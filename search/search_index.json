{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AetherGraph","text":"<p>A Python\u2011first agentic framework for R&amp;D workflows. The core surface is <code>@graph_fn</code> plus Context Services (LLM, memory, channel, artifacts, KV) and Tools for safe capability calls.</p> <p>\ud83d\udc49 Get hands-on in 5 minutes: Quickstart</p> <p>Why AetherGraph? - Python functions as first-class graph nodes (<code>@graph_fn</code>). - Built-in services: no bolt\u2011ons required, but easy to swap. - Artifacts + memory for traceable, reproducible research.</p>"},{"location":"concept/","title":"AetherGraph \u2014 Architecture Overview (1\u2011page)","text":"<p>Goal: Give newcomers a single \"big picture\" of how AetherGraph fits together, then provide a tiny legend so they know what to look up next.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          AetherGraph Runtime                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                    \u2502\n\u2502  Python Code (your repo)                                           \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500          \u2502\n\u2502  @graph_fn nodes           Tools (@tool)            Services       \u2502\n\u2502  (code-native agents)      (reusable ops,           (external ctx) \u2502\n\u2502                            checkpointable)                          \u2502\n\u2502       \u2502                           \u2502                    \u2502            \u2502\n\u2502       \u25bc                           \u25bc                    \u25bc            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  Node Exec   \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  Tool Exec  \u2502      \u2502  Service API \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502        \u2502                                                         \u2502    \u2502\n\u2502        \u25bc                                                         \u2502    \u2502\n\u2502                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502                       \u2502        NodeContext       \u2502  (per node call)  \u2502\n\u2502                       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                   \u2502\n\u2502                       \u2502 channel()   \u2192  chat/CLI/GUI (send/ask)       \u2502\n\u2502                       \u2502 memory()    \u2192  record/recent/query           \u2502\n\u2502                       \u2502 artifacts() \u2192  write/read refs (provenance)  \u2502\n\u2502                       \u2502 kv()        \u2192  small fast key\u2013value          \u2502\n\u2502                       \u2502 logger()    \u2192  structured logs               \u2502\n\u2502                       \u2502 services()  \u2192  external ctx (domain APIs)    \u2502\n\u2502                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502                                \u2502                                     \u2502\n\u2502                                \u25bc                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502               Sidecar / Adapters (inline server)               \u2502 \u2502\n\u2502  \u2502  - Console/CLI channel                                         \u2502 \u2502\n\u2502  \u2502  - Slack / PyQt / HTTP webhooks                                \u2502 \u2502\n\u2502  \u2502  - File/artifact endpoints (optional, later hosted)            \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concept/#legend-skim-first","title":"Legend (skim-first)","text":"<ul> <li><code>@graph_fn</code> (code\u2011native agents): Turn a plain async Python function into a node with a <code>NodeContext</code> injected.</li> <li>Tools (<code>@tool</code>): Small, explicit, reusable operations. Great for checkpoints, retries, and sharing across graphs.</li> <li>NodeContext: Where your node talks to the world: <code>channel()</code>, <code>memory()</code>, <code>artifacts()</code>, <code>kv()</code>, <code>logger()</code>, <code>services()</code>.</li> <li>Channel: Unifies human I/O (console/Slack/PyQt). Use <code>send_text</code>, <code>ask_text</code>, and progress APIs.</li> <li>Memory &amp; Artifacts: Event\u2011first memory with provenance; artifacts store files/results with stable refs.</li> <li>External Context (Services): Register domain services (e.g., job runner, materials DB) so nodes call them like built\u2011ins.</li> <li>Sidecar: Inline server that powers channels/adapters locally; later you can host these endpoints.</li> </ul> <p>Next: See Memory Internals below, then the Submit \u2192 Poll \u2192 Notify tutorial.</p>"},{"location":"concept/#memory-internals-diagram","title":"Memory Internals (diagram)","text":"<p>Goal: Show how event logging, persistence, indices, and optional RAG hang together.</p> <pre><code>              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502          memory().record(...)           \u2502\n              \u2502   kind \u2022 data \u2022 tags \u2022 entities \u2022 ...   \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                   (in\u2011process event stream / bus)\n                                  \u2502\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u25bc                                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  JSONL Persistence Log    \u2502  append\u2011only     \u2502   KV / Indices            \u2502\n\u2502  (provenance timeline)    \u2502  (durable)       \u2502   (fast lookup/filter)    \u2502\n\u2502  e.g., runs/YYYY/MM/*.jsonl\u2502                 \u2502   tags, kinds, entity ids \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                                 \u2502\n          \u2502                                                 \u2502\n          \u25bc                                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Derived Views / Cursors   \u2502  recent(...)     \u2502   Optional RAG Binding    \u2502\n\u2502 e.g., last_by_name,       \u2502  query(...)      \u2502   (vector index)          \u2502\n\u2502 latest_refs_by_kind       \u2502                  \u2502   embed(data/artifacts)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                                 \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba  NodeContext.memory().query(...) \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Record: Write small structured events with <code>kind</code>, <code>data</code>, <code>tags</code>, <code>entities</code>, <code>metrics</code>.</li> <li>Persist: Append to a JSONL log for durability &amp; replay; perfect for provenance.</li> <li>Index: Maintain fast KV/indices for quick filters (by kind/tags/entity/time).</li> <li>RAG (optional): Bind a vector index to selectively embed text/artifact content for semantic search.</li> <li>Query: Use <code>recent</code>, <code>query</code>, and helpers like <code>latest_refs_by_kind</code> to drive summaries &amp; reports.</li> </ul> <p>When to use RAG? When you want semantic retrieval over larger text blocks or artifact\u2011derived content; otherwise rely on indices and tags for speed/clarity.</p>"},{"location":"concept/#tutorial-submit-poll-notify-single-file-runnable","title":"Tutorial \u2014 Submit \u2192 Poll \u2192 Notify (single file, runnable)","text":"<p>Goal: Minimal end\u2011to\u2011end job orchestration with human notification and provenance. Keep it console\u2011only, no external infra.</p> <pre><code># examples/tutorial_submit_poll_notify.py\nfrom __future__ import annotations\nimport asyncio, random, time\nfrom typing import Dict, Optional\n\n# AetherGraph imports (adjust paths/names to your package layout)\nfrom aethergraph import graph_fn, NodeContext\nfrom aethergraph.server import start\nfrom aethergraph.v3.core.runtime.runtime_services import register_context_service\n\n# --- 1) Start the inline sidecar so channel/memory/artifacts work locally ---\nstart()  # prints a local URL; not required to save here\n\n# --- 2) A tiny external service: fake job runner (auto\u2011bound to NodeContext) ---\nclass FakeJobRunner:\n    \"\"\"Pretend to submit a remote job and poll until it finishes.\n    In a real impl, call your cloud API here.\n    \"\"\"\n    def __init__(self):\n        self._jobs: Dict[str, Dict[str, Optional[str]]] = {}\n\n    async def submit(self, spec: Dict) -&gt; str:\n        job_id = f\"job_{int(time.time()*1000)}_{random.randint(100,999)}\"\n        # status can be: queued \u2192 running \u2192 (succeeded | failed)\n        self._jobs[job_id] = {\"status\": \"queued\", \"result\": None}\n        # Background simulation\n        asyncio.create_task(self._simulate(job_id, spec))\n        return job_id\n\n    async def poll(self, job_id: str) -&gt; Dict[str, Optional[str]]:\n        return self._jobs[job_id]\n\n    async def _simulate(self, job_id: str, spec: Dict):\n        # Fake lifecycle with sleeps\n        await asyncio.sleep(0.5)\n        self._jobs[job_id][\"status\"] = \"running\"\n        await asyncio.sleep(1.2)\n        if random.random() &lt; 0.85:\n            self._jobs[job_id][\"status\"] = \"succeeded\"\n            self._jobs[job_id][\"result\"] = f\"Result for {spec.get('name','demo')}\"\n        else:\n            self._jobs[job_id][\"status\"] = \"failed\"\n            self._jobs[job_id][\"result\"] = None\n\n# Register the service under the name \"jobs\" and auto\u2011bind it to context as context.jobs()\nregister_context_service(\"jobs\", FakeJobRunner())\n\n# --- 3) The graph node: submit \u2192 poll \u2192 notify, with artifacts &amp; memory ---\n@graph_fn(name=\"submit_poll_notify\")\nasync def submit_poll_notify(spec: Dict, *, context: NodeContext) -&gt; Dict:\n    ch = context.channel()\n    mem = context.memory()\n    arts = context.artifacts()\n    jobs = context.jobs()  # auto\u2011bound external service\n\n    await ch.send_text(\"Submitting your job\u2026\")\n    job_id = await jobs.submit(spec)\n    await mem.record(kind=\"job_submitted\", data={\"job_id\": job_id, \"spec\": spec}, tags=[\"demo\"])\n\n    # Persist the spec as an artifact\n    spec_ref = await arts.write_text(f\"spec_{job_id}.json\", content=str(spec))\n\n    # Poll until terminal\n    while True:\n        info = await jobs.poll(job_id)\n        status = info.get(\"status\")\n        await ch.send_text(f\"Status: {status}\")\n        if status in {\"succeeded\", \"failed\"}:\n            break\n        await asyncio.sleep(0.6)\n\n    if status == \"succeeded\":\n        result_text = info.get(\"result\") or \"&lt;no result&gt;\"\n        res_ref = await arts.write_text(f\"result_{job_id}.txt\", content=result_text)\n        await mem.record(kind=\"job_succeeded\", data={\"job_id\": job_id, \"result_ref\": res_ref})\n        await ch.send_text(f\"\u2705 Job {job_id} finished. Saved result \u2192 {res_ref}\")\n        return {\"job_id\": job_id, \"status\": status, \"spec_ref\": spec_ref, \"result_ref\": res_ref}\n    else:\n        await mem.record(kind=\"job_failed\", data={\"job_id\": job_id})\n        ans = await ch.ask_text(f\"\u274c Job {job_id} failed. Retry? (yes/no)\")\n        if str(ans).strip().lower().startswith(\"y\"):\n            return await submit_poll_notify(spec=spec, context=context)\n        await ch.send_text(\"Not retrying; stopping here.\")\n        return {\"job_id\": job_id, \"status\": status, \"spec_ref\": spec_ref}\n\n# --- 4) Tiny runner for local testing ---\nif __name__ == \"__main__\":\n    async def main():\n        out = await submit_poll_notify(spec={\"name\": \"toy-sim\", \"steps\": 3})\n        print(\"FINAL OUTPUT:\\n\", out)\n    asyncio.run(main())\n</code></pre>"},{"location":"concept/#what-this-tutorial-demonstrates","title":"What this tutorial demonstrates","text":"<ul> <li>Channel I/O: human\u2011visible status + retry prompt.</li> <li>External Service: a domain API (<code>jobs</code>) registered once, used like a built\u2011in via <code>context.jobs()</code>.</li> <li>Memory: durable events (<code>job_submitted</code>, <code>job_succeeded</code>, <code>job_failed</code>).</li> <li>Artifacts &amp; provenance: spec/result written with stable refs; returned in the node output.</li> <li>Low friction: single file; console channel only; no extra infra.</li> </ul> <p>Next:</p> <ul> <li>Swap <code>FakeJobRunner</code> for your cloud client.</li> <li>Replace <code>ask_text</code> with an approval UI (Slack/PyQt) once you enable those adapters.</li> <li>Emit metrics in <code>mem.record(..., metrics={...})</code> and add a summary node to close the loop.</li> </ul>"},{"location":"external-context-services/","title":"External Context Services (Revised)","text":"<p>Make reusable, lifecycle\u2011aware helpers available as <code>context.&lt;name&gt;</code> inside any <code>@graph_fn</code>.</p> <p>This page explains what an external context service is, why you might use one, how it looks at a high level, and the APIs you\u2019ll use to define and register services. It also clarifies lifecycle behavior today vs. after you add a server/sidecar, and shows how services can access the active <code>NodeContext</code>.</p>"},{"location":"external-context-services/#1-what-is-an-external-context-service","title":"1) What is an external context service?","text":"<p>An external context service is a Python object managed by AetherGraph\u2019s runtime and exposed to your graph functions through the <code>NodeContext</code>. Once registered, you can access it as <code>context.svc(\"name\")</code> or simply <code>context.&lt;name&gt;</code>.</p> <p>Key ideas:</p> <ul> <li>Dependency injection: Centralize clients, caches, and policies in one place and inject them wherever needed.</li> <li>Lifecycle\u2011ready: Services can implement <code>start()</code> and <code>close()</code> for setup/teardown (e.g., open a pool, kick off a background task). Today these hooks are optional and not auto\u2011invoked unless you wire them (see \u00a74.1).</li> <li>Concurrency controls: Built\u2011in mutex and read/write helpers to safely share state across concurrent nodes.</li> <li>Per\u2011run binding: Each call is bound to a <code>NodeContext</code> so the service can access run_id, logger, artifacts, memory, etc.</li> <li>Uniform surface: The same service works in local scripts today and can be proxied or hosted later without changing call sites.</li> </ul> <p>Use services when logic benefits from a long\u2011lived instance, shared state, or orchestration\u2014not for tiny, pure functions (plain imports are fine there).</p>"},{"location":"external-context-services/#2-highlevel-usage-sketch","title":"2) High\u2011level usage sketch","text":"<p>Below is a conceptual outline (intentionally abstract) of how you would define and call a service.</p>"},{"location":"external-context-services/#define-highlevel","title":"Define (high\u2011level)","text":"<pre><code>class MyService(Service):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self._cache = {}\n\n    async def start(self):\n        # optional: warm up connections, threads, or caches\n        ...\n\n    async def close(self):\n        # optional: flush or close resources\n        ...\n\n    async def do_something(self, key: str) -&gt; str:\n        # example: consult cache, maybe call out to an API, return a value\n        ...\n</code></pre>"},{"location":"external-context-services/#register-at-app-startup","title":"Register (at app startup)","text":"<pre><code>register_context_service(\"myservice\", MyService(config={\"mode\": \"dev\"}))\n</code></pre>"},{"location":"external-context-services/#use-in-a-graph-function","title":"Use in a graph function","text":"<pre><code>@graph_fn(name=\"demo\")\nasync def demo(*, context: NodeContext):\n    value = await context.myservice.do_something(\"foo\")\n    return {\"value\": value}\n</code></pre> <p>That\u2019s it: once registered, your service is reachable from any node via <code>context</code>.</p>"},{"location":"external-context-services/#3-why-use-external-context-benefits-use-cases","title":"3) Why use external context? (Benefits + use cases)","text":""},{"location":"external-context-services/#benefits","title":"Benefits","text":"<ul> <li>Replaceable implementations: Swap local vs. remote, mock vs. real, dev vs. prod\u2014without editing call sites.</li> <li>Centralized auth &amp; config: Put tokens, endpoints, retry/timeout policy, telemetry in one place.</li> <li>Lifecycle &amp; performance: Reuse clients, connection pools, thread pools; warm caches once.</li> <li>Concurrency safety: Use the provided <code>critical()</code> mutex or <code>AsyncRWLock</code> to protect shared state.</li> <li>Per\u2011run awareness: Access <code>self.ctx()</code> to reach logger, artifacts, memory, continuations, etc.</li> <li>Future\u2011proof: The same surface can later be proxied (sidecar/hosted) while keeping your graph code unchanged.</li> </ul>"},{"location":"external-context-services/#itemized-scenarios-no-code","title":"Itemized scenarios (no code)","text":"<ul> <li>Model/Tool Clients: Wrap an LLM, embedding service, vector DB, or a simulation engine with retry, rate limit, and consistent API.</li> <li>Job Orchestration: Submit long\u2011running jobs to a queue/cluster and expose <code>submit/status/wait</code> for nodes.</li> <li>Caching/Indexing: Provide a shared in\u2011memory or on\u2011disk cache with strict read (R/W lock) semantics.</li> <li>Policy Enforcement: Centralize tenant limits, quotas, audit logging, and redaction.</li> <li>Data Access Facades: Read domain data (materials table, experiment registry) with local cache + background refresh.</li> <li>Adapters: Present a unified interface over heterogeneous backends (e.g., multiple vendor APIs behind one broker).</li> </ul>"},{"location":"external-context-services/#4-apis-defining-registering-and-binding-services","title":"4) APIs: defining, registering, and binding services","text":"<p>AetherGraph provides small primitives for service registration and a base class with helpful utilities.</p>"},{"location":"external-context-services/#41-lifecycle-today-vs-serversidecar","title":"4.1 Lifecycle (today vs. server/sidecar)","text":"<ul> <li>Today (no server yet): <code>start()</code>/<code>close()</code> exist but are not auto\u2011invoked. You can omit them or leave them as no\u2011ops.</li> <li>When you add a server/sidecar: wire lifecycle once at boot/shutdown (pseudo\u2011code):</li> </ul> <pre><code># After install_services(...) and registrations\nawait start_all_services()\n# ... run your app/sidecar ...\nawait close_all_services()\n</code></pre> <p>Until those hooks are added, services work fine without lifecycle calls.</p>"},{"location":"external-context-services/#42-registry-functions-runtimelevel","title":"4.2 Registry functions (runtime\u2011level)","text":"<ul> <li><code>install_services(container)</code> \u2013 Set the process\u2011wide service container at startup.</li> <li><code>ensure_services_installed(factory)</code> \u2013 Lazily create/install the container if missing.</li> <li><code>register_context_service(name, instance)</code> \u2013 Add a concrete service instance under <code>name</code>.</li> <li><code>get_context_service(name)</code> \u2013 Retrieve a registered instance.</li> <li><code>list_context_services()</code> \u2013 List the names currently registered.</li> </ul>"},{"location":"external-context-services/#43-base-class-service-aka-basecontextservice","title":"4.3 Base class: <code>Service</code> (aka <code>BaseContextService</code>)","text":"<p>The base class gives you batteries\u2011included ergonomics:</p> <ul> <li> <p>Lifecycle</p> </li> <li> <p><code>async def start(self) -&gt; None</code> \u2013 Optional setup hook.</p> </li> <li> <p><code>async def close(self) -&gt; None</code> \u2013 Optional teardown hook.</p> </li> <li> <p>Binding</p> </li> <li> <p><code>def bind(self, *, context: NodeContext) -&gt; Service</code> \u2013 Called by the runtime so <code>self.ctx()</code> works.</p> </li> <li> <p><code>def ctx(self) -&gt; NodeContext</code> \u2013 Access the current node context (logger, memory, artifacts, etc.).</p> </li> <li> <p>Concurrency</p> </li> <li> <p><code>self._lock</code> \u2013 An async mutex available for your own critical sections.</p> </li> <li><code>def critical()(fn)</code> \u2013 Decorator that serializes an async method (easy mutual exclusion).</li> <li> <p><code>class AsyncRWLock</code> \u2013 Many\u2011readers/one\u2011writer lock for shared tables and caches.</p> </li> <li> <p>Offloading</p> </li> <li> <p><code>async def run_blocking(self, fn, *a, **kw)</code> \u2013 Run CPU or blocking I/O on a worker thread (keeps the event loop responsive).</p> </li> </ul>"},{"location":"external-context-services/#44-accessing-services-from-nodes","title":"4.4 Accessing services from nodes","text":"<ul> <li>Dynamic attribute: <code>context.&lt;name&gt;</code> resolves to the registered service (e.g., <code>context.myservice</code>).</li> <li>Explicit lookup: <code>context.svc(\"name\")</code> (equivalent to the dynamic attribute).</li> </ul>"},{"location":"external-context-services/#45-accessing-nodecontext-from-inside-a-service-essential","title":"4.5 Accessing <code>NodeContext</code> from inside a service (essential)","text":"<p>Services frequently need run\u2011scoped utilities (logger, memory, artifacts, kv, llm, rag, etc.). Enable per\u2011call binding so <code>self.ctx()</code> returns the right <code>NodeContext</code>.</p> <p>Use <code>self.ctx()</code> in the service:</p> <pre><code>class MyService(Service):\n    async def do_work(self, x: int) -&gt; int:\n        ctx = self.ctx()  # NodeContext bound for this call\n        ctx.logger().info(\"working\", extra={\"x\": x})\n        await ctx.memory().record(kind=\"note\", data={\"x\": x})\n        uri = ctx.artifacts().put_text(\"result.txt\", f\"value={x}\")\n        return x + 1\n</code></pre>"},{"location":"external-context-services/#46-event-loop-locking-model","title":"4.6 Event loop &amp; locking model","text":"<ul> <li>External services run on the main event loop used by the executing node.</li> <li>Locks (<code>_lock</code>, <code>AsyncRWLock</code>) coordinate on that loop; use <code>run_blocking()</code> for CPU/IO work.</li> </ul>"},{"location":"external-context-services/#5-summary","title":"5) Summary","text":"<p>External context services provide a clean way to share long\u2011lived capabilities across nodes while keeping graph code small and portable:</p> <ul> <li>Inject reusable helpers via <code>context.&lt;name&gt;</code> (or <code>context.svc(name)</code>).</li> <li>Manage concurrency and performance in one place; offload blocking work with <code>run_blocking()</code>.</li> <li>Abstract environments (mock/local/dev/prod) without touching business logic.</li> <li>Bind to <code>NodeContext</code> automatically so services can use logger, memory, artifacts, kv, llm/rag, etc.</li> <li>Lifecycle now vs later: Today you can skip <code>start()</code>/<code>close()</code>; add startup/shutdown hooks when you introduce a server/sidecar.</li> </ul> <p>Use services for shared state, orchestration, specialized clients, or cross\u2011cutting policies. Use plain imports for tiny, stateless helpers.</p>"},{"location":"graph_fn/","title":"Graph Function <code>graph_fn</code> Quickstart &amp; Reference","text":"<p>Make any Python async function a runnable, inspectable Graph Function with a single decorator. You keep normal Python control\u2011flow; AetherGraph wires in runtime services via <code>context</code> and exposes your outputs as graph boundaries.</p>"},{"location":"graph_fn/#tldr","title":"TL;DR","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}! \ud83d\udc4b\")\n    return {\"greeting\": f\"Hello, {name}\"}\n\n# Run (async)\nres = await hello(name=\"Aether\")          # \u2192 {\"greeting\": \"Hello, Aether\"}\n\n# Or run (sync) for quick scripts\nout = hello.sync(name=\"Aether\")            # same result\n</code></pre>"},{"location":"graph_fn/#what-is-a-graph-function","title":"What is a Graph Function?","text":"<p>A Graph Function is a small wrapper around your Python function that:</p> <ul> <li> <p>builds a fresh internal TaskGraph,</p> </li> <li> <p>injects a <code>NodeContext</code> if your function declares <code>*, context</code>,</p> </li> <li> <p>executes your function (awaiting if needed),</p> </li> <li> <p>normalizes the return value into named outputs, and</p> </li> <li> <p>records graph boundary outputs for downstream composition/inspection.</p> </li> </ul> <p>You do not need to learn a new DSL. Write Python; use <code>context.&lt;service&gt;()</code> when you need IO/state.</p>"},{"location":"graph_fn/#decorator-signature","title":"Decorator signature","text":"<pre><code>@graph_fn(\n    name: str,\n    inputs: list[str] | None = None,\n    outputs: list[str] | None = None,\n    version: str = \"0.1.0\",\n    agent: str | None = None,  # optional: also register as an agent name\n)\n</code></pre> <p>Required</p> <ul> <li>name (str) \u2013 Unique identifier for this graph function.</li> </ul> <p>Optional</p> <ul> <li> <p>inputs (list[str]) \u2013 Declares input names for docs/registry (not enforced at call time).</p> </li> <li> <p>outputs (list[str]) \u2013 Declares output names/order; enables single\u2011literal returns.</p> </li> <li> <p>version (str) \u2013 Semantic version for registry/discovery.</p> </li> <li> <p>agent (str) \u2013 Also register in the <code>agent</code> namespace (advanced).</p> </li> </ul>"},{"location":"graph_fn/#function-shape","title":"Function shape","text":"<p><pre><code>@graph_fn(name=\"example\", inputs=[\"x\"], outputs=[\"y\"])\nasync def example(x: int, *, context):\n    # use services via context: channel/memory/artifacts/kv/llm/rag/mcp/logger\n    await context.channel().send_text(f\"x={x}\")\n    return {\"y\": x + 1}\n</code></pre> - Positional/keyword parameters are your API.</p> <ul> <li>Include <code>*, context</code> to receive the <code>NodeContext</code>. If you don\u2019t declare it, nothing is injected.</li> </ul>"},{"location":"graph_fn/#returning-values-normalization-rules","title":"Returning values (normalization rules)","text":"<p>Your return can be:</p> <p>1) Dict of outputs (recommended) <pre><code>return {\"result\": 42, \"note\": \"ok\"}\n</code></pre></p> <p>2) Single literal \u2014 only if you declared exactly one output <pre><code>@graph_fn(name=\"one\", outputs=[\"y\"])\nasync def one(*, context):\n    return 123  # normalized to {\"y\": 123}\n</code></pre></p> <p>3) NodeHandle / Refs (advanced) If you return node handles or refs created by graph utilities, they\u2019re exposed as boundary outputs automatically. For most users, plain dicts/literals are enough.</p> <p>Validation - If <code>outputs</code> are declared, missing keys raise: <code>ValueError(\"Missing declared outputs: ...\")</code>. - Returning a single literal without exactly one declared output raises an error.</p>"},{"location":"graph_fn/#running","title":"Running","text":"<p><pre><code># Async (preferred in apps/servers)\nres = await my_fn(a=1, b=2)\n\n# Sync helper (scripts/CLI/tests)\nout = my_fn.sync(a=1, b=2)\n</code></pre> Internally this builds a fresh runtime environment, constructs a TaskGraph, executes your function in an interpreter, and returns the normalized outputs.</p>"},{"location":"graph_fn/#accessing-context","title":"Accessing Context","text":"<p>Declare <code>*, context</code> to use built\u2011ins: <pre><code>@graph_fn(name=\"report\", outputs=[\"uri\"])\nasync def report(data: dict, *, context):\n    # Log breadcrumbs\n    log = context.logger(); log.info(\"building report\")\n\n    # Save an artifact\n    art = await context.artifacts().save(path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\":\"A\"})\n\n    # Record a typed result in memory\n    await context.memory().write_result(topic=\"report\", outputs=[{\"name\":\"uri\",\"kind\":\"uri\",\"value\": art.uri}])\n\n    # Notify user\n    await context.channel().send_text(f\"Report ready: {art.uri}\")\n    return {\"uri\": art.uri}\n</code></pre></p>"},{"location":"graph_fn/#concurrency-retry-advanced","title":"Concurrency &amp; retry (advanced)","text":"<p><code>GraphFunction.run()</code> accepts knobs used by the interpreter/runtime: <pre><code>await my_fn.run(\n    env=None,                            # supply a prebuilt RuntimeEnv, or let the runner build one\n    retry=RetryPolicy(),                 # backoff/retries for node execution\n    max_concurrency: int | None = None,  # cap parallelism inside the interpreter\n    **inputs,\n)\n</code></pre> For most users, calling <code>await my_fn(...)</code> / <code>.sync(...)</code> is sufficient; the runner chooses sensible defaults.</p>"},{"location":"graph_fn/#minimal-patterns","title":"Minimal patterns","text":"<p>Hello + context <pre><code>@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}\")\n    return {\"greeting\": f\"Hello, {name}\"}\n</code></pre></p> <p>One output (literal) <pre><code>@graph_fn(name=\"square\", outputs=[\"y\"])\nasync def square(x: int, *, context):\n    return x * x\n</code></pre></p> <p>Multi\u2011output dict <pre><code>@graph_fn(name=\"stats\", outputs=[\"mean\",\"std\"])\nasync def stats(xs: list[float], *, context):\n    import statistics as st\n    return {\"mean\": st.mean(xs), \"std\": st.pstdev(xs)}\n</code></pre></p>"},{"location":"graph_fn/#tips-gotchas","title":"Tips &amp; gotchas","text":"<ul> <li>Always include <code>*, context</code> when you need services (channel/memory/llm/etc.).</li> <li>Declare <code>outputs=[...]</code> if you want to return a single literal; otherwise return a dict.</li> <li>Output validation is strict when <code>outputs</code> are declared\u2014return all of them.</li> <li><code>inputs=[...]</code> is for documentation/registry; your Python signature is the source of truth at call time.</li> <li>You can also register the function as an agent by passing <code>agent=\"name\"</code> (covered later).</li> </ul>"},{"location":"graph_fn/#next-steps","title":"Next steps","text":"<ul> <li><code>graphify</code>: combine multiple functions into a larger graph with explicit edges.</li> <li><code>@tool</code>: publish functions as reusable nodes (IO typed), then orchestrate with <code>graphify</code>.</li> <li>Context services: <code>channel</code>, <code>artifacts</code>, <code>memory</code>, <code>kv</code>, <code>llm</code>, <code>rag</code>, `m</li> </ul>"},{"location":"graphify/","title":"AetherGraph \u2014 <code>@graphify</code> (Builder Decorator)","text":"<p><code>@graphify</code> lets you write a plain Python function whose body builds a <code>TaskGraph</code> using tool calls. Instead of executing immediately, the function becomes a graph factory: call <code>.build()</code> to get a concrete graph, <code>.spec()</code> to inspect, and <code>.io()</code> to see its input/output signature.</p>"},{"location":"graphify/#why-graphify-vs-graph_fn","title":"Why <code>graphify</code> vs <code>graph_fn</code>?","text":"Aspect <code>graph_fn</code> <code>graphify</code> Primary purpose Execute now as a single graph node Build a graph (explicit fan\u2011in/fan\u2011out wiring) Return at call Dict of outputs (or awaitable) A builder you later <code>.build()</code> into a graph Control\u2011flow Pythonic, implicit graph behind the scenes Explicit nodes &amp; edges via tool calls (<code>NodeHandle</code>) Best for Orchestration + <code>context.*</code> services Pipelines, DAGs, reusable subgraphs <p>Use <code>graphify</code> when you want:</p> <ul> <li>Multiple tool calls as separate nodes</li> <li>Explicit dependencies (<code>_after</code>) and fan\u2011in/fan\u2011out</li> <li>To inspect/serialize the graph spec for registry/UI</li> <li>To reuse the same pipeline with different inputs</li> </ul> <p>Use <code>graph_fn</code> when you want:</p> <ul> <li>A simple function that runs immediately and returns values</li> <li>Access to <code>context.channel()/memory()/artifacts()/llm()</code> services</li> <li>Minimal ceremony (one decorator and go)</li> </ul>"},{"location":"graphify/#decorator-signature","title":"Decorator Signature","text":"<pre><code>from aethergraph import graphify\n\n@graphify(*, name=\"default_graph\", inputs=(), outputs=None, version=\"0.1.0\", agent: str | None = None)\ndef build_fn(...):\n    ...  # tool calls returning NodeHandles\n    return {\"y\": handle.y}\n</code></pre> <p>Parameters</p> <ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (Iterable[str] or dict) \u2014 Declare required/optional inputs.  </li> <li>If <code>list/tuple</code>: treated as required input names.  </li> <li>If <code>dict</code>: <code>{required_name: ..., ...}</code> for optional mapping; builder will declare required/optional accordingly.</li> <li>outputs (list[str] | None) \u2014 Names to expose. If you return a single literal, you must declare exactly one.</li> <li>version (str) \u2014 Semantic version for registry/spec metadata.</li> <li>agent (str | None) \u2014 Optionally register the built graph under <code>agent</code> namespace.</li> </ul> <p>Return value</p> <p>The decorated symbol becomes a builder function with helpers:</p> <ul> <li><code>.build() -&gt; TaskGraph</code></li> <li><code>.spec() -&gt; GraphSpec</code></li> <li><code>.io() -&gt; IOSignature</code></li> <li>Attributes: <code>.graph_name</code>, <code>.version</code></li> </ul>"},{"location":"graphify/#writing-a-graphify-body","title":"Writing a <code>@graphify</code> Body","text":"<p>Inside the function:</p> <ol> <li>Use <code>arg(\"name\")</code> to reference declared inputs.</li> <li>Call <code>@tool</code> functions (or <code>call_tool(\"pkg.mod:fn\", ...)</code>) \u2014 each returns a <code>NodeHandle</code> in build mode.</li> <li>Return outputs as:</li> <li>A dict mapping names \u2192 <code>NodeHandle</code> outputs or refs/literals, or</li> <li>A single <code>NodeHandle</code> (its outputs will be exposed), or</li> <li>A single literal only if <code>outputs</code> has length 1.</li> </ol> <pre><code>from aethergraph import graphify, tool\nfrom aethergraph.graph import arg\n\n@tool(outputs=[\"embedded\"])\ndef embed(text: str): ...\n\n@tool(outputs=[\"score\"])\ndef score(vec, query_vec): ...\n\n@graphify(name=\"ranker\", inputs=[\"texts\",\"query\"], outputs=[\"scores\"])\ndef ranker(texts, query):\n    q = embed(text=query)\n    # fan\u2011out: call `embed` for each text\n    vecs = [embed(text=t) for t in texts]  # list[NodeHandle]\n    # fan\u2011in: score each against query vec\n    scs = [score(vec=v.embedded, query_vec=q.embedded) for v in vecs]\n    return {\"scores\": [s.score for s in scs]}\n\nG = ranker.build()\n</code></pre>"},{"location":"graphify/#control-dependencies-without-data-edges","title":"Control Dependencies without Data Edges","text":"<p>Use <code>_after</code> when you must enforce order but don\u2019t pass outputs: <pre><code>@tool(outputs=[\"ok\"])\ndef fetch(): return {\"ok\": True}\n\n@tool(outputs=[\"done\"])\ndef train(): return {\"done\": True}\n\n@graphify(name=\"seq\", inputs=[], outputs=[\"done\"])\ndef seq():\n    a = fetch()\n    b = train(_after=a)   # run b after a\n    return {\"done\": b.done}\n</code></pre></p>"},{"location":"graphify/#registration","title":"Registration","text":"<p>If a registry is active, <code>@graphify</code> registers the built graph under <code>nspace=\"graph\"</code> with <code>name</code>/<code>version</code> so it can be listed or launched elsewhere. You can also register it as an <code>agent</code> via the <code>agent=</code> parameter.</p>"},{"location":"graphify/#example-endtoend-pipeline","title":"Example: End\u2011to\u2011End Pipeline","text":"<pre><code>from aethergraph import tool, graphify\nfrom aethergraph.graph import arg\n\n@tool(outputs=[\"rows\"])\ndef load_csv(path: str): ...\n\n@tool(outputs=[\"clean\"])\ndef clean(rows): ...\n\n@tool(outputs=[\"model\"])\ndef train(data): ...\n\n@tool(outputs=[\"uri\"])\ndef report(model): ...\n\n@graphify(name=\"etl_train_report\", inputs=[\"csv_path\"], outputs=[\"uri\"])\ndef etl_train_report(csv_path):\n    raw  = load_csv(path=arg(\"csv_path\"))\n    tidy = clean(rows=raw.rows)\n    mod  = train(data=tidy.clean)\n    rep  = report(model=mod.model)\n    return {\"uri\": rep.uri}\n\nG = etl_train_report.build()\n</code></pre>"},{"location":"graphify/#using-tool-inside-graph_fn-brief","title":"Using <code>@tool</code> Inside <code>@graph_fn</code> (Brief)","text":"<p>While <code>@graph_fn</code> is for immediate execution, you can drop explicit tool nodes inside a <code>graph_fn</code> when you want finer\u2011grained tracing or parallelism:</p> <pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int): return {\"y\": x*x}\n\n@graph_fn(name=\"mix\")\nasync def mix(x: int, *, context):\n    h = square(x=x)                 # schedules a tool node in the implicit graph\n    await context.channel().send_text(\"running square\u2026\")\n    return {\"y\": h.y}               # exposes tool output as graph_fn output\n</code></pre> <p>Prefer <code>@graphify</code> for full pipeline construction; use <code>@graph_fn</code> when you want to orchestrate services (<code>context.*</code>) and run quickly.</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>A 5\u2011minute on\u2011ramp to AetherGraph: install, start the sidecar server, and run your first <code>@graph_fn</code>.</p>"},{"location":"quickstart/#1-install","title":"1) Install","text":"<pre><code>pip install aethergraph\n# or, from source\n# pip install -e .\n</code></pre> <p>Python: 3.10+</p>"},{"location":"quickstart/#2-start-the-sidecar-server-oneliner","title":"2) Start the sidecar server (one\u2011liner)","text":"<p>AetherGraph ships a lightweight sidecar that wires up core services (logger, artifacts, memory, KV, channels, etc.)</p> <pre><code># quickstart_server.py\nfrom aethergraph import start\n\nurl = start(host=\"127.0.0.1\", port=0, log_level=\"warning\")\nprint(\"AetherGraph server:\", url)\n</code></pre> <p>Run it:</p> <pre><code>python quickstart_server.py\n</code></pre> <p>You should see an HTTP URL like <code>http://127.0.0.1:54321</code> printed. (Random free port by default.)</p>"},{"location":"quickstart/#3-your-first-graph-function","title":"3) Your first graph function","text":"<p><code>@graph_fn</code> turns an ordinary async Python function into a runnable graph entrypoint. If you include a <code>context</code> parameter, you get access to built\u2011in services like <code>context.channel()</code> and <code>context.memory()</code>.</p> <pre><code># quickstart_graph_fn.py\nfrom aethergraph import graph_fn, NodeContext\nfrom aethergraph import start\n\n# 1) Start the sidecar so services are available\nstart()\n\n# 2) Define a small graph function\n@graph_fn(name=\"hello.world\", inputs=[\"name\"], outputs=[\"greeting\"], version=\"0.1.0\")\nasync def hello_world(name: str, *, context: NodeContext):\n    # Use the channel to send a message (console by default)\n    await context.channel().send_text(f\"\ud83d\udc4b Hello, {name}! Running graph\u2026\")\n\n    # Do any Python you want here \u2014 call tools, query memory, etc.\n    greeting = f\"Hello, {name}. Nice to meet you from AetherGraph.\"\n\n    # Return outputs as a dict (keys must match `outputs=[...]`)\n    return {\"greeting\": greeting}\n\n# 3) Run it (async wrapper provided)\nif __name__ == \"__main__\":\n    import asyncio\n    async def main():\n        res = await hello_world(name=\"Researcher\")\n        print(\"Result:\", res)\n    asyncio.run(main())\n</code></pre> <p>Run it:</p> <pre><code>python quickstart_graph_fn.py\n</code></pre> <p>You should see a console message from the channel and printed output like:</p> <pre><code>Result: {\"greeting\": \"Hello, Researcher. Nice to meet you from AetherGraph.\"}\n</code></pre>"},{"location":"quickstart/#4-what-just-happened","title":"4) What just happened?","text":"<ul> <li>Sidecar server booted in the background and installed default services (channels, artifacts, memory, KV, logger).</li> <li><code>@graph_fn</code> built a tiny task graph from your function and executed it.</li> <li><code>context.channel()</code> used the default channel (console) to emit a message.</li> </ul> <p>Tip: You can override the channel at call\u2011site with <code>context.channel(\"slack:#research\")</code>, once you\u2019ve configured a Slack adapter.</p>"},{"location":"quickstart/#5-next-steps","title":"5) Next steps","text":"<ul> <li>Add tools with <code>@tool</code> to wrap reusable steps and surface inputs/outputs.</li> <li>Use <code>@graphify</code> for fan\u2011in / fan\u2011out graph construction when the body is mostly tool calls.</li> <li>Explore artifacts (<code>context.artifacts()</code>), memory (<code>context.memory()</code>), and RAG (</li> </ul>"},{"location":"server/","title":"AetherGraph \u2014 Server (Sidecar) Overview","text":"<p>The AetherGraph server is a lightweight sidecar that wires up all runtime services (channels, memory, artifacts, KV, LLM, RAG, MCP, logging, etc.) and exposes a small HTTP/WebSocket surface for adapters and tools. You can run AetherGraph without the server, but the sidecar makes it easy to:</p> <ul> <li>Use GUI/chat adapters (Slack/Telegram/Console UI) that push events back to your runs</li> <li>Host continuation callbacks for <code>ask_text()</code> / <code>ask_approval()</code></li> <li>Centralize service wiring (secrets, paths, corpora, registries)</li> <li>Inspect/trace runs, artifacts, and health in one place</li> </ul> <p>Think of it as your local control plane so your graph functions can stay plain Python.</p>"},{"location":"server/#quick-start","title":"Quick Start","text":"<pre><code>from aethergraph.server import start, stop\n\n# 1) Start the sidecar (in a background thread) and get its base URL\nurl = start(host=\"127.0.0.1\", port=0)   # port=0 \u2192 auto-pick a free port\nprint(\"AetherGraph sidecar:\", url)\n\n# 2) Run your graph functions as usual\nfrom aethergraph import graph_fn\n\n@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}\")\n    return {\"greeting\": f\"Hello, {name}\"}\n\n# ... elsewhere ...\n# res = await hello(name=\"ZC\")\n\n# 3) (Optional) Stop when done (tests/CLI)\nstop()\n</code></pre>"},{"location":"server/#api-start-start_async-stop","title":"API \u2014 <code>start()</code> / <code>start_async()</code> / <code>stop()</code>","text":""},{"location":"server/#start","title":"start","text":"<p><pre><code>start(*, workspace: str = \"./aeg_workspace\", session_id: str | None = None,\n      host: str = \"127.0.0.1\", port: int = 0, log_level: str = \"warning\") -&gt; str\n</code></pre> Start the sidecar in a background thread. Safe to call at the top of scripts or notebook cells.</p> <p>Parameters</p> <ul> <li> <p>workspace (str) \u2013 Root directory for runtime state (artifacts, logs, corpora, temp files). Auto\u2011created.</p> </li> <li> <p>session_id (str, optional) \u2013 Override the logical session. If <code>None</code>, the runtime will create one.</p> </li> <li> <p>host (str) \u2013 Bind address (defaults to loopback).</p> </li> <li> <p>port (int) \u2013 <code>0</code> picks a free port automatically; otherwise bind an explicit port.</p> </li> <li> <p>log_level (str) \u2013 Uvicorn log level (e.g., <code>\"info\"</code>, <code>\"warning\"</code>).</p> </li> </ul> <p>Returns str \u2013 Base URL, e.g., <code>\"http://127.0.0.1:54321\"</code>.</p>"},{"location":"server/#start_async","title":"start_async","text":"<p><pre><code>start_async(**kwargs) -&gt; str\n</code></pre> Async\u2011friendly wrapper that still runs the server in a thread to avoid clashing with your event loop.</p>"},{"location":"server/#stop","title":"stop","text":"<p><pre><code>stop() -&gt; None\n</code></pre> Signal the background server to shut down and join its thread (useful in tests/CI or ephemeral scripts).</p>"},{"location":"server/#why-a-sidecar","title":"Why a sidecar?","text":"<ul> <li>Continuations: <code>context.channel().ask_*</code> creates a continuation token and waits for a resume callback; the server receives user replies (Slack/Telegram/HTTP) and wakes your run.</li> <li>Adapters: chat/file/progress adapters connect over HTTP/WS to publish events (<code>agent.message</code>, <code>agent.progress.*</code>, uploads) into your run.</li> <li>Central config: one place to load settings, secrets, workspace paths, and register services (LLM, RAG, MCP, artifact store, memory backends).</li> <li>Inspection: optional health and tracing endpoints (depending on your app factory) to debug runs locally.</li> </ul>"},{"location":"server/#what-start-actually-does","title":"What <code>start()</code> actually does","text":"<ol> <li>Loads app settings (<code>load_settings()</code>), installs them as current (<code>set_current_settings(...)</code>).</li> <li>Builds a FastAPI app via <code>create_app(workspace=..., cfg=...)</code> \u2014 this registers services and routes.</li> <li>Picks a free port if <code>port=0</code> and launches Uvicorn in a background thread (non\u2011blocking).</li> <li>Returns the base URL so other components (e.g., WS/HTTP MCP clients) can connect.</li> </ol>"},{"location":"server/#typical-usage-patterns","title":"Typical usage patterns","text":""},{"location":"server/#notebooks-quick-scripts","title":"Notebooks &amp; quick scripts","text":"<pre><code>url = start(port=0)\n# \u2026 run several cells that use context.channel()/continuations\n# restart kernel or call stop() when done\n</code></pre>"},{"location":"server/#longrunning-dev-server","title":"Long\u2011running dev server","text":"<ul> <li>Call <code>start(host=\"0.0.0.0\", port=8787, log_level=\"info\")</code> once at process start.</li> <li>Point Slack/Telegram adapters or local tools at <code>http://localhost:8787</code>.</li> </ul>"},{"location":"server/#testsci","title":"Tests/CI","text":"<pre><code>url = start(port=0)\ntry:\n    # run test suite that uses continuations/artifacts\n    ...\nfinally:\n    stop()\n</code></pre>"},{"location":"server/#interop-with-context-services","title":"Interop with context services","text":"<p>Once the sidecar is up, graph functions can rely on bound services:</p> <ul> <li> <p><code>context.channel()</code> \u2013 routes via the server to your chat adapters</p> </li> <li> <p><code>context.artifacts()</code> \u2013 saves to the workspace CAS under the sidecar</p> </li> <li> <p><code>context.memory()</code> \u2013 hotlog/persistence live alongside the server\u2019s config</p> </li> <li> <p><code>context.rag()</code> \u2013 corpora root under workspace; embedders/indices wired here</p> </li> <li> <p><code>context.mcp(...)</code> \u2013 WS/HTTP MCP clients often target sidecar endpoints</p> </li> </ul>"},{"location":"server/#security-notes","title":"Security notes","text":"<ul> <li>Default bind is <code>127.0.0.1</code> (local only). Use <code>0.0.0.0</code> only in trusted networks.</li> <li>Protect WS/HTTP endpoints behind auth headers/tokens if exposing beyond localhost.</li> <li>Never log plaintext API keys; prefer a Secrets store.</li> </ul>"},{"location":"server/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Port already in use: pass <code>port=0</code> or another free port.</li> <li>Nothing happens after ask_text(): ensure the chat adapter posts replies to the sidecar (correct base URL / token).</li> <li>No LLM/kv/rag configured: your <code>create_app()</code> must wire these services (or the accessors will raise \"\u2026 not available\").</li> <li>Jupyter hangs on restart: call <code>stop()</code> before restarting the kernel, or rely on kernel shutdown to terminate the thread.</li> </ul>"},{"location":"server/#minimal-adapter-sketch-optional","title":"Minimal adapter sketch (optional)","text":"<pre><code># Example: WebSocket adapter connecting to sidecar URL\na_sync_ws_client.connect(f\"{url.replace('http','ws')}/events\", headers={\"Authorization\": \"Bearer demo\"})\n# publish OutEvent / listen for Continuation notifications\n</code></pre>"},{"location":"server/#summary","title":"Summary","text":"<p>Run the sidecar server to centralize runtime services, handle continuations/adapters, and keep your graph functions clean. Use <code>start()</code> to launch in\u2011process, <code>start_async()</code> in async apps, and <code>stop()</code> for tests/CI. Configure paths and services once; build everything else in plain Python.</p>"},{"location":"tools/","title":"AetherGraph \u2014 <code>@tool</code> Decorator (Reference &amp; How\u2011to)","text":"<p><code>@tool</code> turns a plain Python function into a tool node that can be executed immediately or added to a graph during build time. You write ordinary Python, declare outputs, and AetherGraph handles result normalization and graph node creation.</p>"},{"location":"tools/#what-is-a-tool","title":"What is a Tool?","text":"<p>A tool is a reusable, IO\u2011typed operation that can be executed on its own or orchestrated inside a graph. Tools are perfect for things like \u201cload CSV\u201d, \u201ctrain model\u201d, \u201cplot chart\u201d, \u201csend_slack\u201d, etc.</p> <ul> <li>Immediate mode (no graph builder active): calling the tool runs the Python function right away and returns a dict of outputs.</li> <li>Graph mode (inside a <code>with graph(...):</code> block or a <code>@graphify</code> body): calling the tool adds a node to the graph and returns a <code>NodeHandle</code> you can wire to other nodes (fan\u2011in/fan\u2011out).</li> <li>Tools automatically register in the runtime registry (<code>nspace=\"tool\"</code>) when a registry is active.</li> </ul> <p>This page covers the simple function form. (The advanced waitable class form is documented separately.)</p>"},{"location":"tools/#decorator-signature","title":"Decorator Signature","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs: list[str], *, inputs: list[str] | None = None,\n      name: str | None = None, version: str = \"0.1.0\")\ndef your_function(...): ...\n</code></pre> <p>Parameters</p> <ul> <li>outputs (list[str], required) \u2014 Declares the output keys your tool will produce.</li> <li>inputs (list[str], optional) \u2014 Explicit input names. Omit to infer from function signature (excluding <code>*args</code>/<code>**kwargs</code>).  </li> <li>name (str, optional) \u2014 Registry/display name. Defaults to the function\u2019s <code>__name__</code>.  </li> <li>version (str, optional) \u2014 Semantic version recorded in the registry (default: <code>\"0.1.0\"</code>).</li> </ul> <p>Return value (call\u2011site dependent)</p> <ul> <li>Immediate mode: returns a <code>dict</code> of outputs.  </li> <li>Graph mode: returns a <code>NodeHandle</code> (or an awaitable handle under an interpreter) to be wired/exposed by the builder.</li> </ul>"},{"location":"tools/#return-normalization","title":"Return Normalization","text":"<p>The wrapped function can return different shapes; the decorator normalizes into a dict that must include every declared output:</p> <ul> <li><code>None</code> \u2192 <code>{}</code></li> <li><code>dict</code> \u2192 used as\u2011is</li> <li><code>tuple</code> \u2192 <code>{\"out0\": v0, \"out1\": v1, ...}</code></li> <li>single value \u2192 <code>{\"result\": value}</code></li> </ul> <p>If any declared <code>outputs</code> are missing from the normalized dict, a <code>ValueError</code> is raised.</p>"},{"location":"tools/#control-keywords-graph-mode","title":"Control Keywords (graph mode)","text":"<p>When calling a tool while building a graph (e.g., inside a <code>with graph(...):</code> or <code>@graphify</code> body), you may pass these special kwargs to influence scheduling/metadata:</p> <ul> <li><code>_after</code> (NodeHandle | list[NodeHandle | node_id]): explicit dependency edges (fan\u2011in).  </li> <li><code>_name</code> (str): display name for UI/spec.  </li> <li><code>_id</code> (str): hard override of the node ID (must be unique in the graph).  </li> <li><code>_alias</code> (str): optional alias for reverse lookups.  </li> <li><code>_labels</code> (Iterable[str]): lightweight tags for search/grouping.</li> </ul> <p>Example:</p> <pre><code>res = my_tool(a=arg_a, b=arg_b, _after=[prev1, prev2], _name=\"preprocess\", _labels=[\"data\",\"prep\"])\n</code></pre> <p>These control keys are stripped before calling your function and only affect graph construction.</p>"},{"location":"tools/#simple-examples","title":"Simple Examples","text":""},{"location":"tools/#1-immediate-execution-no-graph-builder-active","title":"1) Immediate execution (no graph builder active)","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"sum\", \"mean\"])  # outputs you promise to return\ndef stats(xs: list[float]):\n    s = sum(xs)\n    return {\"sum\": s, \"mean\": s / len(xs)}\n\nout = stats([1,2,3,4])   # \u2192 {\"sum\": 10, \"mean\": 2.5}\n</code></pre>"},{"location":"tools/#2-graph-construction-inside-a-builder","title":"2) Graph construction (inside a builder)","text":"<pre><code>from aethergraph import tool\nfrom aethergraph import graphify\nfrom aethergraph.graph import arg  # or from aethergraph.graph.graph_refs import arg\n\n@tool(outputs=[\"y\"])\ndef add(x: int, z: int): return {\"y\": x + z}\n\n@tool(outputs=[\"z\"])\ndef mul(x: int, k: int): return {\"z\": x * k}\n\n@graphify(name=\"pipeline\", inputs=[\"x\"], outputs=[\"y\"])\ndef pipeline(x):\n    a = mul(x=arg(\"x\"), k=2)          # NodeHandle(\"mul_...\")\n    b = add(x=arg(\"x\"), z=a.z)        # depends on `a` automatically via data edge\n    return {\"y\": b.y}\n\nG = pipeline.build()                    # TaskGraph\nspec = pipeline.spec()                  # graph spec for inspection/registry\nio = pipeline.io()                      # IO signature\n</code></pre>"},{"location":"tools/#3-forcing-an-order-with-_after-no-data-edge","title":"3) Forcing an order with <code>_after</code> (no data edge)","text":"<pre><code>@tool(outputs=[\"ok\"])\ndef init(): return {\"ok\": True}\n\n@tool(outputs=[\"ready\"])\ndef warmup(): return {\"ready\": True}\n\n@graphify(name=\"order_demo\", inputs=[], outputs=[\"ready\"])\ndef order_demo():\n    n1 = init()\n    n2 = warmup(_after=n1)   # enforce sequencing without passing data\n    return {\"ready\": n2.ready}\n</code></pre>"},{"location":"tools/#registration-optional","title":"Registration (Optional)","text":"<p>If a runtime registry is active (via <code>current_registry()</code>), the decorator auto\u2011registers your tool under the <code>tool</code> namespace with its <code>name</code> and <code>version</code> so it can be listed and referenced later.</p> <p>You can also call tools by dotted path via <code>call_tool(\"pkg.module:function\", arg1=..., ...)</code> to avoid importing at build sites, but the recommended ergonomic flow is to <code>import</code> the tool and call it directly.</p>"},{"location":"tools/#best-practices","title":"Best Practices","text":"<ul> <li>Keep tools focused and side\u2011effect aware (e.g., write artifacts via <code>context.artifacts()</code> inside <code>@graph_fn</code> wrappers).</li> <li>Always declare <code>outputs</code> and make your function return those keys.</li> <li>Use <code>_after</code> for control dependencies when no data edge exists.</li> <li>Prefer composing tools via <code>@graphify</code> for explicit fan\u2011in/fan\u2011out graphs.</li> <li>Inside <code>@graph_fn</code>, you can call tools to create explicit nodes, but <code>@graph_fn</code> is for immediate orchestration.</li> </ul>"},{"location":"build-graphs/","title":"Build Graphs in AetherGraph","text":"<p>Welcome! This section is the fastest way to grok how to build and run graphs with Python-first ergonomics.</p> <p>We introduce things in the order you will actually use them:</p> <ol> <li><code>@graph_fn</code> \u2014 the on-ramp. Wrap a regular Python function so it runs as a single graph node, with full <code>context.*</code> access. Great for demos, services, notebooks.</li> <li><code>@tool</code> \u2014 make any function a graph node. Use it inside <code>graph_fn</code> for per-step visibility, metrics, artifacts, and reuse.</li> <li><code>@graphify</code> \u2014 build an explicit DAG for fan-out/fan-in, ordering via <code>_after</code>, subgraphs, and reuse.</li> </ol> <p>Tip: Start with <code>@graph_fn</code> (plus a couple of <code>@tool</code> calls). Move to <code>@graphify</code> when you want explicit topology, parallel map/reduce, barriers, or long-lived pipelines.</p>"},{"location":"build-graphs/#what-is-a-graph-here","title":"What is a \"graph\" here?","text":"<ul> <li>AetherGraph executes TaskGraphs \u2014 directed acyclic graphs of nodes.</li> <li>A node can be:</li> <li>a graph function (<code>@graph_fn</code>) \u2014 runs immediately and can call context services.</li> <li>a tool node (<code>@tool</code>) \u2014 a typed, reusable operation with visible inputs/outputs.</li> <li>The Context (<code>context.*</code>) gives every node uniform access to runtime services:   <code>channel()</code>, <code>artifacts()</code>, <code>memory()</code>, <code>kv()</code>, <code>llm()</code>, <code>rag()</code>, <code>mcp()</code>, <code>logger()</code>.</li> </ul>"},{"location":"build-graphs/#quickstart-30-lines","title":"Quickstart (30 lines)","text":"<pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int):\n    return {\"y\": x * x}\n\n@graph_fn(name=\"demo\", outputs=[\"y\"])\nasync def demo(x: int, *, context):\n    await context.channel().send_text(f\"Computing square of {x}\u2026\")\n    h = square(x=x)              # creates a node you can inspect later\n    await context.channel().send_text(\"Done.\")\n    return {\"y\": h.y}            # expose tool output\n</code></pre> <p>Why this design? - You get instant run semantics (like a normal async function), but steps you mark with <code>@tool</code> become visible graph nodes with metrics/artifacts. - When your flow grows and needs explicit fan-out/fan-in or ordering, switch to <code>@graphify</code>.</p>"},{"location":"build-graphs/#next-steps","title":"Next steps","text":"<ul> <li><code>graph_fn</code> (on-ramp) -&gt; graph_fn.md</li> <li><code>@tool</code> reference -&gt; tool.md</li> <li><code>@graphify</code> (explicit DAG + fan-in/out) -&gt; graphify.md</li> <li>Choosing the right approach -&gt; choosing.md</li> </ul>"},{"location":"build-graphs/choosing/","title":"Choosing: <code>graph_fn</code> vs <code>@graphify</code> vs <code>@tool</code>","text":"<p>Use this one-screen guide to pick the right entry point.</p>"},{"location":"build-graphs/choosing/#start-simple","title":"Start simple","text":"<ul> <li><code>@graph_fn</code> \u2014 quickest way to ship a working function with <code>context.*</code>. Add a couple of <code>@tool</code> calls inside if you want visible/inspectable steps.</li> </ul>"},{"location":"build-graphs/choosing/#scale-up-when-needed","title":"Scale up when needed","text":"<ul> <li><code>@graphify</code> \u2014 when you need explicit DAG control:</li> <li>fan-out / fan-in / map-reduce</li> <li><code>_after</code> (barriers) and <code>_alias</code>/<code>_labels</code> for orchestration and UI</li> <li>subgraph reuse and IO/spec inspection</li> </ul>"},{"location":"build-graphs/choosing/#tool-is-a-building-block","title":"<code>@tool</code> is a building block","text":"<ul> <li>Wrap any function to make it a typed node.</li> <li>Works in both: inside <code>@graph_fn</code> (immediate run, visible steps) and in <code>@graphify</code> (adds nodes to DAG).</li> <li>Control kwargs (<code>_after</code>, <code>_alias</code>, <code>_labels</code>, <code>_id</code>) apply only in graph build contexts.</li> </ul>"},{"location":"build-graphs/choosing/#quick-comparison","title":"Quick comparison","text":"Capability <code>@graph_fn</code> <code>@graphify</code> <code>@tool</code> Immediate \"just run\" Yes Build first Yes (outside graph) Full <code>context.*</code> access Yes (via <code>context</code>) via tools/subgraphs when called under <code>graph_fn</code> Visible per-step nodes via <code>@tool</code> calls native yes Fan-out / fan-in (map/reduce) limited (Python loops) Yes (concise) building block Control edges (<code>_after</code>/barrier) No Yes Yes in graph build Graph spec/IO inspection implicit Yes (<code>.spec()/.io()</code>) n/a Best for demos, services pipelines, orchestration atomic operations <p>Rule of thumb: Start with <code>@graph_fn</code>. When you feel the need for explicit topology or orchestration, switch the same steps into <code>@graphify</code> using the exact same <code>@tool</code>s.</p>"},{"location":"build-graphs/graph_fn/","title":"<code>@graph_fn</code> \u2014 Python-first on-ramp","text":"<p>Wrap a normal (async) Python function so it runs as a single graph node with full access to <code>context.*</code> services. Return values are exposed as graph outputs.</p>"},{"location":"build-graphs/graph_fn/#signature","title":"Signature","text":"<pre><code>@graph_fn(name: str, *, inputs: list[str] | None = None, outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\ndef|async def fn(..., *, context: NodeContext) -&gt; dict | value | NodeHandle\n</code></pre> <ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (list[str], optional) \u2014 Declared input keys (used for IO spec; optional for quickstart).</li> <li>outputs (list[str], optional) \u2014 Declared output keys (enables single-value return).</li> <li>version (str) \u2014 SemVer for registry/lineage.</li> <li>agent (str, optional) \u2014 If set, register this graph function as an agent (advanced).</li> </ul>"},{"location":"build-graphs/graph_fn/#return-normalization","title":"Return normalization","text":"<ul> <li>dict -&gt; keys become outputs; NodeHandles/Refs are exposed.</li> <li>single value -&gt; allowed only if exactly one <code>outputs</code> key is declared (collapsed to that name).</li> <li>NodeHandle -&gt; its outputs are exposed (single output collapses).</li> </ul>"},{"location":"build-graphs/graph_fn/#using-tool-inside-graph_fn","title":"Using <code>@tool</code> inside <code>graph_fn</code>","text":"<p>You can call <code>@tool</code> functions to create visible/inspectable nodes while keeping immediate Python control flow:</p> <pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int): return {\"y\": x*x}\n\n@graph_fn(name=\"demo\", outputs=[\"y\"])\nasync def demo(x: int, *, context):\n    h = square(x=x)          # creates a node\n    await context.channel().send_text(\"computed\")\n    return {\"y\": h.y}\n</code></pre> <p>Important: Control kwargs like <code>_after</code>, <code>_alias</code>, <code>_labels</code> are only honored in graph build contexts (e.g., <code>@graphify</code>). Inside <code>graph_fn</code>, execution order follows normal Python semantics. If you need control edges without passing data, use <code>@graphify</code>.</p>"},{"location":"build-graphs/graph_fn/#when-to-use-graph_fn","title":"When to use <code>@graph_fn</code>","text":"<ul> <li>Quick demos, notebooks, service-style tasks.</li> <li>One to a few steps, mostly sequential.</li> <li>You want full <code>context.*</code> access and instant execution, with optional visibility via <code>@tool</code> calls.</li> </ul> <p>See also: tool.md, graphify.md.</p>"},{"location":"build-graphs/graphify/","title":"<code>@graphify</code> \u2014 Build an explicit DAG (fan-out, fan-in, ordering)","text":"<p>Use <code>@graphify</code> when you need clear topology: map/fan-out, reduce/fan-in, barriers via <code>_after</code>, subgraphs, or reusable pipelines.</p>"},{"location":"build-graphs/graphify/#signature","title":"Signature","text":"<p><pre><code>@graphify(*, name=\"default_graph\", inputs=(), outputs=None, version=\"0.1.0\", agent: str | None = None)\ndef body(...):\n    # Use tool calls to add nodes and return NodeHandles/Refs\n    return {...}\n</code></pre> - The decorated function returns a builder: call <code>.build()</code> to get a <code>TaskGraph</code> instance; <code>.spec()</code> for a serializable spec; <code>.io()</code> for IO signature.</p>"},{"location":"build-graphs/graphify/#control-edges-and-labels-graph-build-only","title":"Control edges and labels (graph build only)","text":"<p><code>@tool</code> control kwargs are honored here: - <code>_after</code>, <code>_alias</code>, <code>_labels</code>, <code>_id</code>, <code>_name</code></p>"},{"location":"build-graphs/graphify/#patterns","title":"Patterns","text":""},{"location":"build-graphs/graphify/#fan-out-map-over-inputs","title":"Fan-out (map over inputs)","text":"<pre><code>from aethergraph import tool, graphify\n\n@tool(outputs=[\"vec\"])\ndef embed(text: str): ...\n\n@graphify(name=\"fanout_demo\", inputs=[\"texts\"], outputs=[\"vecs\"])\ndef fanout_demo(texts):\n    handles = [embed(text=t) for t in texts]          # fan-out\n    return {\"vecs\": [h.vec for h in handles]}         # expose list of outputs\n</code></pre>"},{"location":"build-graphs/graphify/#fan-in-reduce","title":"Fan-in (reduce)","text":"<pre><code>@tool(outputs=[\"score\"])\ndef dot(a, b): ...\n\n@graphify(name=\"fanin_demo\", inputs=[\"query\", \"vecs\"], outputs=[\"scores\"])\ndef fanin_demo(query, vecs):\n    q = embed(text=query)\n    scores = [dot(a=v, b=q.vec) for v in vecs]        # fan-in through q\n    return {\"scores\": [s.score for s in scores]}\n</code></pre>"},{"location":"build-graphs/graphify/#control-edge-without-data","title":"Control edge without data","text":"<pre><code>@tool(outputs=[\"ok\"])   def init(): ...\n@tool(outputs=[\"done\"]) def train(): ...\n\n@graphify(name=\"order\", outputs=[\"done\"])\ndef order():\n    a = init()\n    b = train(_after=a)            # sequence a -&gt; b\n    return {\"done\": b.done}\n</code></pre>"},{"location":"build-graphs/graphify/#subgraph-reuse-optional","title":"Subgraph reuse (optional)","text":"<p>You can register graphs and call them as nodes (advanced). For most cases, compose <code>@tool</code>s directly inside <code>@graphify</code>.</p>"},{"location":"build-graphs/graphify/#when-to-use-graphify","title":"When to use <code>@graphify</code>","text":"<ul> <li>You need parallelism (map) or aggregation (reduce).</li> <li>You need ordering without data flow (<code>_after</code>/barriers).</li> <li>You want a reusable / inspectable DAG (e.g., schedule in a UI).</li> </ul> <p>See also: graph_fn.md, tool.md, choosing.md.</p>"},{"location":"build-graphs/tool/","title":"<code>@tool</code> \u2014 Turn any function into a graph node","text":"<p>Make a plain function a typed, reusable node with explicit inputs/outputs. Works in both <code>@graph_fn</code> (immediate run with visible steps) and <code>@graphify</code> (graph build).</p>"},{"location":"build-graphs/tool/#decorator","title":"Decorator","text":"<pre><code>@tool(outputs: list[str], inputs: list[str] | None = None, *, name: str | None = None, version: str = \"0.1.0\")\ndef fn(...): ...\n</code></pre> <ul> <li>outputs (list[str]) \u2014 Output field names this tool produces.</li> <li>inputs (list[str], optional) \u2014 Input names; inferred from signature if omitted.</li> <li>name (str, optional) \u2014 Registry name (defaults to function name).</li> <li>version (str) \u2014 SemVer for registry/lineage.</li> </ul>"},{"location":"build-graphs/tool/#return-normalization","title":"Return normalization","text":"<ul> <li><code>None</code> -&gt; <code>{}</code></li> <li><code>dict</code> -&gt; as-is</li> <li><code>tuple</code> -&gt; <code>{ \"out0\": v0, \"out1\": v1, ... }</code></li> <li>single value -&gt; <code>{ \"result\": value }</code></li> </ul> <p>Contract check: Declared <code>outputs</code> must be present in the normalized return, otherwise a <code>ValueError</code> is raised.</p>"},{"location":"build-graphs/tool/#two-modes-same-decorator","title":"Two modes (same decorator)","text":"Where called from Behavior Outside any graph Runs immediately and returns a dict. Inside <code>@graph_fn</code> Creates a node handle you can expose. Inside <code>@graphify</code> Adds a node to the DAG (honors control kw). <p>Control kwargs (graph build only): - <code>_after</code> (NodeHandle | list) \u2014 add control-edge dependency. - <code>_alias</code> / <code>_id</code> \u2014 override node id / alias. - <code>_labels</code> (list[str]) \u2014 annotate node for UI/search. - <code>_name</code> \u2014 display name hint.</p>"},{"location":"build-graphs/tool/#minimal-example","title":"Minimal example","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"y\"])\ndef square(x:int) -&gt; dict:\n    return {\"y\": x*x}\n</code></pre> <p>Use in <code>graph_fn</code> or <code>@graphify</code> as shown in their pages.</p>"},{"location":"key-concepts/agent-via-graph-fn/","title":"Agents via <code>@graph_fn</code>","text":"<p>This chapter introduces agents in AetherGraph through the <code>@graph_fn</code> decorator. You\u2019ll learn how <code>@tool</code> functions become nodes on the fly, when and why to use async functions, and how to chain or nest them to form structured yet reactive agentic workflows.</p>"},{"location":"key-concepts/agent-via-graph-fn/#1-what-is-a-graph_fn","title":"1. What is a <code>graph_fn</code>?","text":"<p>A <code>graph_fn</code> turns a plain Python function into an agent with access to rich context services\u2014channel, memory, artifacts, logger, and more. It runs in the normal Python runtime by default; no DAG is captured automatically when you invoke it. For most interactive or agentic workflows, this lightweight mode is ideal: you get an ergonomic async function with context utilities for I/O, persistence, and orchestration without committing to graph capture.</p>"},{"location":"key-concepts/agent-via-graph-fn/#function-shape","title":"Function shape","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"example\")\nasync def example(x: int, *, context: NodeContext):\n    # Access runtime services from the context\n    await context.channel().send_text(f\"x={x}\")\n    return {\"y\": x + 1}\n</code></pre> <ul> <li>Define your own API through standard parameters.</li> <li>Include <code>*, context</code> to access the <code>NodeContext</code>; if omitted, nothing is injected.</li> </ul> <p>Minimal example:</p> <pre><code>@graph_fn(name=\"hello_agent\")\nasync def hello_agent(name: str = \"world\", *, context: NodeContext):\n    await context.channel().send_text(f\"\ud83d\udc4b Hello, {name}!\")\n    context.memory().record(kind=\"usr_data\", data={\"name\": name})\n    context.logger().info(\"Greeted user\", extra={\"name\": name})\n    return {\"message\": f\"Hello, {name}\"}\n</code></pre> <p>Key idea: <code>@graph_fn</code> provides a reactive agent interface\u2014async execution with contextual power\u2014while keeping runtime overhead minimal. Nodes are only added when you explicitly use <code>@tool</code> or call other graphs.</p>"},{"location":"key-concepts/agent-via-graph-fn/#2-tools-nodes-on-the-fly","title":"2. Tools: nodes on the fly","text":"<p>The <code>@tool</code> decorator marks a Python function as a tool node. When called inside a <code>graph_fn</code>, the runtime creates a node on the fly and records its inputs/outputs for provenance, inspection, or future resumptions.</p> <p>Rule of thumb: for exploratory, reactive development, call regular Python functions freely. Reach for <code>@tool</code> when you need traceable state, durability, or resume checkpoints.</p>"},{"location":"key-concepts/agent-via-graph-fn/#example-a-simple-sum-tool","title":"Example: a simple sum tool","text":"<pre><code>from typing import List\nfrom aethergraph import tool\n\n@tool(name=\"sum_vec\", outputs=[\"total\"])\ndef sum_vec(xs: List[float]) -&gt; dict:\n    return {\"total\": float(sum(xs))}\n</code></pre> <p>Use inside a <code>graph_fn</code>:</p> <pre><code>@graph_fn(name=\"tool_demo\", outputs=[\"total\"])\nasync def tool_demo(values: list[float], *, context: NodeContext):\n    stats = {\"n\": len(values)}             # executed inline\n    out = sum_vec(values)                  # \u2190 captured as a node\n    await context.channel().send_text(f\"n={stats['n']}, sum={out['total']}\")\n    return {\"total\": out[\"total\"]}\n</code></pre> <p>You can mix normal Python code and <code>@tool</code> calls seamlessly. Only <code>@tool</code> calls create nodes.</p>"},{"location":"key-concepts/agent-via-graph-fn/#example-lightweight-http-fetch-tool","title":"Example: lightweight HTTP fetch tool","text":"<pre><code>from aethergraph import tool\nimport json, urllib.request\n\n@tool(name=\"fetch_json\", outputs=[\"data\"])\ndef fetch_json(url: str) -&gt; dict:\n    with urllib.request.urlopen(url) as r:\n        return {\"data\": json.load(r)}\n</code></pre> <p>Then call it inside a <code>graph_fn</code>:</p> <pre><code>@graph_fn(name=\"use_fetch\", outputs=[\"data\"])\nasync def use_fetch(url: str, *, context: NodeContext):\n    res = fetch_json(url)                   # node created dynamically\n    context.logger().info(\"fetched\", extra={\"url\": url})\n    return {\"data\": res[\"data\"]}\n</code></pre> <p>To inspect the implicit graph created during execution, call <code>graph_fn.last_graph()</code> \u2014 it returns the captured <code>TaskGraph</code> for visualization or reuse.</p>"},{"location":"key-concepts/agent-via-graph-fn/#3-async-first-chaining-nesting-and-concurrency","title":"3. Async-first: chaining, nesting, and concurrency","text":"<p>AetherGraph adopts async-first design because agents often:</p> <ul> <li>Wait for user input (<code>ask_text</code>, <code>ask_approval</code>)</li> <li>Perform I/O (HTTP, file writes, DB queries)</li> <li>Launch parallel sub-tasks</li> </ul>"},{"location":"key-concepts/agent-via-graph-fn/#chaining-and-nesting-graph_fns","title":"Chaining and nesting <code>graph_fn</code>s","text":"<p>You can call one <code>graph_fn</code> from another. Each call creates a child subgraph node:</p> <pre><code>@graph_fn(name=\"step1\", outputs=[\"y\"])\nasync def step1(x: int, *, context: NodeContext) -&gt; dict:\n    return {\"y\": x + 1}\n\n@graph_fn(name=\"step2\", outputs=[\"z\"])\nasync def step2(y: int, *, context: NodeContext) -&gt; dict:\n    return {\"z\": y * 2}\n\n@graph_fn(name=\"pipeline\", outputs=[\"z\"])\nasync def pipeline(x: int, *, context: NodeContext) -&gt; dict:\n    a = await step1(x)       # \u2192 child node\n    b = await step2(a[\"y\"]) # \u2192 child node\n    return {\"z\": b[\"z\"]}\n</code></pre>"},{"location":"key-concepts/agent-via-graph-fn/#fan-out-concurrency","title":"Fan-out concurrency","text":"<p>Launch multiple subgraphs concurrently with <code>asyncio.gather</code>:</p> <pre><code>import asyncio\n\n@graph_fn(name=\"concurrent_steps\", outputs=[\"r1\", \"r2\"])\nasync def concurrent_steps(a: int, b: int, *, context: NodeContext) -&gt; dict:\n    r1, r2 = await asyncio.gather(step1(a), step2(b))\n    return {\"r1\": r1[\"y\"], \"r2\": r2[\"z\"]}\n</code></pre> <p>This pattern enables natural fan-out/fan-in parallelism within a single reactive agent.</p>"},{"location":"key-concepts/agent-via-graph-fn/#4-running-a-graph_fn","title":"4. Running a <code>graph_fn</code>","text":"<p>You can execute a <code>graph_fn</code> directly from async code or through the provided runners.</p>"},{"location":"key-concepts/agent-via-graph-fn/#option-a-direct-await","title":"Option A \u2013 Direct await","text":"<pre><code># In an async function\nresult = await pipeline(3)\n</code></pre>"},{"location":"key-concepts/agent-via-graph-fn/#option-b-synchronous-helper","title":"Option B \u2013 Synchronous helper","text":"<pre><code>from aethergraph.runner import run\nfinal = run(pipeline(3))\n</code></pre>"},{"location":"key-concepts/agent-via-graph-fn/#option-c-explicit-async-runner","title":"Option C \u2013 Explicit async runner","text":"<pre><code>from aethergraph.runner import run_async\n# In an async function\nresult = await run_async(pipeline)\n</code></pre> <p>The <code>run_*</code> helpers drive the event loop and normalize execution for both reactive and static graphs.</p>"},{"location":"key-concepts/agent-via-graph-fn/#5-summary","title":"5. Summary","text":"<ul> <li><code>@graph_fn</code> wraps a Python function into an async agent with an injected <code>NodeContext</code> exposing rich runtime services.</li> <li>Execution stays in normal Python until you invoke <code>@tool</code> or another <code>graph_fn</code>\u2014only those create nodes.</li> <li><code>@tool</code> functions let you capture intermediate steps for provenance and durability.</li> <li>Agents are composable: call one <code>graph_fn</code> from another or fan out with <code>asyncio.gather</code>.</li> <li>Use <code>run()</code> or <code>run_async()</code> for simple orchestration; prefer plain calls + context for lightweight workflows.</li> </ul> <p>AetherGraph\u2019s agent model combines Pythonic simplicity with event-driven introspection\u2014reactive first, deterministic when needed.</p>"},{"location":"key-concepts/artifacts-memory/","title":"Artifacts and Memory","text":"<p>This chapter covers two foundational pillars of AetherGraph\u2019s runtime: Artifacts and Memory. Together, they form the provenance backbone \u2014 making every result, file, and intermediate step traceable, reproducible, and retrievable long after execution.</p> <p>Mental model: Artifacts capture what was produced; Memory captures what happened (events, results, metrics) and why (context, summaries, links).</p>"},{"location":"key-concepts/artifacts-memory/#1-why-artifacts-memory-exist","title":"1. Why Artifacts &amp; Memory Exist","text":"<p>Most Python workflows scatter outputs across temp folders and logs with no consistent linkage. AetherGraph fixes this by binding everything to the active run/graph/node and exposing consistent, high\u2011level APIs for saving and recalling state.</p> Concern Manual management With AetherGraph Provenance Files &amp; logs scattered; hard to link Every record stamped with <code>{run_id, graph_id, node_id}</code> + tool metadata Reproducibility Filenames drift; env unknown Content\u2011addressed + typed records \u2192 deterministic recall Discoverability Grep and guess Query by <code>kind</code>, <code>labels</code>, <code>metrics</code>, scope; ask \u201cbest by metric\u201d Durability Ad\u2011hoc paths; stale temp dirs CAS store + index; pins; export/replay Collaboration Tribal conventions Shared schema (URIs/records) + searchable index <p>Takeaway: Use artifacts for durable assets; use memory for structured, queryable history. Both are scoped to your execution so you can reconstruct the story of a run.</p>"},{"location":"key-concepts/artifacts-memory/#2-artifacts-persistent-assets","title":"2. Artifacts \u2014 Persistent Assets","text":"<p>Artifacts are immutable, content\u2011addressed assets produced or consumed by agents/tools: files, directories, JSON payloads, or serialized objects.</p>"},{"location":"key-concepts/artifacts-memory/#why-artifacts-vs-manual-files","title":"Why Artifacts (vs. manual files)?","text":"<ul> <li>Content\u2011addressed: the URI reflects the content (CAS) \u2014 no silent overwrites.</li> <li>Typed + labeled: add <code>kind</code>, <code>labels</code>, and <code>metrics</code> to organize results.</li> <li>Indexed: query by scope or rank by metric (e.g., best checkpoint by <code>val_acc</code>).</li> <li>Provenance\u2011stamped: <code>{run_id, graph_id, node_id, tool_name, tool_version}</code> baked in.</li> <li>Portable: <code>to_local_path(uri)</code> resolves for local or remote stores.</li> </ul>"},{"location":"key-concepts/artifacts-memory/#architecture","title":"Architecture","text":"<pre><code>[ Your Agent / Tool ]\n          \u2502   (context)\n          \u25bc\n[ NodeContext ]\n          \u2502\n          \u25bc\n[ context.artifacts() \u2014 Artifact Facade ]\n     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502               \u2502                \u2502                 \u2502\n     \u2502 save / writer \u2502 stage / ingest \u2502 list / search   \u2502 best / pin\n     \u25bc               \u25bc                \u25bc                 \u25bc\n[ Artifact Store ]  [ Staging Area ]  [ Artifact Index ]  [ Retention ]\n     (CAS/FS)          (tmp)             (SQLite/KV)        (pins)\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#core-api","title":"Core API","text":"Method Purpose <code>stage()</code> / <code>stage_dir()</code> Reserve a temp path for producing files/dirs safely. <code>save(path, kind, labels=None, metrics=None, pin=False)</code> Save an existing path and index it. Returns an artifact with <code>uri</code>. <code>save_text(content, *, kind=\"text\", labels=None)</code> Store small text payloads. <code>save_json(obj, *, kind=\"json\", labels=None)</code> Store a JSON payload. <code>writer(kind, planned_ext)</code> Context manager to stream\u2011write binary content; atomically indexes on close. <code>list(scope)</code> / <code>search(...)</code> / <code>best(kind, metric, mode, scope)</code> Query and rank artifacts by descriptors or metrics. <code>pin(artifact_id)</code> Mark as retained (skip cleanup policies). <code>to_local_path(uri)</code> Resolve a CAS URI to a local filesystem path."},{"location":"key-concepts/artifacts-memory/#examples","title":"Examples","text":"<p>Save a file</p> <pre><code>@graph_fn(name=\"produce_artifact\", outputs=[\"report_uri\"])\nasync def produce_artifact(*, context):\n    art = await context.artifacts().save(\n        path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\": \"A\"}\n    )\n    return {\"report_uri\": art.uri}\n</code></pre> <p>Rank by metric or search</p> <pre><code>@graph_fn(name=\"search_reports\", outputs=[\"top_uri\"])\nasync def search_reports(*, context):\n    top = await context.artifacts().best(\n        kind=\"checkpoint\", metric=\"val_acc\", mode=\"max\", scope=\"run\"\n    )\n    if top:\n        return {\"top_uri\": top.uri}\n    results = await context.artifacts().search(\n        kind=\"report\", labels={\"exp\": \"A\"}\n    )\n    return {\"top_uri\": results[0].uri if results else None}\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#3-memory-structured-event-result-log","title":"3. Memory \u2014 Structured Event &amp; Result Log","text":"<p>Memory is a unified fa\u00e7ade for recording, persisting, and querying events during an agent\u2019s lifetime: raw logs, typed results, metrics, and their relationships with artifacts.</p>"},{"location":"key-concepts/artifacts-memory/#why-memory-design-intent","title":"Why Memory (design intent)","text":"<ul> <li>Contextual recall: agents can react based on recent or historical state.</li> <li>Typed outputs: <code>write_result</code> records semantic outputs with names/kinds/values.</li> <li>Summarization: distill long conversations or runs into compact context.</li> <li>RAG\u2011ready: promote events to a vector index for retrieval\u2011augmented answers.</li> <li>Analytics: compute \u201clast by name,\u201d track trends, or export for BI.</li> </ul>"},{"location":"key-concepts/artifacts-memory/#architecture_1","title":"Architecture","text":"<pre><code>[ Your Agent / Tool ]\n          \u2502   (context)\n          \u25bc\n[ NodeContext ]\n          \u2502\n          \u25bc\n[ context.memory() \u2014 Memory Facade ]\n        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502               \u2502                  \u2502               \u2502              \u2502\n        \u25bc               \u25bc                  \u25bc               \u25bc              \u25bc\n   [ HotLog ]      [ Persistence ]     [ Indices ]     [ Summaries ]  [ RAG ]\n (ephemeral KV)   (JSONL append-only)   (SQLite/KV)     (LLM-based)   (FAISS)\n        \u2502               \u2502                  \u2502               \u2502              \u2502\n   recent()/tail   replay/export      last_by_name     distill_*      search/answer\n                                      last_outputs                     promote_events\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#core-api_1","title":"Core API","text":"Method Purpose <code>record_raw(base, text=None, metrics=None)</code> Append a low\u2011level event. <code>record(kind, data, tags=None, **extra)</code> Convenience structured logging. <code>write_result(topic, inputs=None, outputs=None, tags=None)</code> Log a typed output; updates indices. <code>recent(kinds=None, limit=50)</code> Fetch most recent events. <code>last_by_name(name)</code> Get the latest output value by name. <code>latest_refs_by_kind(kind)</code> Retrieve latest artifact/message refs of a kind. <code>distill_rolling_chat(max_turns=20)</code> Generate a compact chat/run summary (LLM\u2011assisted). <code>rag_bind(scope)</code> / <code>rag_promote_events(...)</code> / <code>rag_answer(...)</code> RAG lifecycle helpers (requires LLM)."},{"location":"key-concepts/artifacts-memory/#examples_1","title":"Examples","text":"<p>Log a typed result</p> <pre><code>@graph_fn(name=\"remember_output\", outputs=[\"y\"])\nasync def remember_output(x: int, *, context):\n    y = x + 1\n    await context.memory().write_result(\n        topic=\"calc\",\n        outputs=[{\"name\": \"y\", \"kind\": \"number\", \"value\": y}],\n        tags=[\"demo\"],\n    )\n    return {\"y\": y}\n</code></pre> <p>Recall + summarization</p> <pre><code>recent = await context.memory().recent(limit=10)\nlast_y = await context.memory().last_by_name(\"y\")\nsummary = await context.memory().distill_rolling_chat(max_turns=20)\n</code></pre> <p>Promote to RAG</p> <pre><code>corpus = await context.memory().rag_bind(scope=\"project\")\nawait context.memory().rag_promote_events(corpus_id=corpus)\nans = await context.memory().rag_answer(corpus_id=corpus, question=\"What was the best run?\")\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#4-artifacts-memory-better-together","title":"4. Artifacts \u00d7 Memory \u2014 Better Together","text":"<p>Artifacts and Memory reference each other: results and metrics point to artifact URIs; artifact metadata references the node that produced them. This bi\u2011directional linking enables:</p> <ul> <li>Reconstructing the full story of a result (inputs \u2192 tools \u2192 outputs \u2192 files).</li> <li>Ranking/searching results across runs/experiments.</li> <li>Efficient clean\u2011up strategies (e.g., keep pinned/best; GC the rest).</li> </ul>"},{"location":"key-concepts/artifacts-memory/#5-extensibility-external-systems","title":"5. Extensibility &amp; External Systems","text":"<p>AetherGraph\u2019s built\u2011ins for Artifacts and Memory are part of the OSS core runtime and are not swappable in place. That is intentional: we rely on their stable semantics for provenance, lineage, and tooling.</p> <p>Important: You cannot replace <code>context.artifacts()</code> or <code>context.memory()</code> with custom implementations. Instead, add new services with new names and keep agent code explicit about which storage it is using.</p>"},{"location":"key-concepts/artifacts-memory/#recommended-extension-pattern","title":"Recommended Extension Pattern","text":"<ul> <li>Register new services under distinct names, e.g. <code>context.datasets()</code>, <code>context.vault()</code>, <code>context.lineage_store()</code>, <code>context.vector_db()</code> or <code>context.external_artifacts()</code>.</li> <li>Keep provenance links by storing artifact URIs or memory event IDs alongside your external records when appropriate.</li> <li>Export/mirror selectively: you can mirror core artifacts/memory events to external systems for BI/compliance without changing the core stores.</li> </ul> <p>See Extending Context Services for <code>Service</code> APIs.</p>"},{"location":"key-concepts/artifacts-memory/#6-design-principles","title":"6. Design Principles","text":"<ul> <li>Python\u2011first: simple, composable APIs; no DSL required.</li> <li>Immutable by default: artifacts are write\u2011once; updates create new versions.</li> <li>Typed results: names/kinds/values make analytics and recall precise.</li> <li>Provenance everywhere: run/graph/node IDs are attached automatically.</li> <li>Separation of concerns: artifacts hold assets; memory holds events/results; they reference each other for lineage.</li> </ul>"},{"location":"key-concepts/artifacts-memory/#summary","title":"Summary","text":"<ul> <li>Artifacts make outputs durable, searchable, and reproducible with CAS URIs and rich indexing.</li> <li>Memory records the event stream and typed results for contextual recall, analytics, and RAG.</li> <li>Together they provide end\u2011to\u2011end provenance and effortless \u201ctime travel\u201d across runs.</li> <li>The fa\u00e7ades are extensible: swap in enterprise stores/indices without changing agent code.</li> </ul> <p>See also: <code>context.artifacts()</code> \u00b7 <code>context.memory()</code> \u00b7 <code>context.rag()</code> \u00b7 External Context Services</p>"},{"location":"key-concepts/channels-interaction/","title":"Channels and Interaction","text":"<p>A channel is how an agent communicates with the outside world \u2014 Slack, Telegram, Console, Web, or any other adapter. The <code>context.channel()</code> method returns a ChannelSession, a lightweight helper that provides a consistent Python API for sending and receiving messages, buttons, files, streams, and progress updates \u2014 regardless of which adapter you use.</p> <p>Default behavior: If no adapters are configured, AetherGraph automatically uses the console (<code>\"console:stdin\"</code>) as the default channel for input/output. To target Slack, Telegram, or Web, configure their adapters with valid credentials; your agent code remains unchanged.</p> <p>In short: Switch communication targets freely. The agent logic stays identical.</p>"},{"location":"key-concepts/channels-interaction/#1-what-is-a-channel","title":"1. What Is a Channel?","text":"<p>A channel is a routing target for interaction. It represents where your agent sends or receives messages. You can specify a channel key (e.g., <code>\"slack:#research\"</code>) or rely on the system default.</p>"},{"location":"key-concepts/channels-interaction/#resolution-order","title":"Resolution Order","text":"<ol> <li>Per-call override: <code>await context.channel().send_text(\"hi\", channel=\"slack:#alerts\")</code></li> <li>Bound session key: <code>ch = context.channel(\"slack:#research\"); await ch.send_text(\"hi\")</code></li> <li>Bus default: taken from <code>services.channels.get_default_channel_key()</code></li> <li>Fallback: <code>console:stdin</code></li> </ol>"},{"location":"key-concepts/channels-interaction/#2-quick-start","title":"2. Quick Start","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"channel_demo\")\nasync def channel_demo(*, context):\n    ch = context.channel(\"slack:#research\")\n    await ch.send_text(\"Starting experiment\u2026\")\n    resp = await ch.ask_approval(\"Proceed?\", options=[\"Yes\", \"No\"])\n    if resp[\"approved\"]:\n        await ch.send_text(\"\u2705 Launching run.\")\n</code></pre> <p>Channels handle both output and input asynchronously \u2014 messages, approvals, file uploads, and more \u2014 using cooperative waits under the hood.</p>"},{"location":"key-concepts/channels-interaction/#3-core-methods","title":"3. Core Methods","text":"<p>Availability depends on the adapter\u2019s capabilities (e.g., file uploads are not supported in the console channel).</p> Method Purpose <code>send_text(text, *, channel=None)</code> Send a plain text message. <code>send_file(url=None, *, file_bytes=None, filename=\"file.bin\", title=None, channel=None)</code> Upload or attach a file. <code>send_buttons(text, buttons, *, channel=None)</code> Send a message with interactive buttons. <code>ask_text(prompt, *, timeout_s=3600, channel=None)</code> Ask for free-text input (cooperative wait). <code>ask_approval(prompt, options=(\"Approve\",\"Reject\"), *, timeout_s=3600, channel=None)</code> Request approval or a choice. <code>ask_files(prompt, *, accept=None, multiple=True, timeout_s=3600, channel=None)</code> Request file upload(s). <code>stream(channel=None)</code> Open a streaming session for incremental updates. <code>progress(title=\"Working...\", total=None, *, channel=None)</code> Create a live progress bar context. <p>All <code>ask_*</code> methods use event-driven continuations, ensuring replies are properly correlated to their originating node.</p>"},{"location":"key-concepts/channels-interaction/#4-streaming-and-progress","title":"4. Streaming and Progress","text":""},{"location":"key-concepts/channels-interaction/#streaming","title":"Streaming","text":"<p>Use streams for live token or log updates:</p> <pre><code>@graph_fn(name=\"stream_demo\")\nasync def stream_demo(*, context):\n    async with context.channel().stream() as s:\n        for chunk in [\"Hello\", \" \", \"world\", \"\u2026\"]:\n            await s.delta(chunk)\n        await s.end(\"Hello world!\")\n    return {\"ok\": True}\n</code></pre>"},{"location":"key-concepts/channels-interaction/#progress","title":"Progress","text":"<p>Track task progress with structured updates (current, total, percent, ETA):</p> <pre><code>@graph_fn(name=\"progress_demo\")\nasync def progress_demo(*, context):\n    async with context.channel().progress(title=\"Crunching\", total=5) as bar:\n        for i in range(5):\n            await bar.update(current=i+1, eta_seconds=(4-i)*0.5, subtitle=f\"step {i+1}/5\")\n        await bar.end(subtitle=\"All done!\", success=True)\n    return {\"ok\": True}\n</code></pre>"},{"location":"key-concepts/channels-interaction/#5-file-uploads-and-mixed-replies","title":"5. File Uploads and Mixed Replies","text":"<p>Request files, optionally with a text comment:</p> <pre><code>@graph_fn(name=\"upload_demo\")\nasync def upload_demo(*, context):\n    ans = await context.channel().ask_files(\n        prompt=\"Upload your dataset and add a note:\",\n        accept=[\".csv\", \"application/zip\"],\n        multiple=True,\n    )\n    await context.channel().send_text(f\"Received {len(ans['files'])} file(s). Thanks!\")\n    return {\"ok\": True}\n</code></pre> <p><code>ans</code> contains both <code>text</code> and <code>files</code>, where each file is a structured <code>FileRef</code> object with metadata and a retrievable URI.</p>"},{"location":"key-concepts/channels-interaction/#6-concurrency-and-fan-out","title":"6. Concurrency and Fan-Out","text":"<p>You can launch multiple concurrent asks in the same bound channel session and correlate the results:</p> <pre><code>import asyncio\n\n@graph_fn(name=\"concurrent_asks\")\nasync def concurrent_asks(*, context):\n    ch = context.channel(\"slack:#research\")\n\n    async def one(tag):\n        name = await ch.ask_text(f\"[{tag}] What\u2019s your name?\")\n        await ch.send_text(f\"[{tag}] Thanks, {name}!\")\n        return {tag: name}\n\n    a, b = await asyncio.gather(one(\"A\"), one(\"B\"))\n    return {\"names\": a | b}\n</code></pre>"},{"location":"key-concepts/channels-interaction/#7-extensibility","title":"7. Extensibility","text":"<p>The channel interface can be extended to support any platform with a compatible API (HTTP, WebSocket, SDK). In practice, the inbound method for resuming interactions depends heavily on the target platform\u2019s event model.</p> <ul> <li>For notification-only channels, the API is straightforward \u2014 send events, no continuations.</li> <li>For interactive channels (e.g., Slack, Telegram, Web), resumptions rely on correlation IDs and continuation stores.</li> </ul> <p>In the OSS edition, AetherGraph currently includes built-in support for Console, Slack, and Telegram. Support for additional adapters (e.g., Web or REST endpoints) will be provided in future releases.</p>"},{"location":"key-concepts/channels-interaction/#8-guarantees-and-notes","title":"8. Guarantees and Notes","text":"<ul> <li>Idempotent updates: <code>stream()</code> and <code>progress()</code> use stable keys derived from <code>(run_id, node_id, suffix)</code>.</li> <li>Thread-safe correlation: all <code>ask_*</code> calls automatically bind correlators for proper reply routing.</li> <li>Adapter-agnostic: switch destinations by changing the channel key \u2014 agent logic remains identical.</li> <li>Unified abstraction: all adapters implement the same interface; only configuration changes per environment.</li> </ul>"},{"location":"key-concepts/channels-interaction/#summary","title":"Summary","text":"<ul> <li>Channels unify all interaction patterns (text, files, approvals, progress, and streaming) under one async API.</li> <li>Default channel is console; others (Slack, Telegram, Web) are pluggable.</li> <li>All <code>ask_*</code> methods suspend execution via event-driven continuations, resuming seamlessly upon reply.</li> <li>Channels are adapter-agnostic and fully extensible \u2014 swap backends, not code.</li> </ul> <p>Write once, interact anywhere \u2014 your agents stay Pythonic, event\u2011driven, and platform\u2011neutral.</p>"},{"location":"key-concepts/concurrency-orchestration/","title":"Concurrency, Fan\u2011In/Fan\u2011Out &amp; Graph\u2011Level Orchestration","text":"<p>AetherGraph provides Python\u2011first concurrency that scales from small reactive agents to globally scheduled DAGs. You can orchestrate parallelism naturally in Python, while the runtime enforces safe scheduling and per\u2011run concurrency caps.</p>"},{"location":"key-concepts/concurrency-orchestration/#1-graph_fn-pythonic-concurrency-for-reactive-agents","title":"1. <code>@graph_fn</code> \u2014 Pythonic Concurrency for Reactive Agents","text":"<p><code>@graph_fn</code> functions execute through normal Python async semantics. Plain Python awaits run directly on the event loop, while any <code>@tool</code> calls inside a <code>@graph_fn</code> become implicit nodes managed by the agent\u2019s internal scheduler.</p> <p>Example: bounded fan\u2011out using a semaphore</p> <pre><code>import asyncio\nfrom aethergraph import graph_fn\n\nsem = asyncio.Semaphore(4)  # cap concurrent jobs (user-managed)\n\nasync def run_capped(fn, **kw):\n    async with sem:\n        return await fn(**kw)\n\n@graph_fn(name=\"batch_agent\")\nasync def batch_agent(items: list[str], *, context):\n    async def one(x):\n        await context.channel().send_text(f\"processing {x}\")\n        return {\"y\": x.upper()}\n\n    # fan\u2011out with manual cap\n    tasks = [run_capped(one, x=v) for v in items]\n    results = await asyncio.gather(*tasks)\n\n    # fan\u2011in\n    return {\"ys\": [r[\"y\"] for r in results]}\n</code></pre> <p>Notes:</p> <ul> <li>Plain Python steps execute immediately \u2014 not capped by the scheduler.</li> <li><code>@tool</code> calls are scheduled and counted toward the agent\u2019s concurrency cap through <code>max_concurrency</code> (default = 4).</li> <li>You can override per\u2011run limits by passing <code>max_concurrency=&lt;int&gt;</code> to <code>run()</code> or <code>run_async()</code> or use <code>graph_fn(.., max_concurrency=&lt;int&gt;)</code>.</li> <li>For nested or composed agents, effective concurrency multiplies; use semaphores or pools to control load.</li> <li>Ideal for reactive, exploratory agents or mixed I/O + compute logic.</li> </ul>"},{"location":"key-concepts/concurrency-orchestration/#2-graphify-schedulercontrolled-static-dags","title":"2. <code>@graphify</code> \u2014 Scheduler\u2011Controlled Static DAGs","text":"<p>In static DAGs built with <code>@graphify</code>, every <code>@tool</code> call becomes a node in a TaskGraph. Concurrency is automatically managed by the runtime scheduler, respecting per\u2011run limits.</p> <p>Minimal fan\u2011in/fan\u2011out example:</p> <pre><code>from aethergraph import graphify, tool\n\n@tool(outputs=[\"out\"])\nasync def work(x: int):\n    return {\"out\": x * 2}\n\n@tool(outputs=[\"sum\"])\nasync def reduce_sum(xs: list[int]):\n    return {\"sum\": sum(xs)}\n\n@graphify(name=\"map_reduce\", inputs=[\"vals\"], outputs=[\"sum\"])\ndef map_reduce(vals):\n    outs = [work(x=v) for v in vals]       # fan\u2011out\n    total = reduce_sum(xs=[o.out for o in outs])  # fan\u2011in\n    return {\"sum\": total.sum}\n</code></pre> <p>Key points:</p> <ul> <li>The scheduler enforces <code>max_concurrency</code> automatically (default = 4).</li> <li>You can override per\u2011run limits by passing <code>max_concurrency=&lt;int&gt;</code> to <code>run()</code>, or <code>run_async()</code>.</li> <li>Static DAG concurrency is global and consistent across all tool nodes.</li> <li>Each node runs once dependencies resolve; no explicit <code>await</code> is required.</li> </ul>"},{"location":"key-concepts/concurrency-orchestration/#3-graphlevel-orchestration-patterns","title":"3. Graph\u2011Level Orchestration Patterns","text":"<p>All orchestration in AetherGraph is just Python. You can run sequentially or concurrently using standard async primitives.</p>"},{"location":"key-concepts/concurrency-orchestration/#a-sequential-orchestration-plain-python","title":"A) Sequential orchestration (plain Python)","text":"<pre><code>res1 = await graph_fn1(a=1)\nres2 = await graph_fn2(b=2)\n</code></pre>"},{"location":"key-concepts/concurrency-orchestration/#b-concurrent-graph_fn-runs-asyncfriendly","title":"B) Concurrent <code>graph_fn</code> runs (async\u2011friendly)","text":"<pre><code>res1, res2 = await asyncio.gather(\n    graph_fn1(a=1),\n    graph_fn2(b=2),\n)\n</code></pre>"},{"location":"key-concepts/concurrency-orchestration/#c-concurrent-graph-runner-works-for-both-graph_fn-and-graphify","title":"C) Concurrent graph runner (works for both <code>graph_fn</code> and <code>graphify</code>)","text":"<pre><code>from aethergraph.runner import run_async\n\nres1, res2 = await asyncio.gather(\n    run_async(graph1, inputs={\"a\": 1}, max_concurrency=8),\n    run_async(graph2, inputs={\"b\": 2}, max_concurrency=2),\n)\n</code></pre> <p>Default concurrency for each graph is 4, but you can override it per call with <code>max_concurrency</code> in either <code>run()</code> or <code>run_async()</code>. Becareful of global concurrency limit. Use semaphores or pools to control load. </p>"},{"location":"key-concepts/concurrency-orchestration/#4-nested-and-multigraph-execution","title":"4. Nested and Multi\u2011Graph Execution","text":"<p>Nested <code>@graph_fn</code> \u2014 supported. Each agent has its own scheduler; nested agents may multiply total concurrency. Use global semaphores or resource pools to cap total parallelism.</p> <p>Nested <code>@graphify</code> \u2014 not supported yet. Static graphs cannot call other static graphs as nodes; compose them at the orchestration layer instead.</p>"},{"location":"key-concepts/concurrency-orchestration/#5-concurrency-comparison","title":"5. Concurrency Comparison","text":"Aspect <code>@graph_fn</code> (Reactive) <code>@graphify</code> (Static) Concurrency Control Automatic via scheduler (<code>max_concurrency</code>) Automatic via scheduler (<code>max_concurrency</code>) Default Limit Default 4 per run, multiply with nested calls Default 4 per run Plain Python Awaitables Run immediately, outside scheduler Not applicable (only tool nodes) Nested Calls Supported Not yet supported Failure Behavior Caught at runtime; user decides Scheduler stops on first error (configurable) Use Case Agents, exploration, hybrid control Pipelines, batch workflows, reproducible DAGs"},{"location":"key-concepts/concurrency-orchestration/#takeaways","title":"Takeaways","text":"<ul> <li>Reactive vs Deterministic: <code>graph_fn</code> for interactive exploration; <code>graphify</code> for reproducible pipelines.</li> <li>Fan\u2011In/Fan\u2011Out: Async patterns in <code>graph_fn</code>; data edges in <code>graphify</code>.</li> <li>Concurrency Control: Default cap = 4; override per run with <code>max_concurrency</code>.</li> <li>Scalability: Local schedulers per agent; a global scheduler orchestrates multiple runs.</li> <li>Everything is Python: The runtime extends standard async execution into persistent, inspectable DAG scheduling.</li> </ul>"},{"location":"key-concepts/context-services/","title":"Context Services Overview","text":"<p>Context is the lightweight runtime handle that every agent and tool receives during execution. It represents the active run, graph, and node scope and exposes AetherGraph\u2019s built-in runtime services\u2014channels, memory, artifacts, logs, and more\u2014through a clean, Pythonic interface.</p> <p>In short: Context is what makes an AetherGraph program \u201calive.\u201d It bridges your pure Python logic with interactive I/O, persistence, orchestration, and AI-powered capabilities\u2014without introducing a new DSL or framework-specific syntax.</p>"},{"location":"key-concepts/context-services/#1-why-context-matters","title":"1. Why Context Matters","text":"<p>AetherGraph\u2019s guiding principle is Python-first orchestration. The context system makes that possible by providing a unified way to connect logic and infrastructure.</p> <p>Core benefits:</p> <ul> <li>Decoupled logic: Agents and tools can call <code>context.&lt;service&gt;()</code> without worrying about back-end details or deployment environment.</li> <li>Automatic provenance: Each call carries its <code>run_id</code>, <code>graph_id</code>, and <code>node_id</code>, ensuring full traceability.</li> <li>Zero-friction orchestration: Handles message passing, persistence, and coordination transparently.</li> <li>Optional intelligence: Attach LLMs, RAG corpora, or MCP servers only when needed\u2014no dependencies until configured.</li> </ul> <p>In practice, <code>NodeContext</code> turns plain async functions into interactive, stateful agents that can communicate, remember, reason, and orchestrate\u2014all from Python.</p>"},{"location":"key-concepts/context-services/#2-quick-start","title":"2. Quick Start","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"hello_context\")\nasync def hello_context(*, context):\n    await context.channel().send_text(\"Hello from AetherGraph!\")\n    await context.memory().write_result(\n        topic=\"hello\",\n        outputs=[{\"name\": \"msg\", \"kind\": \"text\", \"value\": \"hello\"}],\n        tags=[\"demo\"],\n    )\n    context.logger().info(\"finished\", extra={\"stage\": \"done\"})\n    return {\"ok\": True}\n</code></pre> <p>Each call operates within a specific node scope. The runtime automatically provides <code>run_id</code>, <code>graph_id</code>, and <code>node_id</code> to maintain context and provenance.</p>"},{"location":"key-concepts/context-services/#3-context-structure","title":"3. Context Structure","text":"<p>Each <code>NodeContext</code> carries stable identifiers and bound service references.</p> <pre><code>@dataclass\nclass NodeContext:\n    run_id: str\n    graph_id: str\n    node_id: str\n    services: NodeServices  # all bound runtime services\n</code></pre> <p>Identifiers</p> <ul> <li>run_id \u2014 unique per execution run.</li> <li>graph_id \u2014 identifies which graph the node belongs to.</li> <li>node_id \u2014 unique ID for the node invocation.</li> </ul>"},{"location":"key-concepts/context-services/#4-context-services","title":"4. Context Services","text":"<p>AetherGraph organizes its context services into core, optional, and utility layers.</p>"},{"location":"key-concepts/context-services/#core-services","title":"Core Services","text":"Method Purpose <code>context.channel(key: str | None = None)</code> Message and interaction bus \u2014 send text, files, progress, or streaming events. Defaults to the configured session channel. <code>context.memory()</code> Memory fa\u00e7ade \u2014 record events, write results, query history, or manage RAG-ready logs. <code>context.artifacts()</code> Artifact store fa\u00e7ade \u2014 save/retrieve files, track outputs, and query experiment artifacts. <code>context.kv()</code> Lightweight key\u2013value store for ephemeral coordination and small caches. <code>context.logger()</code> Structured logger with <code>{run_id, graph_id, node_id}</code> metadata automatically included."},{"location":"key-concepts/context-services/#optional-services-config-dependent","title":"Optional Services (config-dependent)","text":"<p>Optional services require API keys or runtime configuration. They are injected dynamically when available.</p> Method Purpose <code>context.llm(profile=\"default\")</code> Access an LLM client for chat or embeddings (OpenAI, Anthropic, or local backends). <code>context.rag()</code> Retrieval-augmented generation fa\u00e7ade \u2014 build corpora, upsert documents, search, and answer queries. <code>context.mcp(name)</code> Connect to external MCP tool servers via stdio, WebSocket, or HTTP."},{"location":"key-concepts/context-services/#utility-helpers","title":"Utility Helpers","text":"Method Purpose <code>context.clock()</code> Clock utilities for timestamps, delays, and scheduling. <code>context.continuations()</code> Access continuation store; used internally for dual-stage waits (<code>ask_text</code>, <code>ask_approval</code>). <p>If a service is unavailable, its accessor raises a clear runtime error (e.g., <code>LLMService not available</code>). Configure them globally or per-environment to enable.</p>"},{"location":"key-concepts/context-services/#5-typical-patterns","title":"5. Typical Patterns","text":""},{"location":"key-concepts/context-services/#1-ask-wait-resume","title":"1 Ask \u2192 Wait \u2192 Resume","text":"<pre><code>text = await context.channel().ask_text(\"Provide a dataset path\")\n# Runtime yields, stores a continuation, and resumes when input arrives.\n</code></pre>"},{"location":"key-concepts/context-services/#2-streaming-progress","title":"2 Streaming &amp; Progress","text":"<pre><code>async with context.channel().stream() as s:\n    await s.delta(\"Parsing\u2026 \")\n    await s.delta(\"OK \u2705\")\n\nasync with context.channel().progress(title=\"Training\", total=100) as p:\n    for i in range(0, 101, 5):\n        await p.update(current=i)\n</code></pre>"},{"location":"key-concepts/context-services/#3-artifacts-memory","title":"3 Artifacts + Memory","text":"<pre><code>art = await context.artifacts().save(path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\": \"A\"})\nawait context.memory().write_result(\n    topic=\"report\",\n    outputs=[{\"name\": \"uri\", \"kind\": \"uri\", \"value\": art.uri}],\n)\n</code></pre>"},{"location":"key-concepts/context-services/#4-rag-llm-answers","title":"4 RAG + LLM Answers","text":"<pre><code>hits = await context.rag().search(\"notes\", query=\"What is MTF?\", k=5)\nans = await context.rag().answer(\"notes\", question=\"What is MTF?\", style=\"concise\")\nawait context.channel().send_text(ans[\"answer\"])\n</code></pre>"},{"location":"key-concepts/context-services/#5-external-tools-via-mcp","title":"5 External Tools via MCP","text":"<pre><code>res = await context.mcp(\"ws\").call(\"search\", {\"q\": \"tolerance analysis\", \"k\": 5})\n</code></pre>"},{"location":"key-concepts/context-services/#6-custom-context-services","title":"6. Custom Context Services","text":"<p>The context system is fully extensible. You can define your own service and expose it via <code>context.&lt;name&gt;()</code> using <code>register_context_service()</code>.</p> <p>Use cases:</p> <ul> <li>Add domain-specific APIs (e.g., simulation, materials DB, experiment tracking).</li> <li>Provide custom persistence or distributed coordination layers.</li> <li>Implement bridges between external systems (e.g., job schedulers, cloud storage, or lab devices).</li> </ul> <p>See External Context Services for API details and examples.</p>"},{"location":"key-concepts/context-services/#7-design-philosophy","title":"7. Design Philosophy","text":"<ul> <li>Python-first: use direct calls, not DSL syntax.</li> <li>Minimal surface: each service follows a small, composable API.</li> <li>Composable orchestration: mix local and remote services freely.</li> <li>Swappable backends: replace LLM, KV, or artifact backends without touching agent logic.</li> </ul>"},{"location":"key-concepts/context-services/#see-also","title":"See Also","text":"<ul> <li>[<code>context.channel()</code>] \u2014 cooperative waits, streaming, progress updates</li> <li>[<code>context.memory()</code>] \u2014 event log, typed results, summaries, and RAG helpers</li> <li>[<code>context.artifacts()</code>] \u2014 content-addressable storage and retrieval</li> <li>[<code>context.llm()</code>] \u2014 chat, completion, and embeddings</li> <li>[<code>context.rag()</code>] \u2014 corpus creation and QA retrieval</li> <li>[<code>context.mcp()</code>] \u2014 bridges to external tool servers</li> <li>[<code>context.kv()</code>] \u2014 transient coordination and state passing</li> <li>[<code>context.logger()</code>] \u2014 structured</li> </ul>"},{"location":"key-concepts/event-driven-waits/","title":"Event\u2011Driven Waits: Cooperative vs Dual\u2011Stage","text":"<p>AetherGraph agents are event\u2011driven: they can pause mid\u2011flow and safely resume when a reply, upload, or callback arrives. There are two complementary wait modes, and you can use them flexibly in both <code>@graph_fn</code> and <code>@graphify</code>\u2011built graphs.</p> <ul> <li>Cooperative waits \u2014 via <code>context.channel().ask_*</code>. Simplest way to prompt + wait in reactive agents.</li> <li>Dual\u2011stage waits \u2014 via <code>@tool</code> nodes that split into Stage A (prompt/setup) and Stage B (resume/produce). Best for static graphs and reliable orchestration.</li> </ul> <p>Flexibility: <code>context.*</code> methods are available inside <code>@tool</code> nodes (therefore inside <code>@graphify</code>). Dual\u2011stage tools can also be <code>await</code>\u2011ed directly inside <code>@graph_fn</code>. In either case, they form a node and persist a continuation.</p>"},{"location":"key-concepts/event-driven-waits/#1-cooperative-waits-channelfirst","title":"1 Cooperative Waits (Channel\u2011first)","text":"<p>What: <code>context.channel().ask_text / ask_approval / ask_files</code> send a prompt and yield until a reply or timeout. The runtime persists a continuation token so the run can resume after restarts.</p> <p>Where: Primarily inside <code>@graph_fn</code>. Can also be called from within a <code>@tool</code> if you want cooperative logic inside a node.</p> <p>Example</p> <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"cooperative_demo\", outputs=[\"msg\"]) \nasync def cooperative_demo(*, context):\n    name = await context.channel().ask_text(\"Your name?\")\n    await context.channel().send_text(f\"Hi, {name}!\")\n    return {\"msg\": f\"greeted:{name}\"}\n</code></pre> <p>Properties</p> <ul> <li>Minimal code, great for exploratory, chat\u2011style agents.</li> <li>Thread/channel\u2011aware correlation.</li> <li>Durable continuations; survives restarts.</li> </ul>"},{"location":"key-concepts/event-driven-waits/#2-dualstage-waits-toolfirst","title":"2 Dual\u2011Stage Waits (Tool\u2011first)","text":"<p>What: A node splits into two stages: A emits the prompt/sets up state, B resumes once the event arrives and produces outputs. Maps cleanly to static DAGs and lets the global scheduler manage resumptions and retries.</p> <p>Use in both places:</p> <ul> <li>In <code>@graphify</code> as standard tool nodes.</li> <li>In <code>@graph_fn</code> with <code>await</code> for immediate use \u2014 they still become nodes under the hood.</li> </ul> <p>Built\u2011in channel tools</p> <pre><code># Use these in either style:\nfrom aethergraph.tools import ask_text, ask_approval, ask_files\n\n# A) Inside a static graph\nfrom aethergraph import graphify\n\n@graphify(name=\"collect_input\", inputs=[], outputs=[\"greeting\"]) \ndef collect_input():\n    name = ask_text(prompt=\"Your name?\")      # node yields \u2192 resumes on reply\n    return {\"greeting\": name.text}\n\n# B) Await directly in a graph_fn\nfrom aethergraph import graph_fn\n\n@graph_fn(name=\"dualstage_in_fn\", outputs=[\"choice\"]) \nasync def dualstage_in_fn(*, context):\n    res = await ask_approval(prompt=\"Proceed?\", options=(\"Yes\",\"No\"))\n    return {\"choice\": res[\"choice\"]}\n</code></pre> <p>Properties</p> <ul> <li>Node\u2011level persistence, retries, and metrics.</li> <li>Works seamlessly with global scheduling (centralized control, resumptions at scale).</li> <li>Great for UI + pipeline hybrids (prompt in Stage A, compute in Stage B).</li> </ul>"},{"location":"key-concepts/event-driven-waits/#3-using-context-inside-graphify","title":"3 Using <code>context.*</code> inside <code>@graphify</code>","text":"<p><code>context</code> methods (channels, memory, artifacts, kv, logger, etc.) are available inside <code>@tool</code> nodes. This means your static graphs can still interact, log, and persist during node execution while retaining DAG inspectability.</p> <pre><code>from aethergraph import tool\n\n@tool(outputs=[\"ok\"]) \nasync def notify_and_tag(*, context):\n    await context.channel().send_text(\"Started node\u2026\")\n    await context.memory().record(kind=\"status\", data={\"stage\":\"start\"})\n    return {\"ok\": True}\n</code></pre>"},{"location":"key-concepts/event-driven-waits/#4-comparison-cooperative-vs-dualstage-vs-manual-checkpoints","title":"4 Comparison: Cooperative vs Dual\u2011Stage vs Manual Checkpoints","text":"Aspect Cooperative (<code>context.channel().ask_*</code>) Dual-Stage (<code>@tool</code> ask_*) Manual checkpoints Authoring style Inline, minimal Explicit node with A/B stages N/A in AG (not built-in) Resumability Hard \u2014 stateless unless save the state to memory manually Native continuations per node (resumeable after restart) Possible but manual/fragile Retry / Idempotency Coarse (re-invoke the whole function) Fine (node-level retry, idempotent resumes) Manual Scale Great for interactive sessions, small graphs Excellent for large runs / thousands of waits Limited by implementation CPU load (waiting) Keeps process / event loop alive; lightweight but not zero Zero CPU \u2014 node is dormant until resumed Depends on checkpointing backend Memory footprint Held in local task heap (light) Released after serialization; only metadata retained Depends on snapshot granularity Disk usage Optional if memory writes used Tiny (~1\u201310 KB per node) \u2014 correlator + inputs serialized Potentially heavy (full state dump) Latency to resume Instant within current process Slightly higher (resume event \u2192 lookup \u2192 dispatch) Potentially high (manual restore) <p>Why Dual\u2011Stage scales</p> <ul> <li>Node\u2011granular control: retries, backoff, and resumption are local to the waiting node.</li> <li>Central orchestration: the global scheduler can queue, shard, or migrate blocked nodes.</li> <li>Observability: each wait is a first\u2011class node with metrics and logs.</li> <li>Determinism: Stage boundaries clarify side\u2011effects and make runs reproducible.</li> </ul> <p>Manual checkpoints (framework\u2011agnostic snapshots) aren\u2019t part of AetherGraph. Dual\u2011stage nodes cover the same reliability space with less boilerplate and better provenance.</p>"},{"location":"key-concepts/event-driven-waits/#5-extending-dualstage-tools","title":"5 Extending Dual\u2011Stage Tools","text":"<p>You can author custom dual\u2011stage nodes with <code>DualStageTool</code> to model your own A/B waits (e.g., submit job \u2192 wait \u2192 collect). Some examples of the usage include </p> <ul> <li>custom channel waits</li> <li>submit/run long simualtion on cloud</li> <li>data/model training pipeline on external systems</li> <li>external API Polling that reports a compleltion asynchronously</li> </ul> <p>A compact public API for this is planned; detailed docs will ship soon.</p>"},{"location":"key-concepts/event-driven-waits/#6-takeaways","title":"6 Takeaways","text":"<ul> <li>All <code>context.channel().ask_*</code> calls are cooperative waits by default.</li> <li>Dual\u2011stage tools work in both <code>@graphify</code> and <code>@graph_fn</code> (awaitable) and always materialize as nodes.</li> <li>For large, reliable systems: prefer dual\u2011stage for node\u2011level retries, metrics, and scheduler control.</li> <li><code>context.*</code> is available inside <code>@tool</code> nodes, so static graphs can still interact, log, and persist cleanly.</li> <li>Manual checkpointing isn\u2019t needed; dual\u2011stage nodes give better reliability with less boilerplate.</li> </ul>"},{"location":"key-concepts/extending-context/","title":"Extending Context Services","text":"<p>AetherGraph lets you extend the runtime by adding your own <code>context.&lt;name&gt;</code> methods. These external context services live alongside built\u2011ins like <code>channel</code>, <code>memory</code>, and <code>artifacts</code>, and provide reusable, lifecycle\u2011aware helpers for clients, caches, orchestration, or domain APIs \u2014 without changing your agent code.</p> <p>Key idea: keep agent logic pure\u2011Python; move integration glue and shared state into services that the runtime injects per node.</p>"},{"location":"key-concepts/extending-context/#1-what-is-an-external-context-service","title":"1. What is an External Context Service?","text":"<p>A context service is a registered Python object bound into every <code>NodeContext</code>. After registration, you can use it anywhere inside a graph or tool:</p> <pre><code>@graph_fn(name=\"demo\")\nasync def demo(*, context):\n    info = await context.trainer().inspect_job(job_id=\"abc123\")\n    return {\"status\": info[\"status\"]}\n</code></pre>"},{"location":"key-concepts/extending-context/#whywhen-to-use","title":"Why/When to Use","text":"<ul> <li>Reusable helpers \u2014 share clients (e.g., HPC, S3, DB, solver), connection pools, or token buckets.</li> <li>Shared state \u2014 memoize expensive lookups; coordinate across nodes within a run.</li> <li>Centralized config \u2014 keep API keys, timeouts, routing, or policies in one place.</li> <li>Lifecycle control \u2014 optional <code>start()/close()</code> hooks for setup/teardown.</li> <li>Per\u2011node awareness \u2014 access <code>run_id/graph_id/node_id</code> for provenance or multi\u2011tenancy.</li> </ul> <p>Use a service for long\u2011lived instances or cross\u2011node coordination. For tiny stateless helpers, plain imports are fine.</p>"},{"location":"key-concepts/extending-context/#2-naming-boundaries-important","title":"2. Naming &amp; Boundaries (Important)","text":"<p>Built\u2011ins (<code>context.artifacts()</code>, <code>context.memory()</code>, etc.) are not swappable in OSS. To extend the system, register new services with new names (e.g., <code>context.trainer()</code>, <code>context.datasets()</code>, <code>context.lineage_store()</code>).</p> <ul> <li>Keep agent code explicit about which storage or API it\u2019s using.</li> <li>If mirroring/exporting, record links (artifact URIs, memory event IDs) inside your external system for provenance.</li> </ul>"},{"location":"key-concepts/extending-context/#3-minimal-service-instancebased","title":"3. Minimal Service (Instance\u2011based)","text":""},{"location":"key-concepts/extending-context/#define-a-service-and-use-selfctx","title":"Define a service and use <code>self.ctx()</code>","text":"<pre><code>from aethergraph import Service\n\nclass RunAware(Service):\n    async def tag_current(self) -&gt; str:\n        # Access the *current* NodeContext lazily\n        ctx = self.ctx()\n        return f\"run={ctx.run_id} node={ctx.node_id}\"\n\nclass Trainer(Service):\n    async def submit(self, spec: dict) -&gt; str:\n        # Submit a training job to your HPC/cluster\n        job_id = await self._submit_to_cluster(spec)\n        # Log provenance to the *current* node\n        self.ctx().logger().info(\"trainer.submit\", extra={\"job_id\": job_id})\n        return job_id\n\n    async def inspect_job(self, job_id: str) -&gt; dict:\n        status = await self._query_cluster(job_id)\n        return {\"job_id\": job_id, \"status\": status}\n</code></pre>"},{"location":"key-concepts/extending-context/#register-at-startup-pass-an-instance","title":"Register at startup (pass an instance)","text":"<pre><code>from aethergraph.runtime import register_context_service\n\nregister_context_service(\"runaware\", RunAware())\nregister_context_service(\"trainer\",  Trainer())\n</code></pre> <p>After this, <code>context.runaware()</code> and <code>context.trainer()</code> are available everywhere in the runtime.</p>"},{"location":"key-concepts/extending-context/#4-usage-patterns","title":"4. Usage Patterns","text":""},{"location":"key-concepts/extending-context/#a-submit-training-link-artifacts","title":"A) Submit training &amp; link artifacts","text":"<pre><code>@graph_fn(name=\"train_model\", outputs=[\"job_id\", \"ckpt_uri\"]) \nasync def train_model(spec: dict, *, context):\n    # 1) Submit to your cluster via the custom service\n    job_id = await context.trainer().submit(spec)\n    # other waiting or inspection logic ...\n\n    # 2) Imagine your trainer writes a checkpoint to /tmp/ckpt.bin \n    # (NOTE: you need to ensure the job finishes, this is a simplification)\n    ckpt = await context.artifacts().save(\"/tmp/ckpt.bin\", kind=\"checkpoint\", labels={\"job\": job_id})\n    return {\"job_id\": job_id, \"ckpt_uri\": ckpt.uri}\n</code></pre>"},{"location":"key-concepts/extending-context/#b-inspect-status-in-another-nodetool","title":"B) Inspect status in another node/tool","text":"<pre><code>@tool(name=\"wait_for_training\", outputs=[\"ready\"]) \nasync def wait_for_training(job_id: str, *, context) -&gt; dict:\n    info = await context.trainer().inspect_job(job_id)\n    return {\"ready\": info[\"status\"] == \"COMPLETED\"}\n</code></pre>"},{"location":"key-concepts/extending-context/#c-backgroundblocking-helpers","title":"C) Background/Blocking helpers","text":"<pre><code>class Heavy(Service):\n    async def compute(self, x: int) -&gt; int:\n        # Offload CPU/binding operations without blocking the event loop\n        return await self.run_blocking(lambda: slow_cpu_fn(x))\n</code></pre>"},{"location":"key-concepts/extending-context/#5-concurrency-lifecycle","title":"5. Concurrency &amp; Lifecycle","text":"<ul> <li>Lifecycle hooks: <code>start()</code> / <code>close()</code> are optional; call them from your app/server bootstrap.</li> <li>Shared access: use <code>self.critical()</code> or an <code>AsyncRWLock</code> to protect mutable shared state.</li> <li>Per\u2011node context: call <code>self.ctx()</code> whenever you need <code>{run_id, graph_id, node_id}</code>.</li> <li>Backpressure: expose async APIs; if integrating queues, consider <code>asyncio.Queue</code> or your platform\u2019s client backpressure.</li> </ul>"},{"location":"key-concepts/extending-context/#6-testing-mocking","title":"6. Testing &amp; Mocking","text":"<ul> <li>Provide a fake implementation with the same interface for unit tests.</li> <li>Register your fake with the same name (e.g., <code>\"trainer\"</code>) in the test harness.</li> <li>Use in\u2011memory structures (dicts, temp dirs) for deterministic tests.</li> </ul>"},{"location":"key-concepts/extending-context/#7-error-handling-observability","title":"7. Error Handling &amp; Observability","text":"<ul> <li>Emit structured logs via <code>context.logger()</code> with operation names and durations.</li> <li>Normalize exceptions from vendor SDKs into your own error types.</li> <li>Consider retry/backoff wrappers in the service (centralized, consistent).</li> </ul>"},{"location":"key-concepts/extending-context/#8-common-service-patterns-examples","title":"8. Common Service Patterns (Examples)","text":"Scenario Suggested accessor What it abstracts Typical operations HPC / Training orchestration <code>context.trainer()</code> Submit/track jobs on Slurm/K8s/Ray <code>submit(spec)</code>, <code>inspect_job(id)</code>, <code>cancel(id)</code> External object storage <code>context.storage()</code> S3/GCS/MinIO buckets &amp; signed URLs <code>put(path)</code>, <code>get(uri)</code>, <code>sign(uri)</code>, <code>list(prefix)</code> Vendor API client <code>context.apiclient()</code> Rate\u2011limited, retried HTTP SDK <code>get/put/post</code>, <code>batch()</code>, <code>retry/backoff</code> In\u2011house AI models <code>context.models()</code> Local inference endpoints <code>embed(texts)</code>, <code>generate(prompt)</code> Materials DB / domain registry <code>context.materials()</code> Domain lookups &amp; cached tables <code>get_index(name)</code>, <code>search(filters)</code> Lineage export <code>context.lineage_store()</code> Mirror core provenance to BI/warehouse <code>export_run(run_id)</code>, <code>push(events)</code> <p>Pick names that are explicit in your org (e.g., <code>context.k8s_jobs()</code>, <code>context.minio()</code>). Avoid names that shadow built\u2011ins.</p>"},{"location":"key-concepts/extending-context/#9-optional-callable-services","title":"9. Optional callable services","text":"<p>You may define <code>__call__</code> on a service to allow a compact form like <code>await context.trainer(spec)</code> in addition to <code>await context.trainer().submit(spec)</code>. This can be handy when switching model profiles or submitting a quick spec inline. It's supported, but for clarity we generally recommend explicit method calls.</p> <pre><code>class Trainer(Service):\n    async def __call__(self, spec: dict) -&gt; str:\n        return await self.submit(spec)\n\n    async def submit(self, spec: dict) -&gt; str:\n        ...\n\n# Both are valid\njob_id = await context.trainer(spec)\njob_id = await context.trainer().submit(spec)\n</code></pre>"},{"location":"key-concepts/extending-context/#10-recap","title":"10. Recap","text":"<ul> <li>External services add named capabilities to <code>context</code> without changing agent code.</li> <li>Built\u2011ins remain stable; extend via new names (no in\u2011place swaps).</li> <li>Register instances, not factories; services run on the main event loop.</li> <li><code>ServiceHandle</code> supports both <code>context.svc()</code> (instance) and optional callable forwarding if your service implements <code>__call__</code>.</li> <li>Use <code>self.ctx()</code> to fetch per\u2011node provenance on demand; protect shared state with <code>AsyncRWLock</code> or <code>critical()</code>.</li> </ul> <p>See also: External Context Deep Dive \u2192 \u00b7 Channels &amp; Interaction \u2192 \u00b7 Artifacts &amp; Memory \u2192</p>"},{"location":"key-concepts/introduction/","title":"AetherGraph \u2014 Key Concepts Overview","text":"<p>AetherGraph rethinks how agentic systems are built: graphs are agents, context is the runtime fabric, and execution is event\u2011driven. This document lays out the philosophy and the essential architecture so you can quickly reason about how it differs from typical frameworks\u2014and how to use it effectively.</p>"},{"location":"key-concepts/introduction/#1-introduction-graphs-context","title":"1. Introduction: Graphs + Context","text":"<p>AetherGraph departs from most agent frameworks in two fundamental ways:</p> <ol> <li>Every agent is a graph. You model behavior as a directed acyclic graph (DAG) with nodes that can expand dynamically (reactive) or be planned statically.</li> <li>Every node runs inside a <code>NodeContext</code>. The context is your per\u2011node control plane that exposes rich, injectable services.</li> </ol> <p>NodeContext services include (built\u2011ins plus anything you add):</p> <ul> <li>Channels (Slack, Console/Web, \u2026) for I/O and interaction</li> <li>Artifacts (blob store) for large files and generated assets</li> <li>Memory (history + summaries + optional RAG)</li> <li>KV &amp; Logger for quick state and observability</li> <li>LLM / MCP / RAG bridges for model calls and tool use</li> <li>Your custom services registered at runtime</li> </ul> <p>The system is pythonic, reactive, and extensible: author graphs directly in Python, then let context services handle communication, persistence, and orchestration details.</p>"},{"location":"key-concepts/introduction/#2-graphs-as-agents","title":"2. Graphs as Agents","text":"<p>AetherGraph unifies two modes under one mental model:</p>"},{"location":"key-concepts/introduction/#reactive-agent-graph_fn","title":"Reactive agent \u2014 <code>@graph_fn</code>","text":"<ul> <li>Executes immediately when called (behaves like an async Python function).</li> <li>Expands into an implicit DAG as tools run.</li> <li>Perfect for interactive, service\u2011rich, or conversational workflows.</li> </ul>"},{"location":"key-concepts/introduction/#static-agent-graphify","title":"Static agent \u2014 <code>@graphify</code>","text":"<ul> <li>First builds a concrete <code>TaskGraph</code>, then executes it via the scheduler.</li> <li>Ideal for stable, deterministic, or large\u2011scale workflows.</li> </ul> <p>One model, two tempos. Iterate fast with <code>@graph_fn</code>; snap to repeatable DAGs with <code>@graphify</code>.</p>"},{"location":"key-concepts/introduction/#3-context-as-the-runtime-fabric","title":"3. Context as the Runtime Fabric","text":"<p>Every node (and each <code>graph_fn</code> invocation) receives a NodeContext. Think of it as a scoped service locator with lifecycle hooks.</p> <ul> <li>Per\u2011node scoping \u2192 independent logging, persistence, messaging.</li> <li>Injectable services \u2192 bind built\u2011ins or your own with simple registration.</li> <li>Sidecar\u2011friendly \u2192 channel adapters and stores can run out\u2011of\u2011process.</li> </ul> <p>Common patterns:</p> <ul> <li><code>context.channel().ask_text()</code> to collect input from a user/UI.</li> <li><code>context.artifacts().save(...)</code> to persist files and structured outputs.</li> <li><code>context.memory().record(...)</code> to capture provenance and summaries.</li> <li><code>context.llm().chat(...)</code> to call your configured model provider.</li> </ul>"},{"location":"key-concepts/introduction/#4-eventdriven-execution-waits","title":"4. Event\u2011Driven Execution &amp; Waits","text":"<p>AetherGraph is event\u2011driven (not polling). Nodes suspend when waiting for input or an external event; the runtime persists and resumes continuations on demand so suspended nodes consume little to no CPU/RAM. Two primary wait styles are exposed:</p> <ul> <li>Cooperative waits \u2014 inline <code>await</code> for short, interactive flows where the coroutine remains in memory for low\u2011latency response.</li> <li>Dual\u2011stage waits \u2014 split a node into setup and resume phases; the continuation is durably stored so the process can free resources until an event triggers resume.</li> </ul> Mode Description CPU/RAM Ideal for Cooperative waits Inline <code>await</code> (e.g., <code>await context.channel().ask_text()</code>); coroutine stays in memory Minimal Small reactive loops, interactive sessions Dual\u2011stage waits Split node into setup/resume; continuation stored in a durable store Zero when paused Large DAGs, long\u2011lived pipelines Manual checkpoints User\u2011managed state snapshot/restore User\u2011managed Legacy integration, custom control <p>Dual\u2011stage waits + event routing enable massive concurrency with tiny footprint.</p>"},{"location":"key-concepts/introduction/#5-execution-scheduling","title":"5. Execution &amp; Scheduling","text":"<p>Each graph runs under a scheduler that selects ready nodes and respects your concurrency caps. Within a graph you get async DAG orchestration by default.</p>"},{"location":"key-concepts/introduction/#crossgraph-orchestration","title":"Cross\u2011graph orchestration","text":"<p>AetherGraph intentionally avoids a heavyweight orchestrator in the OSS core. For concurrent runs across multiple agents/graphs, use idiomatic Python:</p> <pre><code># Concurrent reactive runs\nawait asyncio.gather(\n    graph_fn_agent_a(...),   # a @graph_fn with async __call__\n    graph_fn_agent_b(...),\n)\n\n# Sequential orchestration\nawait graph_fn_agent_a(...)\nawait graph_fn_agent_b(...)\n</code></pre>"},{"location":"key-concepts/introduction/#6-extensibility-everywhere","title":"6. Extensibility Everywhere","text":"<p>Pure Python\u2014no DSLs, no codegen.</p> <ul> <li>Tools: <code>@tool</code> to define reusable, typed primitives.</li> <li>Graphs: <code>@graphify</code> to materialize a DAG for repeatable runs.</li> <li>Services: <code>register_context_service()</code> to inject your own capabilities.</li> <li>Adapters: extend ChannelBus to new transports (Slack, Web, PyQt, \u2026).</li> </ul> <p>Example (sketch)</p> <p>Define a <code>@tool</code>: <pre><code>@tool(name=\"analyze\", outputs=[\"val\"])\ndef analyze(x: int) -&gt; int:\n    return {\"val\": x * 2}\n</code></pre></p> <p>Define a <code>@graph_fn</code> (<code>@tool</code> is optional) <pre><code>@graph_fn(name=\"demo\")\nasync def demo(*, context):\n    y = await analyze(21)\n    await context.channel().send_text(f\"Answer: {y}\")\n</code></pre></p>"},{"location":"key-concepts/introduction/#7-how-aethergraph-differs","title":"7. How AetherGraph Differs","text":"Area AetherGraph Typical frameworks Runtime Unified Python, event\u2011driven Split control planes, mixed models Waits Zero\u2011CPU dual\u2011stage waits Blocking awaits, threads, polling Context Rich per\u2011node runtime fabric Globals, hidden singletons Composition Dynamic (<code>graph_fn</code>) + Static (<code>@graphify</code>) Usually one or the other Scheduling Async DAG scheduling per graph Step\u2011based loops, less reactive Extensibility Decorators + Python APIs Fixed plugin systems Provenance Built\u2011in memory/artifacts External or ad\u2011hoc"},{"location":"key-concepts/introduction/#8-takeaways","title":"8. Takeaways","text":"<p>AetherGraph bridges interactive research and deterministic automation:</p> <ul> <li>React first, then structure for production.</li> <li>Keep state, context, and scheduling where they belong\u2014with the nodes.</li> <li>Use event\u2011driven waits to scale without idle compute.</li> </ul>"},{"location":"key-concepts/other-services-overview/","title":"Other Services Overview","text":"<p>AetherGraph\u2019s context exposes a set of lightweight, composable runtime services that complement Channels, Artifacts, and Memory. Use them when your agent needs coordination, observability, or intelligent reasoning \u2014 while keeping your core logic pure Python.</p> <p>Philosophy: keep code idiomatic; reach for <code>context.&lt;service&gt;()</code> only when you need I/O, coordination, or intelligence.</p>"},{"location":"key-concepts/other-services-overview/#1-kv-ephemeral-coordination","title":"1. KV \u2014 Ephemeral Coordination","text":"<p>A minimal key\u2013value store for transient state, synchronization, and quick signals between nodes/agents.</p> <pre><code>@graph_fn(name=\"kv_demo\", outputs=[\"ok\"])\nasync def kv_demo(*, context):\n    kv = context.kv()\n    await kv.set(\"stage\", \"preflight\", ttl_s=300)\n    stage = await kv.get(\"stage\")  # \u2192 \"preflight\"\n    return {\"ok\": stage == \"preflight\"}\n</code></pre> <p>Why/When: feature flags, locks/counters, short-lived coordination.</p> <p>Default backend: in-memory KV.</p> <p>Deep dive: KV Service \u2192</p>"},{"location":"key-concepts/other-services-overview/#2-logger-structured-logs-with-provenance","title":"2. Logger \u2014 Structured Logs with Provenance","text":"<p>Structured logging with <code>{run_id, graph_id, node_id}</code> automatically injected.</p> <pre><code>@graph_fn(name=\"log_demo\", outputs=[\"done\"])\nasync def log_demo(*, context):\n    log = context.logger()\n    log.info(\"starting\", extra={\"component\": \"ingest\"})\n    try:\n        ...\n        log.info(\"finished\", extra={\"component\": \"ingest\"})\n        return {\"done\": True}\n    except Exception:\n        log.exception(\"ingest failed\")\n        return {\"done\": False}\n</code></pre> <p>Why/When: lifecycle traces, metrics, error reporting.</p> <p>Default backend: Python <code>logging</code>.</p> <p>Deep dive: Logging \u2192</p>"},{"location":"key-concepts/other-services-overview/#3-llm-unified-chat-embeddings-optional","title":"3. LLM \u2014 Unified Chat &amp; Embeddings (optional)","text":"<p>Provider-agnostic interface for chat/completions and embeddings. Requires configuration (API keys, profile).</p> <pre><code>@graph_fn(name=\"llm_demo\", outputs=[\"reply\"])\nasync def llm_demo(prompt: str, *, context):\n    llm = context.llm(profile=\"default\")\n    msg = await llm.chat([{\"role\": \"user\", \"content\": prompt}])\n    return {\"reply\": msg[\"content\"]}\n</code></pre> <p>Why/When: summarization, drafting, tool-use planning, embeddings.</p> <p>Backends: OpenAI, Anthropic, local, etc.</p> <p>Deep dive: LLM Service \u2192</p>"},{"location":"key-concepts/other-services-overview/#4-rag-longterm-semantic-recall-optional","title":"4. RAG \u2014 Long\u2011Term Semantic Recall (optional)","text":"<p>Build searchable corpora from events/docs; retrieve or answer with citations. Requires an LLM for answering.</p> <pre><code>@graph_fn(name=\"rag_demo\", outputs=[\"answer\"])\nasync def rag_demo(q: str, *, context):\n    mem = context.memory()\n    corpus = await mem.rag_bind(scope=\"project\")\n    await mem.rag_promote_events(corpus_id=corpus)\n    ans = await mem.rag_answer(corpus_id=corpus, question=q)\n    return {\"answer\": ans[\"answer\"]}\n</code></pre> <p>Why/When: semantic search, project recall, retrieval\u2011augmented QA.</p> <p>Default backend: FAISS (local).</p> <p>Deep dive: RAG \u2192 \u00b7 External Context \u2192</p>"},{"location":"key-concepts/other-services-overview/#5-mcp-external-tool-bridges-optional","title":"5. MCP \u2014 External Tool Bridges (optional)","text":"<p>Connect to external tool servers over stdio/WebSocket/HTTP via Model Context Protocol (MCP).</p> <pre><code>@graph_fn(name=\"mcp_demo\", outputs=[\"hits\"])\nasync def mcp_demo(*, context):\n    ws = context.mcp(\"ws\")  # adapter name\n    res = await ws.call(\"search\", {\"q\": \"tolerance analysis\", \"k\": 3})\n    return {\"hits\": res.get(\"items\", [])}\n</code></pre> <p>Why/When: integrate non-Python tools or remote services with structured contracts.</p> <p>Deep dive: MCP \u2192</p>"},{"location":"key-concepts/other-services-overview/#takeaways","title":"Takeaways","text":"<ul> <li>Access everything through <code>context.&lt;service&gt;()</code> \u2014 no globals or custom wiring.</li> <li>KV and Logger work out of the box; LLM/RAG/MCP are optional and enabled by config.</li> <li>Backends are pluggable; you can move from local to managed services without changing agent code.</li> </ul> <p>Next: Explore [Channels &amp; Interaction \u2192] or [Artifacts &amp; Memory \u2192]</p>"},{"location":"key-concepts/server-start-sidecar/","title":"Server (Sidecar) Overview","text":"<p>The AetherGraph sidecar is a lightweight process that boots your runtime services and exposes a tiny HTTP/WebSocket surface for adapters (Slack, Web, Telegram, \u2026) and continuations (event\u2011driven waits). You can run <code>@graph_fn</code> in pure Python without it (console only), but start the sidecar when you need:</p> <ul> <li>Real interactions: <code>ask_text/approval/files</code> from Slack/Web/Telegram and resume the run</li> <li>Centralized service wiring: artifacts, memory, kv, llm, rag, mcp, logger</li> <li>A shared control plane: health, upload hooks, progress streams, basic inspect</li> </ul> <p>In short: keep your agents plain Python; start the sidecar for I/O, resumability, and shared services.</p>"},{"location":"key-concepts/server-start-sidecar/#quick-start","title":"Quick Start","text":"<pre><code>from aethergraph.server import start, stop\n\nurl = start(host=\"127.0.0.1\", port=0)  # FastAPI + Uvicorn in a background thread\nprint(\"sidecar:\", url)\n\n# ... run @graph_fn / @graphify normally ...\n\nstop()  # optional (handy in tests/CI)\n</code></pre> <p>Tips</p> <ul> <li>Use <code>port=0</code> to pick a free port automatically. </li> <li>Start it once per process; reuse the base URL across adapters/UI.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/#what-start-does","title":"What <code>start()</code> Does","text":"<ol> <li>Load config &amp; workspace \u2014 resolve paths, secrets, and profiles; make the workspace if needed.</li> <li>Build &amp; register services \u2014 channels, artifacts (store/index), memory (hotlog/persistence/indices), kv, llm, rag, mcp, logger.</li> <li> <p>Expose endpoints \u2014</p> <ul> <li>Continuations: resume callbacks for <code>ask_text / ask_approval / ask_files</code></li> <li>Adapters: chat/events, uploads, progress streams</li> </ul> </li> <li> <p>Launch Uvicorn \u2014 run the app in a background thread and return the base URL.</p> </li> </ol>"},{"location":"key-concepts/server-start-sidecar/#minimal-api","title":"Minimal API","text":"<p><code>start(host=\"127.0.0.1\", port=0, workspace=None, log_level=\"info\") -&gt; str</code></p> <p>Starts the sidecar in\u2011process and returns the base URL.</p> <p><code>start_async(...) -&gt; str</code></p> <p>Async\u2011friendly variant (still hosts the server in a thread), convenient inside async apps/tests.</p> <p><code>stop() -&gt; None</code></p> <p>Stops the background server. Useful for teardown in tests/CI.</p>"},{"location":"key-concepts/server-start-sidecar/#when-to-use-the-sidecar","title":"When to Use the Sidecar","text":"<ul> <li>Event\u2011driven waits from non\u2011console adapters (Slack/Web/Telegram)</li> <li>File uploads &amp; artifact routing from a browser/UI</li> <li>Progress/streaming to external UIs</li> <li>Shared settings &amp; service wiring across multiple agents</li> </ul> <p>You don\u2019t need it for: simple console demos, unit tests without external I/O, or pure compute.</p>"},{"location":"key-concepts/server-start-sidecar/#common-issues-fixes","title":"Common Issues &amp; Fixes","text":"<ul> <li>No reply after <code>ask_text()</code> \u2192 The adapter isn\u2019t posting resume events to the sidecar. Verify the sidecar base URL and token in the adapter config.</li> <li>CORS blocked in web UI \u2192 Allow your UI origin in sidecar settings (CORS <code>allow_origins</code>).</li> <li>Port busy \u2192 Use <code>port=0</code> or pick an open port.</li> <li>Service not available (e.g., LLM/RAG) \u2192 Ensure your <code>create_app()</code> wires those services or provide the required credentials.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/#notes-on-architecture","title":"Notes on Architecture","text":"<ul> <li>The sidecar runs its own event loop/thread; your agents/tools run on the main loop. They communicate via the ChannelBus/HTTP hooks.</li> <li>External context services you register as instances run on the main loop, so <code>asyncio</code> locks work as expected.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/#takeaways","title":"Takeaways","text":"<ul> <li>The sidecar is your local control plane: services + continuations + adapters.</li> <li>Start it with <code>start()</code> when you need interactions, persistence, or shared wiring.</li> <li>Your agent code stays plain Python either way; the sidecar simply adds I/O and resumability.</li> </ul>"},{"location":"key-concepts/static-graph-agent/","title":"Static Graphs with <code>@graphify</code>","text":"<p><code>@graphify</code> transforms a plain Python function into a graph builder. Instead of running immediately (like <code>@graph_fn</code>), it constructs a TaskGraph from <code>@tool</code> calls \u2014 a reusable, explicit DAG that you can run later.</p> <p>In short:</p> <ul> <li><code>@graph_fn</code> \u2192 executes now (reactive, dynamic)</li> <li><code>@graphify</code> \u2192 builds first, runs later (deterministic DAG)</li> </ul>"},{"location":"key-concepts/static-graph-agent/#1-what-is-a-static-graph","title":"1. What Is a Static Graph?","text":"<p>A static graph is a declarative DAG of tool nodes and dependencies. Each node is a <code>@tool</code> call; edges represent data flow or forced ordering. You build it once, then you can inspect, persist, visualize, and run it repeatedly.</p> <p>Why static? Repeatability, inspectability, and clear fan\u2011in/fan\u2011out. Static graphs shine for pipelines and reproducible experiments where determinism and analysis matter.</p>"},{"location":"key-concepts/static-graph-agent/#2-graphify-vs-graph_fn","title":"2. <code>@graphify</code> vs <code>@graph_fn</code>","text":"Aspect <code>@graph_fn</code> (Reactive) <code>@graphify</code> (Static) Execution Runs immediately when called Builds a DAG first; run later Composition Mix plain Python + <code>@tool</code> (implicit nodes) Only <code>@tool</code> nodes are valid steps Context usage Rich <code>context.*</code> available inline Need to wrap <code>context.*</code> in a tool to access it Inspectability Inspect implicit graph via <code>graph_fn.last_graph()</code> Full spec via <code>.io()</code>, <code>.spec()</code>, <code>TaskGraph.pretty()</code> Best for Interactive agents, quick iteration Pipelines, reproducible runs, analytics <p>Note: Nested static\u2011graph calls are not supported at the moment (no calling one <code>@graphify</code> from another as a node). Compose via tools or run graphs separately.</p>"},{"location":"key-concepts/static-graph-agent/#3-define-and-build-a-graph","title":"3. Define and Build a Graph","text":"<pre><code>from aethergraph import graphify, tool\n\n@tool(outputs=[\"rows\"])       \ndef load_csv(path: str): ...\n\n@tool(outputs=[\"clean\"])      \ndef clean(rows): ...\n\n@tool(outputs=[\"model\"])      \ndef train(data): ...\n\n@tool(outputs=[\"uri\"])        \ndef report(model): ...\n\n@graphify(name=\"etl_train_report\", inputs=[\"csv_path\"], outputs=[\"uri\"]) \ndef etl_train_report(csv_path):\n    raw  = load_csv(path=csv_path)\n    tidy = clean(rows=raw.rows)\n    mod  = train(data=tidy.clean)\n    rep  = report(model=mod.model)\n    return {\"uri\": rep.uri}\n\nG = etl_train_report.build()     # \u2192 TaskGraph\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#control-ordering-without-data-edges","title":"Control ordering without data edges","text":"<p>Use <code>_after</code> to enforce sequence when there\u2019s no data dependency:</p> <pre><code>@tool(outputs=[\"ok\"])    \ndef fetch(): ...\n\n@tool(outputs=[\"done\"])  \ndef train(): ...\n\n@graphify(name=\"seq\", inputs=[], outputs=[\"done\"]) \ndef seq():\n    a = fetch()\n    b = train(_after=a)        # run `train` after `fetch`\n    return {\"done\": b.done}\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#referencing-tool-outputs-dot-vs-key","title":"Referencing Tool Outputs (dot vs. key)","text":"<p>Each <code>@tool</code> must declare its outputs. AetherGraph wraps the call in a handle whose fields mirror those names, so you can access them either as attributes or dict keys \u2014 both are equivalent.</p> <pre><code>@tool(outputs=[\"rows\"])\ndef load_csv(path: str):\n# must return a dict matching declared outputs\nreturn {\"rows\": parse_csv(path)}\n\n\n@tool(outputs=[\"clean\"])\ndef clean(rows):\nreturn {\"clean\": tidy(rows)}\n\n\n@graphify(name=\"etl\", inputs=[\"csv_path\"], outputs=[\"clean\"])\ndef etl(csv_path):\nraw = load_csv(path=csv_path)\n# Access either way; these are equivalent\ntidy1 = clean(rows=raw.rows) # dot access\ntidy2 = clean(rows=raw[\"rows\"]) # key access\nreturn {\"clean\": tidy1.clean}\n</code></pre> <p>Consistency matters: declared output names (e.g.,<code>outputs=[\"rows\"]</code>) must match the keys you return from the tool (e.g., <code>{\"rows\": ...}</code>). Mismatches raise clear build/runtime errors.</p> <p>Multiple outputs <pre><code>@tool(outputs=[\"mean\", \"std\"])\ndef stats(xs: list[float]):\nreturn {\"mean\": avg(xs), \"std\": stdev(xs)}\n\n\n@graphify(name=\"use_stats\", inputs=[\"xs\"], outputs=[\"m\"])\ndef use_stats(xs):\ns = stats(xs=xs)\nreturn {\"m\": s.mean} # or s[\"mean\"]\n</code></pre></p> <p>Think of tool calls as typed nodes whose declared outputs become fields on the node handle.</p>"},{"location":"key-concepts/static-graph-agent/#4-fanin-fanout-patterns","title":"4. Fan\u2011in / Fan\u2011out Patterns","text":"<pre><code>@tool(outputs=[\"v\"]) \ndef step(x: int): ...\n\n@tool(outputs=[\"z\"]) \ndef join(a, b): ...\n\n@graphify(name=\"fan\", inputs=[\"x1\", \"x2\"], outputs=[\"z\"]) \ndef fan(x1, x2):\n    a = step(x=arg(\"x1\"))  # fan\u2011out 1\n    b = step(x=arg(\"x2\"))  # fan\u2011out 2\n    j = join(a=a.v, b=b.v)  # fan\u2011in\n    return {\"z\": j.z}\n</code></pre> <p>Tips: you can use for loop to create fan-in and fan-out</p>"},{"location":"key-concepts/static-graph-agent/#5-run-a-built-graph","title":"5. Run a Built Graph","text":"<p>Run the materialized DAG with the runner (sync or async):</p> <pre><code>from aethergraph.runner import run, run_async\n\nresult = run(G, inputs={\"csv_path\": \"data/train.csv\"})\n# \u2192 {\"uri\": \"file://...\"}\n\n# Async form (e.g., inside another async function)\nfinal = await run_async(G, inputs={\"csv_path\": \"data/train.csv\"})\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#6-inspect-and-explore","title":"6. Inspect and Explore","text":"<p><code>@graphify</code> builders expose helpers for IO/signature and full spec:</p> <pre><code>sig  = etl_train_report.io()     # inputs/outputs signature\nspec = etl_train_report.spec()   # GraphSpec (nodes, edges, metadata)\n</code></pre> <p>Runtime helpers on <code>TaskGraph</code>:</p> <pre><code>print(G.pretty())                # human\u2011friendly table\nprint(G.ascii_overview())        # compact overview\n\n# Select / find nodes\nids     = G.list_nodes()                         # visible node_ids\nfirst_c = G.find_by_logic(\"clean\", first=True)  # by tool/logic name\nsome    = G.find_by_label(\"train\")              # by label\nsel     = G.select(\"@my_alias\")                 # mini\u2011DSL (@alias, #label, logic:, name:, id:, /regex/)\n\n# Topology &amp; subgraphs\norder   = G.topological_order()                  # raises if cycles\nup      = G.get_upstream_nodes(first_c)          # dependency closure\nsub     = G.get_subgraph_nodes(first_c)          # downstream closure\n</code></pre> <p>Export / visualize</p> <pre><code>dot = G.to_dot()                 # Graphviz DOT\n# G.visualize()                  # if enabled: render to file/viewer\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#7-recall-use-tool-inside-graph_fn","title":"7. Recall: Use <code>@tool</code> inside <code>@graph_fn</code>","text":"<p>While <code>@graph_fn</code> executes immediately, you can embed <code>@tool</code> calls to create explicit nodes for tracing or parallelism within a reactive agent:</p> <pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"]) \ndef square(x: int):\n    return {\"y\": x*x}\n\n@graph_fn(name=\"mix\") \nasync def mix(x: int, *, context):\n    h = square(x=x)                 # schedules a tool node in the implicit graph\n    await context.channel().send_text(\"running square\u2026\")\n    return {\"y\": h.y}\n</code></pre> <p>Prefer <code>@graphify</code> for full, reproducible pipelines; prefer <code>@graph_fn</code> for interactive/reactive agents that lean on <code>context.*</code>.</p> <p>If you executed a <code>@graph_fn</code> and want to inspect the implicit graph of tool nodes it created:</p> <pre><code>G_last = graph_fn.last_graph()    # TaskGraph of the most recent run (if available)\nprint(G_last.pretty())\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#8-key-points","title":"8. Key Points","text":"<ul> <li><code>@graphify</code> builds a DAG from <code>@tool</code> calls; you run it later.</li> <li>Use <code>arg(\"name\")</code> to reference declared inputs inside the builder.</li> <li>Use <code>_after</code> to force ordering without data edges.</li> <li>Fan\u2011out/fan\u2011in is natural with multiple <code>@tool</code> calls and a later join.</li> <li>Inspect via <code>.io()</code>, <code>.spec()</code>, <code>TaskGraph.pretty()</code>, <code>ascii_overview()</code>, <code>to_dot()</code>.</li> <li>No nested static graphs currently (don\u2019t call one <code>@graphify</code> from another as a node).</li> <li>For reactive agents, stick with <code>@graph_fn</code>; for pipelines, prefer <code>@graphify</code>.</li> </ul>"},{"location":"recipes/data-analysis-loop/","title":"Recipe: Iterative Data Analysis","text":"<ul> <li>User asks for analysis \u2192 generate code</li> <li>Run code; store figures &amp; tables in <code>artifacts()</code></li> <li>Record metrics in <code>memory()</code> and summarize at the end</li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"recipes/job-submit-poll/","title":"Recipe: Submit + Poll + Notify","text":"<ul> <li>Submit a long-running job</li> <li>Poll status and send progress via <code>channel()</code></li> <li>On failure, ask user to retry or stop; save logs to <code>artifacts()</code></li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"recipes/memory-rag-mini/","title":"Recipe: Mini Memory\u2011RAG","text":"<ul> <li>Ingest notes as <code>memory().record(kind=\"note\", data=...)</code></li> <li>Simple retrieval via <code>memory().recent()/query()</code> into <code>llm().chat()</code></li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"reference/config/","title":"Config Keys","text":"<ul> <li><code>AETH_PORT</code> \u2014 server port</li> <li><code>LOG_LEVEL</code> \u2014 logging level (e.g., INFO)</li> <li><code>MODEL_NAME</code> \u2014 default LLM</li> <li><code>SLACK_*</code> \u2014 Slack integration (bot token, signing secret)</li> </ul> <p>(Expand with your actual config schema.)</p>"},{"location":"reference/context-artifacts/","title":"AetherGraph \u2014 <code>context.artifacts()</code> Reference","text":"<p>This page documents the ArtifactFacade methods returned by <code>context.artifacts()</code> in a concise format: signature, brief description, parameters, and returns \u2014 plus examples for <code>writer()</code> and scoped search.</p>"},{"location":"reference/context-artifacts/#overview","title":"Overview","text":"<p><code>context.artifacts()</code> returns an ArtifactFacade bound to the current <code>run_id</code>, <code>graph_id</code>, and <code>node_id</code>. It wraps an <code>AsyncArtifactStore</code> for persistence and an <code>AsyncArtifactIndex</code> for search/pinning/metrics. Most mutating ops auto\u2011index and record an occurrence.</p> <p>Typical flow</p> <ol> <li> <p>Stage (optional) \u2192 write \u2192 ingest</p> </li> <li> <p>Or directly save an existing file path</p> </li> <li> <p>Or use the built\u2011in <code>writer()</code> context manager to stream bytes and auto\u2011index</p> </li> </ol>"},{"location":"reference/context-artifacts/#artifactsstage","title":"artifacts.stage","text":"<p><pre><code>stage(ext: str = \"\") -&gt; str\n</code></pre> Plan a staging file path (temporary path) with an optional extension.</p> <p>Parameters</p> <ul> <li>ext (str, optional) \u2013 Suggested extension (e.g., \".png\", \".csv\").</li> </ul> <p>Returns str \u2013 Staging file path.</p>"},{"location":"reference/context-artifacts/#artifactsingest","title":"artifacts.ingest","text":"<p><pre><code>ingest(staged_path: str, *, kind: str, labels=None, metrics=None, suggested_uri: str | None = None, pin: bool = False) -&gt; Artifact\n</code></pre> Ingest a previously staged file into the store, attach metadata, and auto\u2011index.</p> <p>Parameters</p> <ul> <li> <p>staged_path (str) \u2013 Path returned by <code>stage()</code> or <code>stage_dir()</code>.</p> </li> <li> <p>kind (str) \u2013 Logical artifact kind (e.g., \"image\", \"table\", \"model\").</p> </li> <li> <p>labels (dict, optional) \u2013 Arbitrary labels; merged into index filters.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics used for <code>best()</code> queries.</p> </li> <li> <p>suggested_uri (str, optional) \u2013 Hint for final URI; store may ignore.</p> </li> <li> <p>pin (bool) \u2013 Mark artifact as pinned in the index.</p> </li> </ul> <p>Returns Artifact \u2013 Indexed artifact record.</p>"},{"location":"reference/context-artifacts/#artifactssave","title":"artifacts.save","text":"<p><pre><code>save(path: str, *, kind: str, labels=None, metrics=None, suggested_uri: str | None = None, pin: bool = False) -&gt; Artifact\n</code></pre> Save an existing on\u2011disk file to the store with metadata; auto\u2011index and record occurrence. Sets <code>last_artifact</code>.</p> <p>Parameters</p> <ul> <li> <p>path (str) \u2013 Existing file path to persist.</p> </li> <li> <p>kind (str) \u2013 Logical artifact kind.</p> </li> <li> <p>labels (dict, optional) \u2013 Arbitrary labels.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics.</p> </li> <li> <p>suggested_uri (str, optional) \u2013 Hint for final URI; store may ignore.</p> </li> <li> <p>pin (bool) \u2013 Mark artifact as pinned.</p> </li> </ul> <p>Returns Artifact \u2013 Indexed artifact record.</p>"},{"location":"reference/context-artifacts/#artifactswriter","title":"artifacts.writer","text":"<p><pre><code>writer(*, kind: str, planned_ext: str | None = None, pin: bool = False) -&gt; AsyncContextManager[Writer]\n</code></pre> Open a binary writer context that persists bytes as an artifact; auto\u2011indexes on exit. Sets <code>last_artifact</code>.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Logical artifact kind.</p> </li> <li> <p>planned_ext (str, optional) \u2013 Extension hint for underlying temp file.</p> </li> <li> <p>pin (bool) \u2013 Mark resulting artifact as pinned.</p> </li> </ul> <p>Yields Writer \u2013 File\u2011like object; write bytes and close by exiting the context.</p> <p>Example <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"make_png\")\nasync def make_png(*, context):\n    import PIL.Image as Image\n    img = Image.new(\"RGB\", (128, 128), (255, 122, 26))\n    async with context.artifacts().writer(kind=\"image\", planned_ext=\".png\") as w:\n        # writer exposes a real file handle underneath\n        img.save(w, format=\"PNG\")\n    art = context.artifacts().last_artifact\n    await context.channel().send_image(url=art.uri, title=\"Generated PNG\")\n    return {\"uri\": art.uri}\n</code></pre></p>"},{"location":"reference/context-artifacts/#artifactsstage_dir","title":"artifacts.stage_dir","text":"<p><pre><code>stage_dir(suffix: str = \"\") -&gt; str\n</code></pre> Plan a staging directory for multi\u2011file artifacts.</p> <p>Parameters</p> <ul> <li>suffix (str, optional) \u2013 Optional directory suffix.</li> </ul> <p>Returns str \u2013 Staging directory path.</p>"},{"location":"reference/context-artifacts/#artifactsingest_dir","title":"artifacts.ingest_dir","text":"<p><pre><code>ingest_dir(staged_dir: str, **kw) -&gt; Artifact\n</code></pre> Ingest a directory of files as a single logical artifact; forwards extra keyword args to the store.</p> <p>Parameters</p> <ul> <li> <p>staged_dir (str) \u2013 Directory created by <code>stage_dir()</code>.</p> </li> <li> <p>kw \u2013 Store\u2011specific options (e.g., kind/labels/metrics/pin).</p> </li> </ul> <p>Returns Artifact \u2013 Indexed artifact record.</p>"},{"location":"reference/context-artifacts/#artifactstmp_path","title":"artifacts.tmp_path","text":"<p><pre><code>tmp_path(suffix: str = \"\") -&gt; str\n</code></pre> Alias of <code>stage()</code> for convenience.</p> <p>Parameters</p> <ul> <li>suffix (str, optional) \u2013 Extension or suffix.</li> </ul> <p>Returns str \u2013 Staging file path.</p>"},{"location":"reference/context-artifacts/#artifactsload_artifact","title":"artifacts.load_artifact","text":"<p><pre><code>load_artifact(uri: str) -&gt; Any\n</code></pre> Load a previously saved artifact by URI, using the store\u2019s type\u2011specific loader.</p> <p>Parameters</p> <ul> <li>uri (str) \u2013 Artifact URI.</li> </ul> <p>Returns Any \u2013 Decoded object (depends on store &amp; artifact type).</p>"},{"location":"reference/context-artifacts/#artifactsload_artifact_bytes","title":"artifacts.load_artifact_bytes","text":"<p><pre><code>load_artifact_bytes(uri: str) -&gt; bytes\n</code></pre> Load raw bytes for a previously saved artifact by URI.</p> <p>Parameters</p> <ul> <li>uri (str) \u2013 Artifact URI.</li> </ul> <p>Returns bytes \u2013 Artifact content.</p>"},{"location":"reference/context-artifacts/#artifactslist","title":"artifacts.list","text":"<p><pre><code>list(*, scope: Literal[\"node\",\"run\",\"graph\",\"project\",\"all\"] = \"run\") -&gt; list[Artifact]\n</code></pre> Quick listing with implicit scoping (defaults to the current run). Under the hood, this uses the index with reasonable filters for the given scope.</p> <p>Parameters</p> <ul> <li> <p>scope (str) \u2013 One of:</p> </li> <li> <p>\"node\" \u2013 filter by (run_id, graph_id, node_id) </p> </li> <li> <p>\"graph\" \u2013 filter by (run_id, graph_id) </p> </li> <li> <p>\"run\" \u2013 filter by (run_id) (default) </p> </li> <li> <p>\"project\" \u2013 filter by project/org if tracked in labels  </p> </li> <li> <p>\"all\" \u2013 no implicit filters (use sparingly)</p> </li> </ul> <p>Returns list[Artifact] \u2013 Matching artifacts.</p>"},{"location":"reference/context-artifacts/#artifactssearch","title":"artifacts.search","text":"<p><pre><code>search(*, kind: str | None = None, labels: dict | None = None, metric: str | None = None, mode: Literal[\"max\",\"min\"] | None = None, scope: Scope = \"run\", extra_scope_labels: dict | None = None) -&gt; list[Artifact]\n</code></pre> Index search with automatic scoping. Merges your <code>labels</code> with scope\u2011derived labels.</p> <p>Parameters</p> <ul> <li> <p>kind (str, optional) \u2013 Filter by artifact kind.</p> </li> <li> <p>labels (dict, optional) \u2013 Arbitrary label filters.</p> </li> <li> <p>metric (str, optional) \u2013 Metric name for ranking.</p> </li> <li> <p>mode ({\"max\",\"min\"}, optional) \u2013 Ranking direction.</p> </li> <li> <p>scope (Scope) \u2013 Implicit scope (default: \"run\").</p> </li> <li> <p>extra_scope_labels (dict, optional) \u2013 Additional scope labels to merge.</p> </li> </ul> <p>Returns list[Artifact] \u2013 Search results.</p> <p>Example (scoped search) <pre><code>best_imgs = await context.artifacts().search(kind=\"image\", scope=\"graph\")\n</code></pre></p>"},{"location":"reference/context-artifacts/#artifactsbest","title":"artifacts.best","text":"<p><pre><code>best(*, kind: str, metric: str, mode: Literal[\"max\",\"min\"], scope: Scope = \"run\", filters: dict | None = None) -&gt; Artifact | None\n</code></pre> Return the best artifact by a metric, with optional filters and implicit scope.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Artifact kind.</p> </li> <li> <p>metric (str) \u2013 Metric key.</p> </li> <li> <p>mode ({\"max\",\"min\"}) \u2013 Ranking direction.</p> </li> <li> <p>scope (Scope) \u2013 Implicit scope (default: \"run\").</p> </li> <li> <p>filters (dict, optional) \u2013 Additional label filters.</p> </li> </ul> <p>Returns Artifact | None \u2013 Best match or <code>None</code> if not found.</p>"},{"location":"reference/context-artifacts/#artifactspin","title":"artifacts.pin","text":"<p><pre><code>pin(artifact_id: str, pinned: bool = True) -&gt; None\n</code></pre> Pin or unpin an artifact in the index.</p> <p>Parameters</p> <ul> <li> <p>artifact_id (str) \u2013 ID of the artifact to (un)pin.</p> </li> <li> <p>pinned (bool) \u2013 <code>True</code> to pin; <code>False</code> to unpin.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-artifacts/#scoping-details","title":"Scoping details","text":"<p>The facade enriches queries with labels depending on the <code>scope</code> argument:</p> <ul> <li> <p>node \u2192 <code>{ graph_id, node_id }</code> </p> </li> <li> <p>graph \u2192 <code>{ graph_id }</code> </p> </li> <li> <p>project \u2192 <code>{ project_id }</code> (if tracked)  </p> </li> <li> <p>run \u2192 uses <code>list_for_run(run_id)</code> </p> </li> <li> <p>all \u2192 passes through to index with no implicit labels</p> </li> </ul>"},{"location":"reference/context-artifacts/#practical-examples","title":"Practical examples","text":"<p>1) Direct save <pre><code>uri = \"/tmp/plot.png\"\n# ... generate image to uri ...\nart = await context.artifacts().save(uri, kind=\"image\", labels={\"task\":\"eval\"}, metrics={\"psnr\": 31.2})\n</code></pre></p> <p>2) Stage \u2192 write \u2192 ingest <pre><code>staged = await context.artifacts().stage(\".csv\")\nwith open(staged, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"x,y\\n1,2\\n3,4\\n\")\nart = await context.artifacts().ingest(staged_path=staged, kind=\"table\", labels={\"split\":\"val\"})\n</code></pre></p> <p>3) Search best <pre><code>winner = await context.artifacts().best(kind=\"model\", metric=\"val_acc\", mode=\"max\", scope=\"run\")\nif winner:\n    await context.channel().send_text(f\"Best model: {winner.uri} acc={winner.metrics['val_acc']:.3f}\")\n</code></pre></p> <p>4) Multi\u2011file directory <pre><code>dir_path = await context.artifacts().stage_dir(\"_report\")\n# ... write several files to dir_path ...\nart = await context.artifacts().ingest_dir(dir_path, kind=\"report\", labels={\"format\":\"html\"})\n</code></pre></p> <p>5) Pin <pre><code>await context.artifacts().pin(artifact_id=art.id, pinned=True)\n</code></pre></p>"},{"location":"reference/context-channel/","title":"AetherGraph \u2014 <code>context.channel()</code> Reference","text":"<p>This page documents the ChannelSession methods returned by <code>context.channel()</code> in a concise, PyTorch\u2011style format: signature, brief description, parameters, and returns.</p>"},{"location":"reference/context-channel/#overview-choosing-a-channel","title":"Overview \u2014 Choosing a channel","text":"<p>Use <code>context.channel(&lt;key&gt;)</code> to bind a ChannelSession to a specific destination for all subsequent calls from that session. You can also override per call with the <code>channel=</code> keyword.</p> <p>Common forms - <code>slack:#research</code> \u2014 a Slack channel by name</p> <ul> <li> <p><code>slack:@alice</code> \u2014 a Slack DM</p> </li> <li> <p><code>telegram:@mychannel</code> \u2014 a Telegram channel</p> </li> <li> <p><code>console:stdin</code> \u2014 console fallback (default if nothing is configured)</p> </li> </ul> <p>Resolution order (what channel is used?) 1. Per\u2011call override: <code>await context.channel().send_text(\"hi\", channel=\"slack:#alerts\")</code></p> <ol> <li> <p>Bound session key: <code>ch = context.channel(\"slack:#research\"); await ch.send_text(\"hi\")</code></p> </li> <li> <p>Bus default: whatever <code>services.channels.get_default_channel_key()</code> returns</p> </li> <li> <p>Fallback: <code>console:stdin</code></p> </li> </ol> <p>Examples <pre><code># Bind a session to #research for many messages\nch = context.channel(\"slack:#research\")\nawait ch.send_text(\"Starting the run\u2026\")\nawait ch.send_text(\"Progress will be posted here.\")\n\n# One\u2011off override to a different channel\nawait context.channel().send_text(\"Heads\u2011up in #alerts\", channel=\"slack:#alerts\")\n\n# Stream to #research explicitly\nasync with context.channel().stream(channel=\"slack:#research\") as s:\n    await s.delta(\"Parsing\u2026 \")\n    await s.delta(\"OK\")\n    await s.end(\"Done\")\n\n# Progress bar to the default (no key passed)\nasync with context.channel().progress(title=\"Crunching\", total=100) as bar:\n    await bar.update(current=30, eta_seconds=90)\n    await bar.end(subtitle=\"All set!\")\n</code></pre></p>"},{"location":"reference/context-channel/#channelsend_text","title":"channel.send_text","text":"<p><pre><code>send_text(text, *, meta: dict | None = None, channel: str | None = None)\n</code></pre> Send a plain text message to a channel.</p> <p>Parameters</p> <ul> <li> <p>text (str) \u2013 Message body to send.</p> </li> <li> <p>meta (dict, optional) \u2013 Arbitrary metadata for adapters/analytics.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override (e.g., <code>\"slack:#research\"</code>).</p> </li> </ul> <p>Returns </p> <p><code>None</code></p>"},{"location":"reference/context-channel/#channelsend_image","title":"channel.send_image","text":"<p><pre><code>send_image(url: str | None = None, *, alt: str = \"image\", title: str | None = None, channel: str | None = None)\n</code></pre> Post an image by URL with <code>alt</code>/<code>title</code> text.</p> <p>Parameters</p> <ul> <li> <p>url (str, optional) \u2013 Image URL. Use <code>send_file</code> for file bytes.</p> </li> <li> <p>alt (str) \u2013 Alt text (default: <code>\"image\"</code>).</p> </li> <li> <p>title (str, optional) \u2013 Optional title/caption.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p><code>None</code></p>"},{"location":"reference/context-channel/#channelsend_file","title":"channel.send_file","text":"<p><pre><code>send_file(url: str | None = None, *, file_bytes: bytes | None = None, filename: str = \"file.bin\", title: str | None = None, channel: str | None = None)\n</code></pre> Upload or link a file to the channel. Provide either <code>url</code> or <code>file_bytes</code>.</p> <p>Parameters</p> <ul> <li> <p>url (str, optional) \u2013 Remote file URL to attach.</p> </li> <li> <p>file_bytes (bytes, optional) \u2013 Raw bytes to upload.</p> </li> <li> <p>filename (str) \u2013 Display filename (default: <code>\"file.bin\"</code>).</p> </li> <li> <p>title (str, optional) \u2013 Optional caption/label.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p><code>None</code></p>"},{"location":"reference/context-channel/#channelsend_buttons","title":"channel.send_buttons","text":"<p><pre><code>send_buttons(text: str, buttons: list[Button], *, meta: dict | None = None, channel: str | None = None)\n</code></pre> Send a short message with interactive buttons (links or postbacks depending on adapter).</p> <p>Parameters</p> <ul> <li> <p>text (str) \u2013 Leading text.</p> </li> <li> <p>buttons (list[Button]) \u2013 Button list; at minimum a <code>label</code> per button.</p> </li> <li> <p>meta (dict, optional) \u2013 Arbitrary metadata.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-channel/#channelask_text","title":"channel.ask_text","text":"<p><pre><code>ask_text(prompt: str, *, timeout_s: int = 3600, silent: bool = False, channel: str | None = None)\n</code></pre> Ask the user for free\u2011text using cooperative wait/continuations.</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Prompt text shown to the user. (Ignored if <code>silent=True</code>.)</p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>silent (bool) \u2013 If <code>True</code>, binds to current thread/channel without posting a prompt.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>str \u2013 The user\u2019s text (empty string if none).</p>"},{"location":"reference/context-channel/#channelwait_text","title":"channel.wait_text","text":"<p><pre><code>wait_text(*, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Wait for the next text reply in the current thread/channel without sending a prompt.</p> <p>Parameters</p> <ul> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>str \u2013 The user\u2019s text.</p>"},{"location":"reference/context-channel/#channelask_approval","title":"channel.ask_approval","text":"<p><pre><code>ask_approval(prompt: str, options: Iterable[str] = (\"Approve\", \"Reject\"), *, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Ask the user to approve or pick an option.</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Title or question.</p> </li> <li> <p>options (Iterable[str]) \u2013 Button labels (default: <code>(\"Approve\",\"Reject\")</code>).</p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>dict \u2013 <code>{ \"approved\": bool, \"choice\": str }</code>.</p>"},{"location":"reference/context-channel/#channelget_latest_uploads","title":"channel.get_latest_uploads","text":"<p><pre><code>get_latest_uploads(*, clear: bool = True)\n</code></pre> Fetch latest uploaded files for this channel (Ephemeral KV required).</p> <p>Parameters</p> <ul> <li>clear (bool) \u2013 If <code>True</code>, consume and clear the inbox (default: <code>True</code>).</li> </ul> <p>Returns </p> <p>list[FileRef] \u2013 Recent file references.</p> <p>Raises </p> <p><code>RuntimeError</code> \u2013 if KV is not available.</p>"},{"location":"reference/context-channel/#channelask_files","title":"channel.ask_files","text":"<p><pre><code>ask_files(*, prompt: str, accept: list[str] | None = None, multiple: bool = True, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Ask the user to upload file(s) with optional text input.</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Prompt text.</p> </li> <li> <p>accept (list[str], optional) \u2013 MIME types or extensions (adapter\u2011hint only).</p> </li> <li> <p>multiple (bool) \u2013 Allow selecting multiple files (default: <code>True</code>). </p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>dict \u2013 <code>{ \"text\": str, \"files\": list[FileRef] }</code>.</p>"},{"location":"reference/context-channel/#channelask_text_or_files","title":"channel.ask_text_or_files","text":"<p><pre><code>ask_text_or_files(*, prompt: str, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Let the user respond with either text or file(s).</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Prompt text.</p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>dict \u2013 <code>{ \"text\": str, \"files\": list[FileRef] }</code>.</p>"},{"location":"reference/context-channel/#channelstream","title":"channel.stream","text":"<p><pre><code>stream(channel: str | None = None)  # async context manager\n</code></pre> Create a stream for incremental message updates (token/delta style). Within the context, use <code>s.delta()</code> to append text and <code>s.end()</code> to finalize.</p> <p>Parameters</p> <ul> <li>channel (str, optional) \u2013 Per\u2011stream channel override.</li> </ul> <p>Yields StreamSender \u2013 with methods:</p> <ul> <li> <p><code>start()</code> \u2013 explicitly start the stream (optional; auto on first delta).</p> </li> <li> <p><code>delta(text_piece: str)</code> \u2013 append a delta (adapter receives upsert with full text).</p> </li> <li> <p><code>end(full_text: str | None = None)</code> \u2013 finalize; optionally set final text.</p> </li> </ul> <p>Example <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"stream_demo\")\nasync def stream_demo(*, context):\n    async with context.channel().stream() as s:\n        for chunk in [\"Hello\", \" \", \"world\", \"\u2026\"]:\n            await s.delta(chunk)\n        await s.end(\"Hello world!\")\n</code></pre></p>"},{"location":"reference/context-channel/#channelprogress","title":"channel.progress","text":"<p><pre><code>progress(*, title: str = \"Working...\", total: int | None = None, key_suffix: str = \"progress\", channel: str | None = None)  # async context manager\n</code></pre> Create a progress reporter (start/update/end) bound to the current run/node.</p> <p>Parameters</p> <ul> <li> <p>title (str) \u2013 Progress title (default: <code>\"Working...\"</code>).</p> </li> <li> <p>total (int, optional) \u2013 If set, progress is shown as <code>current/total</code>; allows <code>percent</code> updates.</p> </li> <li> <p>key_suffix (str) \u2013 Included in the internal upsert key (default: <code>\"progress\"</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011progress channel override.</p> </li> </ul> <p>Yields ProgressSender \u2013 with methods:</p> <ul> <li> <p><code>start(subtitle: str | None = None)</code> \u2013 start (auto on first update).</p> </li> <li> <p><code>update(current: int | None = None, inc: int | None = None, subtitle: str | None = None, percent: float | None = None, eta_seconds: float | None = None)</code></p> </li> <li> <p><code>end(subtitle: str | None = \"Done.\", success: bool = True)</code></p> </li> </ul> <p>Example <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"progress_demo\")\nasync def progress_demo(*, context):\n    async with context.channel().progress(title=\"Crunching\", total=5) as bar:\n        for i in range(5):\n            await bar.update(current=i+1, eta_seconds=(4-i)*0.5, subtitle=f\"step {i+1}/5\")\n        await bar.end(subtitle=\"All set!\")\n</code></pre></p>"},{"location":"reference/context-channel/#channel-resolution-notes","title":"Channel resolution notes","text":"<ul> <li> <p>Per\u2011call <code>channel=</code> overrides everything.</p> </li> <li> <p>Otherwise, the session\u2019s bound key (from <code>context.channel(bound_key)</code>) is used.</p> </li> <li> <p>Else, the bus default via <code>services.channels.get_default_channel_key()</code>.</p> </li> <li> <p>Else, fallback <code>\"console:stdin\"</code>.</p> </li> </ul>"},{"location":"reference/context-channel/#guarantees","title":"Guarantees","text":"<ul> <li> <p>Streams/progress use idempotent upsert keys derived from <code>(run_id, node_id, suffix)</code>.</p> </li> <li> <p>Ask methods bind correlators at both message and thread level to capture replies.</p> </li> </ul>"},{"location":"reference/context-kv/","title":"AetherGraph \u2014 <code>context.kv()</code> Reference","text":"<p>This page documents the Key\u2013Value API available via <code>context.kv()</code> in a concise format. The KV store is process\u2011local and transient \u2014 ideal for coordination, small caches, inboxes, and short\u2011lived lists. Not intended for large blobs or durability.</p>"},{"location":"reference/context-kv/#overview","title":"Overview","text":"<ul> <li>Keys are simple strings; values can be any JSON\u2011serializable Python object (adapters may allow arbitrary picklables, but keep it small).</li> <li>Most methods support TTL (time\u2011to\u2011live in seconds). Expired entries are pruned lazily or via <code>purge_expired()</code>.</li> <li>For namespacing, prefer prefixes like <code>\"run:&lt;id&gt;:...\"</code>, <code>\"inbox:&lt;channel&gt;\"</code>, etc.</li> </ul>"},{"location":"reference/context-kv/#kvget","title":"kv.get","text":"<p><pre><code>get(key: str, default: Any = None) -&gt; Any\n</code></pre> Fetch a value by key; returns <code>default</code> if missing or expired.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 Lookup key.</p> </li> <li> <p>default (Any, optional) \u2013 Value to return when absent/expired (default <code>None</code>).</p> </li> </ul> <p>Returns Any \u2013 Stored value or <code>default</code>.</p>"},{"location":"reference/context-kv/#kvset","title":"kv.set","text":"<p><pre><code>set(key: str, value: Any, *, ttl_s: int | None = None) -&gt; None\n</code></pre> Set a key to a value with optional TTL.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 Key to write.</p> </li> <li> <p>value (Any) \u2013 Value to store.</p> </li> <li> <p>ttl_s (int, optional) \u2013 Expiration in seconds.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvdelete","title":"kv.delete","text":"<p><pre><code>delete(key: str) -&gt; None\n</code></pre> Remove a key if present.</p> <p>Parameters</p> <ul> <li>key (str) \u2013 Key to delete.</li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvlist_append_unique","title":"kv.list_append_unique","text":"<p><pre><code>list_append_unique(key: str, items: list[dict], *, id_key: str = \"id\", ttl_s: int | None = None) -&gt; list[dict]\n</code></pre> Append unique dict items to a list value under <code>key</code>. Uniqueness is determined by <code>item[id_key]</code>.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 List container key.</p> </li> <li> <p>items (list[dict]) \u2013 Items to append.</p> </li> <li> <p>id_key (str) \u2013 Field name used for uniqueness (default: <code>\"id\"</code>).</p> </li> <li> <p>ttl_s (int, optional) \u2013 Reset TTL for the list.</p> </li> </ul> <p>Returns list[dict] \u2013 Updated list.</p>"},{"location":"reference/context-kv/#kvlist_pop_all","title":"kv.list_pop_all","text":"<p><pre><code>list_pop_all(key: str) -&gt; list\n</code></pre> Pop and return the entire list stored at <code>key</code>. Empties the container.</p> <p>Parameters</p> <ul> <li>key (str) \u2013 List container key.</li> </ul> <p>Returns list \u2013 Previous list content (empty list if none or not a list).</p>"},{"location":"reference/context-kv/#kvmget","title":"kv.mget","text":"<p><pre><code>mget(keys: list[str]) -&gt; list[Any]\n</code></pre> Batch get multiple keys.</p> <p>Parameters</p> <ul> <li>keys (list[str]) \u2013 Keys to read.</li> </ul> <p>Returns list[Any] \u2013 Values in the same order as <code>keys</code>.</p>"},{"location":"reference/context-kv/#kvmset","title":"kv.mset","text":"<p><pre><code>mset(kv: dict[str, Any], *, ttl_s: int | None = None) -&gt; None\n</code></pre> Batch set multiple keys with an optional shared TTL.</p> <p>Parameters</p> <ul> <li> <p>kv (dict[str, Any]) \u2013 Key\u2013value pairs.</p> </li> <li> <p>ttl_s (int, optional) \u2013 TTL to apply to all entries.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvexpire","title":"kv.expire","text":"<p><pre><code>expire(key: str, ttl_s: int) -&gt; None\n</code></pre> Update/assign a TTL for an existing key.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 Key to expire.</p> </li> <li> <p>ttl_s (int) \u2013 Time\u2011to\u2011live in seconds from now.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvpurge_expired","title":"kv.purge_expired","text":"<p><pre><code>purge_expired(limit: int = 1000) -&gt; int\n</code></pre> Remove up to <code>limit</code> expired keys.</p> <p>Parameters</p> <ul> <li>limit (int) \u2013 Maximum removals per call (default: 1000).</li> </ul> <p>Returns int \u2013 Number of keys purged.</p>"},{"location":"reference/context-kv/#practical-examples","title":"Practical examples","text":"<p>1) Channel inbox (files/messages) <pre><code># Adapter pushes uploads into an inbox list\nawait context.kv().list_append_unique(\n    key=f\"inbox:{channel_key}\",\n    items=[{\"id\": file_id, \"filename\": name, \"url\": url}],\n    ttl_s=3600,\n)\n\n# Agent consumes the inbox later\nfiles = await context.kv().list_pop_all(f\"inbox:{channel_key}\")\nif files:\n    await context.channel().send_text(f\"Received {len(files)} file(s)\")\n</code></pre></p> <p>2) Short\u2011lived cache with TTL <pre><code>k = f\"run:{context.run_id}:spec\"\nspec = await context.kv().get(k)\nif spec is None:\n    spec = await expensive_fetch()\n    await context.kv().set(k, spec, ttl_s=300)  # cache for 5 minutes\n</code></pre></p> <p>3) Batch write/read <pre><code>await context.kv().mset({\n    f\"run:{context.run_id}:step\": 42,\n    f\"run:{context.run_id}:eta\": 120,\n}, ttl_s=600)\n\nvals = await context.kv().mget([\n    f\"run:{context.run_id}:step\",\n    f\"run:{context.run_id}:eta\",\n])\nstep, eta = vals\n</code></pre></p> <p>4) Update TTL <pre><code>await context.kv().expire(f\"run:{context.run_id}:spec\", ttl_s=900)\n</code></pre></p>"},{"location":"reference/context-kv/#notes-behaviors","title":"Notes &amp; behaviors","text":"<ul> <li> <p>Transient: data is in\u2011process only; it disappears on restart.</p> </li> <li> <p>Small values: do not store large binaries; use <code>context.artifacts()</code> for blobs.</p> </li> <li> <p>Concurrency: the implementation uses an internal lock; operations are atomic per call.</p> </li> <li> <p>TTL semantics: reads lazily drop expired entries; use <code>purge_expired()</code> to actively clean.</p> </li> </ul>"},{"location":"reference/context-llm/","title":"AetherGraph \u2014 <code>context.llm()</code> Reference","text":"<p>This page documents the LLM client retrieved via <code>context.llm(profile=\"default\")</code>, in a concise format. The client implements two core calls:</p> <ul> <li><code>chat(messages, **kwargs) -&gt; (text: str, usage: dict)</code></li> <li><code>embed(texts, **kwargs) -&gt; list[list[float]]</code></li> </ul> <p>Profiles are managed by an <code>LLMService</code> that holds one or more configured clients (\"default\", \"azure\", \"local\", etc.).</p>"},{"location":"reference/context-llm/#quick-start","title":"Quick start","text":"<pre><code># 1) Use the default LLM profile\nllm = context.llm()                # == context.llm(\"default\")\ntext, usage = await llm.chat([\n    {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n    {\"role\":\"user\",   \"content\":\"Summarize AetherGraph in one sentence.\"},\n])\n\n# 2) Switch/set a key at runtime (in\u2011memory only)\ncontext.llm_set_key(provider=\"openai\", api_key=\"sk-...\", profile=\"default\")\n\n# 3) Use a named profile (must exist in LLMService)\nllm = context.llm(\"azure\")\n</code></pre>"},{"location":"reference/context-llm/#supported-providers-via-genericllmclient","title":"Supported providers (via GenericLLMClient)","text":"<p><code>{\"openai\",\"azure\",\"anthropic\",\"google\",\"openrouter\",\"lmstudio\",\"ollama\"}</code></p> <p>Credentials and endpoints are read from environment by default, but can be provided at construction time. Runtime key overrides are allowed via <code>context.llm_set_key(...)</code>.</p> <p>Common env vars:</p> <ul> <li><code>OPENAI_API_KEY</code>, <code>OPENAI_BASE_URL</code></li> <li><code>AZURE_OPENAI_KEY</code>, <code>AZURE_OPENAI_ENDPOINT</code>, <code>AZURE_OPENAI_DEPLOYMENT</code></li> <li><code>ANTHROPIC_API_KEY</code></li> <li><code>GOOGLE_API_KEY</code></li> <li><code>OPENROUTER_API_KEY</code></li> <li><code>LMSTUDIO_BASE_URL</code> (default <code>http://localhost:1234/v1</code>)</li> <li><code>OLLAMA_BASE_URL</code>   (default <code>http://localhost:11434/v1</code>)</li> </ul>"},{"location":"reference/context-llm/#llmchat","title":"llm.chat","text":"<p><pre><code>chat(messages: list[dict], **kw) -&gt; tuple[str, dict]\n</code></pre> Send a chat completion request and return <code>(text, usage)</code>.</p> <p>Parameters</p> <ul> <li> <p>messages (list[dict]) \u2013 OpenAI\u2011style conversation turns, e.g. <code>{\"role\":\"system\"|\"user\"|\"assistant\", \"content\": str}</code>.</p> </li> <li> <p>kw \u2013 Common knobs (provider\u2011dependent):  </p> </li> <li> <p>model (str, optional) \u2013 Model name (defaults to client\u2019s configured model).  </p> </li> <li> <p>temperature (float, optional) \u2013 Sampling temperature (default <code>0.5</code>).  </p> </li> <li> <p>top_p (float, optional) \u2013 Nucleus sampling (default <code>1.0</code>).  </p> </li> <li> <p>max_tokens (int, optional) \u2013 Max tokens (Anthropic/Azure/Google paths).  </p> </li> </ul> <p>Returns tuple[str, dict] \u2013 Generated <code>text</code> and a <code>usage</code> dict (token counts where supported).</p> <p>Example <pre><code>sys = {\"role\":\"system\",\"content\":\"Be concise.\"}\nusr = {\"role\":\"user\",\"content\":\"What is AetherGraph?\"}\ntext, usage = await context.llm().chat([sys, usr], temperature=0.2)\nawait context.channel().send_text(text)\n</code></pre></p>"},{"location":"reference/context-llm/#llmembed","title":"llm.embed","text":"<p><pre><code>embed(texts: list[str], **kw) -&gt; list[list[float]]\n</code></pre> Return embeddings for a list of strings.</p> <p>Parameters</p> <ul> <li> <p>texts (list[str]) \u2013 Text strings to embed.</p> </li> <li> <p>kw \u2013 Common knobs (provider\u2011dependent):  </p> </li> <li> <p>model (str, optional) \u2013 Embedding model name (default <code>text-embedding-3-small</code> for OpenAI\u2011like providers).</p> </li> </ul> <p>Returns list[list[float]] \u2013 Embedding vectors.</p> <p>Example <pre><code>vecs = await context.llm().embed([\"lens design\", \"holography basics\"])  # [[...], [...]]\n</code></pre></p>"},{"location":"reference/context-llm/#profiles-and-keys","title":"Profiles and keys","text":""},{"location":"reference/context-llm/#contextllmprofile-str-default-llmclient","title":"<code>context.llm(profile: str = \"default\") -&gt; LLMClient</code>","text":"<p>Retrieve the configured LLM client for a named profile. Raises if <code>LLMService</code> is not bound or profile missing.</p>"},{"location":"reference/context-llm/#contextllm_set_keyprovider-str-api_key-str-profile-str-default-none","title":"<code>context.llm_set_key(provider: str, api_key: str, profile: str = \"default\") -&gt; None</code>","text":"<p>Override an API key in memory for the given profile (good for demos/notebooks). Does not persist.</p> <p>Example <pre><code># Switch the default profile to use a local LM Studio server at runtime\ncontext.llm_set_key(provider=\"lmstudio\", api_key=\"sk-ignore\", profile=\"default\")\ntext, _ = await context.llm().chat([\n    {\"role\":\"user\",\"content\":\"Say hi from LM Studio\"}\n])\n</code></pre></p> <p>For long\u2011lived storage, use your project\u2019s Secrets provider and <code>LLMService.persist_key(secret_name, api_key)</code> if available.</p>"},{"location":"reference/context-llm/#providerspecific-notes","title":"Provider\u2011specific notes","text":"<ul> <li> <p>OpenAI / OpenRouter / LM Studio / Ollama \u2013 uses OpenAI\u2011style <code>/chat/completions</code> and <code>/embeddings</code> routes. <code>usage</code> is included where supported.</p> </li> <li> <p>Azure OpenAI \u2013 requires <code>AZURE_OPENAI_ENDPOINT</code> and <code>AZURE_OPENAI_DEPLOYMENT</code>; uses Azure routes.</p> </li> <li> <p>Anthropic (Claude) \u2013 converts OpenAI\u2011style messages to Anthropic\u2019s message format; returns concatenated text blocks.</p> </li> <li> <p>Google (Gemini) \u2013 uses <code>:generateContent</code> and <code>:embedContent</code>; <code>usage</code> shape differs and may be empty.</p> </li> <li> <p>Embeddings \u2013 not supported for Anthropic in this client.</p> </li> </ul>"},{"location":"reference/context-llm/#error-handling-retries","title":"Error handling &amp; retries","text":"<p>The client wraps calls with exponential backoff (<code>_Retry</code>) for transient HTTP errors (<code>ReadTimeout</code>, <code>ConnectError</code>, <code>HTTPStatusError</code>). You may still want to catch and surface provider\u2011specific errors around quota/keys.</p> <p>Example <pre><code>try:\n    text, usage = await context.llm().chat([{ \"role\":\"user\", \"content\":\"ping\" }])\nexcept Exception as e:\n    await context.channel().send_text(f\"LLM error: {e}\")\n</code></pre></p>"},{"location":"reference/context-llm/#patterns-with-context","title":"Patterns with Context","text":"<p>Router\u2011then\u2011Act <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"router_then_act\")\nasync def router_then_act(*, context):\n    sys = {\"role\":\"system\",\"content\":\"Route to 'summarize' or 'plot'\"}\n    usr = {\"role\":\"user\",\"content\":\"Summarize this research log.\"}\n    decision, _ = await context.llm().chat([sys, usr], temperature=0.0)\n    if \"summarize\" in decision.lower():\n        # Call downstream tool and write a result\n        await context.memory().write_result(\n            topic=\"router\",\n            outputs=[{\"name\":\"route\",\"kind\":\"text\",\"value\":\"summarize\"}],\n        )\n        await context.channel().send_text(\"Routing \u2192 summarize\")\n</code></pre></p> <p>RAG: retrieve \u2192 answer <pre><code>@graph_fn(name=\"rag_answer\")\nasync def rag_answer(*, context, q: str):\n    hits = await context.memory().rag_search(corpus_id=\"notes\", query=q, k=6)\n    prompt = [{\"role\":\"system\",\"content\":\"Answer using the provided notes.\"},\n              {\"role\":\"user\",\"content\":\"\\n\\n\".join(h.get(\"text\",\"\") for h in hits) + \"\\n\\nQ: \" + q}]\n    text, usage = await context.llm().chat(prompt, temperature=0.2, model=\"gpt-4o-mini\")\n    return {\"answer\": text, \"tokens\": usage.get(\"total_tokens\")}\n</code></pre></p>"},{"location":"reference/context-llm/#summary","title":"Summary","text":"<ul> <li>Use <code>context.llm()</code> to get a ready\u2011to\u2011use client for the current profile.</li> <li><code>chat()</code> returns <code>(text, usage)</code>; <code>embed()</code> returns vectors.</li> <li>Switch keys ad\u2011hoc with <code>context.llm_set_key(...)</code>; persist via your Secrets provider when available.</li> </ul>"},{"location":"reference/context-logger/","title":"AetherGraph \u2014 <code>context.logger()</code> Quick Reference","text":"<p><code>context.logger()</code> returns a pre\u2011scoped Python <code>logging.Logger</code> bound to the current run/graph/node via the project\u2019s <code>StdLoggerService</code>.</p> <ul> <li>Namespace: <code>node.&lt;node_id&gt;</code></li> <li>Extra context on every record: <code>{run_id, graph_id, node_id}</code></li> <li>Outputs: console (text) + rotating file (<code>$LOG_DIR/aethergraph.log</code>), optional JSON, optional async QueueHandler</li> </ul>"},{"location":"reference/context-logger/#basics","title":"Basics","text":"<pre><code>log = context.logger()\nlog.info(\"starting step\")\nlog.debug(\"inputs\", extra={\"shape\": [n, d]})\nlog.warning(\"retrying\", extra={\"attempt\": i})\ntry:\n    ...\nexcept Exception:\n    log.exception(\"failed tool call\")  # includes traceback\n</code></pre> <p>Returns <code>logging.Logger</code> \u2014 fully configured for the current node/run.</p>"},{"location":"reference/context-logger/#formatting-levels-service-defaults","title":"Formatting &amp; levels (service defaults)","text":"<p>Configured by <code>StdLoggerService.build(cfg)</code> and <code>LoggingConfig</code>:</p> <ul> <li>Global level: <code>cfg.level</code> (e.g., <code>INFO</code>) with optional per\u2011namespace overrides</li> <li>Console formatter: <code>cfg.console_pattern</code></li> <li>File formatter: text (<code>cfg.file_pattern</code>) or JSON (<code>cfg.use_json = True</code>)</li> <li>File rotation: <code>cfg.max_bytes</code>, <code>cfg.backup_count</code></li> <li>Non\u2011blocking file I/O: <code>cfg.enable_queue = True</code> (QueueHandler + Listener)</li> </ul> <p>You can rebuild the service on server start to apply new settings.</p>"},{"location":"reference/context-logger/#structured-fields","title":"Structured fields","text":"<p>Every call accepts <code>extra={...}</code> for structured, searchable fields. The service injects <code>{run_id, graph_id, node_id}</code> automatically. <pre><code>log.info(\"optimizer step\", extra={\"lr\": 3e-4, \"batch\": 64, \"phase\": \"warmup\"})\n</code></pre></p>"},{"location":"reference/context-logger/#good-practices","title":"Good practices","text":"<ul> <li>Use <code>debug</code> for noisy internals; rely on <code>INFO</code> for milestone breadcrumbs.</li> <li>Prefer <code>extra={...}</code> over string concatenation for metrics/values.</li> <li>Use <code>exception()</code> within <code>except</code> blocks to capture tracebacks.</li> <li>Log artifact URIs (<code>extra={\"artifact\": uri}</code>) instead of large payloads.</li> </ul>"},{"location":"reference/context-logger/#oneliner-pattern-in-tools","title":"One\u2011liner pattern in tools","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"demo\")\nasync def demo(*, context):\n    log = context.logger()\n    log.info(\"hello\", extra={\"stage\": \"start\"})\n    # ... work ...\n    log.info(\"done\", extra={\"duration_ms\": 123})\n</code></pre>"},{"location":"reference/context-logger/#summary","title":"Summary","text":"<ul> <li>Call <code>context.logger()</code> inside graph/tools for a scoped logger.</li> <li>Structured fields available via <code>extra={...}</code>; run/graph/node auto\u2011injected.</li> </ul>"},{"location":"reference/context-mcp/","title":"AetherGraph \u2014 <code>context.mcp()</code> Reference","text":"<p>This page documents the Model Context Protocol (MCP) client you obtain with <code>context.mcp(name)</code>. Use it to call tools, list resources, or read resources exposed by a remote/local MCP server over stdio, WebSocket, or HTTP.</p> <p>Import surface (for examples below): <pre><code>from aethergraph.services.mcp import (\n    MCPService,\n    StdioMCPClient,\n    WsMCPClient,\n    HttpMCPClient,\n)\n</code></pre></p>"},{"location":"reference/context-mcp/#concepts","title":"Concepts","text":"<ul> <li>MCPService: registry of named MCP clients (e.g., <code>\"local\"</code>, <code>\"ws\"</code>, <code>\"http\"</code>), handles lazy open/close and convenience calls.</li> <li>MCPClientProtocol: transport\u2011specific client implementing <code>open()</code>, <code>close()</code>, <code>call(tool, params)</code>, <code>list_tools()</code>, <code>list_resources()</code>, <code>read_resource(uri)</code>.</li> <li>Tools: remote RPCs exposed by the MCP server (e.g., <code>readFile</code>, <code>search</code>, <code>stat</code>).</li> <li>Resources: server\u2011advertised URIs you can <code>read_resource()</code> (e.g., <code>file://\u2026</code>, <code>repo://\u2026</code>).</li> </ul> <p><code>context.mcp(name)</code> returns the client registered under <code>name</code> via your process\u2011global <code>MCPService</code>.</p>"},{"location":"reference/context-mcp/#mcpservice-registry","title":"MCPService (registry)","text":""},{"location":"reference/context-mcp/#register","title":"register","text":"<p><pre><code>register(name: str, client: MCPClientProtocol) -&gt; None\n</code></pre> Register a client under a name.</p>"},{"location":"reference/context-mcp/#remove","title":"remove","text":"<p><pre><code>remove(name: str) -&gt; None\n</code></pre> Unregister a client.</p>"},{"location":"reference/context-mcp/#has-names-get","title":"has / names / get","text":"<p><pre><code>has(name: str) -&gt; bool\nnames() -&gt; list[str]\nget(name: str = \"default\") -&gt; MCPClientProtocol\n</code></pre> Query and retrieve clients by name.</p>"},{"location":"reference/context-mcp/#open-close","title":"open / close","text":"<p><pre><code>open(name: str) -&gt; None\nclose(name: str) -&gt; None\nopen_all() -&gt; None\nclose_all() -&gt; None\n</code></pre> Manage client lifecycles. <code>call()/list_*()</code> implicitly <code>open()</code> on first use.</p>"},{"location":"reference/context-mcp/#call-helpers","title":"call helpers","text":"<p><pre><code>call(name: str, tool: str, params: dict | None = None) -&gt; dict\nlist_tools(name: str) -&gt; list[MCPTool]\nlist_resources(name: str) -&gt; list[MCPResource]\nread_resource(name: str, uri: str) -&gt; dict\n</code></pre> Thin wrappers to keep call sites small; auto\u2011open if needed.</p>"},{"location":"reference/context-mcp/#optional-secretsruntime-headers","title":"optional secrets/runtime headers","text":"<p><pre><code>set_header(name: str, key: str, value: str) -&gt; None\npersist_secret(secret_name: str, value: str) -&gt; None\n</code></pre> <code>set_header()</code> is handy for WS/HTTP auth tokens at runtime. <code>persist_secret()</code> stores a credential via your Secrets provider (if writable).</p>"},{"location":"reference/context-mcp/#transport-clients","title":"Transport clients","text":""},{"location":"reference/context-mcp/#stdiomcpclient","title":"StdioMCPClient","text":"<p><pre><code>StdioMCPClient(cmd: list[str], env: dict[str,str] | None = None, timeout: float = 60.0)\n</code></pre> Spawn a subprocess and speak JSON\u2011RPC over stdio.</p>"},{"location":"reference/context-mcp/#wsmcpclient","title":"WsMCPClient","text":"<p><pre><code>WsMCPClient(url: str, *, headers: dict[str,str] | None = None, timeout: float = 60.0, ping_interval: float = 20.0, ping_timeout: float = 10.0)\n</code></pre> Connect to an MCP server over WebSocket.</p>"},{"location":"reference/context-mcp/#httpmcpclient","title":"HttpMCPClient","text":"<p><pre><code>HttpMCPClient(base_url: str, *, headers: dict[str,str] | None = None, timeout: float = 60.0)\n</code></pre> Call an MCP server over HTTP (JSON).</p>"},{"location":"reference/context-mcp/#contextmcpname","title":"context.mcp(name)","text":"<p><pre><code>context.mcp(name: str) -&gt; MCPClientProtocol\n</code></pre> Return the named client. Typically you register names like <code>\"local\"</code>, <code>\"ws\"</code>, <code>\"http\"</code> during app startup, then retrieve them inside tools/agents.</p> <p>Example <pre><code>client = context.mcp(\"ws\")\nout = await client.call(\"search\", {\"q\": \"holography\", \"k\": 5})\n</code></pre></p>"},{"location":"reference/context-mcp/#calling-tools","title":"Calling tools","text":"<p><pre><code>client.call(tool: str, params: dict | None = None) -&gt; dict\n</code></pre> Invoke a remote tool by name with JSON\u2011serializable params.</p> <p>Parameters - tool (str) \u2013 Tool name (server\u2011defined). - params (dict, optional) \u2013 Arguments for the tool.</p> <p>Returns dict \u2013 Tool result payload (shape defined by the server).</p> <p>Example <pre><code># Filesystem\u2011like server\nres = await context.mcp(\"local\").call(\"readFile\", {\"path\": \"/data/notes.txt\"})\ntext = res.get(\"text\") or res.get(\"content\") or \"\"\nawait context.channel().send_text(f\"len={len(text)}\")\n</code></pre></p>"},{"location":"reference/context-mcp/#listing-tools-resources","title":"Listing tools &amp; resources","text":"<p><pre><code>client.list_tools() -&gt; list[MCPTool]\nclient.list_resources() -&gt; list[MCPResource]\nclient.read_resource(uri: str) -&gt; dict\n</code></pre> Enumerate server capabilities and read advertised resources.</p> <p>Example <pre><code># Tool discovery\nfor t in await context.mcp(\"http\").list_tools():\n    await context.channel().send_text(f\"tool: {t.name} \u2014 {t.description}\")\n\n# Resource fetch\nfor r in await context.mcp(\"ws\").list_resources():\n    if r.uri.startswith(\"file://\"):\n        blob = await context.mcp(\"ws\").read_resource(r.uri)\n        await context.channel().send_text(f\"read {r.uri} \u2192 {len(blob.get('text',''))} chars\")\n</code></pre></p>"},{"location":"reference/context-mcp/#endtoend-setup-startup","title":"End\u2011to\u2011end setup (startup)","text":"<pre><code>from aethergraph.services.mcp import MCPService, StdioMCPClient, WsMCPClient, HttpMCPClient\nfrom aethergraph.v3.core.runtime.runtime_services import set_mcp_service\nimport os, sys\n\nDEMO_HTTP_TOKEN = os.environ.setdefault(\"DEMO_HTTP_TOKEN\", \"demo_token_123\")\n\nmcp = MCPService()\nmcp.register(\"local\", StdioMCPClient(cmd=[sys.executable, \"-m\", \"aethergraph.plugins.mcp.fs_server\"]))\nmcp.register(\"ws\", WsMCPClient(url=\"ws://localhost:8765\", headers={\"Authorization\": \"Bearer demo_token_123\"}))\nmcp.register(\"http\", HttpMCPClient(\"http://127.0.0.1:8769\", headers={\"Authorization\": f\"Bearer {DEMO_HTTP_TOKEN}\"}))\n\nset_mcp_service(mcp)  # make available to NodeContext\n</code></pre>"},{"location":"reference/context-mcp/#using-inside-a-graph-function","title":"Using inside a graph function","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"mcp_search_demo\", inputs=[\"q\"], outputs=[\"text\"], version=\"0.1.0\")\nasync def mcp_search_demo(q: str, *, context):\n    out = await context.mcp(\"ws\").call(\"search\", {\"q\": q, \"k\": 5})\n    text = out.get(\"text\") or out.get(\"content\") or \"\"\n    await context.channel().send_text(text[:200] + (\"\u2026\" if len(text) &gt; 200 else \"\"))\n    return {\"text\": text}\n</code></pre>"},{"location":"reference/context-mcp/#choosing-a-transport","title":"Choosing a transport","text":"<ul> <li>stdio: best when you ship or control the server process (local tools, file system, Git, CLI wrappers). Minimal latency, simple auth via env.</li> <li>WebSocket: interactive servers that push events, need long\u2011lived sessions, or custom headers/tokens.</li> <li>HTTP: stateless request/response, easy to deploy behind gateways; good fit for cloud MCP services.</li> </ul> <p>Tip: You can register multiple transports to the same logical backend under different names (<code>\"fs-local\"</code>, <code>\"fs-ws\"</code>) and switch per call.</p>"},{"location":"reference/context-mcp/#auth-headers","title":"Auth &amp; headers","text":"<ul> <li>Pass headers at client construction (<code>headers={\"Authorization\": \"Bearer \u2026\"}</code>).</li> <li>Update at runtime via <code>MCPService.set_header(name, key, value)</code> for WS/HTTP clients.</li> <li>Persist tokens via <code>MCPService.persist_secret(...)</code> when your Secrets provider supports writes.</li> </ul>"},{"location":"reference/context-mcp/#error-handling","title":"Error handling","text":"<p>Wrap calls to surface clear messages back to the user. <pre><code>try:\n    res = await context.mcp(\"http\").call(\"search\", {\"q\": \"mtf\"})\nexcept KeyError:\n    await context.channel().send_text(\"Unknown MCP profile. Did you register it?\")\nexcept Exception as e:\n    await context.channel().send_text(f\"MCP error: {e}\")\n</code></pre></p>"},{"location":"reference/context-mcp/#summary","title":"Summary","text":"<ul> <li>Register your clients at startup with <code>MCPService.register()</code> and wire the service into runtime so <code>context.mcp(name)</code> can retrieve them.</li> <li>Use <code>.call()</code> for tools, <code>.list_tools()/.list_resources()</code> for discovery, and <code>.read_resource()</code> to fetch URIs.</li> <li>Choose stdio/WS/HTTP based on deployment and interaction needs; manage auth via headers/</li> </ul>"},{"location":"reference/context-memory/","title":"AetherGraph \u2014 <code>context.memory()</code> Reference","text":"<p>This page documents the MemoryFacade returned by <code>context.memory()</code> in a concise format: signature, brief description, parameters, returns, and practical examples. The facade coordinates three core components \u2014 HotLog (recent, transient), Persistence (durable JSONL/appends), and Indices (fast derived views) \u2014 with optional ArtifactStore, RAG, and LLM services.</p>"},{"location":"reference/context-memory/#overview","title":"Overview","text":"<p><code>context.memory()</code> is bound to your current runtime scope (<code>session_id</code>, <code>run_id</code>, <code>graph_id</code>, <code>node_id</code>, <code>agent_id</code>). Typical operations:</p> <ol> <li> <p>Record events (raw or typed results)</p> </li> <li> <p>Query recent/last/by\u2011kind outputs via indices/hotlog</p> </li> <li> <p>Distill (rolling summaries, episode summaries)</p> </li> <li> <p>RAG (optional): upsert, search, answer using a configured RAG + LLM</p> </li> </ol>"},{"location":"reference/context-memory/#memoryrecord_raw","title":"memory.record_raw","text":"<p><pre><code>record_raw(*, base: dict, text: str | None = None, metrics: dict | None = None, sources: list[str] | None = None) -&gt; Event\n</code></pre> Append a normalized event to HotLog (fast) and Persistence (durable). Computes a stable <code>event_id</code> and a lightweight <code>signal</code> if absent.</p> <p>Parameters</p> <ul> <li> <p>base (dict) \u2013 Canonical fields describing the event (e.g., <code>kind</code>, <code>stage</code>, <code>severity</code>, <code>tool</code>, <code>tags</code>, <code>entities</code>, <code>inputs</code>, <code>outputs</code>, \u2026). Missing scope keys are filled from the bound context.</p> </li> <li> <p>text (str, optional) \u2013 Human\u2011readable message/body.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics (latency, token counts, costs, etc.).</p> </li> <li> <p>sources (list[str], optional) \u2013 Event IDs this event summarizes/derives from.</p> </li> </ul> <p>Returns Event \u2013 The appended event.</p> <p>Notes Does not update <code>indices</code> automatically. Use <code>write_result()</code> when you want indices updated for typed outputs.</p>"},{"location":"reference/context-memory/#memoryrecord","title":"memory.record","text":"<p><pre><code>record(kind, data, tags=None, entities=None, severity=2, stage=None, inputs_ref=None, outputs_ref=None, metrics=None, sources=None, signal=None) -&gt; Event\n</code></pre> Convenience wrapper around <code>record_raw()</code> for common fields; stringifies <code>data</code> if needed.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Event kind (e.g., <code>\"user_msg\"</code>, <code>\"tool_call\"</code>).</p> </li> <li> <p>data (Any) \u2013 JSON\u2011serializable payload; will be stringified for <code>text</code>.</p> </li> <li> <p>tags (list[str], optional) \u2013 Tag list.</p> </li> <li> <p>entities (list[str], optional) \u2013 Entity IDs.</p> </li> <li> <p>severity (int) \u2013 1\u20135 scale (default 2).</p> </li> <li> <p>stage (str, optional) \u2013 Phase label (e.g., <code>\"observe\"</code>, <code>\"act\"</code>).</p> </li> <li> <p>inputs_ref (list[dict], optional) \u2013 Typed input references (Value[]).</p> </li> <li> <p>outputs_ref (list[dict], optional) \u2013 Typed output references (Value[]).</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics.</p> </li> <li> <p>sources (list[str], optional) \u2013 Upstream event IDs.</p> </li> <li> <p>signal (float, optional) \u2013 0.0\u20131.0; if omitted, computed heuristically.</p> </li> </ul> <p>Returns Event \u2013 The appended event.</p>"},{"location":"reference/context-memory/#memorywrite_result","title":"memory.write_result","text":"<p><pre><code>write_result(*, topic: str, inputs: list[dict] | None = None, outputs: list[dict] | None = None, tags: list[str] | None = None, metrics: dict | None = None, message: str | None = None, severity: int = 3) -&gt; Event\n</code></pre> Record a typed result (tool/agent/flow) and update indices for quick retrieval.</p> <p>Parameters</p> <ul> <li> <p>topic (str) \u2013 Tool/agent/flow identifier (used by <code>indices.last_outputs_by_topic</code>).</p> </li> <li> <p>inputs (list[dict], optional) \u2013 Typed inputs (Value[]).</p> </li> <li> <p>outputs (list[dict], optional) \u2013 Typed outputs (Value[]). Indices derive from these.</p> </li> <li> <p>tags (list[str], optional) \u2013 Tag list.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics.</p> </li> <li> <p>message (str, optional) \u2013 Human\u2011readable summary.</p> </li> <li> <p>severity (int) \u2013 Default 3.</p> </li> </ul> <p>Returns Event \u2013 The normalized <code>tool_result</code> event.</p> <p>Effect Auto\u2011appends to HotLog &amp; Persistence and calls <code>indices.update(session_id, evt)</code>.</p>"},{"location":"reference/context-memory/#memoryrecent","title":"memory.recent","text":"<p><pre><code>recent(*, kinds: list[str] | None = None, limit: int = 50) -&gt; list[Event]\n</code></pre> Return recent events from HotLog (most recent last), optionally filtering by <code>kinds</code>.</p> <p>Parameters</p> <ul> <li> <p>kinds (list[str], optional) \u2013 Filter kinds.</p> </li> <li> <p>limit (int) \u2013 Max events (default 50).</p> </li> </ul> <p>Returns list[Event] \u2013 Recent events.</p>"},{"location":"reference/context-memory/#memorylast_by_name","title":"memory.last_by_name","text":"<p><pre><code>last_by_name(name: str)\n</code></pre> Return the last output value by <code>name</code> from Indices (fast path).</p> <p>Parameters</p> <ul> <li>name (str) \u2013 Output name.</li> </ul> <p>Returns Any \u2013 The stored value for that name (adapter\u2011dependent) or <code>None</code>.</p>"},{"location":"reference/context-memory/#memorylatest_refs_by_kind","title":"memory.latest_refs_by_kind","text":"<p><pre><code>latest_refs_by_kind(kind: str, *, limit: int = 50)\n</code></pre> Return latest ref outputs by <code>ref.kind</code> (fast path, KV\u2011backed) from Indices.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Reference kind.</p> </li> <li> <p>limit (int) \u2013 Max items (default 50).</p> </li> </ul> <p>Returns list[Any] \u2013 Recent references.</p>"},{"location":"reference/context-memory/#memorylast_outputs_by_topic","title":"memory.last_outputs_by_topic","text":"<p><pre><code>last_outputs_by_topic(topic: str)\n</code></pre> Return the last output map for a given topic (tool/flow/agent) from Indices.</p> <p>Parameters</p> <ul> <li>topic (str) \u2013 Topic identifier.</li> </ul> <p>Returns dict | None \u2013 Latest outputs or <code>None</code> if absent.</p>"},{"location":"reference/context-memory/#memorydistill_rolling_chat","title":"memory.distill_rolling_chat","text":"<p><pre><code>distill_rolling_chat(*, max_turns: int = 20, min_signal: float | None = None) -&gt; dict\n</code></pre> Build a rolling chat summary from recent user/assistant turns (reads HotLog; typically writes a JSON summary via Persistence).</p> <p>Parameters</p> <ul> <li> <p>max_turns (int) \u2013 Window of turns to include (default 20).</p> </li> <li> <p>min_signal (float, optional) \u2013 Signal threshold; uses facade default if omitted.</p> </li> </ul> <p>Returns dict \u2013 Descriptor (e.g., <code>{ \"uri\": ..., \"sources\": [...] }</code>).</p>"},{"location":"reference/context-memory/#memorydistill_episode","title":"memory.distill_episode","text":"<p><pre><code>distill_episode(*, tool: str, run_id: str, include_metrics: bool = True) -&gt; dict\n</code></pre> Summarize a tool/agent episode (all events for a given <code>run_id</code> + <code>tool</code>). Reads HotLog/Persistence; writes back a summary JSON (and optionally CAS bundle).</p> <p>Parameters</p> <ul> <li> <p>tool (str) \u2013 Tool/agent identifier.</p> </li> <li> <p>run_id (str) \u2013 Run to summarize.</p> </li> <li> <p>include_metrics (bool) \u2013 Include metrics in the summary (default True).</p> </li> </ul> <p>Returns dict \u2013 Descriptor (e.g., <code>{ \"uri\": ..., \"sources\": [...], \"metrics\": {...} }</code>).</p>"},{"location":"reference/context-memory/#rag-helpers-optional","title":"RAG helpers (optional)","text":""},{"location":"reference/context-memory/#memoryrag_upsert","title":"memory.rag_upsert","text":"<p><pre><code>rag_upsert(*, corpus_id: str, docs: Sequence[dict], topic: str | None = None) -&gt; dict\n</code></pre> Upsert documents into a RAG corpus via the configured RAG facade.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>docs (Sequence[dict]) \u2013 Documents/chunks with text and metadata.</p> </li> <li> <p>topic (str, optional) \u2013 Optional topic name to attribute the upsert.</p> </li> </ul> <p>Returns dict \u2013 Upsert stats (shape adapter\u2011specific).</p> <p>Raises <code>RuntimeError</code> \u2013 if RAG facade is not configured.</p>"},{"location":"reference/context-memory/#memoryrag_search","title":"memory.rag_search","text":"<p><pre><code>rag_search(*, corpus_id: str, query: str, k: int = 8) -&gt; list[dict]\n</code></pre> Retrieve best\u2011matching chunks for a query.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>query (str) \u2013 Natural language query.</p> </li> <li> <p>k (int) \u2013 Max results (default 8), reranked.</p> </li> </ul> <p>Returns list[dict] \u2013 Ranked hits.</p> <p>Raises <code>RuntimeError</code> \u2013 if RAG facade is not configured.</p>"},{"location":"reference/context-memory/#memoryrag_answer","title":"memory.rag_answer","text":"<p><pre><code>rag_answer(*, corpus_id: str, question: str, style: str = \"concise\", k: int = 6, llm_profile: str = \"default\") -&gt; dict\n</code></pre> Answer a question using RAG + LLM (both must be configured).</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>question (str) \u2013 User question.</p> </li> <li> <p>style (str) \u2013 Answering style (e.g., <code>\"concise\"</code>).</p> </li> <li> <p>k (int) \u2013 Max retrieved chunks (default 6).</p> </li> <li> <p>llm_profile (str) \u2013 Profile name to select an LLM client.</p> </li> </ul> <p>Returns dict \u2013 Answer payload (adapter\u2011specific).</p> <p>Raises <code>RuntimeError</code> \u2013 if RAG or LLM is not configured.</p>"},{"location":"reference/context-memory/#practical-examples","title":"Practical examples","text":"<p>1) Record + recent <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"mem_record_recent\")\nasync def mem_record_recent(*, context):\n    evt = await context.memory().record(\n        kind=\"user_msg\",\n        data={\"text\":\"hello world\",\"lang\":\"en\"},\n        tags=[\"demo\",\"quickstart\"],\n        severity=2,\n    )\n    recent = await context.memory().recent(kinds=[\"user_msg\"], limit=5)\n    await context.channel().send_text(f\"recent user_msg count={len(recent)}\")\n    return {\"event_id\": evt.event_id, \"recent_count\": len(recent)}\n</code></pre></p> <p>2) Write a typed result and fetch last outputs <pre><code>@graph_fn(name=\"mem_write_result\")\nasync def mem_write_result(*, context):\n    await context.memory().write_result(\n        topic=\"eval.step\",\n        outputs=[{\"name\":\"acc\",\"kind\":\"number\",\"value\":0.912}],\n        metrics={\"latency_ms\": 120},\n        message=\"evaluation complete\",\n    )\n    last = await context.memory().last_outputs_by_topic(\"eval.step\")\n    await context.channel().send_text(f\"last acc={last['acc']:.3f}\")\n</code></pre></p> <p>3) Rolling chat summary <pre><code>@graph_fn(name=\"mem_rolling\")\nasync def mem_rolling(*, context):\n    summary = await context.memory().distill_rolling_chat(max_turns=16)\n    await context.channel().send_text(f\"rolling summary uri: {summary.get('uri','&lt;none&gt;')}\")\n</code></pre></p> <p>4) Episode summary <pre><code>@graph_fn(name=\"mem_episode\")\nasync def mem_episode(*, context, run_id: str, tool: str):\n    desc = await context.memory().distill_episode(tool=tool, run_id=run_id)\n    await context.channel().send_text(f\"episode summary: {desc.get('uri','&lt;none&gt;')}\")\n</code></pre></p> <p>5) RAG (if configured) <pre><code>@graph_fn(name=\"mem_rag\")\nasync def mem_rag(*, context):\n    # Upsert a few docs\n    await context.memory().rag_upsert(\n        corpus_id=\"notes\",\n        docs=[{\"id\":\"1\",\"text\":\"Optics basics: Snell's law\"}],\n    )\n    # Search\n    hits = await context.memory().rag_search(corpus_id=\"notes\", query=\"Snell\")\n    # Answer\n    ans = await context.memory().rag_answer(corpus_id=\"notes\", question=\"What is Snell's law?\", style=\"concise\")\n    await context.channel().send_text(ans.get(\"answer\",\"&lt;no answer&gt;\"))\n</code></pre></p>"},{"location":"reference/context-memory/#notes-behaviors","title":"Notes &amp; behaviors","text":"<ul> <li> <p>Signal heuristic: if not provided, <code>record(_raw)</code> computes a 0.0\u20131.0 <code>signal</code> from severity + presence/length of text + metrics.</p> </li> <li> <p>Durability: every <code>record_raw</code> &amp; <code>write_result</code> appends to Persistence; <code>recent()</code> reads from HotLog.</p> </li> <li> <p>Indices: <code>write_result()</code> updates fast views used by <code>last_by_name</code>, <code>latest_refs_by_kind</code>, <code>last_outputs_by_topic</code>.</p> </li> <li> <p>Artifacts: distillers may produce CAS artifacts when an <code>ArtifactStore</code> is provided.</p> </li> <li> <p>Performance: methods are async; backends should avoid blocking the event loop (use <code>asyncio.to_thread</code> for heavy IO).</p> </li> </ul>"},{"location":"reference/context-rag/","title":"AetherGraph \u2014 <code>context.rag()</code> Reference","text":"<p>This page documents the RAGFacade returned by <code>context.rag()</code> in a concise format: signature, brief description, parameters, returns, and practical examples.</p> <p>The facade covers: corpus management, document ingestion (upsert), retrieval (search/retrieve), and question answering with optional citation resolution.</p>"},{"location":"reference/context-rag/#overview","title":"Overview","text":"<p><code>context.rag()</code> provides high\u2011level helpers backed by:</p> <ul> <li>an Artifact Store (for persisted doc assets),</li> <li>an Embedding client (e.g., <code>context.llm().embed()</code>),</li> <li>a Vector index backend (add/search),</li> <li>a TextSplitter (chunking before embedding), and</li> <li>an optional LLM client for QA.</li> </ul>"},{"location":"reference/context-rag/#ragadd_corpus","title":"rag.add_corpus","text":"<p><pre><code>add_corpus(corpus_id: str, meta: dict | None = None) -&gt; None\n</code></pre> Create a new corpus directory with metadata if it does not exist.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Unique identifier for the corpus.</p> </li> <li> <p>meta (dict, optional) \u2013 Arbitrary metadata stored alongside the corpus.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-rag/#ragupsert_docs","title":"rag.upsert_docs","text":"<p><pre><code>upsert_docs(corpus_id: str, docs: list[dict]) -&gt; dict\n</code></pre> Ingest and index a list of documents (file\u2011based or inline text). Handles artifact persistence, chunking, embedding, and index add.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>docs (list[dict]) \u2013 Each doc is either:</p> </li> <li> <p>File doc: <code>{ \"path\": \"/path/to/file.pdf\", \"labels\": {...}, \"title\": \"Optional\" }</code></p> </li> <li> <p>Inline text doc: <code>{ \"text\": \"...\", \"title\": \"Optional\", \"labels\": {...} }</code></p> </li> </ul> <p>Returns dict \u2013 Summary like <code>{ \"added\": int, \"chunks\": int, \"index\": \"BackendName\" }</code>.</p> <p>Notes - PDFs and Markdown are parsed with built\u2011in extractors; other files default to text.</p> <ul> <li>Each doc and chunk is assigned a stable SHA\u2011derived ID and recorded in <code>docs.jsonl</code> / <code>chunks.jsonl</code> under the corpus folder.</li> </ul>"},{"location":"reference/context-rag/#ragsearch","title":"rag.search","text":"<p><pre><code>search(corpus_id: str, query: str, k: int = 8, filters: dict | None = None, mode: str = \"hybrid\") -&gt; list[SearchHit]\n</code></pre> Hybrid retrieval: dense vector search with optional lexical fusion, returning the top\u2011k chunks.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>query (str) \u2013 Natural language query.</p> </li> <li> <p>k (int) \u2013 Number of results (default 8).</p> </li> <li> <p>filters (dict, optional) \u2013 Reserved for metadata filtering (adapter\u2011specific).</p> </li> <li> <p>mode ({\"dense\",\"hybrid\"}) \u2013 Retrieval mode (default <code>\"hybrid\"</code>).</p> </li> </ul> <p>Returns list[SearchHit] \u2013 Ranked hits with <code>chunk_id</code>, <code>doc_id</code>, <code>corpus_id</code>, <code>score</code>, <code>text</code>, <code>meta</code>.</p>"},{"location":"reference/context-rag/#ragretrieve","title":"rag.retrieve","text":"<p><pre><code>retrieve(corpus_id: str, query: str, k: int = 6, rerank: bool = True) -&gt; list[SearchHit]\n</code></pre> Convenience wrapper over <code>search(..., mode=\"hybrid\")</code> for top\u2011k retrieval.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>query (str) \u2013 Natural language query.</p> </li> <li> <p>k (int) \u2013 Number of results (default 6).</p> </li> <li> <p>rerank (bool) \u2013 Currently ignored (hybrid already fuses scores).</p> </li> </ul> <p>Returns list[SearchHit] \u2013 Ranked hits.</p>"},{"location":"reference/context-rag/#raganswer","title":"rag.answer","text":"<p><pre><code>answer(corpus_id: str, question: str, *, llm: GenericLLMClient | None = None, style: str = \"concise\", with_citations: bool = True, k: int = 6) -&gt; dict\n</code></pre> Answer a question using retrieved context and an LLM.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>question (str) \u2013 End\u2011user question.</p> </li> <li> <p>llm (GenericLLMClient, optional) \u2013 LLM to use; defaults to the facade\u2019s configured client.</p> </li> <li> <p>style ({\"concise\",\"detailed\"}) \u2013 Answer verbosity/style.</p> </li> <li> <p>with_citations (bool) \u2013 Whether to include resolved citations.</p> </li> <li> <p>k (int) \u2013 Retrieval depth (default 6).</p> </li> </ul> <p>Returns dict \u2013 <code>{ \"answer\": str, \"citations\": [...], \"usage\": {...}, \"resolved_citations\": [...]? }</code>.</p> <p>Behavior - Builds a context block from top\u2011k chunks (numbered <code>[1]</code>, <code>[2]</code>, ...).</p> <ul> <li>Prompts the LLM to answer only from the provided context and cite chunk numbers.</li> </ul>"},{"location":"reference/context-rag/#ragresolve_citations","title":"rag.resolve_citations","text":"<p><pre><code>resolve_citations(corpus_id: str, citations: list[dict]) -&gt; list[dict]\n</code></pre> Resolve citation metadata for display/download.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>citations (list[dict]) \u2013 Items like <code>{ \"chunk_id\", \"doc_id\", \"rank\" }</code>.</p> </li> </ul> <p>Returns list[dict] \u2013 Sorted by <code>rank</code>, each <code>{ rank, doc_id, title, uri, chunk_id, snippet }</code>.</p>"},{"location":"reference/context-rag/#practical-examples","title":"Practical examples","text":"<p>1) Create a corpus and ingest docs <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"rag_ingest\")\nasync def rag_ingest(*, context):\n    await context.rag().add_corpus(\"notes\")\n    stats = await context.rag().upsert_docs(\n        corpus_id=\"notes\",\n        docs=[\n            {\"text\": \"Optics basics: Snell's law relates angles of incidence and refraction.\" , \"title\": \"optics\"},\n            {\"path\": \"/data/papers/holography.md\", \"labels\": {\"topic\": \"holography\"}},\n        ],\n    )\n    await context.channel().send_text(f\"RAG upsert: {stats}\")\n</code></pre></p> <p>2) Search and preview hits <pre><code>@graph_fn(name=\"rag_search_preview\")\nasync def rag_search_preview(*, context, q: str):\n    hits = await context.rag().search(corpus_id=\"notes\", query=q, k=5)\n    for i, h in enumerate(hits, 1):\n        await context.channel().send_text(f\"[{i}] score={h.score:.3f}  doc={h.doc_id}\\n{h.text[:200]}\")\n</code></pre></p> <p>3) Answer with citations <pre><code>@graph_fn(name=\"rag_answer_with_citations\")\nasync def rag_answer_with_citations(*, context, q: str):\n    out = await context.rag().answer(corpus_id=\"notes\", question=q, style=\"concise\", k=6)\n    ans = out.get(\"answer\", \"\")\n    cites = out.get(\"resolved_citations\", [])\n    await context.channel().send_text(ans)\n    for c in cites[:3]:\n        await context.channel().send_text(f\"[#{c['rank']}] {c['title']} \u2014 {c['snippet']}\")\n</code></pre></p>"},{"location":"reference/context-rag/#notes-behaviors","title":"Notes &amp; behaviors","text":"<ul> <li> <p>Chunking &amp; Embedding: Documents are split via <code>TextSplitter</code> then embedded in batch; the index stores <code>(chunk_id, vector, meta)</code>.</p> </li> <li> <p>Artifacts: File docs and inline text are persisted to the Artifact Store; returned URIs appear in doc metadata and resolved citations.</p> </li> <li> <p>IDs: <code>doc_id</code> and <code>chunk_id</code> are stable SHA\u2011derived IDs; re\u2011ingesting the same content usually yields the same IDs (subject to meta changes).</p> </li> <li> <p>Filters: <code>filters</code> is reserved for future adapter support (label\u2011based narrowing).</p> </li> <li> <p>LLM &amp; Usage: <code>answer()</code> returns provider usage where available; some providers may omit it.</p> </li> </ul>"},{"location":"reference/decorators/","title":"Decorator API \u2014 <code>@graph_fn</code>, <code>@graphify</code>, <code>@tool</code>","text":"<p>A single reference page for the three core decorators you\u2019ll use to build with AetherGraph.</p>"},{"location":"reference/decorators/#quick-chooser","title":"Quick chooser","text":"Use this when\u2026 Pick Why You want the quickest way to make a Python function runnable as a graph entrypoint and get a <code>context</code> for services <code>@graph_fn</code> Small, ergonomic, ideal for tutorials, notebooks, single\u2011entry tools/agents You need to expose reusable steps with typed I/O that can run standalone or as graph nodes <code>@tool</code> Dual\u2011mode decorator; gives you fine control of inputs/outputs; portable and composable Your function body is mostly tool wiring (fan\u2011in/fan\u2011out) and you want a static graph spec from Python syntax <code>@graphify</code> Author graphs declaratively; returns a <code>TaskGraph</code> factory; great for orchestration patterns"},{"location":"reference/decorators/#graph_fn","title":"<code>@graph_fn</code>","text":"<p>Wrap a normal async function into a runnable graph with optional <code>context</code> injection.</p>"},{"location":"reference/decorators/#signature","title":"Signature","text":"<pre><code>@graph_fn(name: str, *, inputs: list[str] | None = None, outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\nasync def my_fn(..., *, context: NodeContext): ...\n</code></pre>"},{"location":"reference/decorators/#description","title":"Description","text":"<ul> <li>Builds a fresh <code>TaskGraph</code> under the hood and executes it immediately.</li> <li>If your function signature includes <code>context: NodeContext</code>, AetherGraph injects a <code>NodeContext</code> so you can call <code>context.channel()</code>, <code>context.memory()</code>, <code>context.artifacts()</code>, <code>context.llm()</code>, etc.</li> <li>Ideal for single\u2011file demos, CLI/notebook usage, and simple agents.</li> </ul>"},{"location":"reference/decorators/#parameters","title":"Parameters","text":"<ul> <li>name (str, required) \u2014 Graph ID and human\u2011readable name.</li> <li>inputs (list[str], optional) \u2014 Declared input keys. Purely declarative; your function still gets normal Python args.</li> <li>outputs (list[str], optional) \u2014 Declared output keys. If you return a single literal, declare exactly one.</li> <li>version (str, optional) \u2014 Semantic version for registry.</li> <li>agent (str, optional) \u2014 If provided, registers this graph function as an agent under the given name.</li> </ul>"},{"location":"reference/decorators/#returns","title":"Returns","text":"<ul> <li>The decorator returns a <code>GraphFunction</code> object. Calling/awaiting it executes the graph and returns a <code>dict</code> of outputs keyed by <code>outputs</code>.</li> </ul>"},{"location":"reference/decorators/#minimal-example","title":"Minimal example","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"hello.world\", inputs=[\"name\"], outputs=[\"greeting\"], version=\"0.1.0\")\nasync def hello_world(name: str, *, context: NodeContext):\n    await context.channel().send_text(f\"\ud83d\udc4b Hello {name}\")\n    return {\"greeting\": f\"Hello, {name}!\"}\n\n# Run (async)\nres = await hello_world(name=\"Aether\")\nprint(res[\"greeting\"])  # \u2192 \"Hello, Aether!\"\n</code></pre>"},{"location":"reference/decorators/#tips","title":"Tips","text":"<ul> <li>Return a <code>dict</code> where keys match <code>outputs</code>. If you return a single literal, declare one output.</li> <li>You can call <code>@tool</code> functions inside a <code>@graph_fn</code> (they\u2019ll run immediately, not build nodes). Use this for small, fast helper steps.</li> <li>For complex orchestration (fan\u2011in/fan\u2011out), prefer <code>@graphify</code> so <code>@tool</code> calls become nodes.</li> </ul>"},{"location":"reference/decorators/#tool","title":"<code>@tool</code>","text":"<p>Dual\u2011mode decorator for reusable steps with explicit inputs/outputs.</p>"},{"location":"reference/decorators/#signature_1","title":"Signature","text":"<pre><code>@tool(outputs: list[str], *, inputs: list[str] | None = None, name: str | None = None, version: str = \"0.1.0\")\ndef/async def my_tool(...): ...\n</code></pre>"},{"location":"reference/decorators/#description_1","title":"Description","text":"<ul> <li>Immediate mode (no builder/interpreter active): calling the function executes it now and returns a <code>dict</code> of outputs.</li> <li>Graph mode (inside a <code>graph(...)</code> / <code>@graphify</code> body or during <code>@graph_fn</code> build): calling the proxy adds a node to the current graph and returns a <code>NodeHandle</code> with typed outputs.</li> <li>Registers the underlying implementation in the runtime registry for portability.</li> </ul>"},{"location":"reference/decorators/#parameters_1","title":"Parameters","text":"<ul> <li>outputs (list[str], required) \u2014 Names of output values (e.g., <code>[\"result\"]</code>, <code>[\"image\", \"stats\"]</code>).</li> <li>inputs (list[str], optional) \u2014 Input names (auto\u2011inferred from signature if omitted).</li> <li>name (str, optional) \u2014 Registry/display name; defaults to function name.</li> <li>version (str, optional) \u2014 Semantic version for registry.</li> </ul>"},{"location":"reference/decorators/#returns_1","title":"Returns","text":"<ul> <li>In immediate mode: <code>dict</code> of outputs.</li> <li>In graph mode: <code>NodeHandle</code> with <code>.out_key</code> attributes (e.g., <code>node.result</code>).</li> </ul>"},{"location":"reference/decorators/#example-reusable-step","title":"Example \u2014 reusable step","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"sum\", \"count\"])\ndef aggregate(xs: list[int]):\n    return {\"sum\": sum(xs), \"count\": len(xs)}\n\n# Immediate mode\nprint(aggregate([1,2,3]))  # {\"sum\": 6, \"count\": 3}\n</code></pre>"},{"location":"reference/decorators/#example-using-tool-inside-graph_fn-immediate-execution","title":"Example \u2014 using <code>@tool</code> inside <code>@graph_fn</code> (immediate execution)","text":"<pre><code>from aethergraph import graph_fn, tool, NodeContext\n\n@tool(outputs=[\"sum\"])  \ndef add(x: int, y: int):\n    return {\"sum\": x + y}\n\n@graph_fn(name=\"calc.pipeline\", inputs=[\"a\",\"b\"], outputs=[\"total\"])\nasync def calc(a: int, b: int, *, context: NodeContext):\n    out = add(a, b)                 # immediate mode here\n    await context.channel().send_text(f\"sum = {out['sum']}\")\n    return {\"total\": out[\"sum\"]}\n</code></pre>"},{"location":"reference/decorators/#tips_1","title":"Tips","text":"<ul> <li>Use <code>@tool</code> to make steps portable and inspectable (typed I/O makes graphs predictable).</li> <li>In <code>@graph_fn</code> the <code>@tool</code> call executes immediately; in <code>@graphify</code> the same call becomes a graph node.</li> <li>Control\u2011flow knobs like <code>_after</code>, <code>_id</code>, <code>_alias</code> apply only in graph\u2011building contexts (e.g., <code>@graphify</code>), not in <code>@graph_fn</code> bodies.</li> </ul>"},{"location":"reference/decorators/#graphify","title":"<code>@graphify</code>","text":"<p>Author a static TaskGraph by writing normal Python that calls <code>@tool</code>s. The function body executes during build to register nodes and edges; returned node handles/literals define graph outputs.</p>"},{"location":"reference/decorators/#signature_2","title":"Signature","text":"<pre><code>@graphify(*, name: str = \"default_graph\", inputs: Iterable[str] | dict = (), outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\ndef my_graph(...):\n    ...  # body calls @tool proxies (graph mode)\n    return {...}  # NodeHandle(s) and/or literal refs\n</code></pre>"},{"location":"reference/decorators/#description_2","title":"Description","text":"<ul> <li>The decorated function becomes a factory: calling <code>my_graph.build()</code> returns a <code>TaskGraph</code> spec.</li> <li>When the body runs under the builder, calls to <code>@tool</code> proxies add nodes to the graph and return <code>NodeHandle</code>s.</li> <li>Perfect for fan\u2011out (parallel branches) and fan\u2011in (join/aggregate) patterns.</li> </ul>"},{"location":"reference/decorators/#parameters_2","title":"Parameters","text":"<ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (iterable[str] or dict) \u2014 Required/optional input names. If dict, keys are optional names with defaults in the body.</li> <li>outputs (list[str], optional) \u2014 Names of exposed boundary outputs. If body returns a single literal, declare exactly one.</li> <li>version (str) \u2014 Semantic version.</li> <li>agent (str, optional) \u2014 Register this graph as an agent (factory registered).</li> </ul>"},{"location":"reference/decorators/#returns_2","title":"Returns","text":"<ul> <li> <p>The decorator returns a builder function with:</p> </li> <li> <p><code>.build() -&gt; TaskGraph</code></p> </li> <li><code>.spec() -&gt; TaskGraphSpec</code></li> <li><code>.io() -&gt; IO signature</code></li> </ul>"},{"location":"reference/decorators/#example-fanout-fanin","title":"Example \u2014 fan\u2011out + fan\u2011in","text":"<pre><code>from aethergraph import graphify, tool\n\n@tool(outputs=[\"y\"])\ndef f(x: int):\n    return {\"y\": x * x}\n\n@tool(outputs=[\"z\"])\ndef g(x: int):\n    return {\"z\": x + 1}\n\n@tool(outputs=[\"sum\"])  \ndef add(a: int, b: int):\n    return {\"sum\": a + b}\n\n@graphify(name=\"fan_in_out\", inputs=[\"x\"], outputs=[\"total\"]) \ndef pipe(x):\n    a = f(x=x)          # node A (graph mode)  \u2510\n    b = g(x=x)          # node B (graph mode)  \u2518  \u2190 fan\u2011out\n    c = add(a=a.y, b=b.z)   # node C depends on A,B \u2190 fan\u2011in\n    return {\"total\": c.sum}\n\nG = pipe.build()\n</code></pre>"},{"location":"reference/decorators/#example-ordering-with-_after-and-aliasing","title":"Example \u2014 ordering with <code>_after</code> and aliasing","text":"<pre><code>@tool(outputs=[\"out\"]) \ndef step(name: str):\n    return {\"out\": name}\n\n@graphify(name=\"ordered\", inputs=[]) \ndef ordered():\n    a = step(name=\"A\", _alias=\"first\")\n    b = step(name=\"B\", _after=a)\n    c = step(name=\"C\", _after=[a, b], _id=\"third\")\n    return {\"final\": c.out}\n\nG = ordered.build()\n</code></pre>"},{"location":"reference/decorators/#using-tool-inside-graph_fn-vs-graphify","title":"Using <code>@tool</code> inside <code>@graph_fn</code> vs <code>@graphify</code>","text":"<ul> <li>Inside <code>@graph_fn</code>: <code>@tool</code> calls execute immediately (no <code>_after</code>/alias). Great for quick helpers.</li> <li>Inside <code>@graphify</code>: <code>@tool</code> calls define nodes (support <code>_after</code>, <code>_alias</code>, <code>_id</code>, <code>_labels</code>). Ideal for orchestration.</li> </ul>"},{"location":"reference/decorators/#interop-best-practices","title":"Interop &amp; best practices","text":"<ol> <li>Start simple with <code>@graph_fn</code> \u2014 it\u2019s the easiest way to get <code>context</code> and ship a working demo.</li> <li>Extract reusable steps with <code>@tool</code> \u2014 typed I/O makes debugging, tracing, and promotion to graphs trivial.</li> <li> <p>Promote to <code>@graphify</code> when you need:</p> </li> <li> <p>Parallel branches (fan\u2011out), joins (fan\u2011in)</p> </li> <li>Explicit ordering with <code>_after</code></li> <li>Reuse via <code>NodeHandle</code> composition and aliasing</li> <li> <p>Context access:</p> </li> <li> <p><code>@graph_fn</code> gives you <code>context: NodeContext</code> directly.</p> </li> <li>In <code>@graphify</code>, nodes don\u2019t get <code>context</code>; tools run with context at execution time when the graph is interpreted. Use <code>@tool</code> implementations to call <code>context.*</code>.</li> <li>Outputs discipline \u2014 keep outputs small and typed (e.g., <code>{ \"image\": ref, \"metrics\": {\u2026} }</code>).</li> <li>Registry \u2014 all three decorators register artifacts (graph fn as runnable, tool impls, graph factories) so you can call by name later.</li> </ol>"},{"location":"reference/decorators/#see-also","title":"See also","text":"<ul> <li>Quick Start: install, start server, first <code>@graph_fn</code>.</li> <li>**Contex</li> </ul>"},{"location":"reference/rest-api/","title":"REST API","text":"<ul> <li><code>GET /health</code> \u2192 200 OK</li> <li><code>POST /execute</code> \u2192 Execute a graph function</li> <li><code>GET/PUT /artifacts/*</code> \u2192 Retrieve/store artifacts</li> </ul> <p>(Add OpenAPI/Redoc when ready.)</p>"},{"location":"reference/tools-facade/","title":"Tools Facade","text":""},{"location":"reference/tools-facade/#registerfunc-namenone-inputsnone-outputsnone-str","title":"register(func, *, name=None, inputs=None, outputs=None) \u2192 str","text":"<p>Registers a tool and returns its name/id.</p>"},{"location":"reference/tools-facade/#callname-args-dict-dict","title":"call(name, args: dict) \u2192 dict","text":"<p>Invokes a tool by name with validated args.</p>"}]}