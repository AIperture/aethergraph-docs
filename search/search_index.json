{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AetherGraph","text":"<p>A Python\u2011first agentic framework for R&amp;D workflows. The core surface is <code>@graph_fn</code> plus Context Services (LLM, memory, channel, artifacts, KV) and Tools for safe capability calls.</p> <p>\ud83d\udc49 Get hands-on in 5 minutes: Quickstart</p> <p>Why AetherGraph? - Python functions as first-class graph nodes (<code>@graph_fn</code>). - Built-in services: no bolt\u2011ons required, but easy to swap. - Artifacts + memory for traceable, reproducible research.</p>"},{"location":"concept/","title":"AetherGraph \u2014 Architecture Overview (1\u2011page)","text":"<p>Goal: Give newcomers a single \"big picture\" of how AetherGraph fits together, then provide a tiny legend so they know what to look up next.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          AetherGraph Runtime                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                    \u2502\n\u2502  Python Code (your repo)                                           \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500          \u2502\n\u2502  @graph_fn nodes           Tools (@tool)            Services       \u2502\n\u2502  (code-native agents)      (reusable ops,           (external ctx) \u2502\n\u2502                            checkpointable)                          \u2502\n\u2502       \u2502                           \u2502                    \u2502            \u2502\n\u2502       \u25bc                           \u25bc                    \u25bc            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  Node Exec   \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  Tool Exec  \u2502      \u2502  Service API \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502        \u2502                                                         \u2502    \u2502\n\u2502        \u25bc                                                         \u2502    \u2502\n\u2502                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502                       \u2502        NodeContext       \u2502  (per node call)  \u2502\n\u2502                       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                   \u2502\n\u2502                       \u2502 channel()   \u2192  chat/CLI/GUI (send/ask)       \u2502\n\u2502                       \u2502 memory()    \u2192  record/recent/query           \u2502\n\u2502                       \u2502 artifacts() \u2192  write/read refs (provenance)  \u2502\n\u2502                       \u2502 kv()        \u2192  small fast key\u2013value          \u2502\n\u2502                       \u2502 logger()    \u2192  structured logs               \u2502\n\u2502                       \u2502 services()  \u2192  external ctx (domain APIs)    \u2502\n\u2502                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502                                \u2502                                     \u2502\n\u2502                                \u25bc                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502               Sidecar / Adapters (inline server)               \u2502 \u2502\n\u2502  \u2502  - Console/CLI channel                                         \u2502 \u2502\n\u2502  \u2502  - Slack / PyQt / HTTP webhooks                                \u2502 \u2502\n\u2502  \u2502  - File/artifact endpoints (optional, later hosted)            \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concept/#legend-skim-first","title":"Legend (skim-first)","text":"<ul> <li><code>@graph_fn</code> (code\u2011native agents): Turn a plain async Python function into a node with a <code>NodeContext</code> injected.</li> <li>Tools (<code>@tool</code>): Small, explicit, reusable operations. Great for checkpoints, retries, and sharing across graphs.</li> <li>NodeContext: Where your node talks to the world: <code>channel()</code>, <code>memory()</code>, <code>artifacts()</code>, <code>kv()</code>, <code>logger()</code>, <code>services()</code>.</li> <li>Channel: Unifies human I/O (console/Slack/PyQt). Use <code>send_text</code>, <code>ask_text</code>, and progress APIs.</li> <li>Memory &amp; Artifacts: Event\u2011first memory with provenance; artifacts store files/results with stable refs.</li> <li>External Context (Services): Register domain services (e.g., job runner, materials DB) so nodes call them like built\u2011ins.</li> <li>Sidecar: Inline server that powers channels/adapters locally; later you can host these endpoints.</li> </ul> <p>Next: See Memory Internals below, then the Submit \u2192 Poll \u2192 Notify tutorial.</p>"},{"location":"concept/#memory-internals-diagram","title":"Memory Internals (diagram)","text":"<p>Goal: Show how event logging, persistence, indices, and optional RAG hang together.</p> <pre><code>              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502          memory().record(...)           \u2502\n              \u2502   kind \u2022 data \u2022 tags \u2022 entities \u2022 ...   \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                   (in\u2011process event stream / bus)\n                                  \u2502\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u25bc                                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  JSONL Persistence Log    \u2502  append\u2011only     \u2502   KV / Indices            \u2502\n\u2502  (provenance timeline)    \u2502  (durable)       \u2502   (fast lookup/filter)    \u2502\n\u2502  e.g., runs/YYYY/MM/*.jsonl\u2502                 \u2502   tags, kinds, entity ids \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                                 \u2502\n          \u2502                                                 \u2502\n          \u25bc                                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Derived Views / Cursors   \u2502  recent(...)     \u2502   Optional RAG Binding    \u2502\n\u2502 e.g., last_by_name,       \u2502  query(...)      \u2502   (vector index)          \u2502\n\u2502 latest_refs_by_kind       \u2502                  \u2502   embed(data/artifacts)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                                 \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba  NodeContext.memory().query(...) \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Record: Write small structured events with <code>kind</code>, <code>data</code>, <code>tags</code>, <code>entities</code>, <code>metrics</code>.</li> <li>Persist: Append to a JSONL log for durability &amp; replay; perfect for provenance.</li> <li>Index: Maintain fast KV/indices for quick filters (by kind/tags/entity/time).</li> <li>RAG (optional): Bind a vector index to selectively embed text/artifact content for semantic search.</li> <li>Query: Use <code>recent</code>, <code>query</code>, and helpers like <code>latest_refs_by_kind</code> to drive summaries &amp; reports.</li> </ul> <p>When to use RAG? When you want semantic retrieval over larger text blocks or artifact\u2011derived content; otherwise rely on indices and tags for speed/clarity.</p>"},{"location":"concept/#tutorial-submit-poll-notify-single-file-runnable","title":"Tutorial \u2014 Submit \u2192 Poll \u2192 Notify (single file, runnable)","text":"<p>Goal: Minimal end\u2011to\u2011end job orchestration with human notification and provenance. Keep it console\u2011only, no external infra.</p> <pre><code># examples/tutorial_submit_poll_notify.py\nfrom __future__ import annotations\nimport asyncio, random, time\nfrom typing import Dict, Optional\n\n# AetherGraph imports (adjust paths/names to your package layout)\nfrom aethergraph import graph_fn, NodeContext\nfrom aethergraph.server import start\nfrom aethergraph.v3.core.runtime.runtime_services import register_context_service\n\n# --- 1) Start the inline sidecar so channel/memory/artifacts work locally ---\nstart()  # prints a local URL; not required to save here\n\n# --- 2) A tiny external service: fake job runner (auto\u2011bound to NodeContext) ---\nclass FakeJobRunner:\n    \"\"\"Pretend to submit a remote job and poll until it finishes.\n    In a real impl, call your cloud API here.\n    \"\"\"\n    def __init__(self):\n        self._jobs: Dict[str, Dict[str, Optional[str]]] = {}\n\n    async def submit(self, spec: Dict) -&gt; str:\n        job_id = f\"job_{int(time.time()*1000)}_{random.randint(100,999)}\"\n        # status can be: queued \u2192 running \u2192 (succeeded | failed)\n        self._jobs[job_id] = {\"status\": \"queued\", \"result\": None}\n        # Background simulation\n        asyncio.create_task(self._simulate(job_id, spec))\n        return job_id\n\n    async def poll(self, job_id: str) -&gt; Dict[str, Optional[str]]:\n        return self._jobs[job_id]\n\n    async def _simulate(self, job_id: str, spec: Dict):\n        # Fake lifecycle with sleeps\n        await asyncio.sleep(0.5)\n        self._jobs[job_id][\"status\"] = \"running\"\n        await asyncio.sleep(1.2)\n        if random.random() &lt; 0.85:\n            self._jobs[job_id][\"status\"] = \"succeeded\"\n            self._jobs[job_id][\"result\"] = f\"Result for {spec.get('name','demo')}\"\n        else:\n            self._jobs[job_id][\"status\"] = \"failed\"\n            self._jobs[job_id][\"result\"] = None\n\n# Register the service under the name \"jobs\" and auto\u2011bind it to context as context.jobs()\nregister_context_service(\"jobs\", FakeJobRunner())\n\n# --- 3) The graph node: submit \u2192 poll \u2192 notify, with artifacts &amp; memory ---\n@graph_fn(name=\"submit_poll_notify\")\nasync def submit_poll_notify(spec: Dict, *, context: NodeContext) -&gt; Dict:\n    ch = context.channel()\n    mem = context.memory()\n    arts = context.artifacts()\n    jobs = context.jobs()  # auto\u2011bound external service\n\n    await ch.send_text(\"Submitting your job\u2026\")\n    job_id = await jobs.submit(spec)\n    await mem.record(kind=\"job_submitted\", data={\"job_id\": job_id, \"spec\": spec}, tags=[\"demo\"])\n\n    # Persist the spec as an artifact\n    spec_ref = await arts.write_text(f\"spec_{job_id}.json\", content=str(spec))\n\n    # Poll until terminal\n    while True:\n        info = await jobs.poll(job_id)\n        status = info.get(\"status\")\n        await ch.send_text(f\"Status: {status}\")\n        if status in {\"succeeded\", \"failed\"}:\n            break\n        await asyncio.sleep(0.6)\n\n    if status == \"succeeded\":\n        result_text = info.get(\"result\") or \"&lt;no result&gt;\"\n        res_ref = await arts.write_text(f\"result_{job_id}.txt\", content=result_text)\n        await mem.record(kind=\"job_succeeded\", data={\"job_id\": job_id, \"result_ref\": res_ref})\n        await ch.send_text(f\"\u2705 Job {job_id} finished. Saved result \u2192 {res_ref}\")\n        return {\"job_id\": job_id, \"status\": status, \"spec_ref\": spec_ref, \"result_ref\": res_ref}\n    else:\n        await mem.record(kind=\"job_failed\", data={\"job_id\": job_id})\n        ans = await ch.ask_text(f\"\u274c Job {job_id} failed. Retry? (yes/no)\")\n        if str(ans).strip().lower().startswith(\"y\"):\n            return await submit_poll_notify(spec=spec, context=context)\n        await ch.send_text(\"Not retrying; stopping here.\")\n        return {\"job_id\": job_id, \"status\": status, \"spec_ref\": spec_ref}\n\n# --- 4) Tiny runner for local testing ---\nif __name__ == \"__main__\":\n    async def main():\n        out = await submit_poll_notify(spec={\"name\": \"toy-sim\", \"steps\": 3})\n        print(\"FINAL OUTPUT:\\n\", out)\n    asyncio.run(main())\n</code></pre>"},{"location":"concept/#what-this-tutorial-demonstrates","title":"What this tutorial demonstrates","text":"<ul> <li>Channel I/O: human\u2011visible status + retry prompt.</li> <li>External Service: a domain API (<code>jobs</code>) registered once, used like a built\u2011in via <code>context.jobs()</code>.</li> <li>Memory: durable events (<code>job_submitted</code>, <code>job_succeeded</code>, <code>job_failed</code>).</li> <li>Artifacts &amp; provenance: spec/result written with stable refs; returned in the node output.</li> <li>Low friction: single file; console channel only; no extra infra.</li> </ul> <p>Next:</p> <ul> <li>Swap <code>FakeJobRunner</code> for your cloud client.</li> <li>Replace <code>ask_text</code> with an approval UI (Slack/PyQt) once you enable those adapters.</li> <li>Emit metrics in <code>mem.record(..., metrics={...})</code> and add a summary node to close the loop.</li> </ul>"},{"location":"external-context-services/","title":"External Context Services (Revised)","text":"<p>Make reusable, lifecycle\u2011aware helpers available as <code>context.&lt;name&gt;</code> inside any <code>@graph_fn</code>.</p> <p>This page explains what an external context service is, why you might use one, how it looks at a high level, and the APIs you\u2019ll use to define and register services. It also clarifies lifecycle behavior today vs. after you add a server/sidecar, and shows how services can access the active <code>NodeContext</code>.</p>"},{"location":"external-context-services/#1-what-is-an-external-context-service","title":"1) What is an external context service?","text":"<p>An external context service is a Python object managed by AetherGraph\u2019s runtime and exposed to your graph functions through the <code>NodeContext</code>. Once registered, you can access it as <code>context.svc(\"name\")</code> or simply <code>context.&lt;name&gt;</code>.</p> <p>Key ideas:</p> <ul> <li>Dependency injection: Centralize clients, caches, and policies in one place and inject them wherever needed.</li> <li>Lifecycle\u2011ready: Services can implement <code>start()</code> and <code>close()</code> for setup/teardown (e.g., open a pool, kick off a background task). Today these hooks are optional and not auto\u2011invoked unless you wire them (see \u00a74.1).</li> <li>Concurrency controls: Built\u2011in mutex and read/write helpers to safely share state across concurrent nodes.</li> <li>Per\u2011run binding: Each call is bound to a <code>NodeContext</code> so the service can access run_id, logger, artifacts, memory, etc.</li> <li>Uniform surface: The same service works in local scripts today and can be proxied or hosted later without changing call sites.</li> </ul> <p>Use services when logic benefits from a long\u2011lived instance, shared state, or orchestration\u2014not for tiny, pure functions (plain imports are fine there).</p>"},{"location":"external-context-services/#2-highlevel-usage-sketch","title":"2) High\u2011level usage sketch","text":"<p>Below is a conceptual outline (intentionally abstract) of how you would define and call a service.</p>"},{"location":"external-context-services/#define-highlevel","title":"Define (high\u2011level)","text":"<pre><code>class MyService(Service):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self._cache = {}\n\n    async def start(self):\n        # optional: warm up connections, threads, or caches\n        ...\n\n    async def close(self):\n        # optional: flush or close resources\n        ...\n\n    async def do_something(self, key: str) -&gt; str:\n        # example: consult cache, maybe call out to an API, return a value\n        ...\n</code></pre>"},{"location":"external-context-services/#register-at-app-startup","title":"Register (at app startup)","text":"<pre><code>register_context_service(\"myservice\", MyService(config={\"mode\": \"dev\"}))\n</code></pre>"},{"location":"external-context-services/#use-in-a-graph-function","title":"Use in a graph function","text":"<pre><code>@graph_fn(name=\"demo\")\nasync def demo(*, context: NodeContext):\n    value = await context.myservice.do_something(\"foo\")\n    return {\"value\": value}\n</code></pre> <p>That\u2019s it: once registered, your service is reachable from any node via <code>context</code>.</p>"},{"location":"external-context-services/#3-why-use-external-context-benefits-use-cases","title":"3) Why use external context? (Benefits + use cases)","text":""},{"location":"external-context-services/#benefits","title":"Benefits","text":"<ul> <li>Replaceable implementations: Swap local vs. remote, mock vs. real, dev vs. prod\u2014without editing call sites.</li> <li>Centralized auth &amp; config: Put tokens, endpoints, retry/timeout policy, telemetry in one place.</li> <li>Lifecycle &amp; performance: Reuse clients, connection pools, thread pools; warm caches once.</li> <li>Concurrency safety: Use the provided <code>critical()</code> mutex or <code>AsyncRWLock</code> to protect shared state.</li> <li>Per\u2011run awareness: Access <code>self.ctx()</code> to reach logger, artifacts, memory, continuations, etc.</li> <li>Future\u2011proof: The same surface can later be proxied (sidecar/hosted) while keeping your graph code unchanged.</li> </ul>"},{"location":"external-context-services/#itemized-scenarios-no-code","title":"Itemized scenarios (no code)","text":"<ul> <li>Model/Tool Clients: Wrap an LLM, embedding service, vector DB, or a simulation engine with retry, rate limit, and consistent API.</li> <li>Job Orchestration: Submit long\u2011running jobs to a queue/cluster and expose <code>submit/status/wait</code> for nodes.</li> <li>Caching/Indexing: Provide a shared in\u2011memory or on\u2011disk cache with strict read (R/W lock) semantics.</li> <li>Policy Enforcement: Centralize tenant limits, quotas, audit logging, and redaction.</li> <li>Data Access Facades: Read domain data (materials table, experiment registry) with local cache + background refresh.</li> <li>Adapters: Present a unified interface over heterogeneous backends (e.g., multiple vendor APIs behind one broker).</li> </ul>"},{"location":"external-context-services/#4-apis-defining-registering-and-binding-services","title":"4) APIs: defining, registering, and binding services","text":"<p>AetherGraph provides small primitives for service registration and a base class with helpful utilities.</p>"},{"location":"external-context-services/#41-lifecycle-today-vs-serversidecar","title":"4.1 Lifecycle (today vs. server/sidecar)","text":"<ul> <li>Today (no server yet): <code>start()</code>/<code>close()</code> exist but are not auto\u2011invoked. You can omit them or leave them as no\u2011ops.</li> <li>When you add a server/sidecar: wire lifecycle once at boot/shutdown (pseudo\u2011code):</li> </ul> <pre><code># After install_services(...) and registrations\nawait start_all_services()\n# ... run your app/sidecar ...\nawait close_all_services()\n</code></pre> <p>Until those hooks are added, services work fine without lifecycle calls.</p>"},{"location":"external-context-services/#42-registry-functions-runtimelevel","title":"4.2 Registry functions (runtime\u2011level)","text":"<ul> <li><code>install_services(container)</code> \u2013 Set the process\u2011wide service container at startup.</li> <li><code>ensure_services_installed(factory)</code> \u2013 Lazily create/install the container if missing.</li> <li><code>register_context_service(name, instance)</code> \u2013 Add a concrete service instance under <code>name</code>.</li> <li><code>get_context_service(name)</code> \u2013 Retrieve a registered instance.</li> <li><code>list_context_services()</code> \u2013 List the names currently registered.</li> </ul>"},{"location":"external-context-services/#43-base-class-service-aka-basecontextservice","title":"4.3 Base class: <code>Service</code> (aka <code>BaseContextService</code>)","text":"<p>The base class gives you batteries\u2011included ergonomics:</p> <ul> <li> <p>Lifecycle</p> </li> <li> <p><code>async def start(self) -&gt; None</code> \u2013 Optional setup hook.</p> </li> <li> <p><code>async def close(self) -&gt; None</code> \u2013 Optional teardown hook.</p> </li> <li> <p>Binding</p> </li> <li> <p><code>def bind(self, *, context: NodeContext) -&gt; Service</code> \u2013 Called by the runtime so <code>self.ctx()</code> works.</p> </li> <li> <p><code>def ctx(self) -&gt; NodeContext</code> \u2013 Access the current node context (logger, memory, artifacts, etc.).</p> </li> <li> <p>Concurrency</p> </li> <li> <p><code>self._lock</code> \u2013 An async mutex available for your own critical sections.</p> </li> <li><code>def critical()(fn)</code> \u2013 Decorator that serializes an async method (easy mutual exclusion).</li> <li> <p><code>class AsyncRWLock</code> \u2013 Many\u2011readers/one\u2011writer lock for shared tables and caches.</p> </li> <li> <p>Offloading</p> </li> <li> <p><code>async def run_blocking(self, fn, *a, **kw)</code> \u2013 Run CPU or blocking I/O on a worker thread (keeps the event loop responsive).</p> </li> </ul>"},{"location":"external-context-services/#44-accessing-services-from-nodes","title":"4.4 Accessing services from nodes","text":"<ul> <li>Dynamic attribute: <code>context.&lt;name&gt;</code> resolves to the registered service (e.g., <code>context.myservice</code>).</li> <li>Explicit lookup: <code>context.svc(\"name\")</code> (equivalent to the dynamic attribute).</li> </ul>"},{"location":"external-context-services/#45-accessing-nodecontext-from-inside-a-service-essential","title":"4.5 Accessing <code>NodeContext</code> from inside a service (essential)","text":"<p>Services frequently need run\u2011scoped utilities (logger, memory, artifacts, kv, llm, rag, etc.). Enable per\u2011call binding so <code>self.ctx()</code> returns the right <code>NodeContext</code>.</p> <p>Use <code>self.ctx()</code> in the service:</p> <pre><code>class MyService(Service):\n    async def do_work(self, x: int) -&gt; int:\n        ctx = self.ctx()  # NodeContext bound for this call\n        ctx.logger().info(\"working\", extra={\"x\": x})\n        await ctx.memory().record(kind=\"note\", data={\"x\": x})\n        uri = ctx.artifacts().put_text(\"result.txt\", f\"value={x}\")\n        return x + 1\n</code></pre>"},{"location":"external-context-services/#46-event-loop-locking-model","title":"4.6 Event loop &amp; locking model","text":"<ul> <li>External services run on the main event loop used by the executing node.</li> <li>Locks (<code>_lock</code>, <code>AsyncRWLock</code>) coordinate on that loop; use <code>run_blocking()</code> for CPU/IO work.</li> </ul>"},{"location":"external-context-services/#5-summary","title":"5) Summary","text":"<p>External context services provide a clean way to share long\u2011lived capabilities across nodes while keeping graph code small and portable:</p> <ul> <li>Inject reusable helpers via <code>context.&lt;name&gt;</code> (or <code>context.svc(name)</code>).</li> <li>Manage concurrency and performance in one place; offload blocking work with <code>run_blocking()</code>.</li> <li>Abstract environments (mock/local/dev/prod) without touching business logic.</li> <li>Bind to <code>NodeContext</code> automatically so services can use logger, memory, artifacts, kv, llm/rag, etc.</li> <li>Lifecycle now vs later: Today you can skip <code>start()</code>/<code>close()</code>; add startup/shutdown hooks when you introduce a server/sidecar.</li> </ul> <p>Use services for shared state, orchestration, specialized clients, or cross\u2011cutting policies. Use plain imports for tiny, stateless helpers.</p>"},{"location":"graph_fn/","title":"Graph Function <code>graph_fn</code> Quickstart &amp; Reference","text":"<p>Make any Python async function a runnable, inspectable Graph Function with a single decorator. You keep normal Python control\u2011flow; AetherGraph wires in runtime services via <code>context</code> and exposes your outputs as graph boundaries.</p>"},{"location":"graph_fn/#tldr","title":"TL;DR","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}! \ud83d\udc4b\")\n    return {\"greeting\": f\"Hello, {name}\"}\n\n# Run (async)\nres = await hello(name=\"Aether\")          # \u2192 {\"greeting\": \"Hello, Aether\"}\n\n# Or run (sync) for quick scripts\nout = hello.sync(name=\"Aether\")            # same result\n</code></pre>"},{"location":"graph_fn/#what-is-a-graph-function","title":"What is a Graph Function?","text":"<p>A Graph Function is a small wrapper around your Python function that:</p> <ul> <li> <p>builds a fresh internal TaskGraph,</p> </li> <li> <p>injects a <code>NodeContext</code> if your function declares <code>*, context</code>,</p> </li> <li> <p>executes your function (awaiting if needed),</p> </li> <li> <p>normalizes the return value into named outputs, and</p> </li> <li> <p>records graph boundary outputs for downstream composition/inspection.</p> </li> </ul> <p>You do not need to learn a new DSL. Write Python; use <code>context.&lt;service&gt;()</code> when you need IO/state.</p>"},{"location":"graph_fn/#decorator-signature","title":"Decorator signature","text":"<pre><code>@graph_fn(\n    name: str,\n    inputs: list[str] | None = None,\n    outputs: list[str] | None = None,\n    version: str = \"0.1.0\",\n    agent: str | None = None,  # optional: also register as an agent name\n)\n</code></pre> <p>Required</p> <ul> <li>name (str) \u2013 Unique identifier for this graph function.</li> </ul> <p>Optional</p> <ul> <li> <p>inputs (list[str]) \u2013 Declares input names for docs/registry (not enforced at call time).</p> </li> <li> <p>outputs (list[str]) \u2013 Declares output names/order; enables single\u2011literal returns.</p> </li> <li> <p>version (str) \u2013 Semantic version for registry/discovery.</p> </li> <li> <p>agent (str) \u2013 Also register in the <code>agent</code> namespace (advanced).</p> </li> </ul>"},{"location":"graph_fn/#function-shape","title":"Function shape","text":"<p><pre><code>@graph_fn(name=\"example\", inputs=[\"x\"], outputs=[\"y\"])\nasync def example(x: int, *, context):\n    # use services via context: channel/memory/artifacts/kv/llm/rag/mcp/logger\n    await context.channel().send_text(f\"x={x}\")\n    return {\"y\": x + 1}\n</code></pre> - Positional/keyword parameters are your API.</p> <ul> <li>Include <code>*, context</code> to receive the <code>NodeContext</code>. If you don\u2019t declare it, nothing is injected.</li> </ul>"},{"location":"graph_fn/#returning-values-normalization-rules","title":"Returning values (normalization rules)","text":"<p>Your return can be:</p> <p>1) Dict of outputs (recommended) <pre><code>return {\"result\": 42, \"note\": \"ok\"}\n</code></pre></p> <p>2) Single literal \u2014 only if you declared exactly one output <pre><code>@graph_fn(name=\"one\", outputs=[\"y\"])\nasync def one(*, context):\n    return 123  # normalized to {\"y\": 123}\n</code></pre></p> <p>3) NodeHandle / Refs (advanced) If you return node handles or refs created by graph utilities, they\u2019re exposed as boundary outputs automatically. For most users, plain dicts/literals are enough.</p> <p>Validation - If <code>outputs</code> are declared, missing keys raise: <code>ValueError(\"Missing declared outputs: ...\")</code>. - Returning a single literal without exactly one declared output raises an error.</p>"},{"location":"graph_fn/#running","title":"Running","text":"<p><pre><code># Async (preferred in apps/servers)\nres = await my_fn(a=1, b=2)\n\n# Sync helper (scripts/CLI/tests)\nout = my_fn.sync(a=1, b=2)\n</code></pre> Internally this builds a fresh runtime environment, constructs a TaskGraph, executes your function in an interpreter, and returns the normalized outputs.</p>"},{"location":"graph_fn/#accessing-context","title":"Accessing Context","text":"<p>Declare <code>*, context</code> to use built\u2011ins: <pre><code>@graph_fn(name=\"report\", outputs=[\"uri\"])\nasync def report(data: dict, *, context):\n    # Log breadcrumbs\n    log = context.logger(); log.info(\"building report\")\n\n    # Save an artifact\n    art = await context.artifacts().save(path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\":\"A\"})\n\n    # Record a typed result in memory\n    await context.memory().write_result(topic=\"report\", outputs=[{\"name\":\"uri\",\"kind\":\"uri\",\"value\": art.uri}])\n\n    # Notify user\n    await context.channel().send_text(f\"Report ready: {art.uri}\")\n    return {\"uri\": art.uri}\n</code></pre></p>"},{"location":"graph_fn/#concurrency-retry-advanced","title":"Concurrency &amp; retry (advanced)","text":"<p><code>GraphFunction.run()</code> accepts knobs used by the interpreter/runtime: <pre><code>await my_fn.run(\n    env=None,                            # supply a prebuilt RuntimeEnv, or let the runner build one\n    retry=RetryPolicy(),                 # backoff/retries for node execution\n    max_concurrency: int | None = None,  # cap parallelism inside the interpreter\n    **inputs,\n)\n</code></pre> For most users, calling <code>await my_fn(...)</code> / <code>.sync(...)</code> is sufficient; the runner chooses sensible defaults.</p>"},{"location":"graph_fn/#minimal-patterns","title":"Minimal patterns","text":"<p>Hello + context <pre><code>@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}\")\n    return {\"greeting\": f\"Hello, {name}\"}\n</code></pre></p> <p>One output (literal) <pre><code>@graph_fn(name=\"square\", outputs=[\"y\"])\nasync def square(x: int, *, context):\n    return x * x\n</code></pre></p> <p>Multi\u2011output dict <pre><code>@graph_fn(name=\"stats\", outputs=[\"mean\",\"std\"])\nasync def stats(xs: list[float], *, context):\n    import statistics as st\n    return {\"mean\": st.mean(xs), \"std\": st.pstdev(xs)}\n</code></pre></p>"},{"location":"graph_fn/#tips-gotchas","title":"Tips &amp; gotchas","text":"<ul> <li>Always include <code>*, context</code> when you need services (channel/memory/llm/etc.).</li> <li>Declare <code>outputs=[...]</code> if you want to return a single literal; otherwise return a dict.</li> <li>Output validation is strict when <code>outputs</code> are declared\u2014return all of them.</li> <li><code>inputs=[...]</code> is for documentation/registry; your Python signature is the source of truth at call time.</li> <li>You can also register the function as an agent by passing <code>agent=\"name\"</code> (covered later).</li> </ul>"},{"location":"graph_fn/#next-steps","title":"Next steps","text":"<ul> <li><code>graphify</code>: combine multiple functions into a larger graph with explicit edges.</li> <li><code>@tool</code>: publish functions as reusable nodes (IO typed), then orchestrate with <code>graphify</code>.</li> <li>Context services: <code>channel</code>, <code>artifacts</code>, <code>memory</code>, <code>kv</code>, <code>llm</code>, <code>rag</code>, `m</li> </ul>"},{"location":"graphify/","title":"AetherGraph \u2014 <code>@graphify</code> (Builder Decorator)","text":"<p><code>@graphify</code> lets you write a plain Python function whose body builds a <code>TaskGraph</code> using tool calls. Instead of executing immediately, the function becomes a graph factory: call <code>.build()</code> to get a concrete graph, <code>.spec()</code> to inspect, and <code>.io()</code> to see its input/output signature.</p>"},{"location":"graphify/#why-graphify-vs-graph_fn","title":"Why <code>graphify</code> vs <code>graph_fn</code>?","text":"Aspect <code>graph_fn</code> <code>graphify</code> Primary purpose Execute now as a single graph node Build a graph (explicit fan\u2011in/fan\u2011out wiring) Return at call Dict of outputs (or awaitable) A builder you later <code>.build()</code> into a graph Control\u2011flow Pythonic, implicit graph behind the scenes Explicit nodes &amp; edges via tool calls (<code>NodeHandle</code>) Best for Orchestration + <code>context.*</code> services Pipelines, DAGs, reusable subgraphs <p>Use <code>graphify</code> when you want:</p> <ul> <li>Multiple tool calls as separate nodes</li> <li>Explicit dependencies (<code>_after</code>) and fan\u2011in/fan\u2011out</li> <li>To inspect/serialize the graph spec for registry/UI</li> <li>To reuse the same pipeline with different inputs</li> </ul> <p>Use <code>graph_fn</code> when you want:</p> <ul> <li>A simple function that runs immediately and returns values</li> <li>Access to <code>context.channel()/memory()/artifacts()/llm()</code> services</li> <li>Minimal ceremony (one decorator and go)</li> </ul>"},{"location":"graphify/#decorator-signature","title":"Decorator Signature","text":"<pre><code>from aethergraph import graphify\n\n@graphify(*, name=\"default_graph\", inputs=(), outputs=None, version=\"0.1.0\", agent: str | None = None)\ndef build_fn(...):\n    ...  # tool calls returning NodeHandles\n    return {\"y\": handle.y}\n</code></pre> <p>Parameters</p> <ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (Iterable[str] or dict) \u2014 Declare required/optional inputs.  </li> <li>If <code>list/tuple</code>: treated as required input names.  </li> <li>If <code>dict</code>: <code>{required_name: ..., ...}</code> for optional mapping; builder will declare required/optional accordingly.</li> <li>outputs (list[str] | None) \u2014 Names to expose. If you return a single literal, you must declare exactly one.</li> <li>version (str) \u2014 Semantic version for registry/spec metadata.</li> <li>agent (str | None) \u2014 Optionally register the built graph under <code>agent</code> namespace.</li> </ul> <p>Return value</p> <p>The decorated symbol becomes a builder function with helpers:</p> <ul> <li><code>.build() -&gt; TaskGraph</code></li> <li><code>.spec() -&gt; GraphSpec</code></li> <li><code>.io() -&gt; IOSignature</code></li> <li>Attributes: <code>.graph_name</code>, <code>.version</code></li> </ul>"},{"location":"graphify/#writing-a-graphify-body","title":"Writing a <code>@graphify</code> Body","text":"<p>Inside the function:</p> <ol> <li>Use <code>arg(\"name\")</code> to reference declared inputs.</li> <li>Call <code>@tool</code> functions (or <code>call_tool(\"pkg.mod:fn\", ...)</code>) \u2014 each returns a <code>NodeHandle</code> in build mode.</li> <li>Return outputs as:</li> <li>A dict mapping names \u2192 <code>NodeHandle</code> outputs or refs/literals, or</li> <li>A single <code>NodeHandle</code> (its outputs will be exposed), or</li> <li>A single literal only if <code>outputs</code> has length 1.</li> </ol> <pre><code>from aethergraph import graphify, tool\nfrom aethergraph.graph import arg\n\n@tool(outputs=[\"embedded\"])\ndef embed(text: str): ...\n\n@tool(outputs=[\"score\"])\ndef score(vec, query_vec): ...\n\n@graphify(name=\"ranker\", inputs=[\"texts\",\"query\"], outputs=[\"scores\"])\ndef ranker(texts, query):\n    q = embed(text=query)\n    # fan\u2011out: call `embed` for each text\n    vecs = [embed(text=t) for t in texts]  # list[NodeHandle]\n    # fan\u2011in: score each against query vec\n    scs = [score(vec=v.embedded, query_vec=q.embedded) for v in vecs]\n    return {\"scores\": [s.score for s in scs]}\n\nG = ranker.build()\n</code></pre>"},{"location":"graphify/#control-dependencies-without-data-edges","title":"Control Dependencies without Data Edges","text":"<p>Use <code>_after</code> when you must enforce order but don\u2019t pass outputs: <pre><code>@tool(outputs=[\"ok\"])\ndef fetch(): return {\"ok\": True}\n\n@tool(outputs=[\"done\"])\ndef train(): return {\"done\": True}\n\n@graphify(name=\"seq\", inputs=[], outputs=[\"done\"])\ndef seq():\n    a = fetch()\n    b = train(_after=a)   # run b after a\n    return {\"done\": b.done}\n</code></pre></p>"},{"location":"graphify/#registration","title":"Registration","text":"<p>If a registry is active, <code>@graphify</code> registers the built graph under <code>nspace=\"graph\"</code> with <code>name</code>/<code>version</code> so it can be listed or launched elsewhere. You can also register it as an <code>agent</code> via the <code>agent=</code> parameter.</p>"},{"location":"graphify/#example-endtoend-pipeline","title":"Example: End\u2011to\u2011End Pipeline","text":"<pre><code>from aethergraph import tool, graphify\nfrom aethergraph.graph import arg\n\n@tool(outputs=[\"rows\"])\ndef load_csv(path: str): ...\n\n@tool(outputs=[\"clean\"])\ndef clean(rows): ...\n\n@tool(outputs=[\"model\"])\ndef train(data): ...\n\n@tool(outputs=[\"uri\"])\ndef report(model): ...\n\n@graphify(name=\"etl_train_report\", inputs=[\"csv_path\"], outputs=[\"uri\"])\ndef etl_train_report(csv_path):\n    raw  = load_csv(path=arg(\"csv_path\"))\n    tidy = clean(rows=raw.rows)\n    mod  = train(data=tidy.clean)\n    rep  = report(model=mod.model)\n    return {\"uri\": rep.uri}\n\nG = etl_train_report.build()\n</code></pre>"},{"location":"graphify/#using-tool-inside-graph_fn-brief","title":"Using <code>@tool</code> Inside <code>@graph_fn</code> (Brief)","text":"<p>While <code>@graph_fn</code> is for immediate execution, you can drop explicit tool nodes inside a <code>graph_fn</code> when you want finer\u2011grained tracing or parallelism:</p> <pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int): return {\"y\": x*x}\n\n@graph_fn(name=\"mix\")\nasync def mix(x: int, *, context):\n    h = square(x=x)                 # schedules a tool node in the implicit graph\n    await context.channel().send_text(\"running square\u2026\")\n    return {\"y\": h.y}               # exposes tool output as graph_fn output\n</code></pre> <p>Prefer <code>@graphify</code> for full pipeline construction; use <code>@graph_fn</code> when you want to orchestrate services (<code>context.*</code>) and run quickly.</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>A 5\u2011minute on\u2011ramp to AetherGraph: install, start the sidecar server, and run your first <code>@graph_fn</code>.</p>"},{"location":"quickstart/#1-install","title":"1) Install","text":"<pre><code>pip install aethergraph\n# or, from source\n# pip install -e .\n</code></pre> <p>Python: 3.10+</p>"},{"location":"quickstart/#2-start-the-sidecar-server-oneliner","title":"2) Start the sidecar server (one\u2011liner)","text":"<p>AetherGraph ships a lightweight sidecar that wires up core services (logger, artifacts, memory, KV, channels, etc.)</p> <pre><code># quickstart_server.py\nfrom aethergraph import start\n\nurl = start(host=\"127.0.0.1\", port=0, log_level=\"warning\")\nprint(\"AetherGraph server:\", url)\n</code></pre> <p>Run it:</p> <pre><code>python quickstart_server.py\n</code></pre> <p>You should see an HTTP URL like <code>http://127.0.0.1:54321</code> printed. (Random free port by default.)</p>"},{"location":"quickstart/#3-your-first-graph-function","title":"3) Your first graph function","text":"<p><code>@graph_fn</code> turns an ordinary async Python function into a runnable graph entrypoint. If you include a <code>context</code> parameter, you get access to built\u2011in services like <code>context.channel()</code> and <code>context.memory()</code>.</p> <pre><code># quickstart_graph_fn.py\nfrom aethergraph import graph_fn, NodeContext\nfrom aethergraph import start\n\n# 1) Start the sidecar so services are available\nstart()\n\n# 2) Define a small graph function\n@graph_fn(name=\"hello.world\", inputs=[\"name\"], outputs=[\"greeting\"], version=\"0.1.0\")\nasync def hello_world(name: str, *, context: NodeContext):\n    # Use the channel to send a message (console by default)\n    await context.channel().send_text(f\"\ud83d\udc4b Hello, {name}! Running graph\u2026\")\n\n    # Do any Python you want here \u2014 call tools, query memory, etc.\n    greeting = f\"Hello, {name}. Nice to meet you from AetherGraph.\"\n\n    # Return outputs as a dict (keys must match `outputs=[...]`)\n    return {\"greeting\": greeting}\n\n# 3) Run it (async wrapper provided)\nif __name__ == \"__main__\":\n    import asyncio\n    async def main():\n        res = await hello_world(name=\"Researcher\")\n        print(\"Result:\", res)\n    asyncio.run(main())\n</code></pre> <p>Run it:</p> <pre><code>python quickstart_graph_fn.py\n</code></pre> <p>You should see a console message from the channel and printed output like:</p> <pre><code>Result: {\"greeting\": \"Hello, Researcher. Nice to meet you from AetherGraph.\"}\n</code></pre>"},{"location":"quickstart/#4-what-just-happened","title":"4) What just happened?","text":"<ul> <li>Sidecar server booted in the background and installed default services (channels, artifacts, memory, KV, logger).</li> <li><code>@graph_fn</code> built a tiny task graph from your function and executed it.</li> <li><code>context.channel()</code> used the default channel (console) to emit a message.</li> </ul> <p>Tip: You can override the channel at call\u2011site with <code>context.channel(\"slack:#research\")</code>, once you\u2019ve configured a Slack adapter.</p>"},{"location":"quickstart/#5-next-steps","title":"5) Next steps","text":"<ul> <li>Add tools with <code>@tool</code> to wrap reusable steps and surface inputs/outputs.</li> <li>Use <code>@graphify</code> for fan\u2011in / fan\u2011out graph construction when the body is mostly tool calls.</li> <li>Explore artifacts (<code>context.artifacts()</code>), memory (<code>context.memory()</code>), and RAG (</li> </ul>"},{"location":"server/","title":"AetherGraph \u2014 Server (Sidecar) Overview","text":"<p>The AetherGraph server is a lightweight sidecar that wires up all runtime services (channels, memory, artifacts, KV, LLM, RAG, MCP, logging, etc.) and exposes a small HTTP/WebSocket surface for adapters and tools. You can run AetherGraph without the server, but the sidecar makes it easy to:</p> <ul> <li>Use GUI/chat adapters (Slack/Telegram/Console UI) that push events back to your runs</li> <li>Host continuation callbacks for <code>ask_text()</code> / <code>ask_approval()</code></li> <li>Centralize service wiring (secrets, paths, corpora, registries)</li> <li>Inspect/trace runs, artifacts, and health in one place</li> </ul> <p>Think of it as your local control plane so your graph functions can stay plain Python.</p>"},{"location":"server/#quick-start","title":"Quick Start","text":"<pre><code>from aethergraph.server import start, stop\n\n# 1) Start the sidecar (in a background thread) and get its base URL\nurl = start(host=\"127.0.0.1\", port=0)   # port=0 \u2192 auto-pick a free port\nprint(\"AetherGraph sidecar:\", url)\n\n# 2) Run your graph functions as usual\nfrom aethergraph import graph_fn\n\n@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}\")\n    return {\"greeting\": f\"Hello, {name}\"}\n\n# ... elsewhere ...\n# res = await hello(name=\"ZC\")\n\n# 3) (Optional) Stop when done (tests/CLI)\nstop()\n</code></pre>"},{"location":"server/#api-start-start_async-stop","title":"API \u2014 <code>start()</code> / <code>start_async()</code> / <code>stop()</code>","text":""},{"location":"server/#start","title":"start","text":"<p><pre><code>start(*, workspace: str = \"./aeg_workspace\", session_id: str | None = None,\n      host: str = \"127.0.0.1\", port: int = 0, log_level: str = \"warning\") -&gt; str\n</code></pre> Start the sidecar in a background thread. Safe to call at the top of scripts or notebook cells.</p> <p>Parameters</p> <ul> <li> <p>workspace (str) \u2013 Root directory for runtime state (artifacts, logs, corpora, temp files). Auto\u2011created.</p> </li> <li> <p>session_id (str, optional) \u2013 Override the logical session. If <code>None</code>, the runtime will create one.</p> </li> <li> <p>host (str) \u2013 Bind address (defaults to loopback).</p> </li> <li> <p>port (int) \u2013 <code>0</code> picks a free port automatically; otherwise bind an explicit port.</p> </li> <li> <p>log_level (str) \u2013 Uvicorn log level (e.g., <code>\"info\"</code>, <code>\"warning\"</code>).</p> </li> </ul> <p>Returns str \u2013 Base URL, e.g., <code>\"http://127.0.0.1:54321\"</code>.</p>"},{"location":"server/#start_async","title":"start_async","text":"<p><pre><code>start_async(**kwargs) -&gt; str\n</code></pre> Async\u2011friendly wrapper that still runs the server in a thread to avoid clashing with your event loop.</p>"},{"location":"server/#stop","title":"stop","text":"<p><pre><code>stop() -&gt; None\n</code></pre> Signal the background server to shut down and join its thread (useful in tests/CI or ephemeral scripts).</p>"},{"location":"server/#why-a-sidecar","title":"Why a sidecar?","text":"<ul> <li>Continuations: <code>context.channel().ask_*</code> creates a continuation token and waits for a resume callback; the server receives user replies (Slack/Telegram/HTTP) and wakes your run.</li> <li>Adapters: chat/file/progress adapters connect over HTTP/WS to publish events (<code>agent.message</code>, <code>agent.progress.*</code>, uploads) into your run.</li> <li>Central config: one place to load settings, secrets, workspace paths, and register services (LLM, RAG, MCP, artifact store, memory backends).</li> <li>Inspection: optional health and tracing endpoints (depending on your app factory) to debug runs locally.</li> </ul>"},{"location":"server/#what-start-actually-does","title":"What <code>start()</code> actually does","text":"<ol> <li>Loads app settings (<code>load_settings()</code>), installs them as current (<code>set_current_settings(...)</code>).</li> <li>Builds a FastAPI app via <code>create_app(workspace=..., cfg=...)</code> \u2014 this registers services and routes.</li> <li>Picks a free port if <code>port=0</code> and launches Uvicorn in a background thread (non\u2011blocking).</li> <li>Returns the base URL so other components (e.g., WS/HTTP MCP clients) can connect.</li> </ol>"},{"location":"server/#typical-usage-patterns","title":"Typical usage patterns","text":""},{"location":"server/#notebooks-quick-scripts","title":"Notebooks &amp; quick scripts","text":"<pre><code>url = start(port=0)\n# \u2026 run several cells that use context.channel()/continuations\n# restart kernel or call stop() when done\n</code></pre>"},{"location":"server/#longrunning-dev-server","title":"Long\u2011running dev server","text":"<ul> <li>Call <code>start(host=\"0.0.0.0\", port=8787, log_level=\"info\")</code> once at process start.</li> <li>Point Slack/Telegram adapters or local tools at <code>http://localhost:8787</code>.</li> </ul>"},{"location":"server/#testsci","title":"Tests/CI","text":"<pre><code>url = start(port=0)\ntry:\n    # run test suite that uses continuations/artifacts\n    ...\nfinally:\n    stop()\n</code></pre>"},{"location":"server/#interop-with-context-services","title":"Interop with context services","text":"<p>Once the sidecar is up, graph functions can rely on bound services:</p> <ul> <li> <p><code>context.channel()</code> \u2013 routes via the server to your chat adapters</p> </li> <li> <p><code>context.artifacts()</code> \u2013 saves to the workspace CAS under the sidecar</p> </li> <li> <p><code>context.memory()</code> \u2013 hotlog/persistence live alongside the server\u2019s config</p> </li> <li> <p><code>context.rag()</code> \u2013 corpora root under workspace; embedders/indices wired here</p> </li> <li> <p><code>context.mcp(...)</code> \u2013 WS/HTTP MCP clients often target sidecar endpoints</p> </li> </ul>"},{"location":"server/#security-notes","title":"Security notes","text":"<ul> <li>Default bind is <code>127.0.0.1</code> (local only). Use <code>0.0.0.0</code> only in trusted networks.</li> <li>Protect WS/HTTP endpoints behind auth headers/tokens if exposing beyond localhost.</li> <li>Never log plaintext API keys; prefer a Secrets store.</li> </ul>"},{"location":"server/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Port already in use: pass <code>port=0</code> or another free port.</li> <li>Nothing happens after ask_text(): ensure the chat adapter posts replies to the sidecar (correct base URL / token).</li> <li>No LLM/kv/rag configured: your <code>create_app()</code> must wire these services (or the accessors will raise \"\u2026 not available\").</li> <li>Jupyter hangs on restart: call <code>stop()</code> before restarting the kernel, or rely on kernel shutdown to terminate the thread.</li> </ul>"},{"location":"server/#minimal-adapter-sketch-optional","title":"Minimal adapter sketch (optional)","text":"<pre><code># Example: WebSocket adapter connecting to sidecar URL\na_sync_ws_client.connect(f\"{url.replace('http','ws')}/events\", headers={\"Authorization\": \"Bearer demo\"})\n# publish OutEvent / listen for Continuation notifications\n</code></pre>"},{"location":"server/#summary","title":"Summary","text":"<p>Run the sidecar server to centralize runtime services, handle continuations/adapters, and keep your graph functions clean. Use <code>start()</code> to launch in\u2011process, <code>start_async()</code> in async apps, and <code>stop()</code> for tests/CI. Configure paths and services once; build everything else in plain Python.</p>"},{"location":"tools/","title":"AetherGraph \u2014 <code>@tool</code> Decorator (Reference &amp; How\u2011to)","text":"<p><code>@tool</code> turns a plain Python function into a tool node that can be executed immediately or added to a graph during build time. You write ordinary Python, declare outputs, and AetherGraph handles result normalization and graph node creation.</p>"},{"location":"tools/#what-is-a-tool","title":"What is a Tool?","text":"<p>A tool is a reusable, IO\u2011typed operation that can be executed on its own or orchestrated inside a graph. Tools are perfect for things like \u201cload CSV\u201d, \u201ctrain model\u201d, \u201cplot chart\u201d, \u201csend_slack\u201d, etc.</p> <ul> <li>Immediate mode (no graph builder active): calling the tool runs the Python function right away and returns a dict of outputs.</li> <li>Graph mode (inside a <code>with graph(...):</code> block or a <code>@graphify</code> body): calling the tool adds a node to the graph and returns a <code>NodeHandle</code> you can wire to other nodes (fan\u2011in/fan\u2011out).</li> <li>Tools automatically register in the runtime registry (<code>nspace=\"tool\"</code>) when a registry is active.</li> </ul> <p>This page covers the simple function form. (The advanced waitable class form is documented separately.)</p>"},{"location":"tools/#decorator-signature","title":"Decorator Signature","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs: list[str], *, inputs: list[str] | None = None,\n      name: str | None = None, version: str = \"0.1.0\")\ndef your_function(...): ...\n</code></pre> <p>Parameters</p> <ul> <li>outputs (list[str], required) \u2014 Declares the output keys your tool will produce.</li> <li>inputs (list[str], optional) \u2014 Explicit input names. Omit to infer from function signature (excluding <code>*args</code>/<code>**kwargs</code>).  </li> <li>name (str, optional) \u2014 Registry/display name. Defaults to the function\u2019s <code>__name__</code>.  </li> <li>version (str, optional) \u2014 Semantic version recorded in the registry (default: <code>\"0.1.0\"</code>).</li> </ul> <p>Return value (call\u2011site dependent)</p> <ul> <li>Immediate mode: returns a <code>dict</code> of outputs.  </li> <li>Graph mode: returns a <code>NodeHandle</code> (or an awaitable handle under an interpreter) to be wired/exposed by the builder.</li> </ul>"},{"location":"tools/#return-normalization","title":"Return Normalization","text":"<p>The wrapped function can return different shapes; the decorator normalizes into a dict that must include every declared output:</p> <ul> <li><code>None</code> \u2192 <code>{}</code></li> <li><code>dict</code> \u2192 used as\u2011is</li> <li><code>tuple</code> \u2192 <code>{\"out0\": v0, \"out1\": v1, ...}</code></li> <li>single value \u2192 <code>{\"result\": value}</code></li> </ul> <p>If any declared <code>outputs</code> are missing from the normalized dict, a <code>ValueError</code> is raised.</p>"},{"location":"tools/#control-keywords-graph-mode","title":"Control Keywords (graph mode)","text":"<p>When calling a tool while building a graph (e.g., inside a <code>with graph(...):</code> or <code>@graphify</code> body), you may pass these special kwargs to influence scheduling/metadata:</p> <ul> <li><code>_after</code> (NodeHandle | list[NodeHandle | node_id]): explicit dependency edges (fan\u2011in).  </li> <li><code>_name</code> (str): display name for UI/spec.  </li> <li><code>_id</code> (str): hard override of the node ID (must be unique in the graph).  </li> <li><code>_alias</code> (str): optional alias for reverse lookups.  </li> <li><code>_labels</code> (Iterable[str]): lightweight tags for search/grouping.</li> </ul> <p>Example:</p> <pre><code>res = my_tool(a=arg_a, b=arg_b, _after=[prev1, prev2], _name=\"preprocess\", _labels=[\"data\",\"prep\"])\n</code></pre> <p>These control keys are stripped before calling your function and only affect graph construction.</p>"},{"location":"tools/#simple-examples","title":"Simple Examples","text":""},{"location":"tools/#1-immediate-execution-no-graph-builder-active","title":"1) Immediate execution (no graph builder active)","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"sum\", \"mean\"])  # outputs you promise to return\ndef stats(xs: list[float]):\n    s = sum(xs)\n    return {\"sum\": s, \"mean\": s / len(xs)}\n\nout = stats([1,2,3,4])   # \u2192 {\"sum\": 10, \"mean\": 2.5}\n</code></pre>"},{"location":"tools/#2-graph-construction-inside-a-builder","title":"2) Graph construction (inside a builder)","text":"<pre><code>from aethergraph import tool\nfrom aethergraph import graphify\nfrom aethergraph.graph import arg  # or from aethergraph.graph.graph_refs import arg\n\n@tool(outputs=[\"y\"])\ndef add(x: int, z: int): return {\"y\": x + z}\n\n@tool(outputs=[\"z\"])\ndef mul(x: int, k: int): return {\"z\": x * k}\n\n@graphify(name=\"pipeline\", inputs=[\"x\"], outputs=[\"y\"])\ndef pipeline(x):\n    a = mul(x=arg(\"x\"), k=2)          # NodeHandle(\"mul_...\")\n    b = add(x=arg(\"x\"), z=a.z)        # depends on `a` automatically via data edge\n    return {\"y\": b.y}\n\nG = pipeline.build()                    # TaskGraph\nspec = pipeline.spec()                  # graph spec for inspection/registry\nio = pipeline.io()                      # IO signature\n</code></pre>"},{"location":"tools/#3-forcing-an-order-with-_after-no-data-edge","title":"3) Forcing an order with <code>_after</code> (no data edge)","text":"<pre><code>@tool(outputs=[\"ok\"])\ndef init(): return {\"ok\": True}\n\n@tool(outputs=[\"ready\"])\ndef warmup(): return {\"ready\": True}\n\n@graphify(name=\"order_demo\", inputs=[], outputs=[\"ready\"])\ndef order_demo():\n    n1 = init()\n    n2 = warmup(_after=n1)   # enforce sequencing without passing data\n    return {\"ready\": n2.ready}\n</code></pre>"},{"location":"tools/#registration-optional","title":"Registration (Optional)","text":"<p>If a runtime registry is active (via <code>current_registry()</code>), the decorator auto\u2011registers your tool under the <code>tool</code> namespace with its <code>name</code> and <code>version</code> so it can be listed and referenced later.</p> <p>You can also call tools by dotted path via <code>call_tool(\"pkg.module:function\", arg1=..., ...)</code> to avoid importing at build sites, but the recommended ergonomic flow is to <code>import</code> the tool and call it directly.</p>"},{"location":"tools/#best-practices","title":"Best Practices","text":"<ul> <li>Keep tools focused and side\u2011effect aware (e.g., write artifacts via <code>context.artifacts()</code> inside <code>@graph_fn</code> wrappers).</li> <li>Always declare <code>outputs</code> and make your function return those keys.</li> <li>Use <code>_after</code> for control dependencies when no data edge exists.</li> <li>Prefer composing tools via <code>@graphify</code> for explicit fan\u2011in/fan\u2011out graphs.</li> <li>Inside <code>@graph_fn</code>, you can call tools to create explicit nodes, but <code>@graph_fn</code> is for immediate orchestration.</li> </ul>"},{"location":"build-graphs/","title":"Build Graphs in AetherGraph","text":"<p>Welcome! This section is the fastest way to grok how to build and run graphs with Python-first ergonomics.</p> <p>We introduce things in the order you will actually use them:</p> <ol> <li><code>@graph_fn</code> \u2014 the on-ramp. Wrap a regular Python function so it runs as a single graph node, with full <code>context.*</code> access. Great for demos, services, notebooks.</li> <li><code>@tool</code> \u2014 make any function a graph node. Use it inside <code>graph_fn</code> for per-step visibility, metrics, artifacts, and reuse.</li> <li><code>@graphify</code> \u2014 build an explicit DAG for fan-out/fan-in, ordering via <code>_after</code>, subgraphs, and reuse.</li> </ol> <p>Tip: Start with <code>@graph_fn</code> (plus a couple of <code>@tool</code> calls). Move to <code>@graphify</code> when you want explicit topology, parallel map/reduce, barriers, or long-lived pipelines.</p>"},{"location":"build-graphs/#what-is-a-graph-here","title":"What is a \"graph\" here?","text":"<ul> <li>AetherGraph executes TaskGraphs \u2014 directed acyclic graphs of nodes.</li> <li>A node can be:</li> <li>a graph function (<code>@graph_fn</code>) \u2014 runs immediately and can call context services.</li> <li>a tool node (<code>@tool</code>) \u2014 a typed, reusable operation with visible inputs/outputs.</li> <li>The Context (<code>context.*</code>) gives every node uniform access to runtime services:   <code>channel()</code>, <code>artifacts()</code>, <code>memory()</code>, <code>kv()</code>, <code>llm()</code>, <code>rag()</code>, <code>mcp()</code>, <code>logger()</code>.</li> </ul>"},{"location":"build-graphs/#quickstart-30-lines","title":"Quickstart (30 lines)","text":"<pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int):\n    return {\"y\": x * x}\n\n@graph_fn(name=\"demo\", outputs=[\"y\"])\nasync def demo(x: int, *, context):\n    await context.channel().send_text(f\"Computing square of {x}\u2026\")\n    h = square(x=x)              # creates a node you can inspect later\n    await context.channel().send_text(\"Done.\")\n    return {\"y\": h.y}            # expose tool output\n</code></pre> <p>Why this design? - You get instant run semantics (like a normal async function), but steps you mark with <code>@tool</code> become visible graph nodes with metrics/artifacts. - When your flow grows and needs explicit fan-out/fan-in or ordering, switch to <code>@graphify</code>.</p>"},{"location":"build-graphs/#next-steps","title":"Next steps","text":"<ul> <li><code>graph_fn</code> (on-ramp) -&gt; graph_fn.md</li> <li><code>@tool</code> reference -&gt; tool.md</li> <li><code>@graphify</code> (explicit DAG + fan-in/out) -&gt; graphify.md</li> <li>Choosing the right approach -&gt; choosing.md</li> </ul>"},{"location":"build-graphs/choosing/","title":"Choosing: <code>graph_fn</code> vs <code>@graphify</code> vs <code>@tool</code>","text":"<p>Use this one-screen guide to pick the right entry point.</p>"},{"location":"build-graphs/choosing/#start-simple","title":"Start simple","text":"<ul> <li><code>@graph_fn</code> \u2014 quickest way to ship a working function with <code>context.*</code>. Add a couple of <code>@tool</code> calls inside if you want visible/inspectable steps.</li> </ul>"},{"location":"build-graphs/choosing/#scale-up-when-needed","title":"Scale up when needed","text":"<ul> <li><code>@graphify</code> \u2014 when you need explicit DAG control:</li> <li>fan-out / fan-in / map-reduce</li> <li><code>_after</code> (barriers) and <code>_alias</code>/<code>_labels</code> for orchestration and UI</li> <li>subgraph reuse and IO/spec inspection</li> </ul>"},{"location":"build-graphs/choosing/#tool-is-a-building-block","title":"<code>@tool</code> is a building block","text":"<ul> <li>Wrap any function to make it a typed node.</li> <li>Works in both: inside <code>@graph_fn</code> (immediate run, visible steps) and in <code>@graphify</code> (adds nodes to DAG).</li> <li>Control kwargs (<code>_after</code>, <code>_alias</code>, <code>_labels</code>, <code>_id</code>) apply only in graph build contexts.</li> </ul>"},{"location":"build-graphs/choosing/#quick-comparison","title":"Quick comparison","text":"Capability <code>@graph_fn</code> <code>@graphify</code> <code>@tool</code> Immediate \"just run\" Yes Build first Yes (outside graph) Full <code>context.*</code> access Yes (via <code>context</code>) via tools/subgraphs when called under <code>graph_fn</code> Visible per-step nodes via <code>@tool</code> calls native yes Fan-out / fan-in (map/reduce) limited (Python loops) Yes (concise) building block Control edges (<code>_after</code>/barrier) No Yes Yes in graph build Graph spec/IO inspection implicit Yes (<code>.spec()/.io()</code>) n/a Best for demos, services pipelines, orchestration atomic operations <p>Rule of thumb: Start with <code>@graph_fn</code>. When you feel the need for explicit topology or orchestration, switch the same steps into <code>@graphify</code> using the exact same <code>@tool</code>s.</p>"},{"location":"build-graphs/graph_fn/","title":"<code>@graph_fn</code> \u2014 Python-first on-ramp","text":"<p>Wrap a normal (async) Python function so it runs as a single graph node with full access to <code>context.*</code> services. Return values are exposed as graph outputs.</p>"},{"location":"build-graphs/graph_fn/#signature","title":"Signature","text":"<pre><code>@graph_fn(name: str, *, inputs: list[str] | None = None, outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\ndef|async def fn(..., *, context: NodeContext) -&gt; dict | value | NodeHandle\n</code></pre> <ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (list[str], optional) \u2014 Declared input keys (used for IO spec; optional for quickstart).</li> <li>outputs (list[str], optional) \u2014 Declared output keys (enables single-value return).</li> <li>version (str) \u2014 SemVer for registry/lineage.</li> <li>agent (str, optional) \u2014 If set, register this graph function as an agent (advanced).</li> </ul>"},{"location":"build-graphs/graph_fn/#return-normalization","title":"Return normalization","text":"<ul> <li>dict -&gt; keys become outputs; NodeHandles/Refs are exposed.</li> <li>single value -&gt; allowed only if exactly one <code>outputs</code> key is declared (collapsed to that name).</li> <li>NodeHandle -&gt; its outputs are exposed (single output collapses).</li> </ul>"},{"location":"build-graphs/graph_fn/#using-tool-inside-graph_fn","title":"Using <code>@tool</code> inside <code>graph_fn</code>","text":"<p>You can call <code>@tool</code> functions to create visible/inspectable nodes while keeping immediate Python control flow:</p> <pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int): return {\"y\": x*x}\n\n@graph_fn(name=\"demo\", outputs=[\"y\"])\nasync def demo(x: int, *, context):\n    h = square(x=x)          # creates a node\n    await context.channel().send_text(\"computed\")\n    return {\"y\": h.y}\n</code></pre> <p>Important: Control kwargs like <code>_after</code>, <code>_alias</code>, <code>_labels</code> are only honored in graph build contexts (e.g., <code>@graphify</code>). Inside <code>graph_fn</code>, execution order follows normal Python semantics. If you need control edges without passing data, use <code>@graphify</code>.</p>"},{"location":"build-graphs/graph_fn/#when-to-use-graph_fn","title":"When to use <code>@graph_fn</code>","text":"<ul> <li>Quick demos, notebooks, service-style tasks.</li> <li>One to a few steps, mostly sequential.</li> <li>You want full <code>context.*</code> access and instant execution, with optional visibility via <code>@tool</code> calls.</li> </ul> <p>See also: tool.md, graphify.md.</p>"},{"location":"build-graphs/graphify/","title":"<code>@graphify</code> \u2014 Build an explicit DAG (fan-out, fan-in, ordering)","text":"<p>Use <code>@graphify</code> when you need clear topology: map/fan-out, reduce/fan-in, barriers via <code>_after</code>, subgraphs, or reusable pipelines.</p>"},{"location":"build-graphs/graphify/#signature","title":"Signature","text":"<p><pre><code>@graphify(*, name=\"default_graph\", inputs=(), outputs=None, version=\"0.1.0\", agent: str | None = None)\ndef body(...):\n    # Use tool calls to add nodes and return NodeHandles/Refs\n    return {...}\n</code></pre> - The decorated function returns a builder: call <code>.build()</code> to get a <code>TaskGraph</code> instance; <code>.spec()</code> for a serializable spec; <code>.io()</code> for IO signature.</p>"},{"location":"build-graphs/graphify/#control-edges-and-labels-graph-build-only","title":"Control edges and labels (graph build only)","text":"<p><code>@tool</code> control kwargs are honored here: - <code>_after</code>, <code>_alias</code>, <code>_labels</code>, <code>_id</code>, <code>_name</code></p>"},{"location":"build-graphs/graphify/#patterns","title":"Patterns","text":""},{"location":"build-graphs/graphify/#fan-out-map-over-inputs","title":"Fan-out (map over inputs)","text":"<pre><code>from aethergraph import tool, graphify\n\n@tool(outputs=[\"vec\"])\ndef embed(text: str): ...\n\n@graphify(name=\"fanout_demo\", inputs=[\"texts\"], outputs=[\"vecs\"])\ndef fanout_demo(texts):\n    handles = [embed(text=t) for t in texts]          # fan-out\n    return {\"vecs\": [h.vec for h in handles]}         # expose list of outputs\n</code></pre>"},{"location":"build-graphs/graphify/#fan-in-reduce","title":"Fan-in (reduce)","text":"<pre><code>@tool(outputs=[\"score\"])\ndef dot(a, b): ...\n\n@graphify(name=\"fanin_demo\", inputs=[\"query\", \"vecs\"], outputs=[\"scores\"])\ndef fanin_demo(query, vecs):\n    q = embed(text=query)\n    scores = [dot(a=v, b=q.vec) for v in vecs]        # fan-in through q\n    return {\"scores\": [s.score for s in scores]}\n</code></pre>"},{"location":"build-graphs/graphify/#control-edge-without-data","title":"Control edge without data","text":"<pre><code>@tool(outputs=[\"ok\"])   def init(): ...\n@tool(outputs=[\"done\"]) def train(): ...\n\n@graphify(name=\"order\", outputs=[\"done\"])\ndef order():\n    a = init()\n    b = train(_after=a)            # sequence a -&gt; b\n    return {\"done\": b.done}\n</code></pre>"},{"location":"build-graphs/graphify/#subgraph-reuse-optional","title":"Subgraph reuse (optional)","text":"<p>You can register graphs and call them as nodes (advanced). For most cases, compose <code>@tool</code>s directly inside <code>@graphify</code>.</p>"},{"location":"build-graphs/graphify/#when-to-use-graphify","title":"When to use <code>@graphify</code>","text":"<ul> <li>You need parallelism (map) or aggregation (reduce).</li> <li>You need ordering without data flow (<code>_after</code>/barriers).</li> <li>You want a reusable / inspectable DAG (e.g., schedule in a UI).</li> </ul> <p>See also: graph_fn.md, tool.md, choosing.md.</p>"},{"location":"build-graphs/tool/","title":"<code>@tool</code> \u2014 Turn any function into a graph node","text":"<p>Make a plain function a typed, reusable node with explicit inputs/outputs. Works in both <code>@graph_fn</code> (immediate run with visible steps) and <code>@graphify</code> (graph build).</p>"},{"location":"build-graphs/tool/#decorator","title":"Decorator","text":"<pre><code>@tool(outputs: list[str], inputs: list[str] | None = None, *, name: str | None = None, version: str = \"0.1.0\")\ndef fn(...): ...\n</code></pre> <ul> <li>outputs (list[str]) \u2014 Output field names this tool produces.</li> <li>inputs (list[str], optional) \u2014 Input names; inferred from signature if omitted.</li> <li>name (str, optional) \u2014 Registry name (defaults to function name).</li> <li>version (str) \u2014 SemVer for registry/lineage.</li> </ul>"},{"location":"build-graphs/tool/#return-normalization","title":"Return normalization","text":"<ul> <li><code>None</code> -&gt; <code>{}</code></li> <li><code>dict</code> -&gt; as-is</li> <li><code>tuple</code> -&gt; <code>{ \"out0\": v0, \"out1\": v1, ... }</code></li> <li>single value -&gt; <code>{ \"result\": value }</code></li> </ul> <p>Contract check: Declared <code>outputs</code> must be present in the normalized return, otherwise a <code>ValueError</code> is raised.</p>"},{"location":"build-graphs/tool/#two-modes-same-decorator","title":"Two modes (same decorator)","text":"Where called from Behavior Outside any graph Runs immediately and returns a dict. Inside <code>@graph_fn</code> Creates a node handle you can expose. Inside <code>@graphify</code> Adds a node to the DAG (honors control kw). <p>Control kwargs (graph build only): - <code>_after</code> (NodeHandle | list) \u2014 add control-edge dependency. - <code>_alias</code> / <code>_id</code> \u2014 override node id / alias. - <code>_labels</code> (list[str]) \u2014 annotate node for UI/search. - <code>_name</code> \u2014 display name hint.</p>"},{"location":"build-graphs/tool/#minimal-example","title":"Minimal example","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"y\"])\ndef square(x:int) -&gt; dict:\n    return {\"y\": x*x}\n</code></pre> <p>Use in <code>graph_fn</code> or <code>@graphify</code> as shown in their pages.</p>"},{"location":"key-concepts/agent-via-graph-fn/","title":"Agents via <code>@graph_fn</code>","text":"<p>This page introduces agents in AetherGraph through the <code>@graph_fn</code> decorator, shows how <code>@tool</code> functions become nodes on the fly, and explains why/when to write async functions and how to chain/nest them.</p>"},{"location":"key-concepts/agent-via-graph-fn/#what-is-a-graph_fn","title":"What is a <code>graph_fn</code>?","text":"<p>A <code>graph_fn</code> turns a plain Python function into an agent with access to rich context services (channel, memory, artifacts, logger, etc.). By default, it runs in normal Python runtime\u2014no DAG is constructed just because you called the function. For most agentic workflows, this is sufficient and preferred: you get an ergonomic async function with <code>context</code> methods for interaction, memory, and side effects, without forcing a graph capture.</p>"},{"location":"key-concepts/agent-via-graph-fn/#decorator-signature","title":"Decorator signature","text":"<pre><code>@graph_fn(\n    name: str,\n    inputs: list[str] | None = None,\n    outputs: list[str] | None = None,\n    version: str = \"0.1.0\",\n    agent: str | None = None,  # optional: also register as an agent name\n)\n</code></pre> <p>Required</p> <ul> <li>name (str) \u2013 Unique identifier for this graph function.</li> </ul> <p>Optional</p> <ul> <li>inputs (list[str]) \u2013 Declares input names for docs/registry (not enforced at call time).</li> <li>outputs (list[str]) \u2013 Declares output names/order; enables single\u2011literal returns. Required if you do not return a <code>dict</code></li> <li>version (str) \u2013 Semantic version for registry/discovery.</li> <li>agent (str) \u2013 Also register in the <code>agent</code> namespace (advanced).</li> </ul>"},{"location":"key-concepts/agent-via-graph-fn/#function-shape","title":"Function shape","text":"<pre><code>@graph_fn(name=\"example\")\nasync def example(x: int, *, context: NodeContext):\n    # use services via context: channel/memory/artifacts/kv/llm/rag/mcp/logger\n    await context.channel().send_text(f\"x={x}\")\n    return {\"y\": x + 1}\n</code></pre> <ul> <li>Positional/keyword parameters are your API.</li> <li>Include <code>*, context</code> to receive the <code>NodeContext</code>. If you don\u2019t declare it, nothing is injected.</li> </ul> <p>Minimal example:</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"hello_agent\")\nasync def hello_agent(name: str = \"world\", *, context: NodeContext):\n    # Uses standard Python execution; no nodes are created just by running this.\n    await context.channel().send_text(f\"\ud83d\udc4b Hello, {name}!\")\n    context.memory().write_result(name=\"greet\", value={\"who\": name})\n    context.logger().info(\"Greeted user\", extra={\"name\": name})\n    return {\"message\": f\"Hello, {name}\"}\n</code></pre> <p>Key idea: <code>@graph_fn</code> gives you an agent surface (context + async) while keeping execution lightweight and reactive. The runtime will only add nodes when you explicitly opt in (see \u00a72).</p>"},{"location":"key-concepts/agent-via-graph-fn/#quick-intro-to-tool-nodes-on-the-fly","title":"Quick intro to <code>@tool</code> \u2014 nodes on the fly","text":"<p><code>@tool</code> marks a regular Python function as a tool node. When a <code>graph_fn</code> calls a tool, the runtime creates a node on the fly and can optionally capture its inputs/outputs for inspection, provenance, and later features like persisted waits (introduced later). This is useful when you want traceable state or need to persist / resume around boundaries.</p> <p>Pragmatic guidance: for exploratory, reactive research flows, you often don\u2019t need tools\u2014just call normal Python and use <code>context</code> for I/O, memory, and messaging. Reach for <code>@tool</code> when you want graph/state capture or durability guarantees.</p>"},{"location":"key-concepts/agent-via-graph-fn/#decorator-signature_1","title":"Decorator signature","text":"<pre><code>@tool(\n    outputs: list[str],\n    inputs: list[str] | None = None,\n    *,\n    name: str | None = None,\n    version: str = \"0.1.0\",\n)\n</code></pre> <p>Required</p> <ul> <li>outputs (str) \u2013 List of output keys</li> </ul> <p>Optional</p> <ul> <li>inputs (list[str]) \u2013 Declares input names for docs/registry (not enforced at call time).</li> <li>name (str) \u2013 optional name of a tool.</li> <li>version (str) \u2013 Semantic version for registry/discovery.</li> </ul> <p>Works on any plain Python function.</p> <p>Execution mode:</p> <ul> <li>Graph mode: inside a <code>graph_fn</code>, calling the tool builds a node (returns a handle under the hood).</li> <li>Immediate mode: outside a graph, executes immediately (sync returns <code>dict</code>; async returns awaitable).</li> </ul> <p>You can mix regular Python and <code>@tool</code> calls inside a <code>graph_fn</code>; only the <code>@tool</code> calls create nodes.</p> <pre><code>from typing import List\nfrom aethergraph import tool\n\n@tool(name=\"sum_vec\", outputs=[\"total\"])  # declare outgoing fields\ndef sum_vec(xs: List[float]) -&gt; dict:\n    return {\"total\": float(sum(xs))}\n</code></pre> <p>Use inside a <code>graph_fn</code>:</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"tool_demo\", outputs=[\"total\"])\nasync def tool_demo(values: list[float], *, context: NodeContext):\n    # regular Python is just executed; @tool creates a node and is captured\n    stats = {\"n\": len(values)}                 # no node\n    out = sum_vec(values)                       # \u2190 node is captured\n    await context.channel().send_text(f\"n={stats['n']}, sum={out['total']}\")\n    return {\"total\": out[\"total\"]}\n</code></pre> <p>A slightly richer tool (e.g., lightweight HTTP fetch):</p> <pre><code>from aethergraph import tool\nimport json, urllib.request\n\n@tool(name=\"fetch_json\", outputs=[\"data\"])\ndef fetch_json(url: str) -&gt; dict:\n    with urllib.request.urlopen(url) as r:\n        return {\"data\": json.load(r)}\n</code></pre> <p>Then, in a <code>graph_fn</code>:</p> <pre><code>@graph_fn(name=\"use_fetch\", outputs=[\"data\"]) \nasync def use_fetch(url: str, *, context: NodeContext):\n    res = fetch_json(url)                        # node is created on call\n    context.logger().info(\"fetched\", extra={\"url\": url})\n    return {\"data\": res[\"data\"]}\n</code></pre>"},{"location":"key-concepts/agent-via-graph-fn/#async-functions-chaining-nesting-and-running","title":"Async functions: chaining, nesting, and running","text":"<p>AetherGraph is async-first because agents often:</p> <ul> <li>Wait for user input (<code>ask_text</code>, <code>ask_approval</code>),</li> <li>Perform I/O (HTTP, file ops),</li> <li>Launch parallel sub-steps.</li> </ul>"},{"location":"key-concepts/agent-via-graph-fn/#chaining-and-nesting-graph_fns","title":"Chaining and nesting <code>graph_fn</code>s","text":"<p>You can call one <code>graph_fn</code> from another; each call becomes a child subgraph node.</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"step1\", outputs=[\"y\"])\nasync def step1(x: int, *, context: NodeContext) -&gt; dict:\n    return {\"y\": x + 1}\n\n@graph_fn(name=\"step2\", outputs=[\"z\"])\nasync def step2(y: int, *, context: NodeContext) -&gt; dict:\n    return {\"z\": y * 2}\n\n@graph_fn(name=\"pipeline\", outputs=[\"z\"]) \nasync def pipeline(x: int, *, context: NodeContext) -&gt; dict:\n    a = await step1(x)              # child graph node\n    b = await step2(a[\"y\"])        # child graph node\n    return {\"z\": b[\"z\"]}          # =&gt; (x + 1) * 2\n</code></pre>"},{"location":"key-concepts/agent-via-graph-fn/#fan-out-concurrency","title":"Fan-out / concurrency","text":"<p>Launch independent awaits concurrently with <code>asyncio.gather</code>:</p> <pre><code>import asyncio\n\n@graph_fn(name=\"concurrent_steps\", outputs=[\"r1\",\"r2\"]) \nasync def concurrent_steps(a: int, b: int, *, context: NodeContext) -&gt; dict:\n    r1, r2 = await asyncio.gather(step1(a), step2(b))\n    return {\"r1\": r1[\"y\"], \"r2\": r2[\"z\"]}\n</code></pre>"},{"location":"key-concepts/agent-via-graph-fn/#running-a-graph_fn-like-any-async-function","title":"Running a <code>graph_fn</code> like any async function","text":"<p>In scripts/tests you can <code>await</code> a <code>graph_fn</code> directly from an async context, or use the helper runner:</p> <pre><code># Option A: inside another graph_fn or an async test\nresult = await pipeline(3)\n\n# Option B: top-level runner (e.g., in __main__)\nfrom aethergraph import run\n\nif __name__ == \"__main__\":\n    final = run(pipeline(3))        # drives the event loop\n    print(final)                    # {\"z\": 8}\n</code></pre> <p>The <code>run(...)</code> helper drives the event loop and returns the final result, while the runtime records the graph only around <code>@tool</code>/child graph calls.</p>"},{"location":"key-concepts/agent-via-graph-fn/#summary","title":"Summary","text":"<ul> <li><code>graph_fn</code> wraps a Python function into an async \"agent\" with an injected <code>NodeContext</code> (channel, memory, artifacts, logger, etc.) while executing in normal Python runtime\u2014no graph nodes are created unless you opt in.</li> <li>Use <code>@tool</code> to mark functions as tool nodes; calling a tool inside a <code>graph_fn</code> creates a node on the fly and can capture inputs/outputs for provenance or persistence.</li> <li>You can call one <code>graph_fn</code> from another\u2014each call becomes a child subgraph node\u2014so compose agents naturally.</li> <li>The system is async-first for waiting on user input, performing I/O, and running concurrent work (use asyncio.gather for fan-out).</li> <li>For scripts/tests you can await <code>graph_fn</code>s directly or use <code>run(...)</code> to drive the event loop; prefer plain calls + context for lightweight, reactive workflows and use tools when you need traceability/durability.</li> </ul>"},{"location":"key-concepts/artifacts-memory/","title":"Artifacts and Memory","text":"<p>This section explains two foundational components of AetherGraph\u2019s runtime: Artifacts and Memory. Together, they form the system\u2019s provenance backbone \u2014 ensuring every result, file, and intermediate step can be saved, indexed, and revisited.</p>"},{"location":"key-concepts/artifacts-memory/#1-artifacts-persistent-assets","title":"1. Artifacts \u2014 Persistent Assets","text":"<p>Artifacts are immutable, content-addressed assets produced or consumed by agents and tools. They may be files, directories, JSON payloads, or serialized objects.</p>"},{"location":"key-concepts/artifacts-memory/#why-artifacts-matter","title":"Why Artifacts Matter","text":"<ul> <li>Reproducibility: Everything an agent produces can be saved and reloaded later by URI.</li> <li>Traceability: Each artifact is stamped with <code>{run_id, graph_id, node_id, tool_name, tool_version}</code>.</li> <li>Discoverability: Artifacts are indexed by kind, labels, and metrics, making it easy to search or rank results (e.g., best model checkpoint by validation score).</li> </ul>"},{"location":"key-concepts/artifacts-memory/#artifacts-architecture","title":"Artifacts Architecture","text":"<pre><code>[ Your Agent / Tool ]\n          \u2502   (context)\n          \u25bc\n[ NodeContext ]\n          \u2502\n          \u25bc\n[ context.artifacts()  \u2014 Artifact Facade ]\n     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502               \u2502                \u2502\n     \u2502 save / writer \u2502 stage/ingest   \u2502 list/search/best/pin\n     \u25bc               \u25bc                \u25bc\n[ Artifact Store ]  [ Staging Area ]  [ Artifact Index ]\n   (CAS/FS)             (tmp)            (SQLite/kv)\n</code></pre> <p><code>context.artifacts()</code> saves immutable outputs (CAS URIs) and allows you to query them later. Typical flow: save/writer \u2192 (Store) \u2192 upsert \u2192 (Index) \u2192 search/best. </p>"},{"location":"key-concepts/artifacts-memory/#core-features","title":"Core Features","text":"Method Purpose <code>stage()</code> / <code>stage_dir()</code> Reserve a temporary path for producing files or directories. <code>save(path, kind, labels, metrics, pin)</code> Save an existing file and index it. <code>save_text(content)</code> / <code>save_json(payload)</code> Quickly store small artifacts. <code>writer(kind, planned_ext)</code> Context manager to stream-write binary content safely. <code>list(scope)</code> / <code>search(...)</code> / <code>best(...)</code> Query artifacts by scope, kind, label, or metric. <code>pin(artifact_id)</code> Mark an artifact as retained. <code>to_local_path(uri)</code> Resolve file URIs for local access."},{"location":"key-concepts/artifacts-memory/#examples","title":"Examples","text":""},{"location":"key-concepts/artifacts-memory/#save-a-file","title":"Save a File","text":"<pre><code>@graph_fn(name=\"produce_artifact\", outputs=[\"report_uri\"])\nasync def produce_artifact(*, context):\n    # assume the file has been exported to tmp path \"/tmp/report.pdf\"\n    art = await context.artifacts().save(\n        path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\": \"A\"}\n    )\n    return {\"report_uri\": art.uri}\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#search-after-save","title":"Search After Save","text":"<pre><code>@graph_fn(name=\"search_reports\", outputs=[\"top_uri\"]) \nasync def search_reports(*, context):\n    # Find the best report by a metric (if indexed) within current run\n    top = await context.artifacts().best(\n        kind=\"report\", metric=\"val_score\", mode=\"max\", scope=\"run\"\n    )\n    if top:\n        return {\"top_uri\": top.uri}\n    # Or general search by labels/kind\n    reports = await context.artifacts().search(kind=\"report\", labels={\"exp\": \"A\"})\n    return {\"top_uri\": reports[0].uri if reports else None}\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#2-memory-structured-event-result-log","title":"2. Memory \u2014 Structured Event &amp; Result Log","text":"<p>Memory provides a unified fa\u00e7ade for recording, persisting, and querying events during an agent\u2019s lifetime. It keeps raw logs, typed results, and derived indices in sync.</p>"},{"location":"key-concepts/artifacts-memory/#why-memory-matters","title":"Why Memory Matters","text":"<ul> <li>Provenance: Tracks everything your graph or agent does \u2014 messages, results, metrics.</li> <li>Contextual recall: Enables reactive behavior by letting agents recall previous states.</li> <li>Analytics: Supports summaries, distillation, and RAG promotion for long-term memory.</li> </ul>"},{"location":"key-concepts/artifacts-memory/#core-methods","title":"Core Methods","text":"Method Purpose <code>record_raw(base, text, metrics)</code> Append a low-level event to logs and persistence. <code>record(kind, data, tags, ...)</code> Convenience for structured logging. <code>write_result(topic, inputs, outputs)</code> Log a typed output event; updates indices. <code>recent(kinds, limit)</code> Fetch recent events from the hot log. <code>last_by_name(name)</code> Fetch the last output value by name. <code>latest_refs_by_kind(kind)</code> Retrieve last references of a given kind."},{"location":"key-concepts/artifacts-memory/#memory-architecture","title":"Memory Architecture","text":"<pre><code>[ Your Agent / Tool ]\n          \u2502   (context)\n          \u25bc\n[ NodeContext ]\n          \u2502\n          \u25bc\n[ context.memory()  \u2014 Memory Facade ]\n        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502               \u2502                  \u2502               \u2502\n        \u25bc               \u25bc                  \u25bc               \u25bc\n   [ HotLog ]      [ Persistence ]     [ Indices ]     [ RAG (FAISS) ]\n (ephemeral KV)   (JSONL append-only)   (SQLite)      (vector index)\n        \u2502               \u2502                  \u2502               \u2502\n   recent()/tail   replay/export      last_by_name     search/answer*\n                                      last_outputs     promote_events\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#examples_1","title":"Examples","text":""},{"location":"key-concepts/artifacts-memory/#use-in-a-graph_fn","title":"Use in a graph_fn","text":"<pre><code>@graph_fn(name=\"remember_output\", outputs=[\"y\"])\nasync def remember_output(x: int, *, context):\n    y = x + 1\n    await context.memory().write_result(\n        topic=\"calc\",\n        outputs=[{\"name\": \"y\", \"kind\": \"number\", \"value\": y}]\n    )\n    return {\"y\": y}\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#using-memory-facets","title":"Using Memory Facets","text":"<pre><code># Retrieve recent events\nrecent = await context.memory().recent(limit=10)\n\n# Get last stored output\nlast_y = await context.memory().last_by_name(\"y\")\n\n# Summarize conversation or run\nsummary = await context.memory().distill_rolling_chat(max_turns=20)\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#combining-with-rag","title":"Combining with RAG","text":"<p>Memory can promote important events into a RAG corpus for question\u2011answering and retrieval. RAG requires an LLM configuration (<code>context.llm()</code>), and the default vector index is FAISS.</p> <pre><code># Bind or create a RAG corpus for the \"project\" scope and get its id\ncorpus = await context.memory().rag_bind(scope=\"project\")\n\n# Promote recent events into the bound corpus so they become retrievable\nawait context.memory().rag_promote_events(corpus_id=corpus)\n\n# Query the RAG corpus to answer a question (returns the retrieved/LLM answer)\nans = await context.memory().rag_answer(corpus_id=corpus, question=\"What was the best run?\")\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#takeaways","title":"Takeaways","text":"<ul> <li>Artifacts ensure files and datasets are traceable, reproducible, and searchable.</li> <li>Memory provides structured logging and retrieval of all runtime events.</li> <li>RAG is available out\u2011of\u2011the\u2011box via FAISS (LLM required) and can be swapped via external services.</li> <li>If you have your own memory system, External Context Services let you add custom memory with advanced capability (e.g., FAISS \u2192 managed vector DB, FS CAS \u2192 S3/GCS) without changing the main agent code.</li> </ul> <p>See also:</p> <ul> <li><code>context.artifacts()</code> \u2014 artifact storage fa\u00e7ade</li> <li><code>context.memory()</code> \u2014 session memory fa\u00e7ade</li> <li><code>context.rag()</code> \u2014 retrieval\u2011augmented interface (requires LLM)</li> <li>External Context \u2014 extend/replace local services for scale or compliance</li> </ul>"},{"location":"key-concepts/channels-interaction/","title":"Channels and Interaction","text":"<p>A channel is how your agent talks to the outside world (Slack, Telegram, Console, Web, \u2026). <code>context.channel()</code> returns a ChannelSession \u2014 a small helper you use to send/receive messages, buttons, files, streams, and progress with the same Python API, regardless of adapter.</p> <p>Defaults: If you haven't configured any adapters, AetherGraph uses the console (<code>\"console:stdin\"</code>) as the default channel for all communication. To target Slack/Telegram/Web, see the adapter setup guides and provide credentials/keys; your agent code does not change.</p> <p>In short: Switch destinations without changing your agent logic. Use a bound session for many messages, or override per call.</p>"},{"location":"key-concepts/channels-interaction/#1-what-is-a-channel","title":"1. What is a Channel?","text":"<p>A routing target for interaction. You can invoke a channel session using a key like <code>\"slack:#research\"</code>, or rely on the default configuration.</p> <p>Common keys</p> <ul> <li><code>slack:#research</code>  (channel)</li> <li><code>slack:@alice</code>     (DM)</li> <li><code>telegram:@mybot</code>  (chat)</li> <li><code>console:stdin</code>    (default fallback)</li> </ul> <p>Resolution order</p> <ol> <li>Per\u2011call override \u2192 <code>await context.channel().send_text(\"hi\", channel=\"slack:#alerts\")</code></li> <li>Bound session key \u2192 <code>ch = context.channel(\"slack:#research\"); await ch.send_text(\"hi\")</code></li> <li>Bus default \u2192 <code>services.channels.get_default_channel_key()</code></li> <li>Fallback \u2192 <code>console:stdin</code></li> </ol>"},{"location":"key-concepts/channels-interaction/#2-quick-start","title":"2. Quick Start","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"channel_demo\")\nasync def channel_demo(*, context):\n    ch = context.channel(\"slack:#research\")\n    await ch.send_text(\"Starting experiment\u2026\")\n    resp = await ch.ask_approval(\"Proceed?\", options=[\"Yes\", \"No\"])  # {approved, choice}\n    if resp[\"approved\"]:\n        await ch.send_text(\"Great \u2014 launching run.\")\n</code></pre>"},{"location":"key-concepts/channels-interaction/#3-core-methods","title":"3. Core Methods","text":"Method Purpose <code>send_text(text, *, channel=None)</code> Send a plain text message. <code>send_image(url, *, alt=\"image\", title=None, channel=None)</code> Post an image by URL. <code>send_file(url=None, *, file_bytes=None, filename=\"file.bin\", title=None, channel=None)</code> Upload or attach a file. <code>send_buttons(text, buttons, *, channel=None)</code> Message with interactive buttons. <code>ask_text(prompt, *, timeout_s=3600, channel=None)</code> Ask for free\u2011text (cooperative wait). <code>ask_approval(prompt, options=(\"Approve\",\"Reject\"), *, timeout_s=3600, channel=None)</code> Approve/pick an option. <code>ask_files(prompt, *, accept=None, multiple=True, timeout_s=3600, channel=None)</code> Request file upload(s). <code>ask_text_or_files(prompt, *, timeout_s=3600, channel=None)</code> Let user reply with text or files. <code>stream(channel=None)</code> Async context for incremental token/delta updates. <code>progress(title=\"Working...\", total=None, *, channel=None)</code> Async context for live progress. <p>All ask methods use cooperative waits with continuations; replies are correlated to the originating thread.</p>"},{"location":"key-concepts/channels-interaction/#4-streaming-progress","title":"4. Streaming &amp; Progress","text":"<p>Streams are ideal for tokenized outputs or live logs:</p> <pre><code>@graph_fn(name=\"stream_demo\")\nasync def stream_demo(*, context):\n    async with context.channel().stream() as s:\n        for chunk in [\"Hello\", \" \", \"world\", \"\u2026\"]:\n            await s.delta(chunk)\n        await s.end(\"Hello world!\")\n</code></pre> <p>Progress exposes structured updates (current/total/percent/ETA):</p> <pre><code>@graph_fn(name=\"progress_demo\")\nasync def progress_demo(*, context):\n    async with context.channel().progress(title=\"Crunching\", total=5) as bar:\n        for i in range(5):\n            await bar.update(current=i+1, eta_seconds=(4-i)*0.5, subtitle=f\"step {i+1}/5\")\n        await bar.end(subtitle=\"All set!\", success=True)\n</code></pre>"},{"location":"key-concepts/channels-interaction/#5-file-uploads-mixed-replies","title":"5. File Uploads &amp; Mixed Replies","text":"<p>Request files (optionally with a text note):</p> <pre><code>@graph_fn(name=\"upload_demo\")\nasync def upload_demo(*, context):\n    ans = await context.channel().ask_files(\n        prompt=\"Upload your dataset and add a brief note:\", accept=[\".csv\", \"application/zip\"], multiple=True\n    )\n    # ans = { \"text\": str, \"files\": list[FileRef] }\n    await context.channel().send_text(f\"Received {len(ans['files'])} file(s). Thanks!\")\n</code></pre>"},{"location":"key-concepts/channels-interaction/#6-concurrency-pattern-fanout","title":"6. Concurrency Pattern (fan\u2011out)","text":"<p>You can issue concurrent asks to the same bound session and correlate the replies:</p> <pre><code>import asyncio\n\n@graph_fn(name=\"concurrent_asks\")\nasync def concurrent_asks(*, context):\n    ch = context.channel(\"slack:#research\")\n    async def one(tag):\n        name = await ch.ask_text(f\"[{tag}] What's your name?\")\n        await ch.send_text(f\"[{tag}] thanks, {name}!\")\n        return {tag: name}\n    a, b = await asyncio.gather(one(\"A\"), one(\"B\"))\n    return {\"names\": a | b}\n</code></pre>"},{"location":"key-concepts/channels-interaction/#7-guarantees-notes","title":"7. Guarantees &amp; Notes","text":"<ul> <li>Idempotent upserts: streams/progress use stable keys derived from <code>(run_id, node_id, suffix)</code>.</li> <li>Thread correlation: ask/wait methods bind correlators so replies route to the right node.</li> <li>Adapter agnostic: change destinations by key; your agent code stays the same.</li> </ul> <p>See also: Context Overview \u2192 Channels \u00b7 Continuations &amp; Waits \u00b7 Slack/Telegram/Web adapters</p>"},{"location":"key-concepts/context-services/","title":"Context Overview","text":"<p>Context is the lightweight runtime handle that your agents and tools receive at execution time. It represents the current run, graph, and node scope, and provides access to the runtime\u2019s built\u2011in services (like channels, memory, artifacts, etc.) through a clean, composable Python interface.</p> <p>In essence: Context is what lets AetherGraph \u201ccome alive.\u201d It connects your pure Python logic to external I/O, state, orchestration, and AI\u2011augmented services \u2014 without introducing a new DSL.</p>"},{"location":"key-concepts/context-services/#why-context-matters","title":"Why Context Matters","text":"<p>AetherGraph\u2019s design principle is Python\u2011first orchestration. The context system enables this by:</p> <ul> <li>Decoupling logic from infrastructure \u2013 you can build tools and agents that call <code>context.&lt;service&gt;()</code> without caring about the backend implementation.</li> <li>Maintaining provenance and state \u2013 each call is aware of its <code>run_id</code>, <code>graph_id</code>, and <code>node_id</code>, so results and events are recorded with full traceability.</li> <li>Enabling orchestration without overhead \u2013 context manages message passing, persistence, and coordination automatically.</li> <li>Integrating optional intelligence \u2013 attach LLMs, RAG corpora, or external MCP tool servers only when needed.</li> </ul> <p>Ultimately, <code>NodeContext</code> is what transforms plain async functions into interactive, stateful agents \u2014 giving you the ability to talk, remember, reason, and orchestrate in one consistent runtime.</p>"},{"location":"key-concepts/context-services/#quick-start","title":"Quick Start","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"hello_context\")\nasync def hello_context(*, context):\n    await context.channel().send_text(\"Hello from AetherGraph!\")\n    await context.memory().write_result(\n        topic=\"hello\",\n        outputs=[{\"name\": \"msg\", \"kind\": \"text\", \"value\": \"hello\"}],\n        tags=[\"demo\"],\n    )\n    context.logger().info(\"done\", extra={\"stage\": \"finish\"})\n</code></pre>"},{"location":"key-concepts/context-services/#what-the-context-contains","title":"What the Context Contains","text":"<p>Each <code>NodeContext</code> carries stable identifiers and bound services for the current execution scope.</p> <pre><code>@dataclass\nclass NodeContext:\n    run_id: str\n    graph_id: str\n    node_id: str\n    services: NodeServices  # wiring for all built\u2011ins\n</code></pre> <p>Identifiers</p> <ul> <li>run_id \u2014 unique per execution run</li> <li>graph_id \u2014 which graph this node belongs to</li> <li>node_id \u2014 unique node invocation id</li> </ul>"},{"location":"key-concepts/context-services/#context-methods","title":"Context Methods","text":"<p>AetherGraph divides context services into core, optional, and utility groups.</p>"},{"location":"key-concepts/context-services/#core-services","title":"Core Services","text":"Method Purpose <code>context.channel(key: str | None = None)</code> Message &amp; interaction bus (text, buttons, files, streaming, progress). Defaults to configured channel. <code>context.memory()</code> Session/run memory fa\u00e7ade \u2014 record events, write results, query history, build RAG indices. <code>context.artifacts()</code> Artifact store/index fa\u00e7ade \u2014 save or retrieve files, manage experiment outputs. <code>context.kv()</code> Transient key\u2013value store for coordination, small caches, and ephemeral synchronization. <code>context.logger()</code> Structured Python logger with <code>{run_id, graph_id, node_id}</code> automatically injected."},{"location":"key-concepts/context-services/#optional-services-require-extra-configuration","title":"Optional Services (require extra configuration)","text":"<p>These depend on environment/API keys and are only available if configured at runtime.</p> Method Purpose <code>context.llm(profile=\"default\")</code> LLM client for <code>chat()</code> or <code>embed()</code> operations; plug in OpenAI, Anthropic, or local models. <code>context.rag()</code> Retrieval\u2011augmented generation fa\u00e7ade; create corpora, upsert docs, search, and answer with citations. <code>context.mcp(name)</code> MCP client to connect to external tool servers via stdio/websocket/HTTP."},{"location":"key-concepts/context-services/#utility-helpers","title":"Utility Helpers","text":"Method Purpose <code>context.clock()</code> Clock/time helpers for timestamps, scheduling, and delays. <code>context.continuations()</code> Access to continuation store (usually used indirectly by <code>channel().ask_*</code>). <p>If a service is unavailable, its accessor raises a clear error (e.g., <code>LLMService not available</code>). Configure them at startup or through the environment.</p>"},{"location":"key-concepts/context-services/#typical-patterns","title":"Typical Patterns","text":""},{"location":"key-concepts/context-services/#1-ask-wait-continue","title":"1) Ask \u2192 Wait \u2192 Continue","text":"<pre><code>text = await context.channel().ask_text(\"Provide a dataset path\")\n# runtime yields, persists a continuation, resumes with user input\n</code></pre>"},{"location":"key-concepts/context-services/#2-stream-progress","title":"2) Stream + Progress","text":"<pre><code>async with context.channel().stream() as s:\n    await s.delta(\"Parsing\u2026 \")\n    await s.delta(\"OK. \")\n\nasync with context.channel().progress(title=\"Training\", total=100) as p:\n    for i in range(0, 101, 5):\n        await p.update(current=i)\n</code></pre>"},{"location":"key-concepts/context-services/#3-artifacts-memory","title":"3) Artifacts + Memory","text":"<pre><code>art = await context.artifacts().save(path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\":\"A\"})\nawait context.memory().write_result(\n    topic=\"report\",\n    outputs=[{\"name\": \"uri\", \"kind\": \"uri\", \"value\": art.uri}],\n)\n</code></pre>"},{"location":"key-concepts/context-services/#4-rag-answer-using-llm","title":"4) RAG Answer using LLM","text":"<pre><code>hits = await context.rag().search(\"notes\", query=\"What is MTF?\", k=5)\nans = await context.rag().answer(\"notes\", question=\"What is MTF?\", style=\"concise\")\nawait context.channel().send_text(ans[\"answer\"])\n</code></pre>"},{"location":"key-concepts/context-services/#5-external-tools-via-mcp","title":"5) External Tools via MCP","text":"<pre><code>res = await context.mcp(\"ws\").call(\"search\", {\"q\": \"tolerance analysis\", \"k\": 5})\n</code></pre>"},{"location":"key-concepts/context-services/#custom-context-services","title":"Custom Context Services","text":"<p>AetherGraph\u2019s context system is extensible. You can register your own service and make it available as <code>context.&lt;your_service&gt;()</code>. This allows you to:</p> <ul> <li>Extend the runtime with custom persistence layers, schedulers, or storage models.</li> <li>Encapsulate domain\u2011specific APIs (e.g., simulation, materials database, experiment tracker).</li> <li>Implement persistent stages that survive restarts or act as bridges between distributed components.</li> </ul> <p>Custom contexts are defined and registered via <code>register_context_service()</code> (see the External Context section for details and benefits).</p>"},{"location":"key-concepts/context-services/#philosophy","title":"Philosophy","text":"<ul> <li>Python\u2011first: context calls are helpers, not a DSL. Keep logic in plain Python.</li> <li>Minimal surface: each service has a small, composable API.</li> <li>Composable orchestration: mix local and remote services freely.</li> <li>Swappable backends: replace services (e.g., LLM provider, KV backend) without changing agent code.</li> </ul>"},{"location":"key-concepts/context-services/#see-also","title":"See Also","text":"<ul> <li><code>context.channel()</code> \u2014 cooperative waits, streaming, progress</li> <li><code>context.memory()</code> \u2014 event log, typed results, summaries, RAG helpers</li> <li><code>context.artifacts()</code> \u2014 CAS storage + search</li> <li><code>context.llm()</code> \u2014 chat &amp; embeddings</li> <li><code>context.rag()</code> \u2014 corpora &amp; QA</li> <li><code>context.mcp()</code> \u2014 external tool bridges</li> <li><code>context.kv()</code> \u2014 transient coordination</li> <li><code>context.logger()</code> \u2014 structured logs</li> </ul>"},{"location":"key-concepts/event-driven-waits/","title":"Event\u2011Driven Waits: Cooperative vs Dual\u2011Stage","text":"<p>AetherGraph agents are event\u2011driven: they can pause mid\u2011flow and safely resume when a reply, upload, or callback arrives. There are two complementary wait modes, and you can use them flexibly in both <code>@graph_fn</code> and <code>@graphify</code>\u2011built graphs.</p> <ul> <li>Cooperative waits \u2014 via <code>context.channel().ask_*</code>. Simplest way to prompt + wait in reactive agents.</li> <li>Dual\u2011stage waits \u2014 via <code>@tool</code> nodes that split into Stage A (prompt/setup) and Stage B (resume/produce). Best for static graphs and reliable orchestration.</li> </ul> <p>Flexibility: <code>context.*</code> methods are available inside <code>@tool</code> nodes (therefore inside <code>@graphify</code>). Dual\u2011stage tools can also be <code>await</code>\u2011ed directly inside <code>@graph_fn</code>. In either case, they form a node and persist a continuation.</p>"},{"location":"key-concepts/event-driven-waits/#1-cooperative-waits-channelfirst","title":"1 Cooperative Waits (Channel\u2011first)","text":"<p>What: <code>context.channel().ask_text / ask_approval / ask_files</code> send a prompt and yield until a reply or timeout. The runtime persists a continuation token so the run can resume after restarts.</p> <p>Where: Primarily inside <code>@graph_fn</code>. Can also be called from within a <code>@tool</code> if you want cooperative logic inside a node.</p> <p>Example</p> <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"cooperative_demo\", outputs=[\"msg\"]) \nasync def cooperative_demo(*, context):\n    name = await context.channel().ask_text(\"Your name?\")\n    await context.channel().send_text(f\"Hi, {name}!\")\n    return {\"msg\": f\"greeted:{name}\"}\n</code></pre> <p>Properties</p> <ul> <li>Minimal code, great for exploratory, chat\u2011style agents.</li> <li>Thread/channel\u2011aware correlation.</li> <li>Durable continuations; survives restarts.</li> </ul>"},{"location":"key-concepts/event-driven-waits/#2-dualstage-waits-toolfirst","title":"2 Dual\u2011Stage Waits (Tool\u2011first)","text":"<p>What: A node splits into two stages: A emits the prompt/sets up state, B resumes once the event arrives and produces outputs. Maps cleanly to static DAGs and lets the global scheduler manage resumptions and retries.</p> <p>Use in both places:</p> <ul> <li>In <code>@graphify</code> as standard tool nodes.</li> <li>In <code>@graph_fn</code> with <code>await</code> for immediate use \u2014 they still become nodes under the hood.</li> </ul> <p>Built\u2011in channel tools</p> <pre><code># Use these in either style:\nfrom aethergraph.tools import ask_text, ask_approval, ask_files\n\n# A) Inside a static graph\nfrom aethergraph import graphify\n\n@graphify(name=\"collect_input\", inputs=[], outputs=[\"greeting\"]) \ndef collect_input():\n    name = ask_text(prompt=\"Your name?\")      # node yields \u2192 resumes on reply\n    return {\"greeting\": name.text}\n\n# B) Await directly in a graph_fn\nfrom aethergraph import graph_fn\n\n@graph_fn(name=\"dualstage_in_fn\", outputs=[\"choice\"]) \nasync def dualstage_in_fn(*, context):\n    res = await ask_approval(prompt=\"Proceed?\", options=(\"Yes\",\"No\"))\n    return {\"choice\": res[\"choice\"]}\n</code></pre> <p>Properties</p> <ul> <li>Node\u2011level persistence, retries, and metrics.</li> <li>Works seamlessly with global scheduling (centralized control, resumptions at scale).</li> <li>Great for UI + pipeline hybrids (prompt in Stage A, compute in Stage B).</li> </ul>"},{"location":"key-concepts/event-driven-waits/#3-using-context-inside-graphify","title":"3 Using <code>context.*</code> inside <code>@graphify</code>","text":"<p><code>context</code> methods (channels, memory, artifacts, kv, logger, etc.) are available inside <code>@tool</code> nodes. This means your static graphs can still interact, log, and persist during node execution while retaining DAG inspectability.</p> <pre><code>from aethergraph import tool\n\n@tool(outputs=[\"ok\"]) \nasync def notify_and_tag(*, context):\n    await context.channel().send_text(\"Started node\u2026\")\n    await context.memory().record(kind=\"status\", data={\"stage\":\"start\"})\n    return {\"ok\": True}\n</code></pre>"},{"location":"key-concepts/event-driven-waits/#4-comparison-cooperative-vs-dualstage-vs-manual-checkpoints","title":"4 Comparison: Cooperative vs Dual\u2011Stage vs Manual Checkpoints","text":"Aspect Cooperative (<code>context.channel().ask_*</code>) Dual-Stage (<code>@tool</code> ask_*) Manual checkpoints Authoring style Inline, minimal Explicit node with A/B stages N/A in AG (not built-in) Resumability Hard \u2014 stateless unless save the state to memory manually Native continuations per node (resumeable after restart) Possible but manual/fragile Retry / Idempotency Coarse (re-invoke the whole function) Fine (node-level retry, idempotent resumes) Manual Scale Great for interactive sessions, small graphs Excellent for large runs / thousands of waits Limited by implementation CPU load (waiting) Keeps process / event loop alive; lightweight but not zero Zero CPU \u2014 node is dormant until resumed Depends on checkpointing backend Memory footprint Held in local task heap (light) Released after serialization; only metadata retained Depends on snapshot granularity Disk usage Optional if memory writes used Tiny (~1\u201310 KB per node) \u2014 correlator + inputs serialized Potentially heavy (full state dump) Latency to resume Instant within current process Slightly higher (resume event \u2192 lookup \u2192 dispatch) Potentially high (manual restore) <p>Why Dual\u2011Stage scales</p> <ul> <li>Node\u2011granular control: retries, backoff, and resumption are local to the waiting node.</li> <li>Central orchestration: the global scheduler can queue, shard, or migrate blocked nodes.</li> <li>Observability: each wait is a first\u2011class node with metrics and logs.</li> <li>Determinism: Stage boundaries clarify side\u2011effects and make runs reproducible.</li> </ul> <p>Manual checkpoints (framework\u2011agnostic snapshots) aren\u2019t part of AetherGraph. Dual\u2011stage nodes cover the same reliability space with less boilerplate and better provenance.</p>"},{"location":"key-concepts/event-driven-waits/#5-extending-dualstage-tools","title":"5 Extending Dual\u2011Stage Tools","text":"<p>You can author custom dual\u2011stage nodes with <code>DualStageTool</code> to model your own A/B waits (e.g., submit job \u2192 wait \u2192 collect). Some examples of the usage include </p> <ul> <li>custom channel waits</li> <li>submit/run long simualtion on the cloud</li> <li>data/model training pipeline on external systems</li> <li>external API Polling that reports a compleltion asynchronously</li> </ul> <p>A compact public API for this is planned; detailed docs will ship soon.</p>"},{"location":"key-concepts/event-driven-waits/#6-takeaways","title":"6 Takeaways","text":"<ul> <li>All <code>context.channel().ask_*</code> calls are cooperative waits by default.</li> <li>Dual\u2011stage tools work in both <code>@graphify</code> and <code>@graph_fn</code> (awaitable) and always materialize as nodes.</li> <li>For large, reliable systems: prefer dual\u2011stage for node\u2011level retries, metrics, and scheduler control.</li> <li><code>context.*</code> is available inside <code>@tool</code> nodes, so static graphs can still interact, log, and persist cleanly.</li> <li>Manual checkpointing isn\u2019t needed; dual\u2011stage nodes give better reliability with less boilerplate.</li> </ul>"},{"location":"key-concepts/extending-context/","title":"Extending Context Services","text":"<p>AetherGraph lets you extend the runtime by adding your own <code>context.&lt;name&gt;</code> methods. These external context services are reusable, lifecycle-aware helpers that live alongside the built-ins (<code>channel</code>, <code>memory</code>, <code>artifacts</code>, etc.) and can hold shared state, wrap APIs, or orchestrate external systems \u2014 all without changing your agent code.</p>"},{"location":"key-concepts/extending-context/#what-is-an-external-context-service","title":"What Is an External Context Service?","text":"<p>A context service is a registered Python object that AetherGraph injects into every node\u2019s <code>NodeContext</code>. Once registered, it\u2019s available anywhere inside a graph:</p> <pre><code>@graph_fn(name=\"demo\")\nasync def demo(*, context):\n    val = await context.myservice.do_something(\"foo\")\n    return {\"val\": val}\n</code></pre>"},{"location":"key-concepts/extending-context/#why-use-it","title":"Why Use It","text":"<ul> <li>Reusable helpers \u2013 share clients, caches, or models across nodes.</li> <li>Shared state \u2013 coordinate progress or reuse expensive objects.</li> <li>Centralized config \u2013 keep API keys, timeouts, or policies in one place.</li> <li>Lifecycle control \u2013 optional <code>start()</code>/<code>close()</code> for setup or teardown.</li> <li>Per-run binding \u2013 each call knows its <code>run_id</code>, <code>graph_id</code>, and <code>node_id</code>.</li> </ul> <p>Use a service when you need a long-lived instance or cross-node coordination. For small, stateless helpers, plain imports are simpler.</p>"},{"location":"key-concepts/extending-context/#minimal-example","title":"Minimal Example","text":""},{"location":"key-concepts/extending-context/#define-a-service","title":"Define a Service","text":"<pre><code>from aethergraph.v3.core.runtime.base_service import Service\n\nclass MyService(Service):\n    async def do_something(self, key: str) -&gt; str:\n        ctx = self.ctx()  # current NodeContext\n        ctx.logger().info(f\"working on {key}\")\n        return f\"done:{key}\"\n</code></pre>"},{"location":"key-concepts/extending-context/#register-at-startup","title":"Register at Startup","text":"<pre><code>from aethergraph.v3.core.runtime.runtime_services import register_context_service\nregister_context_service(\"myservice\", MyService())\n</code></pre> <p>Now <code>context.myservice</code> is available to all nodes in that runtime.</p>"},{"location":"key-concepts/extending-context/#when-to-use-custom-services","title":"When to Use Custom Services","text":"Scenario Example Model/Tool Wrappers unify access to LLMs, simulation engines, or APIs Shared Caches memoize expensive lookups or material tables Job Orchestration submit/track remote compute jobs Policy / Governance enforce tenant limits, logging, or auditing Adapters / Brokers expose multiple vendor APIs under one interface"},{"location":"key-concepts/extending-context/#lifecycle-concurrency","title":"Lifecycle &amp; Concurrency","text":"<ul> <li>Services can optionally implement <code>start()</code> / <code>close()</code>; these are manual today but can be auto-wired when running under a server/sidecar.</li> <li>Use <code>self.critical()</code> or <code>AsyncRWLock</code> for safe shared access.</li> <li><code>self.run_blocking(fn)</code> helps offload CPU or blocking I/O.</li> </ul>"},{"location":"key-concepts/extending-context/#takeaways","title":"Takeaways","text":"<ul> <li>External services make AetherGraph modular and composable.</li> <li>They behave like built-in context methods \u2014 bound automatically per node.</li> <li>Use them to manage clients, caches, orchestration, or background work.</li> <li>Configure once; your <code>@graph_fn</code> and <code>@tool</code> logic remains unchanged.</li> </ul> <p>See also: External Context Deep Dive \u2192 for advanced registration, lifecycle management, and hosted modes.</p>"},{"location":"key-concepts/graph-executor/","title":"Graph Executor \u2014 Schedulers","text":"<p>AetherGraph\u2019s execution model uses schedulers to drive node execution, handle dependencies, and coordinate events across graphs. There are two major modes:</p> <ul> <li>Local scheduler \u2013 lightweight, per\u2011graph control loop used by <code>@graph_fn</code>.</li> <li>Global scheduler \u2013 centralized, event\u2011driven scheduler managing all graphs submitted by <code>@graphify</code>.</li> </ul>"},{"location":"key-concepts/graph-executor/#1-local-vs-global-schedulers","title":"1. Local vs Global Schedulers","text":""},{"location":"key-concepts/graph-executor/#local-scheduler-used-by-graph_fn","title":"Local Scheduler (used by <code>@graph_fn</code>)","text":"<ul> <li>Each <code>@graph_fn</code> invocation runs in its own event loop context, orchestrated directly by Python\u2019s async runtime.</li> <li>It executes sequentially or with lightweight concurrency (via <code>await</code> and tasks).</li> <li>No graph queueing \u2014 tool calls execute immediately or schedule via local awaits.</li> <li>Ideal for reactive, interactive agents where quick iteration matters.</li> <li>Regular Python functions (non\u2011tool calls) bypass scheduling entirely, running through the native Python runtime.</li> </ul>"},{"location":"key-concepts/graph-executor/#global-scheduler-used-by-graphify","title":"Global Scheduler (used by <code>@graphify</code>)","text":"<ul> <li>A single global event loop drives all active graphs and nodes across runs.</li> <li>Implements fair, event\u2011driven scheduling with per\u2011run capacity, retry policies, and backoff.</li> <li>Nodes enter the global queue when ready, and yield back control when waiting for continuations or dependencies.</li> <li>Perfect for static DAGs, multi\u2011run orchestration, and large\u2011scale deployments.</li> </ul>"},{"location":"key-concepts/graph-executor/#2-conceptual-diagram","title":"2. Conceptual Diagram","text":"<pre><code>          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502        Local Scheduler     \u2502\n          \u2502  (per @graph_fn runtime)   \u2502\n          \u2502                            \u2502\n   async \u2192\u2502 executes Python awaitables \u2502\n  calls \u2192 \u2502 immediate tool invocations \u2502\n          \u2502  small scale, no registry  \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502 submits TaskGraph (optional)\n                       \u25bc\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502       Global Scheduler     \u2502\n          \u2502 (multi\u2011graph orchestrator) \u2502\n          \u2502  \u2022 One event loop          \u2502\n          \u2502  \u2022 Runs across all graphs  \u2502\n          \u2502  \u2022 Event\u2011driven resumes    \u2502\n          \u2502  \u2022 Backoff &amp; retries       \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"key-concepts/graph-executor/#3-scheduler-algorithm-overview","title":"3. Scheduler Algorithm Overview","text":""},{"location":"key-concepts/graph-executor/#core-loop-type","title":"Core Loop Type","text":"<p>The GlobalForwardScheduler is event\u2011driven, not polling\u2011based.</p> <ul> <li>It sleeps until an event (resume, wakeup, or node completion) is emitted.</li> <li>Once awakened, it drains queued events, checks for ready nodes, and dispatches eligible tasks.</li> <li>Each run has bounded concurrency, and optionally, a global cap across all runs.</li> </ul>"},{"location":"key-concepts/graph-executor/#simplified-flow","title":"Simplified Flow","text":"<pre><code>while not terminated:\n    drain control events (resume, wakeup)\n    schedule resumed nodes first\n    schedule ready nodes (deps satisfied)\n    await first_completed(running_tasks or new events)\n    on node_done \u2192 emit event \u2192 unblock dependents\n</code></pre> <ul> <li>Event sources: resume signals, new runs, wakeups, and node completions.</li> <li>Scheduling order: resumed nodes \u2192 explicitly pending \u2192 dependency\u2011ready nodes.</li> <li>Fairness: round\u2011robin across runs.</li> <li>Idle handling: if no nodes running, block until the next event.</li> </ul>"},{"location":"key-concepts/graph-executor/#4-resource-model","title":"4. Resource Model","text":"Resource Local Scheduler (<code>@graph_fn</code>) Global Scheduler (<code>@graphify</code>) CPU Uses local async tasks; minimal overhead Centralized asyncio event loop with per\u2011run task pools Memory In\u2011process; short\u2011lived per function Persistent per\u2011run state tables (node states, queues) Disk None unless memory/artifact writes used Writes resumable states and continuations (~1\u201310 KB/node) Network Only if context services use remote backends Communicates via Resume/Wakeup buses for distributed runs"},{"location":"key-concepts/graph-executor/#5-why-this-design-works","title":"5. Why This Design Works","text":"<ul> <li>Local scheduler \u2014 fast, minimal overhead, ideal for reactive research and small DAGs.</li> <li>Global scheduler \u2014 durable, resumable, and optimized for long\u2011running or large workflows.</li> <li>Event\u2011driven architecture \u2014 ensures zero busy\u2011waiting and minimal CPU when idle.</li> <li>Per\u2011run independence \u2014 each graph maintains its own capacity, backoff, and retry settings.</li> </ul>"},{"location":"key-concepts/graph-executor/#6-summary","title":"6. Summary","text":"Aspect Local Scheduler (<code>@graph_fn</code>) Global Scheduler (<code>@graphify</code>) Scope One runtime function Many graphs, many runs Control Immediate async execution Central event bus + scheduling queues Scheduling granularity Function level Node level (with retries/resume) Persistence Ephemeral Durable; resumable Best for Interactive or exploratory runs Deterministic, large\u2011scale orchestration <p>In short: <code>@graph_fn</code> runs live, reactive agents; <code>@graphify</code> builds orchestrated pipelines under a global, event\u2011driven scheduler.</p>"},{"location":"key-concepts/server-start-sidecar/","title":"Server (Sidecar) Overview","text":"<p>The AetherGraph server is a lightweight sidecar that boots your runtime services and exposes a tiny HTTP/WebSocket surface for adapters and continuations. You can run <code>@graph_fn</code> without it (pure Python, console I/O), but the sidecar is required for:</p> <ul> <li>Event\u2011driven waits (<code>ask_*</code> via Slack/Telegram/Web UI beyong Console \u2192 resume your run)</li> <li>Centralized service wiring (artifacts, memory, kv, rag, llm, mcp)</li> <li>Global scheduling for <code>@graphify</code> pipelines</li> </ul> <p>In short: start the sidecar when you need real interactions, resumability, or a shared control plane.</p>"},{"location":"key-concepts/server-start-sidecar/#quick-start","title":"Quick Start","text":"<pre><code>from aethergraph.server import start, stop\n\nurl = start(host=\"127.0.0.1\", port=0)  # launches FastAPI+Uvicorn in a background thread\nprint(\"sidecar:\", url)\n\n# ... run @graph_fn / @graphify normally ...\n\nstop()  # optional (useful for tests/CI)\n</code></pre>"},{"location":"key-concepts/server-start-sidecar/#what-start-actually-does","title":"What <code>start()</code> actually does","text":"<ol> <li>Load config &amp; workspace (paths, secrets, profiles) and install them as current settings.</li> <li>Build services and register them (channels, artifacts, memory hotlog/persistence/indices, kv, llm, rag, mcp, logger).</li> <li> <p>Expose endpoints for:</p> <ul> <li>Continuations (resume callbacks for <code>ask_text/approval/files</code>),</li> <li>Adapters (chat/events, uploads, progress),</li> <li>Health/inspect (minimal status routes).</li> </ul> </li> <li> <p>Launch Uvicorn in a background thread and return the base URL.</p> </li> </ol>"},{"location":"key-concepts/server-start-sidecar/#minimal-api","title":"Minimal API","text":""},{"location":"key-concepts/server-start-sidecar/#start-str","title":"<code>start(...) -&gt; str</code>","text":"<p>Starts the sidecar in\u2011process and returns the base URL.</p> <ul> <li><code>workspace</code>: root dir for artifacts/logs/corpora (auto\u2011created)</li> <li><code>host</code>, <code>port</code>: bind address; <code>port=0</code> picks a free port</li> <li><code>log_level</code>: Uvicorn verbosity</li> </ul>"},{"location":"key-concepts/server-start-sidecar/#start_async-str","title":"<code>start_async(...) -&gt; str</code>","text":"<p>Async\u2011friendly variant (still runs the server in a thread).</p>"},{"location":"key-concepts/server-start-sidecar/#stop-none","title":"<code>stop() -&gt; None</code>","text":"<p>Stops the background server (useful in tests/CI).</p>"},{"location":"key-concepts/server-start-sidecar/#when-do-i-not-need-it","title":"When do I not need it?","text":"<ul> <li>Pure <code>@graph_fn</code> runs that only print to console and don\u2019t use <code>ask_*</code>, external channels, or global scheduling. Those run directly in Python\u2019s event loop. </li> </ul>"},{"location":"key-concepts/server-start-sidecar/#when-do-i-need-it","title":"When do I need it?","text":"<ul> <li>Any interactive or resumable flow (cooperative or dual\u2011stage waits).</li> <li>Using Slack/Telegram/Web channels or file uploads.</li> <li>Running static DAGs built with <code>@graphify</code> under the global scheduler.</li> </ul> <p>If you don't know whether to use it, start the server anyway. </p>"},{"location":"key-concepts/server-start-sidecar/#security-networking-short","title":"Security &amp; Networking (short)","text":"<ul> <li>Default bind: <code>127.0.0.1</code> (local only). Use <code>0.0.0.0</code> only on trusted networks.</li> <li>Put auth on WS/HTTP if exposed beyond localhost. Never log plaintext API keys.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/#troubleshooting-quick","title":"Troubleshooting (quick)","text":"<ul> <li>No reply after <code>ask_text()</code> \u2192 adapter isn\u2019t posting resumes to the sidecar URL/token.</li> <li>CORS error from web UI \u2192 allow the UI origin in sidecar settings.</li> <li>Port busy \u2192 pass <code>port=0</code> or another free port.</li> <li>Services unavailable \u2192 ensure your <code>create_app()</code> wires llm/rag/kv/etc., or use defaults.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/#takeaways","title":"Takeaways","text":"<ul> <li>The sidecar is your local control plane: services + continuations + adapters + global scheduling.</li> <li>Start it once with <code>start()</code>; keep your agents plain Python.</li> <li>Use it whenever you need interactions, persistence, or scale.</li> </ul>"},{"location":"key-concepts/services-at-a-glance/","title":"Core Services at a Glance","text":"<p>AetherGraph\u2019s context provides a unified interface to access runtime services \u2014 from lightweight coordination to AI-powered reasoning. This page offers a concise overview of these services with minimal examples, showing what\u2019s available beyond Channels, Artifacts, and Memory.</p> <p>Goal: Keep your logic pure-Python and call <code>context.&lt;service&gt;()</code> only when you need I/O, coordination, or intelligence.</p>"},{"location":"key-concepts/services-at-a-glance/#1-kv-ephemeral-coordination","title":"1. KV \u2014 Ephemeral Coordination","text":"<p>A lightweight key\u2013value store for transient synchronization, small caches, and locks.</p> <pre><code>@graph_fn(name=\"kv_demo\", outputs=[\"ok\"])\nasync def kv_demo(*, context):\n    kv = context.kv()\n    await kv.set(\"stage\", \"preflight\", ttl_s=300)\n    stage = await kv.get(\"stage\")  # \"preflight\"\n    return {\"ok\": stage == \"preflight\"}\n</code></pre> <p>Default backend: ephemeral in-memory KV. Use for: feature flags, shared state, short coordination. See: KV Service Deep Dive \u2192</p>"},{"location":"key-concepts/services-at-a-glance/#2-logger-structured-logs-with-provenance","title":"2. Logger \u2014 Structured Logs with Provenance","text":"<p>Structured Python logger that automatically includes <code>{run_id, graph_id, node_id}</code> in every record.</p> <pre><code>@graph_fn(name=\"log_demo\", outputs=[\"done\"])\nasync def log_demo(*, context):\n    log = context.logger()\n    log.info(\"starting\", extra={\"component\": \"ingest\"})\n    try:\n        ...\n        log.info(\"finished\", extra={\"component\": \"ingest\"})\n        return {\"done\": True}\n    except Exception:\n        log.exception(\"ingest failed\")\n        return {\"done\": False}\n</code></pre> <p>Default backend: standard Python logging. Use for: lifecycle traces, metrics, structured error reports. See: Logging Deep Dive \u2192</p>"},{"location":"key-concepts/services-at-a-glance/#3-llm-unified-chat-embeddings-optional","title":"3. LLM \u2014 Unified Chat &amp; Embeddings (optional)","text":"<p>An abstraction over multiple model providers. Requires configuration (API keys, profile).</p> <pre><code>@graph_fn(name=\"llm_demo\", outputs=[\"reply\"])\nasync def llm_demo(prompt: str, *, context):\n    llm = context.llm(profile=\"default\")\n    msg = await llm.chat([{\"role\": \"user\", \"content\": prompt}])\n    return {\"reply\": msg[\"content\"]}\n</code></pre> <p>Use for: chat completions, summarization, embeddings. Backend: pluggable clients (OpenAI, Anthropic, local). See: LLM Service Deep Dive \u2192</p>"},{"location":"key-concepts/services-at-a-glance/#4-rag-long-term-semantic-recall-optional","title":"4. RAG \u2014 Long-Term Semantic Recall (optional)","text":"<p>Build searchable corpora from events or docs, then retrieve or answer with citations. Requires an LLM for answering.</p> <pre><code>@graph_fn(name=\"rag_demo\", outputs=[\"answer\"])\nasync def rag_demo(q: str, *, context):\n    mem = context.memory()\n    corpus = await mem.rag_bind(scope=\"project\")\n    await mem.rag_promote_events(corpus_id=corpus)\n    ans = await mem.rag_answer(corpus_id=corpus, question=q)\n    return {\"answer\": ans[\"answer\"]}\n</code></pre> <p>Default backend: FAISS (local). Use for: semantic search and retrieval-augmented QA. See: RAG Deep Dive \u2192 \u00b7 External Context \u2192</p>"},{"location":"key-concepts/services-at-a-glance/#5-mcp-external-tool-bridges","title":"5. MCP \u2014 External Tool Bridges","text":"<p>Connect to external tool servers over stdio, WebSocket, or HTTP using the Model Context Protocol (MCP).</p> <pre><code>@graph_fn(name=\"mcp_demo\", outputs=[\"hits\"])\nasync def mcp_demo(*, context):\n    ws = context.mcp(\"ws\")  # adapter name\n    res = await ws.call(\"search\", {\"q\": \"tolerance analysis\", \"k\": 3})\n    return {\"hits\": res.get(\"items\", [])}\n</code></pre> <p>Use for: safe integration with non-Python tools and structured external APIs. See: MCP Deep Dive \u2192</p>"},{"location":"key-concepts/services-at-a-glance/#takeaways","title":"Takeaways","text":"<ul> <li>All services are accessible through <code>context.&lt;service&gt;()</code> \u2014 no imports or globals.</li> <li>Core defaults (KV, Logger) work locally out of the box.</li> <li>LLM, RAG, and MCP are optional; enable them via environment or external context configuration.</li> <li>Backends are swappable \u2014 you can move from local to managed services without changing agent code.</li> </ul> <p>Next: Explore Channels &amp; Interaction \u2192 or Artifacts &amp; Memory \u2192</p>"},{"location":"key-concepts/static-graph-agent/","title":"Static Graphs with <code>@graphify</code>","text":"<p><code>@graphify</code> turns a plain Python function into a graph builder. Instead of executing immediately (like <code>@graph_fn</code>), it constructs a TaskGraph from <code>@tool</code> calls, which you build once and run later.</p> <p>In short: <code>@graph_fn</code> = execute now (implicit, reactive). <code>@graphify</code> = build first, then run (explicit DAG).</p>"},{"location":"key-concepts/static-graph-agent/#1-what-is-a-static-graph","title":"1 What is a Static Graph?","text":"<p>A static graph is an explicit DAG of tool nodes and dependencies. All internal steps must be <code>@tool</code> calls; the builder wires nodes and returns exposed outputs. You can then:</p> <ul> <li>inspect the spec / IO signature</li> <li>persist or visualize the graph</li> <li>run it under the global scheduler</li> </ul> <p>Why static? Repeatability, inspectability, and clear fan\u2011in/fan\u2011out. Ideal for pipelines and reproducible experiments.</p> <p>How it differs from <code>@graph_fn</code>:</p> <ul> <li><code>@graph_fn</code> executes immediately with a per\u2011function scheduler and rich <code>context.*</code> calls.</li> <li><code>@graphify</code> builds a DAG; you run it later (typically using the global scheduler).</li> </ul> <p>When to use it: <code>@graphify</code> asks for a bit more code up front (declared inputs, explicit <code>@tool</code> nodes, <code>_after</code> for ordering), but you get stability, efficiency, and inspectability in return\u2014deterministic runs, clearer fan-in/out, easier caching/retries, and better debugging/analytics. For reactive exploration, stick with <code>@graph_fn</code> until your flow settles; switch to <code>@graphify</code> when the pipeline is stabilized or when product environments demand determinism and performance.</p>"},{"location":"key-concepts/static-graph-agent/#2-define-and-build-a-graph","title":"2 Define and Build a Graph","text":"<pre><code>from aethergraph import graphify, tool\nfrom aethergraph.graph import arg\n\n@tool(outputs=[\"rows\"])       \ndef load_csv(path: str): ...\n\n@tool(outputs=[\"clean\"])      \ndef clean(rows): ...\n\n@tool(outputs=[\"model\"])      \ndef train(data): ...\n\n@tool(outputs=[\"uri\"])        \ndef report(model): ...\n\n@graphify(name=\"etl_train_report\", inputs=[\"csv_path\"], outputs=[\"uri\"])\ndef etl_train_report(csv_path):\n    raw  = load_csv(path=arg(\"csv_path\"))\n    tidy = clean(rows=raw.rows)\n    mod  = train(data=tidy.clean)\n    rep  = report(model=mod.model)\n    return {\"uri\": rep.uri}\n\nG = etl_train_report.build()     # \u2192 TaskGraph\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#control-ordering-without-data-edges","title":"Control ordering without data edges","text":"<p>Use <code>_after</code> to enforce sequence when you don\u2019t pass outputs:</p> <pre><code>@tool(outputs=[\"ok\"])    \ndef fetch(): ...\n\n@tool(outputs=[\"done\"])  \ndef train(): ...\n\n@graphify(name=\"seq\", inputs=[], outputs=[\"done\"])\ndef seq():\n    a = fetch()\n    b = train(_after=a)        # run `train` after `fetch`\n    return {\"done\": b.done}\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#3-run-a-built-graph","title":"3 Run a Built Graph","text":"<p>Static graphs typically run under the global scheduler (centralized control over node scheduling). You can only run a static graph using runner function. </p> <pre><code># Pseudocode \u2014 exact runner API depends on your hosting layer\nfrom aethergraph.runner import run\n\nresult = run(G, inputs={\"csv_path\": \"data/train.csv\"})\n# `result` contains graph-level outputs, e.g., {\"uri\": \"file://...\"}\n</code></pre> <p>Note: <code>@graph_fn</code> uses its own lightweight scheduler for immediate execution, while <code>@graphify</code> graphs are designed to be scheduled by the global scheduler for observability, resumability, and concurrency control.</p>"},{"location":"key-concepts/static-graph-agent/#4-inspect-and-explore-a-graph","title":"4 Inspect and Explore a Graph","text":"<p><code>@graphify</code> builders expose helpers:</p> <pre><code>sig = etl_train_report.io()      # inputs/outputs signature\nspec = etl_train_report.spec()   # GraphSpec (nodes, edges, metadata)\n</code></pre> <p>Runtime helpers on <code>TaskGraph</code>:</p> <pre><code>print(G.pretty())                # human-friendly table\nprint(G.ascii_overview())        # compact ASCII view\n\n# Select / find nodes\nids     = G.list_nodes()                         # all non-internal node_ids\nfirst_c = G.find_by_logic(\"clean\", first=True)  # by tool/logic name\nsome    = G.find_by_label(\"train\")              # by label\nsel     = G.select(\"@my_alias\")                  # mini-DSL (@alias, #label, logic:, name:, id:, /regex/)\n\n# Topology &amp; subgraphs\norder   = G.topological_order()                  # raise if cycles\nup      = G.get_upstream_nodes(first_c)          # closure of dependencies\nsub     = G.get_subgraph_nodes(first_c)          # downstream dependents\n</code></pre> <p>Export / visualize</p> <pre><code>dot = G.to_dot()                 # Graphviz DOT\n# G.visualize()                  # if enabled: render to file/viewer\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#5-recall-use-tool-inside-graph_fn","title":"5 Recall: Use <code>@tool</code> inside <code>@graph_fn</code>","text":"<p>While <code>@graph_fn</code> executes immediately, you can embed <code>@tool</code> calls to create explicit nodes for tracing or parallelism within a reactive agent:</p> <pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"]) \ndef square(x: int): return {\"y\": x*x}\n\n@graph_fn(name=\"mix\")\nasync def mix(x: int, *, context):\n    h = square(x=x)                 # schedules a tool node in the implicit graph\n    await context.channel().send_text(\"running square\u2026\")\n    return {\"y\": h.y}\n</code></pre> <p>Prefer <code>@graphify</code> for full pipelines and reproducible DAGs; prefer <code>@graph_fn</code> for interactive/reactive agents that lean on <code>context.*</code>.</p> <p>If you executed a <code>@graph_fn</code> and want to inspect the implicit graph of tool nodes it created:</p> <pre><code>from aethergraph import graph_fn\nG_last = graph_fn.last_graph()    # TaskGraph of the most recent run (if available)\nprint(G_last.pretty())\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#6-key-points","title":"6 Key Points","text":"<ul> <li><code>@graphify</code> builds a DAG from <code>@tool</code> calls; you run it later (usually with the global scheduler).</li> <li>Use <code>arg(\"name\")</code> inside the builder to reference declared inputs.</li> <li>Use <code>_after</code> to force ordering without data edges.</li> <li>Inspect via <code>.io()</code>, <code>.spec()</code>, <code>TaskGraph.pretty()</code>, <code>ascii_overview()</code>, <code>to_dot()</code>.</li> <li>For reactive agents, stick with <code>@graph_fn</code>; for pipelines, prefer <code>@graphify</code>.</li> </ul>"},{"location":"recipes/data-analysis-loop/","title":"Recipe: Iterative Data Analysis","text":"<ul> <li>User asks for analysis \u2192 generate code</li> <li>Run code; store figures &amp; tables in <code>artifacts()</code></li> <li>Record metrics in <code>memory()</code> and summarize at the end</li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"recipes/job-submit-poll/","title":"Recipe: Submit + Poll + Notify","text":"<ul> <li>Submit a long-running job</li> <li>Poll status and send progress via <code>channel()</code></li> <li>On failure, ask user to retry or stop; save logs to <code>artifacts()</code></li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"recipes/memory-rag-mini/","title":"Recipe: Mini Memory\u2011RAG","text":"<ul> <li>Ingest notes as <code>memory().record(kind=\"note\", data=...)</code></li> <li>Simple retrieval via <code>memory().recent()/query()</code> into <code>llm().chat()</code></li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"reference/config/","title":"Config Keys","text":"<ul> <li><code>AETH_PORT</code> \u2014 server port</li> <li><code>LOG_LEVEL</code> \u2014 logging level (e.g., INFO)</li> <li><code>MODEL_NAME</code> \u2014 default LLM</li> <li><code>SLACK_*</code> \u2014 Slack integration (bot token, signing secret)</li> </ul> <p>(Expand with your actual config schema.)</p>"},{"location":"reference/context-artifacts/","title":"AetherGraph \u2014 <code>context.artifacts()</code> Reference","text":"<p>This page documents the ArtifactFacade methods returned by <code>context.artifacts()</code> in a concise format: signature, brief description, parameters, and returns \u2014 plus examples for <code>writer()</code> and scoped search.</p>"},{"location":"reference/context-artifacts/#overview","title":"Overview","text":"<p><code>context.artifacts()</code> returns an ArtifactFacade bound to the current <code>run_id</code>, <code>graph_id</code>, and <code>node_id</code>. It wraps an <code>AsyncArtifactStore</code> for persistence and an <code>AsyncArtifactIndex</code> for search/pinning/metrics. Most mutating ops auto\u2011index and record an occurrence.</p> <p>Typical flow</p> <ol> <li> <p>Stage (optional) \u2192 write \u2192 ingest</p> </li> <li> <p>Or directly save an existing file path</p> </li> <li> <p>Or use the built\u2011in <code>writer()</code> context manager to stream bytes and auto\u2011index</p> </li> </ol>"},{"location":"reference/context-artifacts/#artifactsstage","title":"artifacts.stage","text":"<p><pre><code>stage(ext: str = \"\") -&gt; str\n</code></pre> Plan a staging file path (temporary path) with an optional extension.</p> <p>Parameters</p> <ul> <li>ext (str, optional) \u2013 Suggested extension (e.g., \".png\", \".csv\").</li> </ul> <p>Returns str \u2013 Staging file path.</p>"},{"location":"reference/context-artifacts/#artifactsingest","title":"artifacts.ingest","text":"<p><pre><code>ingest(staged_path: str, *, kind: str, labels=None, metrics=None, suggested_uri: str | None = None, pin: bool = False) -&gt; Artifact\n</code></pre> Ingest a previously staged file into the store, attach metadata, and auto\u2011index.</p> <p>Parameters</p> <ul> <li> <p>staged_path (str) \u2013 Path returned by <code>stage()</code> or <code>stage_dir()</code>.</p> </li> <li> <p>kind (str) \u2013 Logical artifact kind (e.g., \"image\", \"table\", \"model\").</p> </li> <li> <p>labels (dict, optional) \u2013 Arbitrary labels; merged into index filters.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics used for <code>best()</code> queries.</p> </li> <li> <p>suggested_uri (str, optional) \u2013 Hint for final URI; store may ignore.</p> </li> <li> <p>pin (bool) \u2013 Mark artifact as pinned in the index.</p> </li> </ul> <p>Returns Artifact \u2013 Indexed artifact record.</p>"},{"location":"reference/context-artifacts/#artifactssave","title":"artifacts.save","text":"<p><pre><code>save(path: str, *, kind: str, labels=None, metrics=None, suggested_uri: str | None = None, pin: bool = False) -&gt; Artifact\n</code></pre> Save an existing on\u2011disk file to the store with metadata; auto\u2011index and record occurrence. Sets <code>last_artifact</code>.</p> <p>Parameters</p> <ul> <li> <p>path (str) \u2013 Existing file path to persist.</p> </li> <li> <p>kind (str) \u2013 Logical artifact kind.</p> </li> <li> <p>labels (dict, optional) \u2013 Arbitrary labels.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics.</p> </li> <li> <p>suggested_uri (str, optional) \u2013 Hint for final URI; store may ignore.</p> </li> <li> <p>pin (bool) \u2013 Mark artifact as pinned.</p> </li> </ul> <p>Returns Artifact \u2013 Indexed artifact record.</p>"},{"location":"reference/context-artifacts/#artifactswriter","title":"artifacts.writer","text":"<p><pre><code>writer(*, kind: str, planned_ext: str | None = None, pin: bool = False) -&gt; AsyncContextManager[Writer]\n</code></pre> Open a binary writer context that persists bytes as an artifact; auto\u2011indexes on exit. Sets <code>last_artifact</code>.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Logical artifact kind.</p> </li> <li> <p>planned_ext (str, optional) \u2013 Extension hint for underlying temp file.</p> </li> <li> <p>pin (bool) \u2013 Mark resulting artifact as pinned.</p> </li> </ul> <p>Yields Writer \u2013 File\u2011like object; write bytes and close by exiting the context.</p> <p>Example <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"make_png\")\nasync def make_png(*, context):\n    import PIL.Image as Image\n    img = Image.new(\"RGB\", (128, 128), (255, 122, 26))\n    async with context.artifacts().writer(kind=\"image\", planned_ext=\".png\") as w:\n        # writer exposes a real file handle underneath\n        img.save(w, format=\"PNG\")\n    art = context.artifacts().last_artifact\n    await context.channel().send_image(url=art.uri, title=\"Generated PNG\")\n    return {\"uri\": art.uri}\n</code></pre></p>"},{"location":"reference/context-artifacts/#artifactsstage_dir","title":"artifacts.stage_dir","text":"<p><pre><code>stage_dir(suffix: str = \"\") -&gt; str\n</code></pre> Plan a staging directory for multi\u2011file artifacts.</p> <p>Parameters</p> <ul> <li>suffix (str, optional) \u2013 Optional directory suffix.</li> </ul> <p>Returns str \u2013 Staging directory path.</p>"},{"location":"reference/context-artifacts/#artifactsingest_dir","title":"artifacts.ingest_dir","text":"<p><pre><code>ingest_dir(staged_dir: str, **kw) -&gt; Artifact\n</code></pre> Ingest a directory of files as a single logical artifact; forwards extra keyword args to the store.</p> <p>Parameters</p> <ul> <li> <p>staged_dir (str) \u2013 Directory created by <code>stage_dir()</code>.</p> </li> <li> <p>kw \u2013 Store\u2011specific options (e.g., kind/labels/metrics/pin).</p> </li> </ul> <p>Returns Artifact \u2013 Indexed artifact record.</p>"},{"location":"reference/context-artifacts/#artifactstmp_path","title":"artifacts.tmp_path","text":"<p><pre><code>tmp_path(suffix: str = \"\") -&gt; str\n</code></pre> Alias of <code>stage()</code> for convenience.</p> <p>Parameters</p> <ul> <li>suffix (str, optional) \u2013 Extension or suffix.</li> </ul> <p>Returns str \u2013 Staging file path.</p>"},{"location":"reference/context-artifacts/#artifactsload_artifact","title":"artifacts.load_artifact","text":"<p><pre><code>load_artifact(uri: str) -&gt; Any\n</code></pre> Load a previously saved artifact by URI, using the store\u2019s type\u2011specific loader.</p> <p>Parameters</p> <ul> <li>uri (str) \u2013 Artifact URI.</li> </ul> <p>Returns Any \u2013 Decoded object (depends on store &amp; artifact type).</p>"},{"location":"reference/context-artifacts/#artifactsload_artifact_bytes","title":"artifacts.load_artifact_bytes","text":"<p><pre><code>load_artifact_bytes(uri: str) -&gt; bytes\n</code></pre> Load raw bytes for a previously saved artifact by URI.</p> <p>Parameters</p> <ul> <li>uri (str) \u2013 Artifact URI.</li> </ul> <p>Returns bytes \u2013 Artifact content.</p>"},{"location":"reference/context-artifacts/#artifactslist","title":"artifacts.list","text":"<p><pre><code>list(*, scope: Literal[\"node\",\"run\",\"graph\",\"project\",\"all\"] = \"run\") -&gt; list[Artifact]\n</code></pre> Quick listing with implicit scoping (defaults to the current run). Under the hood, this uses the index with reasonable filters for the given scope.</p> <p>Parameters</p> <ul> <li> <p>scope (str) \u2013 One of:</p> </li> <li> <p>\"node\" \u2013 filter by (run_id, graph_id, node_id) </p> </li> <li> <p>\"graph\" \u2013 filter by (run_id, graph_id) </p> </li> <li> <p>\"run\" \u2013 filter by (run_id) (default) </p> </li> <li> <p>\"project\" \u2013 filter by project/org if tracked in labels  </p> </li> <li> <p>\"all\" \u2013 no implicit filters (use sparingly)</p> </li> </ul> <p>Returns list[Artifact] \u2013 Matching artifacts.</p>"},{"location":"reference/context-artifacts/#artifactssearch","title":"artifacts.search","text":"<p><pre><code>search(*, kind: str | None = None, labels: dict | None = None, metric: str | None = None, mode: Literal[\"max\",\"min\"] | None = None, scope: Scope = \"run\", extra_scope_labels: dict | None = None) -&gt; list[Artifact]\n</code></pre> Index search with automatic scoping. Merges your <code>labels</code> with scope\u2011derived labels.</p> <p>Parameters</p> <ul> <li> <p>kind (str, optional) \u2013 Filter by artifact kind.</p> </li> <li> <p>labels (dict, optional) \u2013 Arbitrary label filters.</p> </li> <li> <p>metric (str, optional) \u2013 Metric name for ranking.</p> </li> <li> <p>mode ({\"max\",\"min\"}, optional) \u2013 Ranking direction.</p> </li> <li> <p>scope (Scope) \u2013 Implicit scope (default: \"run\").</p> </li> <li> <p>extra_scope_labels (dict, optional) \u2013 Additional scope labels to merge.</p> </li> </ul> <p>Returns list[Artifact] \u2013 Search results.</p> <p>Example (scoped search) <pre><code>best_imgs = await context.artifacts().search(kind=\"image\", scope=\"graph\")\n</code></pre></p>"},{"location":"reference/context-artifacts/#artifactsbest","title":"artifacts.best","text":"<p><pre><code>best(*, kind: str, metric: str, mode: Literal[\"max\",\"min\"], scope: Scope = \"run\", filters: dict | None = None) -&gt; Artifact | None\n</code></pre> Return the best artifact by a metric, with optional filters and implicit scope.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Artifact kind.</p> </li> <li> <p>metric (str) \u2013 Metric key.</p> </li> <li> <p>mode ({\"max\",\"min\"}) \u2013 Ranking direction.</p> </li> <li> <p>scope (Scope) \u2013 Implicit scope (default: \"run\").</p> </li> <li> <p>filters (dict, optional) \u2013 Additional label filters.</p> </li> </ul> <p>Returns Artifact | None \u2013 Best match or <code>None</code> if not found.</p>"},{"location":"reference/context-artifacts/#artifactspin","title":"artifacts.pin","text":"<p><pre><code>pin(artifact_id: str, pinned: bool = True) -&gt; None\n</code></pre> Pin or unpin an artifact in the index.</p> <p>Parameters</p> <ul> <li> <p>artifact_id (str) \u2013 ID of the artifact to (un)pin.</p> </li> <li> <p>pinned (bool) \u2013 <code>True</code> to pin; <code>False</code> to unpin.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-artifacts/#scoping-details","title":"Scoping details","text":"<p>The facade enriches queries with labels depending on the <code>scope</code> argument:</p> <ul> <li> <p>node \u2192 <code>{ graph_id, node_id }</code> </p> </li> <li> <p>graph \u2192 <code>{ graph_id }</code> </p> </li> <li> <p>project \u2192 <code>{ project_id }</code> (if tracked)  </p> </li> <li> <p>run \u2192 uses <code>list_for_run(run_id)</code> </p> </li> <li> <p>all \u2192 passes through to index with no implicit labels</p> </li> </ul>"},{"location":"reference/context-artifacts/#practical-examples","title":"Practical examples","text":"<p>1) Direct save <pre><code>uri = \"/tmp/plot.png\"\n# ... generate image to uri ...\nart = await context.artifacts().save(uri, kind=\"image\", labels={\"task\":\"eval\"}, metrics={\"psnr\": 31.2})\n</code></pre></p> <p>2) Stage \u2192 write \u2192 ingest <pre><code>staged = await context.artifacts().stage(\".csv\")\nwith open(staged, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"x,y\\n1,2\\n3,4\\n\")\nart = await context.artifacts().ingest(staged_path=staged, kind=\"table\", labels={\"split\":\"val\"})\n</code></pre></p> <p>3) Search best <pre><code>winner = await context.artifacts().best(kind=\"model\", metric=\"val_acc\", mode=\"max\", scope=\"run\")\nif winner:\n    await context.channel().send_text(f\"Best model: {winner.uri} acc={winner.metrics['val_acc']:.3f}\")\n</code></pre></p> <p>4) Multi\u2011file directory <pre><code>dir_path = await context.artifacts().stage_dir(\"_report\")\n# ... write several files to dir_path ...\nart = await context.artifacts().ingest_dir(dir_path, kind=\"report\", labels={\"format\":\"html\"})\n</code></pre></p> <p>5) Pin <pre><code>await context.artifacts().pin(artifact_id=art.id, pinned=True)\n</code></pre></p>"},{"location":"reference/context-channel/","title":"AetherGraph \u2014 <code>context.channel()</code> Reference","text":"<p>This page documents the ChannelSession methods returned by <code>context.channel()</code> in a concise, PyTorch\u2011style format: signature, brief description, parameters, and returns.</p>"},{"location":"reference/context-channel/#overview-choosing-a-channel","title":"Overview \u2014 Choosing a channel","text":"<p>Use <code>context.channel(&lt;key&gt;)</code> to bind a ChannelSession to a specific destination for all subsequent calls from that session. You can also override per call with the <code>channel=</code> keyword.</p> <p>Common forms - <code>slack:#research</code> \u2014 a Slack channel by name</p> <ul> <li> <p><code>slack:@alice</code> \u2014 a Slack DM</p> </li> <li> <p><code>telegram:@mychannel</code> \u2014 a Telegram channel</p> </li> <li> <p><code>console:stdin</code> \u2014 console fallback (default if nothing is configured)</p> </li> </ul> <p>Resolution order (what channel is used?) 1. Per\u2011call override: <code>await context.channel().send_text(\"hi\", channel=\"slack:#alerts\")</code></p> <ol> <li> <p>Bound session key: <code>ch = context.channel(\"slack:#research\"); await ch.send_text(\"hi\")</code></p> </li> <li> <p>Bus default: whatever <code>services.channels.get_default_channel_key()</code> returns</p> </li> <li> <p>Fallback: <code>console:stdin</code></p> </li> </ol> <p>Examples <pre><code># Bind a session to #research for many messages\nch = context.channel(\"slack:#research\")\nawait ch.send_text(\"Starting the run\u2026\")\nawait ch.send_text(\"Progress will be posted here.\")\n\n# One\u2011off override to a different channel\nawait context.channel().send_text(\"Heads\u2011up in #alerts\", channel=\"slack:#alerts\")\n\n# Stream to #research explicitly\nasync with context.channel().stream(channel=\"slack:#research\") as s:\n    await s.delta(\"Parsing\u2026 \")\n    await s.delta(\"OK\")\n    await s.end(\"Done\")\n\n# Progress bar to the default (no key passed)\nasync with context.channel().progress(title=\"Crunching\", total=100) as bar:\n    await bar.update(current=30, eta_seconds=90)\n    await bar.end(subtitle=\"All set!\")\n</code></pre></p>"},{"location":"reference/context-channel/#channelsend_text","title":"channel.send_text","text":"<p><pre><code>send_text(text, *, meta: dict | None = None, channel: str | None = None)\n</code></pre> Send a plain text message to a channel.</p> <p>Parameters</p> <ul> <li> <p>text (str) \u2013 Message body to send.</p> </li> <li> <p>meta (dict, optional) \u2013 Arbitrary metadata for adapters/analytics.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override (e.g., <code>\"slack:#research\"</code>).</p> </li> </ul> <p>Returns </p> <p><code>None</code></p>"},{"location":"reference/context-channel/#channelsend_image","title":"channel.send_image","text":"<p><pre><code>send_image(url: str | None = None, *, alt: str = \"image\", title: str | None = None, channel: str | None = None)\n</code></pre> Post an image by URL with <code>alt</code>/<code>title</code> text.</p> <p>Parameters</p> <ul> <li> <p>url (str, optional) \u2013 Image URL. Use <code>send_file</code> for file bytes.</p> </li> <li> <p>alt (str) \u2013 Alt text (default: <code>\"image\"</code>).</p> </li> <li> <p>title (str, optional) \u2013 Optional title/caption.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p><code>None</code></p>"},{"location":"reference/context-channel/#channelsend_file","title":"channel.send_file","text":"<p><pre><code>send_file(url: str | None = None, *, file_bytes: bytes | None = None, filename: str = \"file.bin\", title: str | None = None, channel: str | None = None)\n</code></pre> Upload or link a file to the channel. Provide either <code>url</code> or <code>file_bytes</code>.</p> <p>Parameters</p> <ul> <li> <p>url (str, optional) \u2013 Remote file URL to attach.</p> </li> <li> <p>file_bytes (bytes, optional) \u2013 Raw bytes to upload.</p> </li> <li> <p>filename (str) \u2013 Display filename (default: <code>\"file.bin\"</code>).</p> </li> <li> <p>title (str, optional) \u2013 Optional caption/label.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p><code>None</code></p>"},{"location":"reference/context-channel/#channelsend_buttons","title":"channel.send_buttons","text":"<p><pre><code>send_buttons(text: str, buttons: list[Button], *, meta: dict | None = None, channel: str | None = None)\n</code></pre> Send a short message with interactive buttons (links or postbacks depending on adapter).</p> <p>Parameters</p> <ul> <li> <p>text (str) \u2013 Leading text.</p> </li> <li> <p>buttons (list[Button]) \u2013 Button list; at minimum a <code>label</code> per button.</p> </li> <li> <p>meta (dict, optional) \u2013 Arbitrary metadata.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-channel/#channelask_text","title":"channel.ask_text","text":"<p><pre><code>ask_text(prompt: str, *, timeout_s: int = 3600, silent: bool = False, channel: str | None = None)\n</code></pre> Ask the user for free\u2011text using cooperative wait/continuations.</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Prompt text shown to the user. (Ignored if <code>silent=True</code>.)</p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>silent (bool) \u2013 If <code>True</code>, binds to current thread/channel without posting a prompt.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>str \u2013 The user\u2019s text (empty string if none).</p>"},{"location":"reference/context-channel/#channelwait_text","title":"channel.wait_text","text":"<p><pre><code>wait_text(*, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Wait for the next text reply in the current thread/channel without sending a prompt.</p> <p>Parameters</p> <ul> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>str \u2013 The user\u2019s text.</p>"},{"location":"reference/context-channel/#channelask_approval","title":"channel.ask_approval","text":"<p><pre><code>ask_approval(prompt: str, options: Iterable[str] = (\"Approve\", \"Reject\"), *, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Ask the user to approve or pick an option.</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Title or question.</p> </li> <li> <p>options (Iterable[str]) \u2013 Button labels (default: <code>(\"Approve\",\"Reject\")</code>).</p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>dict \u2013 <code>{ \"approved\": bool, \"choice\": str }</code>.</p>"},{"location":"reference/context-channel/#channelget_latest_uploads","title":"channel.get_latest_uploads","text":"<p><pre><code>get_latest_uploads(*, clear: bool = True)\n</code></pre> Fetch latest uploaded files for this channel (Ephemeral KV required).</p> <p>Parameters</p> <ul> <li>clear (bool) \u2013 If <code>True</code>, consume and clear the inbox (default: <code>True</code>).</li> </ul> <p>Returns </p> <p>list[FileRef] \u2013 Recent file references.</p> <p>Raises </p> <p><code>RuntimeError</code> \u2013 if KV is not available.</p>"},{"location":"reference/context-channel/#channelask_files","title":"channel.ask_files","text":"<p><pre><code>ask_files(*, prompt: str, accept: list[str] | None = None, multiple: bool = True, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Ask the user to upload file(s) with optional text input.</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Prompt text.</p> </li> <li> <p>accept (list[str], optional) \u2013 MIME types or extensions (adapter\u2011hint only).</p> </li> <li> <p>multiple (bool) \u2013 Allow selecting multiple files (default: <code>True</code>). </p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>dict \u2013 <code>{ \"text\": str, \"files\": list[FileRef] }</code>.</p>"},{"location":"reference/context-channel/#channelask_text_or_files","title":"channel.ask_text_or_files","text":"<p><pre><code>ask_text_or_files(*, prompt: str, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Let the user respond with either text or file(s).</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Prompt text.</p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>dict \u2013 <code>{ \"text\": str, \"files\": list[FileRef] }</code>.</p>"},{"location":"reference/context-channel/#channelstream","title":"channel.stream","text":"<p><pre><code>stream(channel: str | None = None)  # async context manager\n</code></pre> Create a stream for incremental message updates (token/delta style). Within the context, use <code>s.delta()</code> to append text and <code>s.end()</code> to finalize.</p> <p>Parameters</p> <ul> <li>channel (str, optional) \u2013 Per\u2011stream channel override.</li> </ul> <p>Yields StreamSender \u2013 with methods:</p> <ul> <li> <p><code>start()</code> \u2013 explicitly start the stream (optional; auto on first delta).</p> </li> <li> <p><code>delta(text_piece: str)</code> \u2013 append a delta (adapter receives upsert with full text).</p> </li> <li> <p><code>end(full_text: str | None = None)</code> \u2013 finalize; optionally set final text.</p> </li> </ul> <p>Example <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"stream_demo\")\nasync def stream_demo(*, context):\n    async with context.channel().stream() as s:\n        for chunk in [\"Hello\", \" \", \"world\", \"\u2026\"]:\n            await s.delta(chunk)\n        await s.end(\"Hello world!\")\n</code></pre></p>"},{"location":"reference/context-channel/#channelprogress","title":"channel.progress","text":"<p><pre><code>progress(*, title: str = \"Working...\", total: int | None = None, key_suffix: str = \"progress\", channel: str | None = None)  # async context manager\n</code></pre> Create a progress reporter (start/update/end) bound to the current run/node.</p> <p>Parameters</p> <ul> <li> <p>title (str) \u2013 Progress title (default: <code>\"Working...\"</code>).</p> </li> <li> <p>total (int, optional) \u2013 If set, progress is shown as <code>current/total</code>; allows <code>percent</code> updates.</p> </li> <li> <p>key_suffix (str) \u2013 Included in the internal upsert key (default: <code>\"progress\"</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011progress channel override.</p> </li> </ul> <p>Yields ProgressSender \u2013 with methods:</p> <ul> <li> <p><code>start(subtitle: str | None = None)</code> \u2013 start (auto on first update).</p> </li> <li> <p><code>update(current: int | None = None, inc: int | None = None, subtitle: str | None = None, percent: float | None = None, eta_seconds: float | None = None)</code></p> </li> <li> <p><code>end(subtitle: str | None = \"Done.\", success: bool = True)</code></p> </li> </ul> <p>Example <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"progress_demo\")\nasync def progress_demo(*, context):\n    async with context.channel().progress(title=\"Crunching\", total=5) as bar:\n        for i in range(5):\n            await bar.update(current=i+1, eta_seconds=(4-i)*0.5, subtitle=f\"step {i+1}/5\")\n        await bar.end(subtitle=\"All set!\")\n</code></pre></p>"},{"location":"reference/context-channel/#channel-resolution-notes","title":"Channel resolution notes","text":"<ul> <li> <p>Per\u2011call <code>channel=</code> overrides everything.</p> </li> <li> <p>Otherwise, the session\u2019s bound key (from <code>context.channel(bound_key)</code>) is used.</p> </li> <li> <p>Else, the bus default via <code>services.channels.get_default_channel_key()</code>.</p> </li> <li> <p>Else, fallback <code>\"console:stdin\"</code>.</p> </li> </ul>"},{"location":"reference/context-channel/#guarantees","title":"Guarantees","text":"<ul> <li> <p>Streams/progress use idempotent upsert keys derived from <code>(run_id, node_id, suffix)</code>.</p> </li> <li> <p>Ask methods bind correlators at both message and thread level to capture replies.</p> </li> </ul>"},{"location":"reference/context-kv/","title":"AetherGraph \u2014 <code>context.kv()</code> Reference","text":"<p>This page documents the Key\u2013Value API available via <code>context.kv()</code> in a concise format. The KV store is process\u2011local and transient \u2014 ideal for coordination, small caches, inboxes, and short\u2011lived lists. Not intended for large blobs or durability.</p>"},{"location":"reference/context-kv/#overview","title":"Overview","text":"<ul> <li>Keys are simple strings; values can be any JSON\u2011serializable Python object (adapters may allow arbitrary picklables, but keep it small).</li> <li>Most methods support TTL (time\u2011to\u2011live in seconds). Expired entries are pruned lazily or via <code>purge_expired()</code>.</li> <li>For namespacing, prefer prefixes like <code>\"run:&lt;id&gt;:...\"</code>, <code>\"inbox:&lt;channel&gt;\"</code>, etc.</li> </ul>"},{"location":"reference/context-kv/#kvget","title":"kv.get","text":"<p><pre><code>get(key: str, default: Any = None) -&gt; Any\n</code></pre> Fetch a value by key; returns <code>default</code> if missing or expired.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 Lookup key.</p> </li> <li> <p>default (Any, optional) \u2013 Value to return when absent/expired (default <code>None</code>).</p> </li> </ul> <p>Returns Any \u2013 Stored value or <code>default</code>.</p>"},{"location":"reference/context-kv/#kvset","title":"kv.set","text":"<p><pre><code>set(key: str, value: Any, *, ttl_s: int | None = None) -&gt; None\n</code></pre> Set a key to a value with optional TTL.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 Key to write.</p> </li> <li> <p>value (Any) \u2013 Value to store.</p> </li> <li> <p>ttl_s (int, optional) \u2013 Expiration in seconds.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvdelete","title":"kv.delete","text":"<p><pre><code>delete(key: str) -&gt; None\n</code></pre> Remove a key if present.</p> <p>Parameters</p> <ul> <li>key (str) \u2013 Key to delete.</li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvlist_append_unique","title":"kv.list_append_unique","text":"<p><pre><code>list_append_unique(key: str, items: list[dict], *, id_key: str = \"id\", ttl_s: int | None = None) -&gt; list[dict]\n</code></pre> Append unique dict items to a list value under <code>key</code>. Uniqueness is determined by <code>item[id_key]</code>.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 List container key.</p> </li> <li> <p>items (list[dict]) \u2013 Items to append.</p> </li> <li> <p>id_key (str) \u2013 Field name used for uniqueness (default: <code>\"id\"</code>).</p> </li> <li> <p>ttl_s (int, optional) \u2013 Reset TTL for the list.</p> </li> </ul> <p>Returns list[dict] \u2013 Updated list.</p>"},{"location":"reference/context-kv/#kvlist_pop_all","title":"kv.list_pop_all","text":"<p><pre><code>list_pop_all(key: str) -&gt; list\n</code></pre> Pop and return the entire list stored at <code>key</code>. Empties the container.</p> <p>Parameters</p> <ul> <li>key (str) \u2013 List container key.</li> </ul> <p>Returns list \u2013 Previous list content (empty list if none or not a list).</p>"},{"location":"reference/context-kv/#kvmget","title":"kv.mget","text":"<p><pre><code>mget(keys: list[str]) -&gt; list[Any]\n</code></pre> Batch get multiple keys.</p> <p>Parameters</p> <ul> <li>keys (list[str]) \u2013 Keys to read.</li> </ul> <p>Returns list[Any] \u2013 Values in the same order as <code>keys</code>.</p>"},{"location":"reference/context-kv/#kvmset","title":"kv.mset","text":"<p><pre><code>mset(kv: dict[str, Any], *, ttl_s: int | None = None) -&gt; None\n</code></pre> Batch set multiple keys with an optional shared TTL.</p> <p>Parameters</p> <ul> <li> <p>kv (dict[str, Any]) \u2013 Key\u2013value pairs.</p> </li> <li> <p>ttl_s (int, optional) \u2013 TTL to apply to all entries.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvexpire","title":"kv.expire","text":"<p><pre><code>expire(key: str, ttl_s: int) -&gt; None\n</code></pre> Update/assign a TTL for an existing key.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 Key to expire.</p> </li> <li> <p>ttl_s (int) \u2013 Time\u2011to\u2011live in seconds from now.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvpurge_expired","title":"kv.purge_expired","text":"<p><pre><code>purge_expired(limit: int = 1000) -&gt; int\n</code></pre> Remove up to <code>limit</code> expired keys.</p> <p>Parameters</p> <ul> <li>limit (int) \u2013 Maximum removals per call (default: 1000).</li> </ul> <p>Returns int \u2013 Number of keys purged.</p>"},{"location":"reference/context-kv/#practical-examples","title":"Practical examples","text":"<p>1) Channel inbox (files/messages) <pre><code># Adapter pushes uploads into an inbox list\nawait context.kv().list_append_unique(\n    key=f\"inbox:{channel_key}\",\n    items=[{\"id\": file_id, \"filename\": name, \"url\": url}],\n    ttl_s=3600,\n)\n\n# Agent consumes the inbox later\nfiles = await context.kv().list_pop_all(f\"inbox:{channel_key}\")\nif files:\n    await context.channel().send_text(f\"Received {len(files)} file(s)\")\n</code></pre></p> <p>2) Short\u2011lived cache with TTL <pre><code>k = f\"run:{context.run_id}:spec\"\nspec = await context.kv().get(k)\nif spec is None:\n    spec = await expensive_fetch()\n    await context.kv().set(k, spec, ttl_s=300)  # cache for 5 minutes\n</code></pre></p> <p>3) Batch write/read <pre><code>await context.kv().mset({\n    f\"run:{context.run_id}:step\": 42,\n    f\"run:{context.run_id}:eta\": 120,\n}, ttl_s=600)\n\nvals = await context.kv().mget([\n    f\"run:{context.run_id}:step\",\n    f\"run:{context.run_id}:eta\",\n])\nstep, eta = vals\n</code></pre></p> <p>4) Update TTL <pre><code>await context.kv().expire(f\"run:{context.run_id}:spec\", ttl_s=900)\n</code></pre></p>"},{"location":"reference/context-kv/#notes-behaviors","title":"Notes &amp; behaviors","text":"<ul> <li> <p>Transient: data is in\u2011process only; it disappears on restart.</p> </li> <li> <p>Small values: do not store large binaries; use <code>context.artifacts()</code> for blobs.</p> </li> <li> <p>Concurrency: the implementation uses an internal lock; operations are atomic per call.</p> </li> <li> <p>TTL semantics: reads lazily drop expired entries; use <code>purge_expired()</code> to actively clean.</p> </li> </ul>"},{"location":"reference/context-llm/","title":"AetherGraph \u2014 <code>context.llm()</code> Reference","text":"<p>This page documents the LLM client retrieved via <code>context.llm(profile=\"default\")</code>, in a concise format. The client implements two core calls:</p> <ul> <li><code>chat(messages, **kwargs) -&gt; (text: str, usage: dict)</code></li> <li><code>embed(texts, **kwargs) -&gt; list[list[float]]</code></li> </ul> <p>Profiles are managed by an <code>LLMService</code> that holds one or more configured clients (\"default\", \"azure\", \"local\", etc.).</p>"},{"location":"reference/context-llm/#quick-start","title":"Quick start","text":"<pre><code># 1) Use the default LLM profile\nllm = context.llm()                # == context.llm(\"default\")\ntext, usage = await llm.chat([\n    {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n    {\"role\":\"user\",   \"content\":\"Summarize AetherGraph in one sentence.\"},\n])\n\n# 2) Switch/set a key at runtime (in\u2011memory only)\ncontext.llm_set_key(provider=\"openai\", api_key=\"sk-...\", profile=\"default\")\n\n# 3) Use a named profile (must exist in LLMService)\nllm = context.llm(\"azure\")\n</code></pre>"},{"location":"reference/context-llm/#supported-providers-via-genericllmclient","title":"Supported providers (via GenericLLMClient)","text":"<p><code>{\"openai\",\"azure\",\"anthropic\",\"google\",\"openrouter\",\"lmstudio\",\"ollama\"}</code></p> <p>Credentials and endpoints are read from environment by default, but can be provided at construction time. Runtime key overrides are allowed via <code>context.llm_set_key(...)</code>.</p> <p>Common env vars:</p> <ul> <li><code>OPENAI_API_KEY</code>, <code>OPENAI_BASE_URL</code></li> <li><code>AZURE_OPENAI_KEY</code>, <code>AZURE_OPENAI_ENDPOINT</code>, <code>AZURE_OPENAI_DEPLOYMENT</code></li> <li><code>ANTHROPIC_API_KEY</code></li> <li><code>GOOGLE_API_KEY</code></li> <li><code>OPENROUTER_API_KEY</code></li> <li><code>LMSTUDIO_BASE_URL</code> (default <code>http://localhost:1234/v1</code>)</li> <li><code>OLLAMA_BASE_URL</code>   (default <code>http://localhost:11434/v1</code>)</li> </ul>"},{"location":"reference/context-llm/#llmchat","title":"llm.chat","text":"<p><pre><code>chat(messages: list[dict], **kw) -&gt; tuple[str, dict]\n</code></pre> Send a chat completion request and return <code>(text, usage)</code>.</p> <p>Parameters</p> <ul> <li> <p>messages (list[dict]) \u2013 OpenAI\u2011style conversation turns, e.g. <code>{\"role\":\"system\"|\"user\"|\"assistant\", \"content\": str}</code>.</p> </li> <li> <p>kw \u2013 Common knobs (provider\u2011dependent):  </p> </li> <li> <p>model (str, optional) \u2013 Model name (defaults to client\u2019s configured model).  </p> </li> <li> <p>temperature (float, optional) \u2013 Sampling temperature (default <code>0.5</code>).  </p> </li> <li> <p>top_p (float, optional) \u2013 Nucleus sampling (default <code>1.0</code>).  </p> </li> <li> <p>max_tokens (int, optional) \u2013 Max tokens (Anthropic/Azure/Google paths).  </p> </li> </ul> <p>Returns tuple[str, dict] \u2013 Generated <code>text</code> and a <code>usage</code> dict (token counts where supported).</p> <p>Example <pre><code>sys = {\"role\":\"system\",\"content\":\"Be concise.\"}\nusr = {\"role\":\"user\",\"content\":\"What is AetherGraph?\"}\ntext, usage = await context.llm().chat([sys, usr], temperature=0.2)\nawait context.channel().send_text(text)\n</code></pre></p>"},{"location":"reference/context-llm/#llmembed","title":"llm.embed","text":"<p><pre><code>embed(texts: list[str], **kw) -&gt; list[list[float]]\n</code></pre> Return embeddings for a list of strings.</p> <p>Parameters</p> <ul> <li> <p>texts (list[str]) \u2013 Text strings to embed.</p> </li> <li> <p>kw \u2013 Common knobs (provider\u2011dependent):  </p> </li> <li> <p>model (str, optional) \u2013 Embedding model name (default <code>text-embedding-3-small</code> for OpenAI\u2011like providers).</p> </li> </ul> <p>Returns list[list[float]] \u2013 Embedding vectors.</p> <p>Example <pre><code>vecs = await context.llm().embed([\"lens design\", \"holography basics\"])  # [[...], [...]]\n</code></pre></p>"},{"location":"reference/context-llm/#profiles-and-keys","title":"Profiles and keys","text":""},{"location":"reference/context-llm/#contextllmprofile-str-default-llmclient","title":"<code>context.llm(profile: str = \"default\") -&gt; LLMClient</code>","text":"<p>Retrieve the configured LLM client for a named profile. Raises if <code>LLMService</code> is not bound or profile missing.</p>"},{"location":"reference/context-llm/#contextllm_set_keyprovider-str-api_key-str-profile-str-default-none","title":"<code>context.llm_set_key(provider: str, api_key: str, profile: str = \"default\") -&gt; None</code>","text":"<p>Override an API key in memory for the given profile (good for demos/notebooks). Does not persist.</p> <p>Example <pre><code># Switch the default profile to use a local LM Studio server at runtime\ncontext.llm_set_key(provider=\"lmstudio\", api_key=\"sk-ignore\", profile=\"default\")\ntext, _ = await context.llm().chat([\n    {\"role\":\"user\",\"content\":\"Say hi from LM Studio\"}\n])\n</code></pre></p> <p>For long\u2011lived storage, use your project\u2019s Secrets provider and <code>LLMService.persist_key(secret_name, api_key)</code> if available.</p>"},{"location":"reference/context-llm/#providerspecific-notes","title":"Provider\u2011specific notes","text":"<ul> <li> <p>OpenAI / OpenRouter / LM Studio / Ollama \u2013 uses OpenAI\u2011style <code>/chat/completions</code> and <code>/embeddings</code> routes. <code>usage</code> is included where supported.</p> </li> <li> <p>Azure OpenAI \u2013 requires <code>AZURE_OPENAI_ENDPOINT</code> and <code>AZURE_OPENAI_DEPLOYMENT</code>; uses Azure routes.</p> </li> <li> <p>Anthropic (Claude) \u2013 converts OpenAI\u2011style messages to Anthropic\u2019s message format; returns concatenated text blocks.</p> </li> <li> <p>Google (Gemini) \u2013 uses <code>:generateContent</code> and <code>:embedContent</code>; <code>usage</code> shape differs and may be empty.</p> </li> <li> <p>Embeddings \u2013 not supported for Anthropic in this client.</p> </li> </ul>"},{"location":"reference/context-llm/#error-handling-retries","title":"Error handling &amp; retries","text":"<p>The client wraps calls with exponential backoff (<code>_Retry</code>) for transient HTTP errors (<code>ReadTimeout</code>, <code>ConnectError</code>, <code>HTTPStatusError</code>). You may still want to catch and surface provider\u2011specific errors around quota/keys.</p> <p>Example <pre><code>try:\n    text, usage = await context.llm().chat([{ \"role\":\"user\", \"content\":\"ping\" }])\nexcept Exception as e:\n    await context.channel().send_text(f\"LLM error: {e}\")\n</code></pre></p>"},{"location":"reference/context-llm/#patterns-with-context","title":"Patterns with Context","text":"<p>Router\u2011then\u2011Act <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"router_then_act\")\nasync def router_then_act(*, context):\n    sys = {\"role\":\"system\",\"content\":\"Route to 'summarize' or 'plot'\"}\n    usr = {\"role\":\"user\",\"content\":\"Summarize this research log.\"}\n    decision, _ = await context.llm().chat([sys, usr], temperature=0.0)\n    if \"summarize\" in decision.lower():\n        # Call downstream tool and write a result\n        await context.memory().write_result(\n            topic=\"router\",\n            outputs=[{\"name\":\"route\",\"kind\":\"text\",\"value\":\"summarize\"}],\n        )\n        await context.channel().send_text(\"Routing \u2192 summarize\")\n</code></pre></p> <p>RAG: retrieve \u2192 answer <pre><code>@graph_fn(name=\"rag_answer\")\nasync def rag_answer(*, context, q: str):\n    hits = await context.memory().rag_search(corpus_id=\"notes\", query=q, k=6)\n    prompt = [{\"role\":\"system\",\"content\":\"Answer using the provided notes.\"},\n              {\"role\":\"user\",\"content\":\"\\n\\n\".join(h.get(\"text\",\"\") for h in hits) + \"\\n\\nQ: \" + q}]\n    text, usage = await context.llm().chat(prompt, temperature=0.2, model=\"gpt-4o-mini\")\n    return {\"answer\": text, \"tokens\": usage.get(\"total_tokens\")}\n</code></pre></p>"},{"location":"reference/context-llm/#summary","title":"Summary","text":"<ul> <li>Use <code>context.llm()</code> to get a ready\u2011to\u2011use client for the current profile.</li> <li><code>chat()</code> returns <code>(text, usage)</code>; <code>embed()</code> returns vectors.</li> <li>Switch keys ad\u2011hoc with <code>context.llm_set_key(...)</code>; persist via your Secrets provider when available.</li> </ul>"},{"location":"reference/context-logger/","title":"AetherGraph \u2014 <code>context.logger()</code> Quick Reference","text":"<p><code>context.logger()</code> returns a pre\u2011scoped Python <code>logging.Logger</code> bound to the current run/graph/node via the project\u2019s <code>StdLoggerService</code>.</p> <ul> <li>Namespace: <code>node.&lt;node_id&gt;</code></li> <li>Extra context on every record: <code>{run_id, graph_id, node_id}</code></li> <li>Outputs: console (text) + rotating file (<code>$LOG_DIR/aethergraph.log</code>), optional JSON, optional async QueueHandler</li> </ul>"},{"location":"reference/context-logger/#basics","title":"Basics","text":"<pre><code>log = context.logger()\nlog.info(\"starting step\")\nlog.debug(\"inputs\", extra={\"shape\": [n, d]})\nlog.warning(\"retrying\", extra={\"attempt\": i})\ntry:\n    ...\nexcept Exception:\n    log.exception(\"failed tool call\")  # includes traceback\n</code></pre> <p>Returns <code>logging.Logger</code> \u2014 fully configured for the current node/run.</p>"},{"location":"reference/context-logger/#formatting-levels-service-defaults","title":"Formatting &amp; levels (service defaults)","text":"<p>Configured by <code>StdLoggerService.build(cfg)</code> and <code>LoggingConfig</code>:</p> <ul> <li>Global level: <code>cfg.level</code> (e.g., <code>INFO</code>) with optional per\u2011namespace overrides</li> <li>Console formatter: <code>cfg.console_pattern</code></li> <li>File formatter: text (<code>cfg.file_pattern</code>) or JSON (<code>cfg.use_json = True</code>)</li> <li>File rotation: <code>cfg.max_bytes</code>, <code>cfg.backup_count</code></li> <li>Non\u2011blocking file I/O: <code>cfg.enable_queue = True</code> (QueueHandler + Listener)</li> </ul> <p>You can rebuild the service on server start to apply new settings.</p>"},{"location":"reference/context-logger/#structured-fields","title":"Structured fields","text":"<p>Every call accepts <code>extra={...}</code> for structured, searchable fields. The service injects <code>{run_id, graph_id, node_id}</code> automatically. <pre><code>log.info(\"optimizer step\", extra={\"lr\": 3e-4, \"batch\": 64, \"phase\": \"warmup\"})\n</code></pre></p>"},{"location":"reference/context-logger/#good-practices","title":"Good practices","text":"<ul> <li>Use <code>debug</code> for noisy internals; rely on <code>INFO</code> for milestone breadcrumbs.</li> <li>Prefer <code>extra={...}</code> over string concatenation for metrics/values.</li> <li>Use <code>exception()</code> within <code>except</code> blocks to capture tracebacks.</li> <li>Log artifact URIs (<code>extra={\"artifact\": uri}</code>) instead of large payloads.</li> </ul>"},{"location":"reference/context-logger/#oneliner-pattern-in-tools","title":"One\u2011liner pattern in tools","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"demo\")\nasync def demo(*, context):\n    log = context.logger()\n    log.info(\"hello\", extra={\"stage\": \"start\"})\n    # ... work ...\n    log.info(\"done\", extra={\"duration_ms\": 123})\n</code></pre>"},{"location":"reference/context-logger/#summary","title":"Summary","text":"<ul> <li>Call <code>context.logger()</code> inside graph/tools for a scoped logger.</li> <li>Structured fields available via <code>extra={...}</code>; run/graph/node auto\u2011injected.</li> </ul>"},{"location":"reference/context-mcp/","title":"AetherGraph \u2014 <code>context.mcp()</code> Reference","text":"<p>This page documents the Model Context Protocol (MCP) client you obtain with <code>context.mcp(name)</code>. Use it to call tools, list resources, or read resources exposed by a remote/local MCP server over stdio, WebSocket, or HTTP.</p> <p>Import surface (for examples below): <pre><code>from aethergraph.services.mcp import (\n    MCPService,\n    StdioMCPClient,\n    WsMCPClient,\n    HttpMCPClient,\n)\n</code></pre></p>"},{"location":"reference/context-mcp/#concepts","title":"Concepts","text":"<ul> <li>MCPService: registry of named MCP clients (e.g., <code>\"local\"</code>, <code>\"ws\"</code>, <code>\"http\"</code>), handles lazy open/close and convenience calls.</li> <li>MCPClientProtocol: transport\u2011specific client implementing <code>open()</code>, <code>close()</code>, <code>call(tool, params)</code>, <code>list_tools()</code>, <code>list_resources()</code>, <code>read_resource(uri)</code>.</li> <li>Tools: remote RPCs exposed by the MCP server (e.g., <code>readFile</code>, <code>search</code>, <code>stat</code>).</li> <li>Resources: server\u2011advertised URIs you can <code>read_resource()</code> (e.g., <code>file://\u2026</code>, <code>repo://\u2026</code>).</li> </ul> <p><code>context.mcp(name)</code> returns the client registered under <code>name</code> via your process\u2011global <code>MCPService</code>.</p>"},{"location":"reference/context-mcp/#mcpservice-registry","title":"MCPService (registry)","text":""},{"location":"reference/context-mcp/#register","title":"register","text":"<p><pre><code>register(name: str, client: MCPClientProtocol) -&gt; None\n</code></pre> Register a client under a name.</p>"},{"location":"reference/context-mcp/#remove","title":"remove","text":"<p><pre><code>remove(name: str) -&gt; None\n</code></pre> Unregister a client.</p>"},{"location":"reference/context-mcp/#has-names-get","title":"has / names / get","text":"<p><pre><code>has(name: str) -&gt; bool\nnames() -&gt; list[str]\nget(name: str = \"default\") -&gt; MCPClientProtocol\n</code></pre> Query and retrieve clients by name.</p>"},{"location":"reference/context-mcp/#open-close","title":"open / close","text":"<p><pre><code>open(name: str) -&gt; None\nclose(name: str) -&gt; None\nopen_all() -&gt; None\nclose_all() -&gt; None\n</code></pre> Manage client lifecycles. <code>call()/list_*()</code> implicitly <code>open()</code> on first use.</p>"},{"location":"reference/context-mcp/#call-helpers","title":"call helpers","text":"<p><pre><code>call(name: str, tool: str, params: dict | None = None) -&gt; dict\nlist_tools(name: str) -&gt; list[MCPTool]\nlist_resources(name: str) -&gt; list[MCPResource]\nread_resource(name: str, uri: str) -&gt; dict\n</code></pre> Thin wrappers to keep call sites small; auto\u2011open if needed.</p>"},{"location":"reference/context-mcp/#optional-secretsruntime-headers","title":"optional secrets/runtime headers","text":"<p><pre><code>set_header(name: str, key: str, value: str) -&gt; None\npersist_secret(secret_name: str, value: str) -&gt; None\n</code></pre> <code>set_header()</code> is handy for WS/HTTP auth tokens at runtime. <code>persist_secret()</code> stores a credential via your Secrets provider (if writable).</p>"},{"location":"reference/context-mcp/#transport-clients","title":"Transport clients","text":""},{"location":"reference/context-mcp/#stdiomcpclient","title":"StdioMCPClient","text":"<p><pre><code>StdioMCPClient(cmd: list[str], env: dict[str,str] | None = None, timeout: float = 60.0)\n</code></pre> Spawn a subprocess and speak JSON\u2011RPC over stdio.</p>"},{"location":"reference/context-mcp/#wsmcpclient","title":"WsMCPClient","text":"<p><pre><code>WsMCPClient(url: str, *, headers: dict[str,str] | None = None, timeout: float = 60.0, ping_interval: float = 20.0, ping_timeout: float = 10.0)\n</code></pre> Connect to an MCP server over WebSocket.</p>"},{"location":"reference/context-mcp/#httpmcpclient","title":"HttpMCPClient","text":"<p><pre><code>HttpMCPClient(base_url: str, *, headers: dict[str,str] | None = None, timeout: float = 60.0)\n</code></pre> Call an MCP server over HTTP (JSON).</p>"},{"location":"reference/context-mcp/#contextmcpname","title":"context.mcp(name)","text":"<p><pre><code>context.mcp(name: str) -&gt; MCPClientProtocol\n</code></pre> Return the named client. Typically you register names like <code>\"local\"</code>, <code>\"ws\"</code>, <code>\"http\"</code> during app startup, then retrieve them inside tools/agents.</p> <p>Example <pre><code>client = context.mcp(\"ws\")\nout = await client.call(\"search\", {\"q\": \"holography\", \"k\": 5})\n</code></pre></p>"},{"location":"reference/context-mcp/#calling-tools","title":"Calling tools","text":"<p><pre><code>client.call(tool: str, params: dict | None = None) -&gt; dict\n</code></pre> Invoke a remote tool by name with JSON\u2011serializable params.</p> <p>Parameters - tool (str) \u2013 Tool name (server\u2011defined). - params (dict, optional) \u2013 Arguments for the tool.</p> <p>Returns dict \u2013 Tool result payload (shape defined by the server).</p> <p>Example <pre><code># Filesystem\u2011like server\nres = await context.mcp(\"local\").call(\"readFile\", {\"path\": \"/data/notes.txt\"})\ntext = res.get(\"text\") or res.get(\"content\") or \"\"\nawait context.channel().send_text(f\"len={len(text)}\")\n</code></pre></p>"},{"location":"reference/context-mcp/#listing-tools-resources","title":"Listing tools &amp; resources","text":"<p><pre><code>client.list_tools() -&gt; list[MCPTool]\nclient.list_resources() -&gt; list[MCPResource]\nclient.read_resource(uri: str) -&gt; dict\n</code></pre> Enumerate server capabilities and read advertised resources.</p> <p>Example <pre><code># Tool discovery\nfor t in await context.mcp(\"http\").list_tools():\n    await context.channel().send_text(f\"tool: {t.name} \u2014 {t.description}\")\n\n# Resource fetch\nfor r in await context.mcp(\"ws\").list_resources():\n    if r.uri.startswith(\"file://\"):\n        blob = await context.mcp(\"ws\").read_resource(r.uri)\n        await context.channel().send_text(f\"read {r.uri} \u2192 {len(blob.get('text',''))} chars\")\n</code></pre></p>"},{"location":"reference/context-mcp/#endtoend-setup-startup","title":"End\u2011to\u2011end setup (startup)","text":"<pre><code>from aethergraph.services.mcp import MCPService, StdioMCPClient, WsMCPClient, HttpMCPClient\nfrom aethergraph.v3.core.runtime.runtime_services import set_mcp_service\nimport os, sys\n\nDEMO_HTTP_TOKEN = os.environ.setdefault(\"DEMO_HTTP_TOKEN\", \"demo_token_123\")\n\nmcp = MCPService()\nmcp.register(\"local\", StdioMCPClient(cmd=[sys.executable, \"-m\", \"aethergraph.plugins.mcp.fs_server\"]))\nmcp.register(\"ws\", WsMCPClient(url=\"ws://localhost:8765\", headers={\"Authorization\": \"Bearer demo_token_123\"}))\nmcp.register(\"http\", HttpMCPClient(\"http://127.0.0.1:8769\", headers={\"Authorization\": f\"Bearer {DEMO_HTTP_TOKEN}\"}))\n\nset_mcp_service(mcp)  # make available to NodeContext\n</code></pre>"},{"location":"reference/context-mcp/#using-inside-a-graph-function","title":"Using inside a graph function","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"mcp_search_demo\", inputs=[\"q\"], outputs=[\"text\"], version=\"0.1.0\")\nasync def mcp_search_demo(q: str, *, context):\n    out = await context.mcp(\"ws\").call(\"search\", {\"q\": q, \"k\": 5})\n    text = out.get(\"text\") or out.get(\"content\") or \"\"\n    await context.channel().send_text(text[:200] + (\"\u2026\" if len(text) &gt; 200 else \"\"))\n    return {\"text\": text}\n</code></pre>"},{"location":"reference/context-mcp/#choosing-a-transport","title":"Choosing a transport","text":"<ul> <li>stdio: best when you ship or control the server process (local tools, file system, Git, CLI wrappers). Minimal latency, simple auth via env.</li> <li>WebSocket: interactive servers that push events, need long\u2011lived sessions, or custom headers/tokens.</li> <li>HTTP: stateless request/response, easy to deploy behind gateways; good fit for cloud MCP services.</li> </ul> <p>Tip: You can register multiple transports to the same logical backend under different names (<code>\"fs-local\"</code>, <code>\"fs-ws\"</code>) and switch per call.</p>"},{"location":"reference/context-mcp/#auth-headers","title":"Auth &amp; headers","text":"<ul> <li>Pass headers at client construction (<code>headers={\"Authorization\": \"Bearer \u2026\"}</code>).</li> <li>Update at runtime via <code>MCPService.set_header(name, key, value)</code> for WS/HTTP clients.</li> <li>Persist tokens via <code>MCPService.persist_secret(...)</code> when your Secrets provider supports writes.</li> </ul>"},{"location":"reference/context-mcp/#error-handling","title":"Error handling","text":"<p>Wrap calls to surface clear messages back to the user. <pre><code>try:\n    res = await context.mcp(\"http\").call(\"search\", {\"q\": \"mtf\"})\nexcept KeyError:\n    await context.channel().send_text(\"Unknown MCP profile. Did you register it?\")\nexcept Exception as e:\n    await context.channel().send_text(f\"MCP error: {e}\")\n</code></pre></p>"},{"location":"reference/context-mcp/#summary","title":"Summary","text":"<ul> <li>Register your clients at startup with <code>MCPService.register()</code> and wire the service into runtime so <code>context.mcp(name)</code> can retrieve them.</li> <li>Use <code>.call()</code> for tools, <code>.list_tools()/.list_resources()</code> for discovery, and <code>.read_resource()</code> to fetch URIs.</li> <li>Choose stdio/WS/HTTP based on deployment and interaction needs; manage auth via headers/</li> </ul>"},{"location":"reference/context-memory/","title":"AetherGraph \u2014 <code>context.memory()</code> Reference","text":"<p>This page documents the MemoryFacade returned by <code>context.memory()</code> in a concise format: signature, brief description, parameters, returns, and practical examples. The facade coordinates three core components \u2014 HotLog (recent, transient), Persistence (durable JSONL/appends), and Indices (fast derived views) \u2014 with optional ArtifactStore, RAG, and LLM services.</p>"},{"location":"reference/context-memory/#overview","title":"Overview","text":"<p><code>context.memory()</code> is bound to your current runtime scope (<code>session_id</code>, <code>run_id</code>, <code>graph_id</code>, <code>node_id</code>, <code>agent_id</code>). Typical operations:</p> <ol> <li> <p>Record events (raw or typed results)</p> </li> <li> <p>Query recent/last/by\u2011kind outputs via indices/hotlog</p> </li> <li> <p>Distill (rolling summaries, episode summaries)</p> </li> <li> <p>RAG (optional): upsert, search, answer using a configured RAG + LLM</p> </li> </ol>"},{"location":"reference/context-memory/#memoryrecord_raw","title":"memory.record_raw","text":"<p><pre><code>record_raw(*, base: dict, text: str | None = None, metrics: dict | None = None, sources: list[str] | None = None) -&gt; Event\n</code></pre> Append a normalized event to HotLog (fast) and Persistence (durable). Computes a stable <code>event_id</code> and a lightweight <code>signal</code> if absent.</p> <p>Parameters</p> <ul> <li> <p>base (dict) \u2013 Canonical fields describing the event (e.g., <code>kind</code>, <code>stage</code>, <code>severity</code>, <code>tool</code>, <code>tags</code>, <code>entities</code>, <code>inputs</code>, <code>outputs</code>, \u2026). Missing scope keys are filled from the bound context.</p> </li> <li> <p>text (str, optional) \u2013 Human\u2011readable message/body.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics (latency, token counts, costs, etc.).</p> </li> <li> <p>sources (list[str], optional) \u2013 Event IDs this event summarizes/derives from.</p> </li> </ul> <p>Returns Event \u2013 The appended event.</p> <p>Notes Does not update <code>indices</code> automatically. Use <code>write_result()</code> when you want indices updated for typed outputs.</p>"},{"location":"reference/context-memory/#memoryrecord","title":"memory.record","text":"<p><pre><code>record(kind, data, tags=None, entities=None, severity=2, stage=None, inputs_ref=None, outputs_ref=None, metrics=None, sources=None, signal=None) -&gt; Event\n</code></pre> Convenience wrapper around <code>record_raw()</code> for common fields; stringifies <code>data</code> if needed.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Event kind (e.g., <code>\"user_msg\"</code>, <code>\"tool_call\"</code>).</p> </li> <li> <p>data (Any) \u2013 JSON\u2011serializable payload; will be stringified for <code>text</code>.</p> </li> <li> <p>tags (list[str], optional) \u2013 Tag list.</p> </li> <li> <p>entities (list[str], optional) \u2013 Entity IDs.</p> </li> <li> <p>severity (int) \u2013 1\u20135 scale (default 2).</p> </li> <li> <p>stage (str, optional) \u2013 Phase label (e.g., <code>\"observe\"</code>, <code>\"act\"</code>).</p> </li> <li> <p>inputs_ref (list[dict], optional) \u2013 Typed input references (Value[]).</p> </li> <li> <p>outputs_ref (list[dict], optional) \u2013 Typed output references (Value[]).</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics.</p> </li> <li> <p>sources (list[str], optional) \u2013 Upstream event IDs.</p> </li> <li> <p>signal (float, optional) \u2013 0.0\u20131.0; if omitted, computed heuristically.</p> </li> </ul> <p>Returns Event \u2013 The appended event.</p>"},{"location":"reference/context-memory/#memorywrite_result","title":"memory.write_result","text":"<p><pre><code>write_result(*, topic: str, inputs: list[dict] | None = None, outputs: list[dict] | None = None, tags: list[str] | None = None, metrics: dict | None = None, message: str | None = None, severity: int = 3) -&gt; Event\n</code></pre> Record a typed result (tool/agent/flow) and update indices for quick retrieval.</p> <p>Parameters</p> <ul> <li> <p>topic (str) \u2013 Tool/agent/flow identifier (used by <code>indices.last_outputs_by_topic</code>).</p> </li> <li> <p>inputs (list[dict], optional) \u2013 Typed inputs (Value[]).</p> </li> <li> <p>outputs (list[dict], optional) \u2013 Typed outputs (Value[]). Indices derive from these.</p> </li> <li> <p>tags (list[str], optional) \u2013 Tag list.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics.</p> </li> <li> <p>message (str, optional) \u2013 Human\u2011readable summary.</p> </li> <li> <p>severity (int) \u2013 Default 3.</p> </li> </ul> <p>Returns Event \u2013 The normalized <code>tool_result</code> event.</p> <p>Effect Auto\u2011appends to HotLog &amp; Persistence and calls <code>indices.update(session_id, evt)</code>.</p>"},{"location":"reference/context-memory/#memoryrecent","title":"memory.recent","text":"<p><pre><code>recent(*, kinds: list[str] | None = None, limit: int = 50) -&gt; list[Event]\n</code></pre> Return recent events from HotLog (most recent last), optionally filtering by <code>kinds</code>.</p> <p>Parameters</p> <ul> <li> <p>kinds (list[str], optional) \u2013 Filter kinds.</p> </li> <li> <p>limit (int) \u2013 Max events (default 50).</p> </li> </ul> <p>Returns list[Event] \u2013 Recent events.</p>"},{"location":"reference/context-memory/#memorylast_by_name","title":"memory.last_by_name","text":"<p><pre><code>last_by_name(name: str)\n</code></pre> Return the last output value by <code>name</code> from Indices (fast path).</p> <p>Parameters</p> <ul> <li>name (str) \u2013 Output name.</li> </ul> <p>Returns Any \u2013 The stored value for that name (adapter\u2011dependent) or <code>None</code>.</p>"},{"location":"reference/context-memory/#memorylatest_refs_by_kind","title":"memory.latest_refs_by_kind","text":"<p><pre><code>latest_refs_by_kind(kind: str, *, limit: int = 50)\n</code></pre> Return latest ref outputs by <code>ref.kind</code> (fast path, KV\u2011backed) from Indices.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Reference kind.</p> </li> <li> <p>limit (int) \u2013 Max items (default 50).</p> </li> </ul> <p>Returns list[Any] \u2013 Recent references.</p>"},{"location":"reference/context-memory/#memorylast_outputs_by_topic","title":"memory.last_outputs_by_topic","text":"<p><pre><code>last_outputs_by_topic(topic: str)\n</code></pre> Return the last output map for a given topic (tool/flow/agent) from Indices.</p> <p>Parameters</p> <ul> <li>topic (str) \u2013 Topic identifier.</li> </ul> <p>Returns dict | None \u2013 Latest outputs or <code>None</code> if absent.</p>"},{"location":"reference/context-memory/#memorydistill_rolling_chat","title":"memory.distill_rolling_chat","text":"<p><pre><code>distill_rolling_chat(*, max_turns: int = 20, min_signal: float | None = None) -&gt; dict\n</code></pre> Build a rolling chat summary from recent user/assistant turns (reads HotLog; typically writes a JSON summary via Persistence).</p> <p>Parameters</p> <ul> <li> <p>max_turns (int) \u2013 Window of turns to include (default 20).</p> </li> <li> <p>min_signal (float, optional) \u2013 Signal threshold; uses facade default if omitted.</p> </li> </ul> <p>Returns dict \u2013 Descriptor (e.g., <code>{ \"uri\": ..., \"sources\": [...] }</code>).</p>"},{"location":"reference/context-memory/#memorydistill_episode","title":"memory.distill_episode","text":"<p><pre><code>distill_episode(*, tool: str, run_id: str, include_metrics: bool = True) -&gt; dict\n</code></pre> Summarize a tool/agent episode (all events for a given <code>run_id</code> + <code>tool</code>). Reads HotLog/Persistence; writes back a summary JSON (and optionally CAS bundle).</p> <p>Parameters</p> <ul> <li> <p>tool (str) \u2013 Tool/agent identifier.</p> </li> <li> <p>run_id (str) \u2013 Run to summarize.</p> </li> <li> <p>include_metrics (bool) \u2013 Include metrics in the summary (default True).</p> </li> </ul> <p>Returns dict \u2013 Descriptor (e.g., <code>{ \"uri\": ..., \"sources\": [...], \"metrics\": {...} }</code>).</p>"},{"location":"reference/context-memory/#rag-helpers-optional","title":"RAG helpers (optional)","text":""},{"location":"reference/context-memory/#memoryrag_upsert","title":"memory.rag_upsert","text":"<p><pre><code>rag_upsert(*, corpus_id: str, docs: Sequence[dict], topic: str | None = None) -&gt; dict\n</code></pre> Upsert documents into a RAG corpus via the configured RAG facade.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>docs (Sequence[dict]) \u2013 Documents/chunks with text and metadata.</p> </li> <li> <p>topic (str, optional) \u2013 Optional topic name to attribute the upsert.</p> </li> </ul> <p>Returns dict \u2013 Upsert stats (shape adapter\u2011specific).</p> <p>Raises <code>RuntimeError</code> \u2013 if RAG facade is not configured.</p>"},{"location":"reference/context-memory/#memoryrag_search","title":"memory.rag_search","text":"<p><pre><code>rag_search(*, corpus_id: str, query: str, k: int = 8) -&gt; list[dict]\n</code></pre> Retrieve best\u2011matching chunks for a query.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>query (str) \u2013 Natural language query.</p> </li> <li> <p>k (int) \u2013 Max results (default 8), reranked.</p> </li> </ul> <p>Returns list[dict] \u2013 Ranked hits.</p> <p>Raises <code>RuntimeError</code> \u2013 if RAG facade is not configured.</p>"},{"location":"reference/context-memory/#memoryrag_answer","title":"memory.rag_answer","text":"<p><pre><code>rag_answer(*, corpus_id: str, question: str, style: str = \"concise\", k: int = 6, llm_profile: str = \"default\") -&gt; dict\n</code></pre> Answer a question using RAG + LLM (both must be configured).</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>question (str) \u2013 User question.</p> </li> <li> <p>style (str) \u2013 Answering style (e.g., <code>\"concise\"</code>).</p> </li> <li> <p>k (int) \u2013 Max retrieved chunks (default 6).</p> </li> <li> <p>llm_profile (str) \u2013 Profile name to select an LLM client.</p> </li> </ul> <p>Returns dict \u2013 Answer payload (adapter\u2011specific).</p> <p>Raises <code>RuntimeError</code> \u2013 if RAG or LLM is not configured.</p>"},{"location":"reference/context-memory/#practical-examples","title":"Practical examples","text":"<p>1) Record + recent <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"mem_record_recent\")\nasync def mem_record_recent(*, context):\n    evt = await context.memory().record(\n        kind=\"user_msg\",\n        data={\"text\":\"hello world\",\"lang\":\"en\"},\n        tags=[\"demo\",\"quickstart\"],\n        severity=2,\n    )\n    recent = await context.memory().recent(kinds=[\"user_msg\"], limit=5)\n    await context.channel().send_text(f\"recent user_msg count={len(recent)}\")\n    return {\"event_id\": evt.event_id, \"recent_count\": len(recent)}\n</code></pre></p> <p>2) Write a typed result and fetch last outputs <pre><code>@graph_fn(name=\"mem_write_result\")\nasync def mem_write_result(*, context):\n    await context.memory().write_result(\n        topic=\"eval.step\",\n        outputs=[{\"name\":\"acc\",\"kind\":\"number\",\"value\":0.912}],\n        metrics={\"latency_ms\": 120},\n        message=\"evaluation complete\",\n    )\n    last = await context.memory().last_outputs_by_topic(\"eval.step\")\n    await context.channel().send_text(f\"last acc={last['acc']:.3f}\")\n</code></pre></p> <p>3) Rolling chat summary <pre><code>@graph_fn(name=\"mem_rolling\")\nasync def mem_rolling(*, context):\n    summary = await context.memory().distill_rolling_chat(max_turns=16)\n    await context.channel().send_text(f\"rolling summary uri: {summary.get('uri','&lt;none&gt;')}\")\n</code></pre></p> <p>4) Episode summary <pre><code>@graph_fn(name=\"mem_episode\")\nasync def mem_episode(*, context, run_id: str, tool: str):\n    desc = await context.memory().distill_episode(tool=tool, run_id=run_id)\n    await context.channel().send_text(f\"episode summary: {desc.get('uri','&lt;none&gt;')}\")\n</code></pre></p> <p>5) RAG (if configured) <pre><code>@graph_fn(name=\"mem_rag\")\nasync def mem_rag(*, context):\n    # Upsert a few docs\n    await context.memory().rag_upsert(\n        corpus_id=\"notes\",\n        docs=[{\"id\":\"1\",\"text\":\"Optics basics: Snell's law\"}],\n    )\n    # Search\n    hits = await context.memory().rag_search(corpus_id=\"notes\", query=\"Snell\")\n    # Answer\n    ans = await context.memory().rag_answer(corpus_id=\"notes\", question=\"What is Snell's law?\", style=\"concise\")\n    await context.channel().send_text(ans.get(\"answer\",\"&lt;no answer&gt;\"))\n</code></pre></p>"},{"location":"reference/context-memory/#notes-behaviors","title":"Notes &amp; behaviors","text":"<ul> <li> <p>Signal heuristic: if not provided, <code>record(_raw)</code> computes a 0.0\u20131.0 <code>signal</code> from severity + presence/length of text + metrics.</p> </li> <li> <p>Durability: every <code>record_raw</code> &amp; <code>write_result</code> appends to Persistence; <code>recent()</code> reads from HotLog.</p> </li> <li> <p>Indices: <code>write_result()</code> updates fast views used by <code>last_by_name</code>, <code>latest_refs_by_kind</code>, <code>last_outputs_by_topic</code>.</p> </li> <li> <p>Artifacts: distillers may produce CAS artifacts when an <code>ArtifactStore</code> is provided.</p> </li> <li> <p>Performance: methods are async; backends should avoid blocking the event loop (use <code>asyncio.to_thread</code> for heavy IO).</p> </li> </ul>"},{"location":"reference/context-rag/","title":"AetherGraph \u2014 <code>context.rag()</code> Reference","text":"<p>This page documents the RAGFacade returned by <code>context.rag()</code> in a concise format: signature, brief description, parameters, returns, and practical examples.</p> <p>The facade covers: corpus management, document ingestion (upsert), retrieval (search/retrieve), and question answering with optional citation resolution.</p>"},{"location":"reference/context-rag/#overview","title":"Overview","text":"<p><code>context.rag()</code> provides high\u2011level helpers backed by:</p> <ul> <li>an Artifact Store (for persisted doc assets),</li> <li>an Embedding client (e.g., <code>context.llm().embed()</code>),</li> <li>a Vector index backend (add/search),</li> <li>a TextSplitter (chunking before embedding), and</li> <li>an optional LLM client for QA.</li> </ul>"},{"location":"reference/context-rag/#ragadd_corpus","title":"rag.add_corpus","text":"<p><pre><code>add_corpus(corpus_id: str, meta: dict | None = None) -&gt; None\n</code></pre> Create a new corpus directory with metadata if it does not exist.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Unique identifier for the corpus.</p> </li> <li> <p>meta (dict, optional) \u2013 Arbitrary metadata stored alongside the corpus.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-rag/#ragupsert_docs","title":"rag.upsert_docs","text":"<p><pre><code>upsert_docs(corpus_id: str, docs: list[dict]) -&gt; dict\n</code></pre> Ingest and index a list of documents (file\u2011based or inline text). Handles artifact persistence, chunking, embedding, and index add.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>docs (list[dict]) \u2013 Each doc is either:</p> </li> <li> <p>File doc: <code>{ \"path\": \"/path/to/file.pdf\", \"labels\": {...}, \"title\": \"Optional\" }</code></p> </li> <li> <p>Inline text doc: <code>{ \"text\": \"...\", \"title\": \"Optional\", \"labels\": {...} }</code></p> </li> </ul> <p>Returns dict \u2013 Summary like <code>{ \"added\": int, \"chunks\": int, \"index\": \"BackendName\" }</code>.</p> <p>Notes - PDFs and Markdown are parsed with built\u2011in extractors; other files default to text.</p> <ul> <li>Each doc and chunk is assigned a stable SHA\u2011derived ID and recorded in <code>docs.jsonl</code> / <code>chunks.jsonl</code> under the corpus folder.</li> </ul>"},{"location":"reference/context-rag/#ragsearch","title":"rag.search","text":"<p><pre><code>search(corpus_id: str, query: str, k: int = 8, filters: dict | None = None, mode: str = \"hybrid\") -&gt; list[SearchHit]\n</code></pre> Hybrid retrieval: dense vector search with optional lexical fusion, returning the top\u2011k chunks.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>query (str) \u2013 Natural language query.</p> </li> <li> <p>k (int) \u2013 Number of results (default 8).</p> </li> <li> <p>filters (dict, optional) \u2013 Reserved for metadata filtering (adapter\u2011specific).</p> </li> <li> <p>mode ({\"dense\",\"hybrid\"}) \u2013 Retrieval mode (default <code>\"hybrid\"</code>).</p> </li> </ul> <p>Returns list[SearchHit] \u2013 Ranked hits with <code>chunk_id</code>, <code>doc_id</code>, <code>corpus_id</code>, <code>score</code>, <code>text</code>, <code>meta</code>.</p>"},{"location":"reference/context-rag/#ragretrieve","title":"rag.retrieve","text":"<p><pre><code>retrieve(corpus_id: str, query: str, k: int = 6, rerank: bool = True) -&gt; list[SearchHit]\n</code></pre> Convenience wrapper over <code>search(..., mode=\"hybrid\")</code> for top\u2011k retrieval.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>query (str) \u2013 Natural language query.</p> </li> <li> <p>k (int) \u2013 Number of results (default 6).</p> </li> <li> <p>rerank (bool) \u2013 Currently ignored (hybrid already fuses scores).</p> </li> </ul> <p>Returns list[SearchHit] \u2013 Ranked hits.</p>"},{"location":"reference/context-rag/#raganswer","title":"rag.answer","text":"<p><pre><code>answer(corpus_id: str, question: str, *, llm: GenericLLMClient | None = None, style: str = \"concise\", with_citations: bool = True, k: int = 6) -&gt; dict\n</code></pre> Answer a question using retrieved context and an LLM.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>question (str) \u2013 End\u2011user question.</p> </li> <li> <p>llm (GenericLLMClient, optional) \u2013 LLM to use; defaults to the facade\u2019s configured client.</p> </li> <li> <p>style ({\"concise\",\"detailed\"}) \u2013 Answer verbosity/style.</p> </li> <li> <p>with_citations (bool) \u2013 Whether to include resolved citations.</p> </li> <li> <p>k (int) \u2013 Retrieval depth (default 6).</p> </li> </ul> <p>Returns dict \u2013 <code>{ \"answer\": str, \"citations\": [...], \"usage\": {...}, \"resolved_citations\": [...]? }</code>.</p> <p>Behavior - Builds a context block from top\u2011k chunks (numbered <code>[1]</code>, <code>[2]</code>, ...).</p> <ul> <li>Prompts the LLM to answer only from the provided context and cite chunk numbers.</li> </ul>"},{"location":"reference/context-rag/#ragresolve_citations","title":"rag.resolve_citations","text":"<p><pre><code>resolve_citations(corpus_id: str, citations: list[dict]) -&gt; list[dict]\n</code></pre> Resolve citation metadata for display/download.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>citations (list[dict]) \u2013 Items like <code>{ \"chunk_id\", \"doc_id\", \"rank\" }</code>.</p> </li> </ul> <p>Returns list[dict] \u2013 Sorted by <code>rank</code>, each <code>{ rank, doc_id, title, uri, chunk_id, snippet }</code>.</p>"},{"location":"reference/context-rag/#practical-examples","title":"Practical examples","text":"<p>1) Create a corpus and ingest docs <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"rag_ingest\")\nasync def rag_ingest(*, context):\n    await context.rag().add_corpus(\"notes\")\n    stats = await context.rag().upsert_docs(\n        corpus_id=\"notes\",\n        docs=[\n            {\"text\": \"Optics basics: Snell's law relates angles of incidence and refraction.\" , \"title\": \"optics\"},\n            {\"path\": \"/data/papers/holography.md\", \"labels\": {\"topic\": \"holography\"}},\n        ],\n    )\n    await context.channel().send_text(f\"RAG upsert: {stats}\")\n</code></pre></p> <p>2) Search and preview hits <pre><code>@graph_fn(name=\"rag_search_preview\")\nasync def rag_search_preview(*, context, q: str):\n    hits = await context.rag().search(corpus_id=\"notes\", query=q, k=5)\n    for i, h in enumerate(hits, 1):\n        await context.channel().send_text(f\"[{i}] score={h.score:.3f}  doc={h.doc_id}\\n{h.text[:200]}\")\n</code></pre></p> <p>3) Answer with citations <pre><code>@graph_fn(name=\"rag_answer_with_citations\")\nasync def rag_answer_with_citations(*, context, q: str):\n    out = await context.rag().answer(corpus_id=\"notes\", question=q, style=\"concise\", k=6)\n    ans = out.get(\"answer\", \"\")\n    cites = out.get(\"resolved_citations\", [])\n    await context.channel().send_text(ans)\n    for c in cites[:3]:\n        await context.channel().send_text(f\"[#{c['rank']}] {c['title']} \u2014 {c['snippet']}\")\n</code></pre></p>"},{"location":"reference/context-rag/#notes-behaviors","title":"Notes &amp; behaviors","text":"<ul> <li> <p>Chunking &amp; Embedding: Documents are split via <code>TextSplitter</code> then embedded in batch; the index stores <code>(chunk_id, vector, meta)</code>.</p> </li> <li> <p>Artifacts: File docs and inline text are persisted to the Artifact Store; returned URIs appear in doc metadata and resolved citations.</p> </li> <li> <p>IDs: <code>doc_id</code> and <code>chunk_id</code> are stable SHA\u2011derived IDs; re\u2011ingesting the same content usually yields the same IDs (subject to meta changes).</p> </li> <li> <p>Filters: <code>filters</code> is reserved for future adapter support (label\u2011based narrowing).</p> </li> <li> <p>LLM &amp; Usage: <code>answer()</code> returns provider usage where available; some providers may omit it.</p> </li> </ul>"},{"location":"reference/decorators/","title":"Decorator API \u2014 <code>@graph_fn</code>, <code>@graphify</code>, <code>@tool</code>","text":"<p>A single reference page for the three core decorators you\u2019ll use to build with AetherGraph.</p>"},{"location":"reference/decorators/#quick-chooser","title":"Quick chooser","text":"Use this when\u2026 Pick Why You want the quickest way to make a Python function runnable as a graph entrypoint and get a <code>context</code> for services <code>@graph_fn</code> Small, ergonomic, ideal for tutorials, notebooks, single\u2011entry tools/agents You need to expose reusable steps with typed I/O that can run standalone or as graph nodes <code>@tool</code> Dual\u2011mode decorator; gives you fine control of inputs/outputs; portable and composable Your function body is mostly tool wiring (fan\u2011in/fan\u2011out) and you want a static graph spec from Python syntax <code>@graphify</code> Author graphs declaratively; returns a <code>TaskGraph</code> factory; great for orchestration patterns"},{"location":"reference/decorators/#graph_fn","title":"<code>@graph_fn</code>","text":"<p>Wrap a normal async function into a runnable graph with optional <code>context</code> injection.</p>"},{"location":"reference/decorators/#signature","title":"Signature","text":"<pre><code>@graph_fn(name: str, *, inputs: list[str] | None = None, outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\nasync def my_fn(..., *, context: NodeContext): ...\n</code></pre>"},{"location":"reference/decorators/#description","title":"Description","text":"<ul> <li>Builds a fresh <code>TaskGraph</code> under the hood and executes it immediately.</li> <li>If your function signature includes <code>context: NodeContext</code>, AetherGraph injects a <code>NodeContext</code> so you can call <code>context.channel()</code>, <code>context.memory()</code>, <code>context.artifacts()</code>, <code>context.llm()</code>, etc.</li> <li>Ideal for single\u2011file demos, CLI/notebook usage, and simple agents.</li> </ul>"},{"location":"reference/decorators/#parameters","title":"Parameters","text":"<ul> <li>name (str, required) \u2014 Graph ID and human\u2011readable name.</li> <li>inputs (list[str], optional) \u2014 Declared input keys. Purely declarative; your function still gets normal Python args.</li> <li>outputs (list[str], optional) \u2014 Declared output keys. If you return a single literal, declare exactly one.</li> <li>version (str, optional) \u2014 Semantic version for registry.</li> <li>agent (str, optional) \u2014 If provided, registers this graph function as an agent under the given name.</li> </ul>"},{"location":"reference/decorators/#returns","title":"Returns","text":"<ul> <li>The decorator returns a <code>GraphFunction</code> object. Calling/awaiting it executes the graph and returns a <code>dict</code> of outputs keyed by <code>outputs</code>.</li> </ul>"},{"location":"reference/decorators/#minimal-example","title":"Minimal example","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"hello.world\", inputs=[\"name\"], outputs=[\"greeting\"], version=\"0.1.0\")\nasync def hello_world(name: str, *, context: NodeContext):\n    await context.channel().send_text(f\"\ud83d\udc4b Hello {name}\")\n    return {\"greeting\": f\"Hello, {name}!\"}\n\n# Run (async)\nres = await hello_world(name=\"Aether\")\nprint(res[\"greeting\"])  # \u2192 \"Hello, Aether!\"\n</code></pre>"},{"location":"reference/decorators/#tips","title":"Tips","text":"<ul> <li>Return a <code>dict</code> where keys match <code>outputs</code>. If you return a single literal, declare one output.</li> <li>You can call <code>@tool</code> functions inside a <code>@graph_fn</code> (they\u2019ll run immediately, not build nodes). Use this for small, fast helper steps.</li> <li>For complex orchestration (fan\u2011in/fan\u2011out), prefer <code>@graphify</code> so <code>@tool</code> calls become nodes.</li> </ul>"},{"location":"reference/decorators/#tool","title":"<code>@tool</code>","text":"<p>Dual\u2011mode decorator for reusable steps with explicit inputs/outputs.</p>"},{"location":"reference/decorators/#signature_1","title":"Signature","text":"<pre><code>@tool(outputs: list[str], *, inputs: list[str] | None = None, name: str | None = None, version: str = \"0.1.0\")\ndef/async def my_tool(...): ...\n</code></pre>"},{"location":"reference/decorators/#description_1","title":"Description","text":"<ul> <li>Immediate mode (no builder/interpreter active): calling the function executes it now and returns a <code>dict</code> of outputs.</li> <li>Graph mode (inside a <code>graph(...)</code> / <code>@graphify</code> body or during <code>@graph_fn</code> build): calling the proxy adds a node to the current graph and returns a <code>NodeHandle</code> with typed outputs.</li> <li>Registers the underlying implementation in the runtime registry for portability.</li> </ul>"},{"location":"reference/decorators/#parameters_1","title":"Parameters","text":"<ul> <li>outputs (list[str], required) \u2014 Names of output values (e.g., <code>[\"result\"]</code>, <code>[\"image\", \"stats\"]</code>).</li> <li>inputs (list[str], optional) \u2014 Input names (auto\u2011inferred from signature if omitted).</li> <li>name (str, optional) \u2014 Registry/display name; defaults to function name.</li> <li>version (str, optional) \u2014 Semantic version for registry.</li> </ul>"},{"location":"reference/decorators/#returns_1","title":"Returns","text":"<ul> <li>In immediate mode: <code>dict</code> of outputs.</li> <li>In graph mode: <code>NodeHandle</code> with <code>.out_key</code> attributes (e.g., <code>node.result</code>).</li> </ul>"},{"location":"reference/decorators/#example-reusable-step","title":"Example \u2014 reusable step","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"sum\", \"count\"])\ndef aggregate(xs: list[int]):\n    return {\"sum\": sum(xs), \"count\": len(xs)}\n\n# Immediate mode\nprint(aggregate([1,2,3]))  # {\"sum\": 6, \"count\": 3}\n</code></pre>"},{"location":"reference/decorators/#example-using-tool-inside-graph_fn-immediate-execution","title":"Example \u2014 using <code>@tool</code> inside <code>@graph_fn</code> (immediate execution)","text":"<pre><code>from aethergraph import graph_fn, tool, NodeContext\n\n@tool(outputs=[\"sum\"])  \ndef add(x: int, y: int):\n    return {\"sum\": x + y}\n\n@graph_fn(name=\"calc.pipeline\", inputs=[\"a\",\"b\"], outputs=[\"total\"])\nasync def calc(a: int, b: int, *, context: NodeContext):\n    out = add(a, b)                 # immediate mode here\n    await context.channel().send_text(f\"sum = {out['sum']}\")\n    return {\"total\": out[\"sum\"]}\n</code></pre>"},{"location":"reference/decorators/#tips_1","title":"Tips","text":"<ul> <li>Use <code>@tool</code> to make steps portable and inspectable (typed I/O makes graphs predictable).</li> <li>In <code>@graph_fn</code> the <code>@tool</code> call executes immediately; in <code>@graphify</code> the same call becomes a graph node.</li> <li>Control\u2011flow knobs like <code>_after</code>, <code>_id</code>, <code>_alias</code> apply only in graph\u2011building contexts (e.g., <code>@graphify</code>), not in <code>@graph_fn</code> bodies.</li> </ul>"},{"location":"reference/decorators/#graphify","title":"<code>@graphify</code>","text":"<p>Author a static TaskGraph by writing normal Python that calls <code>@tool</code>s. The function body executes during build to register nodes and edges; returned node handles/literals define graph outputs.</p>"},{"location":"reference/decorators/#signature_2","title":"Signature","text":"<pre><code>@graphify(*, name: str = \"default_graph\", inputs: Iterable[str] | dict = (), outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\ndef my_graph(...):\n    ...  # body calls @tool proxies (graph mode)\n    return {...}  # NodeHandle(s) and/or literal refs\n</code></pre>"},{"location":"reference/decorators/#description_2","title":"Description","text":"<ul> <li>The decorated function becomes a factory: calling <code>my_graph.build()</code> returns a <code>TaskGraph</code> spec.</li> <li>When the body runs under the builder, calls to <code>@tool</code> proxies add nodes to the graph and return <code>NodeHandle</code>s.</li> <li>Perfect for fan\u2011out (parallel branches) and fan\u2011in (join/aggregate) patterns.</li> </ul>"},{"location":"reference/decorators/#parameters_2","title":"Parameters","text":"<ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (iterable[str] or dict) \u2014 Required/optional input names. If dict, keys are optional names with defaults in the body.</li> <li>outputs (list[str], optional) \u2014 Names of exposed boundary outputs. If body returns a single literal, declare exactly one.</li> <li>version (str) \u2014 Semantic version.</li> <li>agent (str, optional) \u2014 Register this graph as an agent (factory registered).</li> </ul>"},{"location":"reference/decorators/#returns_2","title":"Returns","text":"<ul> <li> <p>The decorator returns a builder function with:</p> </li> <li> <p><code>.build() -&gt; TaskGraph</code></p> </li> <li><code>.spec() -&gt; TaskGraphSpec</code></li> <li><code>.io() -&gt; IO signature</code></li> </ul>"},{"location":"reference/decorators/#example-fanout-fanin","title":"Example \u2014 fan\u2011out + fan\u2011in","text":"<pre><code>from aethergraph import graphify, tool\n\n@tool(outputs=[\"y\"])\ndef f(x: int):\n    return {\"y\": x * x}\n\n@tool(outputs=[\"z\"])\ndef g(x: int):\n    return {\"z\": x + 1}\n\n@tool(outputs=[\"sum\"])  \ndef add(a: int, b: int):\n    return {\"sum\": a + b}\n\n@graphify(name=\"fan_in_out\", inputs=[\"x\"], outputs=[\"total\"]) \ndef pipe(x):\n    a = f(x=x)          # node A (graph mode)  \u2510\n    b = g(x=x)          # node B (graph mode)  \u2518  \u2190 fan\u2011out\n    c = add(a=a.y, b=b.z)   # node C depends on A,B \u2190 fan\u2011in\n    return {\"total\": c.sum}\n\nG = pipe.build()\n</code></pre>"},{"location":"reference/decorators/#example-ordering-with-_after-and-aliasing","title":"Example \u2014 ordering with <code>_after</code> and aliasing","text":"<pre><code>@tool(outputs=[\"out\"]) \ndef step(name: str):\n    return {\"out\": name}\n\n@graphify(name=\"ordered\", inputs=[]) \ndef ordered():\n    a = step(name=\"A\", _alias=\"first\")\n    b = step(name=\"B\", _after=a)\n    c = step(name=\"C\", _after=[a, b], _id=\"third\")\n    return {\"final\": c.out}\n\nG = ordered.build()\n</code></pre>"},{"location":"reference/decorators/#using-tool-inside-graph_fn-vs-graphify","title":"Using <code>@tool</code> inside <code>@graph_fn</code> vs <code>@graphify</code>","text":"<ul> <li>Inside <code>@graph_fn</code>: <code>@tool</code> calls execute immediately (no <code>_after</code>/alias). Great for quick helpers.</li> <li>Inside <code>@graphify</code>: <code>@tool</code> calls define nodes (support <code>_after</code>, <code>_alias</code>, <code>_id</code>, <code>_labels</code>). Ideal for orchestration.</li> </ul>"},{"location":"reference/decorators/#interop-best-practices","title":"Interop &amp; best practices","text":"<ol> <li>Start simple with <code>@graph_fn</code> \u2014 it\u2019s the easiest way to get <code>context</code> and ship a working demo.</li> <li>Extract reusable steps with <code>@tool</code> \u2014 typed I/O makes debugging, tracing, and promotion to graphs trivial.</li> <li> <p>Promote to <code>@graphify</code> when you need:</p> </li> <li> <p>Parallel branches (fan\u2011out), joins (fan\u2011in)</p> </li> <li>Explicit ordering with <code>_after</code></li> <li>Reuse via <code>NodeHandle</code> composition and aliasing</li> <li> <p>Context access:</p> </li> <li> <p><code>@graph_fn</code> gives you <code>context: NodeContext</code> directly.</p> </li> <li>In <code>@graphify</code>, nodes don\u2019t get <code>context</code>; tools run with context at execution time when the graph is interpreted. Use <code>@tool</code> implementations to call <code>context.*</code>.</li> <li>Outputs discipline \u2014 keep outputs small and typed (e.g., <code>{ \"image\": ref, \"metrics\": {\u2026} }</code>).</li> <li>Registry \u2014 all three decorators register artifacts (graph fn as runnable, tool impls, graph factories) so you can call by name later.</li> </ol>"},{"location":"reference/decorators/#see-also","title":"See also","text":"<ul> <li>Quick Start: install, start server, first <code>@graph_fn</code>.</li> <li>**Contex</li> </ul>"},{"location":"reference/rest-api/","title":"REST API","text":"<ul> <li><code>GET /health</code> \u2192 200 OK</li> <li><code>POST /execute</code> \u2192 Execute a graph function</li> <li><code>GET/PUT /artifacts/*</code> \u2192 Retrieve/store artifacts</li> </ul> <p>(Add OpenAPI/Redoc when ready.)</p>"},{"location":"reference/tools-facade/","title":"Tools Facade","text":""},{"location":"reference/tools-facade/#registerfunc-namenone-inputsnone-outputsnone-str","title":"register(func, *, name=None, inputs=None, outputs=None) \u2192 str","text":"<p>Registers a tool and returns its name/id.</p>"},{"location":"reference/tools-facade/#callname-args-dict-dict","title":"call(name, args: dict) \u2192 dict","text":"<p>Invokes a tool by name with validated args.</p>"}]}