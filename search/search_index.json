{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AetherGraph","text":"<p>A Python\u2011first agentic framework for R&amp;D workflows. The core surface is <code>@graph_fn</code> plus Context Services (LLM, memory, channel, artifacts, KV) and Tools for safe capability calls.</p> <p>\ud83d\udc49 Get hands-on in 5 minutes: Quickstart</p> <p>Why AetherGraph? - Python functions as first-class graph nodes (<code>@graph_fn</code>). - Built-in services: no bolt\u2011ons required, but easy to swap. - Artifacts + memory for traceable, reproducible research.</p>"},{"location":"concept/","title":"AetherGraph \u2014 Architecture Overview (1\u2011page)","text":"<p>Goal: Give newcomers a single \"big picture\" of how AetherGraph fits together, then provide a tiny legend so they know what to look up next.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          AetherGraph Runtime                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                    \u2502\n\u2502  Python Code (your repo)                                           \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500          \u2502\n\u2502  @graph_fn nodes           Tools (@tool)            Services       \u2502\n\u2502  (code-native agents)      (reusable ops,           (external ctx) \u2502\n\u2502                            checkpointable)                          \u2502\n\u2502       \u2502                           \u2502                    \u2502            \u2502\n\u2502       \u25bc                           \u25bc                    \u25bc            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  Node Exec   \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  Tool Exec  \u2502      \u2502  Service API \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502        \u2502                                                         \u2502    \u2502\n\u2502        \u25bc                                                         \u2502    \u2502\n\u2502                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502                       \u2502        NodeContext       \u2502  (per node call)  \u2502\n\u2502                       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                   \u2502\n\u2502                       \u2502 channel()   \u2192  chat/CLI/GUI (send/ask)       \u2502\n\u2502                       \u2502 memory()    \u2192  record/recent/query           \u2502\n\u2502                       \u2502 artifacts() \u2192  write/read refs (provenance)  \u2502\n\u2502                       \u2502 kv()        \u2192  small fast key\u2013value          \u2502\n\u2502                       \u2502 logger()    \u2192  structured logs               \u2502\n\u2502                       \u2502 services()  \u2192  external ctx (domain APIs)    \u2502\n\u2502                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502                                \u2502                                     \u2502\n\u2502                                \u25bc                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502               Sidecar / Adapters (inline server)               \u2502 \u2502\n\u2502  \u2502  - Console/CLI channel                                         \u2502 \u2502\n\u2502  \u2502  - Slack / PyQt / HTTP webhooks                                \u2502 \u2502\n\u2502  \u2502  - File/artifact endpoints (optional, later hosted)            \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concept/#legend-skim-first","title":"Legend (skim-first)","text":"<ul> <li><code>@graph_fn</code> (code\u2011native agents): Turn a plain async Python function into a node with a <code>NodeContext</code> injected.</li> <li>Tools (<code>@tool</code>): Small, explicit, reusable operations. Great for checkpoints, retries, and sharing across graphs.</li> <li>NodeContext: Where your node talks to the world: <code>channel()</code>, <code>memory()</code>, <code>artifacts()</code>, <code>kv()</code>, <code>logger()</code>, <code>services()</code>.</li> <li>Channel: Unifies human I/O (console/Slack/PyQt). Use <code>send_text</code>, <code>ask_text</code>, and progress APIs.</li> <li>Memory &amp; Artifacts: Event\u2011first memory with provenance; artifacts store files/results with stable refs.</li> <li>External Context (Services): Register domain services (e.g., job runner, materials DB) so nodes call them like built\u2011ins.</li> <li>Sidecar: Inline server that powers channels/adapters locally; later you can host these endpoints.</li> </ul> <p>Next: See Memory Internals below, then the Submit \u2192 Poll \u2192 Notify tutorial.</p>"},{"location":"concept/#memory-internals-diagram","title":"Memory Internals (diagram)","text":"<p>Goal: Show how event logging, persistence, indices, and optional RAG hang together.</p> <pre><code>              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502          memory().record(...)           \u2502\n              \u2502   kind \u2022 data \u2022 tags \u2022 entities \u2022 ...   \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                   (in\u2011process event stream / bus)\n                                  \u2502\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u25bc                                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  JSONL Persistence Log    \u2502  append\u2011only     \u2502   KV / Indices            \u2502\n\u2502  (provenance timeline)    \u2502  (durable)       \u2502   (fast lookup/filter)    \u2502\n\u2502  e.g., runs/YYYY/MM/*.jsonl\u2502                 \u2502   tags, kinds, entity ids \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                                 \u2502\n          \u2502                                                 \u2502\n          \u25bc                                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Derived Views / Cursors   \u2502  recent(...)     \u2502   Optional RAG Binding    \u2502\n\u2502 e.g., last_by_name,       \u2502  query(...)      \u2502   (vector index)          \u2502\n\u2502 latest_refs_by_kind       \u2502                  \u2502   embed(data/artifacts)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                                 \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba  NodeContext.memory().query(...) \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Record: Write small structured events with <code>kind</code>, <code>data</code>, <code>tags</code>, <code>entities</code>, <code>metrics</code>.</li> <li>Persist: Append to a JSONL log for durability &amp; replay; perfect for provenance.</li> <li>Index: Maintain fast KV/indices for quick filters (by kind/tags/entity/time).</li> <li>RAG (optional): Bind a vector index to selectively embed text/artifact content for semantic search.</li> <li>Query: Use <code>recent</code>, <code>query</code>, and helpers like <code>latest_refs_by_kind</code> to drive summaries &amp; reports.</li> </ul> <p>When to use RAG? When you want semantic retrieval over larger text blocks or artifact\u2011derived content; otherwise rely on indices and tags for speed/clarity.</p>"},{"location":"concept/#tutorial-submit-poll-notify-single-file-runnable","title":"Tutorial \u2014 Submit \u2192 Poll \u2192 Notify (single file, runnable)","text":"<p>Goal: Minimal end\u2011to\u2011end job orchestration with human notification and provenance. Keep it console\u2011only, no external infra.</p> <pre><code># examples/tutorial_submit_poll_notify.py\nfrom __future__ import annotations\nimport asyncio, random, time\nfrom typing import Dict, Optional\n\n# AetherGraph imports (adjust paths/names to your package layout)\nfrom aethergraph import graph_fn, NodeContext\nfrom aethergraph.server import start\nfrom aethergraph.v3.core.runtime.runtime_services import register_context_service\n\n# --- 1) Start the inline sidecar so channel/memory/artifacts work locally ---\nstart()  # prints a local URL; not required to save here\n\n# --- 2) A tiny external service: fake job runner (auto\u2011bound to NodeContext) ---\nclass FakeJobRunner:\n    \"\"\"Pretend to submit a remote job and poll until it finishes.\n    In a real impl, call your cloud API here.\n    \"\"\"\n    def __init__(self):\n        self._jobs: Dict[str, Dict[str, Optional[str]]] = {}\n\n    async def submit(self, spec: Dict) -&gt; str:\n        job_id = f\"job_{int(time.time()*1000)}_{random.randint(100,999)}\"\n        # status can be: queued \u2192 running \u2192 (succeeded | failed)\n        self._jobs[job_id] = {\"status\": \"queued\", \"result\": None}\n        # Background simulation\n        asyncio.create_task(self._simulate(job_id, spec))\n        return job_id\n\n    async def poll(self, job_id: str) -&gt; Dict[str, Optional[str]]:\n        return self._jobs[job_id]\n\n    async def _simulate(self, job_id: str, spec: Dict):\n        # Fake lifecycle with sleeps\n        await asyncio.sleep(0.5)\n        self._jobs[job_id][\"status\"] = \"running\"\n        await asyncio.sleep(1.2)\n        if random.random() &lt; 0.85:\n            self._jobs[job_id][\"status\"] = \"succeeded\"\n            self._jobs[job_id][\"result\"] = f\"Result for {spec.get('name','demo')}\"\n        else:\n            self._jobs[job_id][\"status\"] = \"failed\"\n            self._jobs[job_id][\"result\"] = None\n\n# Register the service under the name \"jobs\" and auto\u2011bind it to context as context.jobs()\nregister_context_service(\"jobs\", FakeJobRunner())\n\n# --- 3) The graph node: submit \u2192 poll \u2192 notify, with artifacts &amp; memory ---\n@graph_fn(name=\"submit_poll_notify\")\nasync def submit_poll_notify(spec: Dict, *, context: NodeContext) -&gt; Dict:\n    ch = context.channel()\n    mem = context.memory()\n    arts = context.artifacts()\n    jobs = context.jobs()  # auto\u2011bound external service\n\n    await ch.send_text(\"Submitting your job\u2026\")\n    job_id = await jobs.submit(spec)\n    await mem.record(kind=\"job_submitted\", data={\"job_id\": job_id, \"spec\": spec}, tags=[\"demo\"])\n\n    # Persist the spec as an artifact\n    spec_ref = await arts.write_text(f\"spec_{job_id}.json\", content=str(spec))\n\n    # Poll until terminal\n    while True:\n        info = await jobs.poll(job_id)\n        status = info.get(\"status\")\n        await ch.send_text(f\"Status: {status}\")\n        if status in {\"succeeded\", \"failed\"}:\n            break\n        await asyncio.sleep(0.6)\n\n    if status == \"succeeded\":\n        result_text = info.get(\"result\") or \"&lt;no result&gt;\"\n        res_ref = await arts.write_text(f\"result_{job_id}.txt\", content=result_text)\n        await mem.record(kind=\"job_succeeded\", data={\"job_id\": job_id, \"result_ref\": res_ref})\n        await ch.send_text(f\"\u2705 Job {job_id} finished. Saved result \u2192 {res_ref}\")\n        return {\"job_id\": job_id, \"status\": status, \"spec_ref\": spec_ref, \"result_ref\": res_ref}\n    else:\n        await mem.record(kind=\"job_failed\", data={\"job_id\": job_id})\n        ans = await ch.ask_text(f\"\u274c Job {job_id} failed. Retry? (yes/no)\")\n        if str(ans).strip().lower().startswith(\"y\"):\n            return await submit_poll_notify(spec=spec, context=context)\n        await ch.send_text(\"Not retrying; stopping here.\")\n        return {\"job_id\": job_id, \"status\": status, \"spec_ref\": spec_ref}\n\n# --- 4) Tiny runner for local testing ---\nif __name__ == \"__main__\":\n    async def main():\n        out = await submit_poll_notify(spec={\"name\": \"toy-sim\", \"steps\": 3})\n        print(\"FINAL OUTPUT:\\n\", out)\n    asyncio.run(main())\n</code></pre>"},{"location":"concept/#what-this-tutorial-demonstrates","title":"What this tutorial demonstrates","text":"<ul> <li>Channel I/O: human\u2011visible status + retry prompt.</li> <li>External Service: a domain API (<code>jobs</code>) registered once, used like a built\u2011in via <code>context.jobs()</code>.</li> <li>Memory: durable events (<code>job_submitted</code>, <code>job_succeeded</code>, <code>job_failed</code>).</li> <li>Artifacts &amp; provenance: spec/result written with stable refs; returned in the node output.</li> <li>Low friction: single file; console channel only; no extra infra.</li> </ul> <p>Next:</p> <ul> <li>Swap <code>FakeJobRunner</code> for your cloud client.</li> <li>Replace <code>ask_text</code> with an approval UI (Slack/PyQt) once you enable those adapters.</li> <li>Emit metrics in <code>mem.record(..., metrics={...})</code> and add a summary node to close the loop.</li> </ul>"},{"location":"external-context-services/","title":"External Context Services (Revised)","text":"<p>Make reusable, lifecycle\u2011aware helpers available as <code>context.&lt;name&gt;</code> inside any <code>@graph_fn</code>.</p> <p>This page explains what an external context service is, why you might use one, how it looks at a high level, and the APIs you\u2019ll use to define and register services. It also clarifies lifecycle behavior today vs. after you add a server/sidecar, and shows how services can access the active <code>NodeContext</code>.</p>"},{"location":"external-context-services/#1-what-is-an-external-context-service","title":"1) What is an external context service?","text":"<p>An external context service is a Python object managed by AetherGraph\u2019s runtime and exposed to your graph functions through the <code>NodeContext</code>. Once registered, you can access it as <code>context.svc(\"name\")</code> or simply <code>context.&lt;name&gt;</code>.</p> <p>Key ideas:</p> <ul> <li>Dependency injection: Centralize clients, caches, and policies in one place and inject them wherever needed.</li> <li>Lifecycle\u2011ready: Services can implement <code>start()</code> and <code>close()</code> for setup/teardown (e.g., open a pool, kick off a background task). Today these hooks are optional and not auto\u2011invoked unless you wire them (see \u00a74.1).</li> <li>Concurrency controls: Built\u2011in mutex and read/write helpers to safely share state across concurrent nodes.</li> <li>Per\u2011run binding: Each call is bound to a <code>NodeContext</code> so the service can access run_id, logger, artifacts, memory, etc.</li> <li>Uniform surface: The same service works in local scripts today and can be proxied or hosted later without changing call sites.</li> </ul> <p>Use services when logic benefits from a long\u2011lived instance, shared state, or orchestration\u2014not for tiny, pure functions (plain imports are fine there).</p>"},{"location":"external-context-services/#2-highlevel-usage-sketch","title":"2) High\u2011level usage sketch","text":"<p>Below is a conceptual outline (intentionally abstract) of how you would define and call a service.</p>"},{"location":"external-context-services/#define-highlevel","title":"Define (high\u2011level)","text":"<pre><code>class MyService(Service):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self._cache = {}\n\n    async def start(self):\n        # optional: warm up connections, threads, or caches\n        ...\n\n    async def close(self):\n        # optional: flush or close resources\n        ...\n\n    async def do_something(self, key: str) -&gt; str:\n        # example: consult cache, maybe call out to an API, return a value\n        ...\n</code></pre>"},{"location":"external-context-services/#register-at-app-startup","title":"Register (at app startup)","text":"<pre><code>register_context_service(\"myservice\", MyService(config={\"mode\": \"dev\"}))\n</code></pre>"},{"location":"external-context-services/#use-in-a-graph-function","title":"Use in a graph function","text":"<pre><code>@graph_fn(name=\"demo\")\nasync def demo(*, context: NodeContext):\n    value = await context.myservice.do_something(\"foo\")\n    return {\"value\": value}\n</code></pre> <p>That\u2019s it: once registered, your service is reachable from any node via <code>context</code>.</p>"},{"location":"external-context-services/#3-why-use-external-context-benefits-use-cases","title":"3) Why use external context? (Benefits + use cases)","text":""},{"location":"external-context-services/#benefits","title":"Benefits","text":"<ul> <li>Replaceable implementations: Swap local vs. remote, mock vs. real, dev vs. prod\u2014without editing call sites.</li> <li>Centralized auth &amp; config: Put tokens, endpoints, retry/timeout policy, telemetry in one place.</li> <li>Lifecycle &amp; performance: Reuse clients, connection pools, thread pools; warm caches once.</li> <li>Concurrency safety: Use the provided <code>critical()</code> mutex or <code>AsyncRWLock</code> to protect shared state.</li> <li>Per\u2011run awareness: Access <code>self.ctx()</code> to reach logger, artifacts, memory, continuations, etc.</li> <li>Future\u2011proof: The same surface can later be proxied (sidecar/hosted) while keeping your graph code unchanged.</li> </ul>"},{"location":"external-context-services/#itemized-scenarios-no-code","title":"Itemized scenarios (no code)","text":"<ul> <li>Model/Tool Clients: Wrap an LLM, embedding service, vector DB, or a simulation engine with retry, rate limit, and consistent API.</li> <li>Job Orchestration: Submit long\u2011running jobs to a queue/cluster and expose <code>submit/status/wait</code> for nodes.</li> <li>Caching/Indexing: Provide a shared in\u2011memory or on\u2011disk cache with strict read (R/W lock) semantics.</li> <li>Policy Enforcement: Centralize tenant limits, quotas, audit logging, and redaction.</li> <li>Data Access Facades: Read domain data (materials table, experiment registry) with local cache + background refresh.</li> <li>Adapters: Present a unified interface over heterogeneous backends (e.g., multiple vendor APIs behind one broker).</li> </ul>"},{"location":"external-context-services/#4-apis-defining-registering-and-binding-services","title":"4) APIs: defining, registering, and binding services","text":"<p>AetherGraph provides small primitives for service registration and a base class with helpful utilities.</p>"},{"location":"external-context-services/#41-lifecycle-today-vs-serversidecar","title":"4.1 Lifecycle (today vs. server/sidecar)","text":"<ul> <li>Today (no server yet): <code>start()</code>/<code>close()</code> exist but are not auto\u2011invoked. You can omit them or leave them as no\u2011ops.</li> <li>When you add a server/sidecar: wire lifecycle once at boot/shutdown (pseudo\u2011code):</li> </ul> <pre><code># After install_services(...) and registrations\nawait start_all_services()\n# ... run your app/sidecar ...\nawait close_all_services()\n</code></pre> <p>Until those hooks are added, services work fine without lifecycle calls.</p>"},{"location":"external-context-services/#42-registry-functions-runtimelevel","title":"4.2 Registry functions (runtime\u2011level)","text":"<ul> <li><code>install_services(container)</code> \u2013 Set the process\u2011wide service container at startup.</li> <li><code>ensure_services_installed(factory)</code> \u2013 Lazily create/install the container if missing.</li> <li><code>register_context_service(name, instance)</code> \u2013 Add a concrete service instance under <code>name</code>.</li> <li><code>get_context_service(name)</code> \u2013 Retrieve a registered instance.</li> <li><code>list_context_services()</code> \u2013 List the names currently registered.</li> </ul>"},{"location":"external-context-services/#43-base-class-service-aka-basecontextservice","title":"4.3 Base class: <code>Service</code> (aka <code>BaseContextService</code>)","text":"<p>The base class gives you batteries\u2011included ergonomics:</p> <ul> <li> <p>Lifecycle</p> </li> <li> <p><code>async def start(self) -&gt; None</code> \u2013 Optional setup hook.</p> </li> <li> <p><code>async def close(self) -&gt; None</code> \u2013 Optional teardown hook.</p> </li> <li> <p>Binding</p> </li> <li> <p><code>def bind(self, *, context: NodeContext) -&gt; Service</code> \u2013 Called by the runtime so <code>self.ctx()</code> works.</p> </li> <li> <p><code>def ctx(self) -&gt; NodeContext</code> \u2013 Access the current node context (logger, memory, artifacts, etc.).</p> </li> <li> <p>Concurrency</p> </li> <li> <p><code>self._lock</code> \u2013 An async mutex available for your own critical sections.</p> </li> <li><code>def critical()(fn)</code> \u2013 Decorator that serializes an async method (easy mutual exclusion).</li> <li> <p><code>class AsyncRWLock</code> \u2013 Many\u2011readers/one\u2011writer lock for shared tables and caches.</p> </li> <li> <p>Offloading</p> </li> <li> <p><code>async def run_blocking(self, fn, *a, **kw)</code> \u2013 Run CPU or blocking I/O on a worker thread (keeps the event loop responsive).</p> </li> </ul>"},{"location":"external-context-services/#44-accessing-services-from-nodes","title":"4.4 Accessing services from nodes","text":"<ul> <li>Dynamic attribute: <code>context.&lt;name&gt;</code> resolves to the registered service (e.g., <code>context.myservice</code>).</li> <li>Explicit lookup: <code>context.svc(\"name\")</code> (equivalent to the dynamic attribute).</li> </ul>"},{"location":"external-context-services/#45-accessing-nodecontext-from-inside-a-service-essential","title":"4.5 Accessing <code>NodeContext</code> from inside a service (essential)","text":"<p>Services frequently need run\u2011scoped utilities (logger, memory, artifacts, kv, llm, rag, etc.). Enable per\u2011call binding so <code>self.ctx()</code> returns the right <code>NodeContext</code>.</p> <p>Use <code>self.ctx()</code> in the service:</p> <pre><code>class MyService(Service):\n    async def do_work(self, x: int) -&gt; int:\n        ctx = self.ctx()  # NodeContext bound for this call\n        ctx.logger().info(\"working\", extra={\"x\": x})\n        await ctx.memory().record(kind=\"note\", data={\"x\": x})\n        uri = ctx.artifacts().put_text(\"result.txt\", f\"value={x}\")\n        return x + 1\n</code></pre>"},{"location":"external-context-services/#46-event-loop-locking-model","title":"4.6 Event loop &amp; locking model","text":"<ul> <li>External services run on the main event loop used by the executing node.</li> <li>Locks (<code>_lock</code>, <code>AsyncRWLock</code>) coordinate on that loop; use <code>run_blocking()</code> for CPU/IO work.</li> </ul>"},{"location":"external-context-services/#5-summary","title":"5) Summary","text":"<p>External context services provide a clean way to share long\u2011lived capabilities across nodes while keeping graph code small and portable:</p> <ul> <li>Inject reusable helpers via <code>context.&lt;name&gt;</code> (or <code>context.svc(name)</code>).</li> <li>Manage concurrency and performance in one place; offload blocking work with <code>run_blocking()</code>.</li> <li>Abstract environments (mock/local/dev/prod) without touching business logic.</li> <li>Bind to <code>NodeContext</code> automatically so services can use logger, memory, artifacts, kv, llm/rag, etc.</li> <li>Lifecycle now vs later: Today you can skip <code>start()</code>/<code>close()</code>; add startup/shutdown hooks when you introduce a server/sidecar.</li> </ul> <p>Use services for shared state, orchestration, specialized clients, or cross\u2011cutting policies. Use plain imports for tiny, stateless helpers.</p>"},{"location":"graph_fn/","title":"Graph Function <code>graph_fn</code> Quickstart &amp; Reference","text":"<p>Make any Python async function a runnable, inspectable Graph Function with a single decorator. You keep normal Python control\u2011flow; AetherGraph wires in runtime services via <code>context</code> and exposes your outputs as graph boundaries.</p>"},{"location":"graph_fn/#tldr","title":"TL;DR","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}! \ud83d\udc4b\")\n    return {\"greeting\": f\"Hello, {name}\"}\n\n# Run (async)\nres = await hello(name=\"Aether\")          # \u2192 {\"greeting\": \"Hello, Aether\"}\n\n# Or run (sync) for quick scripts\nout = hello.sync(name=\"Aether\")            # same result\n</code></pre>"},{"location":"graph_fn/#what-is-a-graph-function","title":"What is a Graph Function?","text":"<p>A Graph Function is a small wrapper around your Python function that:</p> <ul> <li> <p>builds a fresh internal TaskGraph,</p> </li> <li> <p>injects a <code>NodeContext</code> if your function declares <code>*, context</code>,</p> </li> <li> <p>executes your function (awaiting if needed),</p> </li> <li> <p>normalizes the return value into named outputs, and</p> </li> <li> <p>records graph boundary outputs for downstream composition/inspection.</p> </li> </ul> <p>You do not need to learn a new DSL. Write Python; use <code>context.&lt;service&gt;()</code> when you need IO/state.</p>"},{"location":"graph_fn/#decorator-signature","title":"Decorator signature","text":"<pre><code>@graph_fn(\n    name: str,\n    inputs: list[str] | None = None,\n    outputs: list[str] | None = None,\n    version: str = \"0.1.0\",\n    agent: str | None = None,  # optional: also register as an agent name\n)\n</code></pre> <p>Required</p> <ul> <li>name (str) \u2013 Unique identifier for this graph function.</li> </ul> <p>Optional</p> <ul> <li> <p>inputs (list[str]) \u2013 Declares input names for docs/registry (not enforced at call time).</p> </li> <li> <p>outputs (list[str]) \u2013 Declares output names/order; enables single\u2011literal returns.</p> </li> <li> <p>version (str) \u2013 Semantic version for registry/discovery.</p> </li> <li> <p>agent (str) \u2013 Also register in the <code>agent</code> namespace (advanced).</p> </li> </ul>"},{"location":"graph_fn/#function-shape","title":"Function shape","text":"<p><pre><code>@graph_fn(name=\"example\", inputs=[\"x\"], outputs=[\"y\"])\nasync def example(x: int, *, context):\n    # use services via context: channel/memory/artifacts/kv/llm/rag/mcp/logger\n    await context.channel().send_text(f\"x={x}\")\n    return {\"y\": x + 1}\n</code></pre> - Positional/keyword parameters are your API.</p> <ul> <li>Include <code>*, context</code> to receive the <code>NodeContext</code>. If you don\u2019t declare it, nothing is injected.</li> </ul>"},{"location":"graph_fn/#returning-values-normalization-rules","title":"Returning values (normalization rules)","text":"<p>Your return can be:</p> <p>1) Dict of outputs (recommended) <pre><code>return {\"result\": 42, \"note\": \"ok\"}\n</code></pre></p> <p>2) Single literal \u2014 only if you declared exactly one output <pre><code>@graph_fn(name=\"one\", outputs=[\"y\"])\nasync def one(*, context):\n    return 123  # normalized to {\"y\": 123}\n</code></pre></p> <p>3) NodeHandle / Refs (advanced) If you return node handles or refs created by graph utilities, they\u2019re exposed as boundary outputs automatically. For most users, plain dicts/literals are enough.</p> <p>Validation - If <code>outputs</code> are declared, missing keys raise: <code>ValueError(\"Missing declared outputs: ...\")</code>. - Returning a single literal without exactly one declared output raises an error.</p>"},{"location":"graph_fn/#running","title":"Running","text":"<p><pre><code># Async (preferred in apps/servers)\nres = await my_fn(a=1, b=2)\n\n# Sync helper (scripts/CLI/tests)\nout = my_fn.sync(a=1, b=2)\n</code></pre> Internally this builds a fresh runtime environment, constructs a TaskGraph, executes your function in an interpreter, and returns the normalized outputs.</p>"},{"location":"graph_fn/#accessing-context","title":"Accessing Context","text":"<p>Declare <code>*, context</code> to use built\u2011ins: <pre><code>@graph_fn(name=\"report\", outputs=[\"uri\"])\nasync def report(data: dict, *, context):\n    # Log breadcrumbs\n    log = context.logger(); log.info(\"building report\")\n\n    # Save an artifact\n    art = await context.artifacts().save(path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\":\"A\"})\n\n    # Record a typed result in memory\n    await context.memory().write_result(topic=\"report\", outputs=[{\"name\":\"uri\",\"kind\":\"uri\",\"value\": art.uri}])\n\n    # Notify user\n    await context.channel().send_text(f\"Report ready: {art.uri}\")\n    return {\"uri\": art.uri}\n</code></pre></p>"},{"location":"graph_fn/#concurrency-retry-advanced","title":"Concurrency &amp; retry (advanced)","text":"<p><code>GraphFunction.run()</code> accepts knobs used by the interpreter/runtime: <pre><code>await my_fn.run(\n    env=None,                            # supply a prebuilt RuntimeEnv, or let the runner build one\n    retry=RetryPolicy(),                 # backoff/retries for node execution\n    max_concurrency: int | None = None,  # cap parallelism inside the interpreter\n    **inputs,\n)\n</code></pre> For most users, calling <code>await my_fn(...)</code> / <code>.sync(...)</code> is sufficient; the runner chooses sensible defaults.</p>"},{"location":"graph_fn/#minimal-patterns","title":"Minimal patterns","text":"<p>Hello + context <pre><code>@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}\")\n    return {\"greeting\": f\"Hello, {name}\"}\n</code></pre></p> <p>One output (literal) <pre><code>@graph_fn(name=\"square\", outputs=[\"y\"])\nasync def square(x: int, *, context):\n    return x * x\n</code></pre></p> <p>Multi\u2011output dict <pre><code>@graph_fn(name=\"stats\", outputs=[\"mean\",\"std\"])\nasync def stats(xs: list[float], *, context):\n    import statistics as st\n    return {\"mean\": st.mean(xs), \"std\": st.pstdev(xs)}\n</code></pre></p>"},{"location":"graph_fn/#tips-gotchas","title":"Tips &amp; gotchas","text":"<ul> <li>Always include <code>*, context</code> when you need services (channel/memory/llm/etc.).</li> <li>Declare <code>outputs=[...]</code> if you want to return a single literal; otherwise return a dict.</li> <li>Output validation is strict when <code>outputs</code> are declared\u2014return all of them.</li> <li><code>inputs=[...]</code> is for documentation/registry; your Python signature is the source of truth at call time.</li> <li>You can also register the function as an agent by passing <code>agent=\"name\"</code> (covered later).</li> </ul>"},{"location":"graph_fn/#next-steps","title":"Next steps","text":"<ul> <li><code>graphify</code>: combine multiple functions into a larger graph with explicit edges.</li> <li><code>@tool</code>: publish functions as reusable nodes (IO typed), then orchestrate with <code>graphify</code>.</li> <li>Context services: <code>channel</code>, <code>artifacts</code>, <code>memory</code>, <code>kv</code>, <code>llm</code>, <code>rag</code>, `m</li> </ul>"},{"location":"graphify/","title":"AetherGraph \u2014 <code>@graphify</code> (Builder Decorator)","text":"<p><code>@graphify</code> lets you write a plain Python function whose body builds a <code>TaskGraph</code> using tool calls. Instead of executing immediately, the function becomes a graph factory: call <code>.build()</code> to get a concrete graph, <code>.spec()</code> to inspect, and <code>.io()</code> to see its input/output signature.</p>"},{"location":"graphify/#why-graphify-vs-graph_fn","title":"Why <code>graphify</code> vs <code>graph_fn</code>?","text":"Aspect <code>graph_fn</code> <code>graphify</code> Primary purpose Execute now as a single graph node Build a graph (explicit fan\u2011in/fan\u2011out wiring) Return at call Dict of outputs (or awaitable) A builder you later <code>.build()</code> into a graph Control\u2011flow Pythonic, implicit graph behind the scenes Explicit nodes &amp; edges via tool calls (<code>NodeHandle</code>) Best for Orchestration + <code>context.*</code> services Pipelines, DAGs, reusable subgraphs <p>Use <code>graphify</code> when you want:</p> <ul> <li>Multiple tool calls as separate nodes</li> <li>Explicit dependencies (<code>_after</code>) and fan\u2011in/fan\u2011out</li> <li>To inspect/serialize the graph spec for registry/UI</li> <li>To reuse the same pipeline with different inputs</li> </ul> <p>Use <code>graph_fn</code> when you want:</p> <ul> <li>A simple function that runs immediately and returns values</li> <li>Access to <code>context.channel()/memory()/artifacts()/llm()</code> services</li> <li>Minimal ceremony (one decorator and go)</li> </ul>"},{"location":"graphify/#decorator-signature","title":"Decorator Signature","text":"<pre><code>from aethergraph import graphify\n\n@graphify(*, name=\"default_graph\", inputs=(), outputs=None, version=\"0.1.0\", agent: str | None = None)\ndef build_fn(...):\n    ...  # tool calls returning NodeHandles\n    return {\"y\": handle.y}\n</code></pre> <p>Parameters</p> <ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (Iterable[str] or dict) \u2014 Declare required/optional inputs.  </li> <li>If <code>list/tuple</code>: treated as required input names.  </li> <li>If <code>dict</code>: <code>{required_name: ..., ...}</code> for optional mapping; builder will declare required/optional accordingly.</li> <li>outputs (list[str] | None) \u2014 Names to expose. If you return a single literal, you must declare exactly one.</li> <li>version (str) \u2014 Semantic version for registry/spec metadata.</li> <li>agent (str | None) \u2014 Optionally register the built graph under <code>agent</code> namespace.</li> </ul> <p>Return value</p> <p>The decorated symbol becomes a builder function with helpers:</p> <ul> <li><code>.build() -&gt; TaskGraph</code></li> <li><code>.spec() -&gt; GraphSpec</code></li> <li><code>.io() -&gt; IOSignature</code></li> <li>Attributes: <code>.graph_name</code>, <code>.version</code></li> </ul>"},{"location":"graphify/#writing-a-graphify-body","title":"Writing a <code>@graphify</code> Body","text":"<p>Inside the function:</p> <ol> <li>Use <code>arg(\"name\")</code> to reference declared inputs.</li> <li>Call <code>@tool</code> functions (or <code>call_tool(\"pkg.mod:fn\", ...)</code>) \u2014 each returns a <code>NodeHandle</code> in build mode.</li> <li>Return outputs as:</li> <li>A dict mapping names \u2192 <code>NodeHandle</code> outputs or refs/literals, or</li> <li>A single <code>NodeHandle</code> (its outputs will be exposed), or</li> <li>A single literal only if <code>outputs</code> has length 1.</li> </ol> <pre><code>from aethergraph import graphify, tool\nfrom aethergraph.graph import arg\n\n@tool(outputs=[\"embedded\"])\ndef embed(text: str): ...\n\n@tool(outputs=[\"score\"])\ndef score(vec, query_vec): ...\n\n@graphify(name=\"ranker\", inputs=[\"texts\",\"query\"], outputs=[\"scores\"])\ndef ranker(texts, query):\n    q = embed(text=query)\n    # fan\u2011out: call `embed` for each text\n    vecs = [embed(text=t) for t in texts]  # list[NodeHandle]\n    # fan\u2011in: score each against query vec\n    scs = [score(vec=v.embedded, query_vec=q.embedded) for v in vecs]\n    return {\"scores\": [s.score for s in scs]}\n\nG = ranker.build()\n</code></pre>"},{"location":"graphify/#control-dependencies-without-data-edges","title":"Control Dependencies without Data Edges","text":"<p>Use <code>_after</code> when you must enforce order but don\u2019t pass outputs: <pre><code>@tool(outputs=[\"ok\"])\ndef fetch(): return {\"ok\": True}\n\n@tool(outputs=[\"done\"])\ndef train(): return {\"done\": True}\n\n@graphify(name=\"seq\", inputs=[], outputs=[\"done\"])\ndef seq():\n    a = fetch()\n    b = train(_after=a)   # run b after a\n    return {\"done\": b.done}\n</code></pre></p>"},{"location":"graphify/#registration","title":"Registration","text":"<p>If a registry is active, <code>@graphify</code> registers the built graph under <code>nspace=\"graph\"</code> with <code>name</code>/<code>version</code> so it can be listed or launched elsewhere. You can also register it as an <code>agent</code> via the <code>agent=</code> parameter.</p>"},{"location":"graphify/#example-endtoend-pipeline","title":"Example: End\u2011to\u2011End Pipeline","text":"<pre><code>from aethergraph import tool, graphify\nfrom aethergraph.graph import arg\n\n@tool(outputs=[\"rows\"])\ndef load_csv(path: str): ...\n\n@tool(outputs=[\"clean\"])\ndef clean(rows): ...\n\n@tool(outputs=[\"model\"])\ndef train(data): ...\n\n@tool(outputs=[\"uri\"])\ndef report(model): ...\n\n@graphify(name=\"etl_train_report\", inputs=[\"csv_path\"], outputs=[\"uri\"])\ndef etl_train_report(csv_path):\n    raw  = load_csv(path=arg(\"csv_path\"))\n    tidy = clean(rows=raw.rows)\n    mod  = train(data=tidy.clean)\n    rep  = report(model=mod.model)\n    return {\"uri\": rep.uri}\n\nG = etl_train_report.build()\n</code></pre>"},{"location":"graphify/#using-tool-inside-graph_fn-brief","title":"Using <code>@tool</code> Inside <code>@graph_fn</code> (Brief)","text":"<p>While <code>@graph_fn</code> is for immediate execution, you can drop explicit tool nodes inside a <code>graph_fn</code> when you want finer\u2011grained tracing or parallelism:</p> <pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int): return {\"y\": x*x}\n\n@graph_fn(name=\"mix\")\nasync def mix(x: int, *, context):\n    h = square(x=x)                 # schedules a tool node in the implicit graph\n    await context.channel().send_text(\"running square\u2026\")\n    return {\"y\": h.y}               # exposes tool output as graph_fn output\n</code></pre> <p>Prefer <code>@graphify</code> for full pipeline construction; use <code>@graph_fn</code> when you want to orchestrate services (<code>context.*</code>) and run quickly.</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>A 5\u2011minute on\u2011ramp to AetherGraph: install, start the sidecar server, and run your first <code>@graph_fn</code>.</p>"},{"location":"quickstart/#1-install","title":"1) Install","text":"<pre><code>pip install aethergraph\n# or, from source\n# pip install -e .\n</code></pre> <p>Python: 3.10+</p>"},{"location":"quickstart/#2-start-the-sidecar-server-oneliner","title":"2) Start the sidecar server (one\u2011liner)","text":"<p>AetherGraph ships a lightweight sidecar that wires up core services (logger, artifacts, memory, KV, channels, etc.)</p> <pre><code># quickstart_server.py\nfrom aethergraph import start\n\nurl = start(host=\"127.0.0.1\", port=0, log_level=\"warning\")\nprint(\"AetherGraph server:\", url)\n</code></pre> <p>Run it:</p> <pre><code>python quickstart_server.py\n</code></pre> <p>You should see an HTTP URL like <code>http://127.0.0.1:54321</code> printed. (Random free port by default.)</p>"},{"location":"quickstart/#3-your-first-graph-function","title":"3) Your first graph function","text":"<p><code>@graph_fn</code> turns an ordinary async Python function into a runnable graph entrypoint. If you include a <code>context</code> parameter, you get access to built\u2011in services like <code>context.channel()</code> and <code>context.memory()</code>.</p> <pre><code># quickstart_graph_fn.py\nfrom aethergraph import graph_fn, NodeContext\nfrom aethergraph import start\n\n# 1) Start the sidecar so services are available\nstart()\n\n# 2) Define a small graph function\n@graph_fn(name=\"hello.world\", inputs=[\"name\"], outputs=[\"greeting\"], version=\"0.1.0\")\nasync def hello_world(name: str, *, context: NodeContext):\n    # Use the channel to send a message (console by default)\n    await context.channel().send_text(f\"\ud83d\udc4b Hello, {name}! Running graph\u2026\")\n\n    # Do any Python you want here \u2014 call tools, query memory, etc.\n    greeting = f\"Hello, {name}. Nice to meet you from AetherGraph.\"\n\n    # Return outputs as a dict (keys must match `outputs=[...]`)\n    return {\"greeting\": greeting}\n\n# 3) Run it (async wrapper provided)\nif __name__ == \"__main__\":\n    import asyncio\n    async def main():\n        res = await hello_world(name=\"Researcher\")\n        print(\"Result:\", res)\n    asyncio.run(main())\n</code></pre> <p>Run it:</p> <pre><code>python quickstart_graph_fn.py\n</code></pre> <p>You should see a console message from the channel and printed output like:</p> <pre><code>Result: {\"greeting\": \"Hello, Researcher. Nice to meet you from AetherGraph.\"}\n</code></pre>"},{"location":"quickstart/#4-what-just-happened","title":"4) What just happened?","text":"<ul> <li>Sidecar server booted in the background and installed default services (channels, artifacts, memory, KV, logger).</li> <li><code>@graph_fn</code> built a tiny task graph from your function and executed it.</li> <li><code>context.channel()</code> used the default channel (console) to emit a message.</li> </ul> <p>Tip: You can override the channel at call\u2011site with <code>context.channel(\"slack:#research\")</code>, once you\u2019ve configured a Slack adapter.</p>"},{"location":"quickstart/#5-next-steps","title":"5) Next steps","text":"<ul> <li>Add tools with <code>@tool</code> to wrap reusable steps and surface inputs/outputs.</li> <li>Use <code>@graphify</code> for fan\u2011in / fan\u2011out graph construction when the body is mostly tool calls.</li> <li>Explore artifacts (<code>context.artifacts()</code>), memory (<code>context.memory()</code>), and RAG (</li> </ul>"},{"location":"server/","title":"AetherGraph \u2014 Server (Sidecar) Overview","text":"<p>The AetherGraph server is a lightweight sidecar that wires up all runtime services (channels, memory, artifacts, KV, LLM, RAG, MCP, logging, etc.) and exposes a small HTTP/WebSocket surface for adapters and tools. You can run AetherGraph without the server, but the sidecar makes it easy to:</p> <ul> <li>Use GUI/chat adapters (Slack/Telegram/Console UI) that push events back to your runs</li> <li>Host continuation callbacks for <code>ask_text()</code> / <code>ask_approval()</code></li> <li>Centralize service wiring (secrets, paths, corpora, registries)</li> <li>Inspect/trace runs, artifacts, and health in one place</li> </ul> <p>Think of it as your local control plane so your graph functions can stay plain Python.</p>"},{"location":"server/#quick-start","title":"Quick Start","text":"<pre><code>from aethergraph.server import start, stop\n\n# 1) Start the sidecar (in a background thread) and get its base URL\nurl = start(host=\"127.0.0.1\", port=0)   # port=0 \u2192 auto-pick a free port\nprint(\"AetherGraph sidecar:\", url)\n\n# 2) Run your graph functions as usual\nfrom aethergraph import graph_fn\n\n@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}\")\n    return {\"greeting\": f\"Hello, {name}\"}\n\n# ... elsewhere ...\n# res = await hello(name=\"ZC\")\n\n# 3) (Optional) Stop when done (tests/CLI)\nstop()\n</code></pre>"},{"location":"server/#api-start-start_async-stop","title":"API \u2014 <code>start()</code> / <code>start_async()</code> / <code>stop()</code>","text":""},{"location":"server/#start","title":"start","text":"<p><pre><code>start(*, workspace: str = \"./aeg_workspace\", session_id: str | None = None,\n      host: str = \"127.0.0.1\", port: int = 0, log_level: str = \"warning\") -&gt; str\n</code></pre> Start the sidecar in a background thread. Safe to call at the top of scripts or notebook cells.</p> <p>Parameters</p> <ul> <li> <p>workspace (str) \u2013 Root directory for runtime state (artifacts, logs, corpora, temp files). Auto\u2011created.</p> </li> <li> <p>session_id (str, optional) \u2013 Override the logical session. If <code>None</code>, the runtime will create one.</p> </li> <li> <p>host (str) \u2013 Bind address (defaults to loopback).</p> </li> <li> <p>port (int) \u2013 <code>0</code> picks a free port automatically; otherwise bind an explicit port.</p> </li> <li> <p>log_level (str) \u2013 Uvicorn log level (e.g., <code>\"info\"</code>, <code>\"warning\"</code>).</p> </li> </ul> <p>Returns str \u2013 Base URL, e.g., <code>\"http://127.0.0.1:54321\"</code>.</p>"},{"location":"server/#start_async","title":"start_async","text":"<p><pre><code>start_async(**kwargs) -&gt; str\n</code></pre> Async\u2011friendly wrapper that still runs the server in a thread to avoid clashing with your event loop.</p>"},{"location":"server/#stop","title":"stop","text":"<p><pre><code>stop() -&gt; None\n</code></pre> Signal the background server to shut down and join its thread (useful in tests/CI or ephemeral scripts).</p>"},{"location":"server/#why-a-sidecar","title":"Why a sidecar?","text":"<ul> <li>Continuations: <code>context.channel().ask_*</code> creates a continuation token and waits for a resume callback; the server receives user replies (Slack/Telegram/HTTP) and wakes your run.</li> <li>Adapters: chat/file/progress adapters connect over HTTP/WS to publish events (<code>agent.message</code>, <code>agent.progress.*</code>, uploads) into your run.</li> <li>Central config: one place to load settings, secrets, workspace paths, and register services (LLM, RAG, MCP, artifact store, memory backends).</li> <li>Inspection: optional health and tracing endpoints (depending on your app factory) to debug runs locally.</li> </ul>"},{"location":"server/#what-start-actually-does","title":"What <code>start()</code> actually does","text":"<ol> <li>Loads app settings (<code>load_settings()</code>), installs them as current (<code>set_current_settings(...)</code>).</li> <li>Builds a FastAPI app via <code>create_app(workspace=..., cfg=...)</code> \u2014 this registers services and routes.</li> <li>Picks a free port if <code>port=0</code> and launches Uvicorn in a background thread (non\u2011blocking).</li> <li>Returns the base URL so other components (e.g., WS/HTTP MCP clients) can connect.</li> </ol>"},{"location":"server/#typical-usage-patterns","title":"Typical usage patterns","text":""},{"location":"server/#notebooks-quick-scripts","title":"Notebooks &amp; quick scripts","text":"<pre><code>url = start(port=0)\n# \u2026 run several cells that use context.channel()/continuations\n# restart kernel or call stop() when done\n</code></pre>"},{"location":"server/#longrunning-dev-server","title":"Long\u2011running dev server","text":"<ul> <li>Call <code>start(host=\"0.0.0.0\", port=8787, log_level=\"info\")</code> once at process start.</li> <li>Point Slack/Telegram adapters or local tools at <code>http://localhost:8787</code>.</li> </ul>"},{"location":"server/#testsci","title":"Tests/CI","text":"<pre><code>url = start(port=0)\ntry:\n    # run test suite that uses continuations/artifacts\n    ...\nfinally:\n    stop()\n</code></pre>"},{"location":"server/#interop-with-context-services","title":"Interop with context services","text":"<p>Once the sidecar is up, graph functions can rely on bound services:</p> <ul> <li> <p><code>context.channel()</code> \u2013 routes via the server to your chat adapters</p> </li> <li> <p><code>context.artifacts()</code> \u2013 saves to the workspace CAS under the sidecar</p> </li> <li> <p><code>context.memory()</code> \u2013 hotlog/persistence live alongside the server\u2019s config</p> </li> <li> <p><code>context.rag()</code> \u2013 corpora root under workspace; embedders/indices wired here</p> </li> <li> <p><code>context.mcp(...)</code> \u2013 WS/HTTP MCP clients often target sidecar endpoints</p> </li> </ul>"},{"location":"server/#security-notes","title":"Security notes","text":"<ul> <li>Default bind is <code>127.0.0.1</code> (local only). Use <code>0.0.0.0</code> only in trusted networks.</li> <li>Protect WS/HTTP endpoints behind auth headers/tokens if exposing beyond localhost.</li> <li>Never log plaintext API keys; prefer a Secrets store.</li> </ul>"},{"location":"server/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Port already in use: pass <code>port=0</code> or another free port.</li> <li>Nothing happens after ask_text(): ensure the chat adapter posts replies to the sidecar (correct base URL / token).</li> <li>No LLM/kv/rag configured: your <code>create_app()</code> must wire these services (or the accessors will raise \"\u2026 not available\").</li> <li>Jupyter hangs on restart: call <code>stop()</code> before restarting the kernel, or rely on kernel shutdown to terminate the thread.</li> </ul>"},{"location":"server/#minimal-adapter-sketch-optional","title":"Minimal adapter sketch (optional)","text":"<pre><code># Example: WebSocket adapter connecting to sidecar URL\na_sync_ws_client.connect(f\"{url.replace('http','ws')}/events\", headers={\"Authorization\": \"Bearer demo\"})\n# publish OutEvent / listen for Continuation notifications\n</code></pre>"},{"location":"server/#summary","title":"Summary","text":"<p>Run the sidecar server to centralize runtime services, handle continuations/adapters, and keep your graph functions clean. Use <code>start()</code> to launch in\u2011process, <code>start_async()</code> in async apps, and <code>stop()</code> for tests/CI. Configure paths and services once; build everything else in plain Python.</p>"},{"location":"tools/","title":"AetherGraph \u2014 <code>@tool</code> Decorator (Reference &amp; How\u2011to)","text":"<p><code>@tool</code> turns a plain Python function into a tool node that can be executed immediately or added to a graph during build time. You write ordinary Python, declare outputs, and AetherGraph handles result normalization and graph node creation.</p>"},{"location":"tools/#what-is-a-tool","title":"What is a Tool?","text":"<p>A tool is a reusable, IO\u2011typed operation that can be executed on its own or orchestrated inside a graph. Tools are perfect for things like \u201cload CSV\u201d, \u201ctrain model\u201d, \u201cplot chart\u201d, \u201csend_slack\u201d, etc.</p> <ul> <li>Immediate mode (no graph builder active): calling the tool runs the Python function right away and returns a dict of outputs.</li> <li>Graph mode (inside a <code>with graph(...):</code> block or a <code>@graphify</code> body): calling the tool adds a node to the graph and returns a <code>NodeHandle</code> you can wire to other nodes (fan\u2011in/fan\u2011out).</li> <li>Tools automatically register in the runtime registry (<code>nspace=\"tool\"</code>) when a registry is active.</li> </ul> <p>This page covers the simple function form. (The advanced waitable class form is documented separately.)</p>"},{"location":"tools/#decorator-signature","title":"Decorator Signature","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs: list[str], *, inputs: list[str] | None = None,\n      name: str | None = None, version: str = \"0.1.0\")\ndef your_function(...): ...\n</code></pre> <p>Parameters</p> <ul> <li>outputs (list[str], required) \u2014 Declares the output keys your tool will produce.</li> <li>inputs (list[str], optional) \u2014 Explicit input names. Omit to infer from function signature (excluding <code>*args</code>/<code>**kwargs</code>).  </li> <li>name (str, optional) \u2014 Registry/display name. Defaults to the function\u2019s <code>__name__</code>.  </li> <li>version (str, optional) \u2014 Semantic version recorded in the registry (default: <code>\"0.1.0\"</code>).</li> </ul> <p>Return value (call\u2011site dependent)</p> <ul> <li>Immediate mode: returns a <code>dict</code> of outputs.  </li> <li>Graph mode: returns a <code>NodeHandle</code> (or an awaitable handle under an interpreter) to be wired/exposed by the builder.</li> </ul>"},{"location":"tools/#return-normalization","title":"Return Normalization","text":"<p>The wrapped function can return different shapes; the decorator normalizes into a dict that must include every declared output:</p> <ul> <li><code>None</code> \u2192 <code>{}</code></li> <li><code>dict</code> \u2192 used as\u2011is</li> <li><code>tuple</code> \u2192 <code>{\"out0\": v0, \"out1\": v1, ...}</code></li> <li>single value \u2192 <code>{\"result\": value}</code></li> </ul> <p>If any declared <code>outputs</code> are missing from the normalized dict, a <code>ValueError</code> is raised.</p>"},{"location":"tools/#control-keywords-graph-mode","title":"Control Keywords (graph mode)","text":"<p>When calling a tool while building a graph (e.g., inside a <code>with graph(...):</code> or <code>@graphify</code> body), you may pass these special kwargs to influence scheduling/metadata:</p> <ul> <li><code>_after</code> (NodeHandle | list[NodeHandle | node_id]): explicit dependency edges (fan\u2011in).  </li> <li><code>_name</code> (str): display name for UI/spec.  </li> <li><code>_id</code> (str): hard override of the node ID (must be unique in the graph).  </li> <li><code>_alias</code> (str): optional alias for reverse lookups.  </li> <li><code>_labels</code> (Iterable[str]): lightweight tags for search/grouping.</li> </ul> <p>Example:</p> <pre><code>res = my_tool(a=arg_a, b=arg_b, _after=[prev1, prev2], _name=\"preprocess\", _labels=[\"data\",\"prep\"])\n</code></pre> <p>These control keys are stripped before calling your function and only affect graph construction.</p>"},{"location":"tools/#simple-examples","title":"Simple Examples","text":""},{"location":"tools/#1-immediate-execution-no-graph-builder-active","title":"1) Immediate execution (no graph builder active)","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"sum\", \"mean\"])  # outputs you promise to return\ndef stats(xs: list[float]):\n    s = sum(xs)\n    return {\"sum\": s, \"mean\": s / len(xs)}\n\nout = stats([1,2,3,4])   # \u2192 {\"sum\": 10, \"mean\": 2.5}\n</code></pre>"},{"location":"tools/#2-graph-construction-inside-a-builder","title":"2) Graph construction (inside a builder)","text":"<pre><code>from aethergraph import tool\nfrom aethergraph import graphify\nfrom aethergraph.graph import arg  # or from aethergraph.graph.graph_refs import arg\n\n@tool(outputs=[\"y\"])\ndef add(x: int, z: int): return {\"y\": x + z}\n\n@tool(outputs=[\"z\"])\ndef mul(x: int, k: int): return {\"z\": x * k}\n\n@graphify(name=\"pipeline\", inputs=[\"x\"], outputs=[\"y\"])\ndef pipeline(x):\n    a = mul(x=arg(\"x\"), k=2)          # NodeHandle(\"mul_...\")\n    b = add(x=arg(\"x\"), z=a.z)        # depends on `a` automatically via data edge\n    return {\"y\": b.y}\n\nG = pipeline.build()                    # TaskGraph\nspec = pipeline.spec()                  # graph spec for inspection/registry\nio = pipeline.io()                      # IO signature\n</code></pre>"},{"location":"tools/#3-forcing-an-order-with-_after-no-data-edge","title":"3) Forcing an order with <code>_after</code> (no data edge)","text":"<pre><code>@tool(outputs=[\"ok\"])\ndef init(): return {\"ok\": True}\n\n@tool(outputs=[\"ready\"])\ndef warmup(): return {\"ready\": True}\n\n@graphify(name=\"order_demo\", inputs=[], outputs=[\"ready\"])\ndef order_demo():\n    n1 = init()\n    n2 = warmup(_after=n1)   # enforce sequencing without passing data\n    return {\"ready\": n2.ready}\n</code></pre>"},{"location":"tools/#registration-optional","title":"Registration (Optional)","text":"<p>If a runtime registry is active (via <code>current_registry()</code>), the decorator auto\u2011registers your tool under the <code>tool</code> namespace with its <code>name</code> and <code>version</code> so it can be listed and referenced later.</p> <p>You can also call tools by dotted path via <code>call_tool(\"pkg.module:function\", arg1=..., ...)</code> to avoid importing at build sites, but the recommended ergonomic flow is to <code>import</code> the tool and call it directly.</p>"},{"location":"tools/#best-practices","title":"Best Practices","text":"<ul> <li>Keep tools focused and side\u2011effect aware (e.g., write artifacts via <code>context.artifacts()</code> inside <code>@graph_fn</code> wrappers).</li> <li>Always declare <code>outputs</code> and make your function return those keys.</li> <li>Use <code>_after</code> for control dependencies when no data edge exists.</li> <li>Prefer composing tools via <code>@graphify</code> for explicit fan\u2011in/fan\u2011out graphs.</li> <li>Inside <code>@graph_fn</code>, you can call tools to create explicit nodes, but <code>@graph_fn</code> is for immediate orchestration.</li> </ul>"},{"location":"build-graphs/","title":"Build Graphs in AetherGraph","text":"<p>Welcome! This section is the fastest way to grok how to build and run graphs with Python-first ergonomics.</p> <p>We introduce things in the order you will actually use them:</p> <ol> <li><code>@graph_fn</code> \u2014 the on-ramp. Wrap a regular Python function so it runs as a single graph node, with full <code>context.*</code> access. Great for demos, services, notebooks.</li> <li><code>@tool</code> \u2014 make any function a graph node. Use it inside <code>graph_fn</code> for per-step visibility, metrics, artifacts, and reuse.</li> <li><code>@graphify</code> \u2014 build an explicit DAG for fan-out/fan-in, ordering via <code>_after</code>, subgraphs, and reuse.</li> </ol> <p>Tip: Start with <code>@graph_fn</code> (plus a couple of <code>@tool</code> calls). Move to <code>@graphify</code> when you want explicit topology, parallel map/reduce, barriers, or long-lived pipelines.</p>"},{"location":"build-graphs/#what-is-a-graph-here","title":"What is a \"graph\" here?","text":"<ul> <li>AetherGraph executes TaskGraphs \u2014 directed acyclic graphs of nodes.</li> <li>A node can be:</li> <li>a graph function (<code>@graph_fn</code>) \u2014 runs immediately and can call context services.</li> <li>a tool node (<code>@tool</code>) \u2014 a typed, reusable operation with visible inputs/outputs.</li> <li>The Context (<code>context.*</code>) gives every node uniform access to runtime services:   <code>channel()</code>, <code>artifacts()</code>, <code>memory()</code>, <code>kv()</code>, <code>llm()</code>, <code>rag()</code>, <code>mcp()</code>, <code>logger()</code>.</li> </ul>"},{"location":"build-graphs/#quickstart-30-lines","title":"Quickstart (30 lines)","text":"<pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int):\n    return {\"y\": x * x}\n\n@graph_fn(name=\"demo\", outputs=[\"y\"])\nasync def demo(x: int, *, context):\n    await context.channel().send_text(f\"Computing square of {x}\u2026\")\n    h = square(x=x)              # creates a node you can inspect later\n    await context.channel().send_text(\"Done.\")\n    return {\"y\": h.y}            # expose tool output\n</code></pre> <p>Why this design? - You get instant run semantics (like a normal async function), but steps you mark with <code>@tool</code> become visible graph nodes with metrics/artifacts. - When your flow grows and needs explicit fan-out/fan-in or ordering, switch to <code>@graphify</code>.</p>"},{"location":"build-graphs/#next-steps","title":"Next steps","text":"<ul> <li><code>graph_fn</code> (on-ramp) -&gt; graph_fn.md</li> <li><code>@tool</code> reference -&gt; tool.md</li> <li><code>@graphify</code> (explicit DAG + fan-in/out) -&gt; graphify.md</li> <li>Choosing the right approach -&gt; choosing.md</li> </ul>"},{"location":"build-graphs/choosing/","title":"Choosing: <code>graph_fn</code> vs <code>@graphify</code> vs <code>@tool</code>","text":"<p>Use this one-screen guide to pick the right entry point.</p>"},{"location":"build-graphs/choosing/#start-simple","title":"Start simple","text":"<ul> <li><code>@graph_fn</code> \u2014 quickest way to ship a working function with <code>context.*</code>. Add a couple of <code>@tool</code> calls inside if you want visible/inspectable steps.</li> </ul>"},{"location":"build-graphs/choosing/#scale-up-when-needed","title":"Scale up when needed","text":"<ul> <li><code>@graphify</code> \u2014 when you need explicit DAG control:</li> <li>fan-out / fan-in / map-reduce</li> <li><code>_after</code> (barriers) and <code>_alias</code>/<code>_labels</code> for orchestration and UI</li> <li>subgraph reuse and IO/spec inspection</li> </ul>"},{"location":"build-graphs/choosing/#tool-is-a-building-block","title":"<code>@tool</code> is a building block","text":"<ul> <li>Wrap any function to make it a typed node.</li> <li>Works in both: inside <code>@graph_fn</code> (immediate run, visible steps) and in <code>@graphify</code> (adds nodes to DAG).</li> <li>Control kwargs (<code>_after</code>, <code>_alias</code>, <code>_labels</code>, <code>_id</code>) apply only in graph build contexts.</li> </ul>"},{"location":"build-graphs/choosing/#quick-comparison","title":"Quick comparison","text":"Capability <code>@graph_fn</code> <code>@graphify</code> <code>@tool</code> Immediate \"just run\" Yes Build first Yes (outside graph) Full <code>context.*</code> access Yes (via <code>context</code>) via tools/subgraphs when called under <code>graph_fn</code> Visible per-step nodes via <code>@tool</code> calls native yes Fan-out / fan-in (map/reduce) limited (Python loops) Yes (concise) building block Control edges (<code>_after</code>/barrier) No Yes Yes in graph build Graph spec/IO inspection implicit Yes (<code>.spec()/.io()</code>) n/a Best for demos, services pipelines, orchestration atomic operations <p>Rule of thumb: Start with <code>@graph_fn</code>. When you feel the need for explicit topology or orchestration, switch the same steps into <code>@graphify</code> using the exact same <code>@tool</code>s.</p>"},{"location":"build-graphs/graph_fn/","title":"<code>@graph_fn</code> \u2014 Python-first on-ramp","text":"<p>Wrap a normal (async) Python function so it runs as a single graph node with full access to <code>context.*</code> services. Return values are exposed as graph outputs.</p>"},{"location":"build-graphs/graph_fn/#signature","title":"Signature","text":"<pre><code>@graph_fn(name: str, *, inputs: list[str] | None = None, outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\ndef|async def fn(..., *, context: NodeContext) -&gt; dict | value | NodeHandle\n</code></pre> <ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (list[str], optional) \u2014 Declared input keys (used for IO spec; optional for quickstart).</li> <li>outputs (list[str], optional) \u2014 Declared output keys (enables single-value return).</li> <li>version (str) \u2014 SemVer for registry/lineage.</li> <li>agent (str, optional) \u2014 If set, register this graph function as an agent (advanced).</li> </ul>"},{"location":"build-graphs/graph_fn/#return-normalization","title":"Return normalization","text":"<ul> <li>dict -&gt; keys become outputs; NodeHandles/Refs are exposed.</li> <li>single value -&gt; allowed only if exactly one <code>outputs</code> key is declared (collapsed to that name).</li> <li>NodeHandle -&gt; its outputs are exposed (single output collapses).</li> </ul>"},{"location":"build-graphs/graph_fn/#using-tool-inside-graph_fn","title":"Using <code>@tool</code> inside <code>graph_fn</code>","text":"<p>You can call <code>@tool</code> functions to create visible/inspectable nodes while keeping immediate Python control flow:</p> <pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int): return {\"y\": x*x}\n\n@graph_fn(name=\"demo\", outputs=[\"y\"])\nasync def demo(x: int, *, context):\n    h = square(x=x)          # creates a node\n    await context.channel().send_text(\"computed\")\n    return {\"y\": h.y}\n</code></pre> <p>Important: Control kwargs like <code>_after</code>, <code>_alias</code>, <code>_labels</code> are only honored in graph build contexts (e.g., <code>@graphify</code>). Inside <code>graph_fn</code>, execution order follows normal Python semantics. If you need control edges without passing data, use <code>@graphify</code>.</p>"},{"location":"build-graphs/graph_fn/#when-to-use-graph_fn","title":"When to use <code>@graph_fn</code>","text":"<ul> <li>Quick demos, notebooks, service-style tasks.</li> <li>One to a few steps, mostly sequential.</li> <li>You want full <code>context.*</code> access and instant execution, with optional visibility via <code>@tool</code> calls.</li> </ul> <p>See also: tool.md, graphify.md.</p>"},{"location":"build-graphs/graphify/","title":"<code>@graphify</code> \u2014 Build an explicit DAG (fan-out, fan-in, ordering)","text":"<p>Use <code>@graphify</code> when you need clear topology: map/fan-out, reduce/fan-in, barriers via <code>_after</code>, subgraphs, or reusable pipelines.</p>"},{"location":"build-graphs/graphify/#signature","title":"Signature","text":"<p><pre><code>@graphify(*, name=\"default_graph\", inputs=(), outputs=None, version=\"0.1.0\", agent: str | None = None)\ndef body(...):\n    # Use tool calls to add nodes and return NodeHandles/Refs\n    return {...}\n</code></pre> - The decorated function returns a builder: call <code>.build()</code> to get a <code>TaskGraph</code> instance; <code>.spec()</code> for a serializable spec; <code>.io()</code> for IO signature.</p>"},{"location":"build-graphs/graphify/#control-edges-and-labels-graph-build-only","title":"Control edges and labels (graph build only)","text":"<p><code>@tool</code> control kwargs are honored here: - <code>_after</code>, <code>_alias</code>, <code>_labels</code>, <code>_id</code>, <code>_name</code></p>"},{"location":"build-graphs/graphify/#patterns","title":"Patterns","text":""},{"location":"build-graphs/graphify/#fan-out-map-over-inputs","title":"Fan-out (map over inputs)","text":"<pre><code>from aethergraph import tool, graphify\n\n@tool(outputs=[\"vec\"])\ndef embed(text: str): ...\n\n@graphify(name=\"fanout_demo\", inputs=[\"texts\"], outputs=[\"vecs\"])\ndef fanout_demo(texts):\n    handles = [embed(text=t) for t in texts]          # fan-out\n    return {\"vecs\": [h.vec for h in handles]}         # expose list of outputs\n</code></pre>"},{"location":"build-graphs/graphify/#fan-in-reduce","title":"Fan-in (reduce)","text":"<pre><code>@tool(outputs=[\"score\"])\ndef dot(a, b): ...\n\n@graphify(name=\"fanin_demo\", inputs=[\"query\", \"vecs\"], outputs=[\"scores\"])\ndef fanin_demo(query, vecs):\n    q = embed(text=query)\n    scores = [dot(a=v, b=q.vec) for v in vecs]        # fan-in through q\n    return {\"scores\": [s.score for s in scores]}\n</code></pre>"},{"location":"build-graphs/graphify/#control-edge-without-data","title":"Control edge without data","text":"<pre><code>@tool(outputs=[\"ok\"])   def init(): ...\n@tool(outputs=[\"done\"]) def train(): ...\n\n@graphify(name=\"order\", outputs=[\"done\"])\ndef order():\n    a = init()\n    b = train(_after=a)            # sequence a -&gt; b\n    return {\"done\": b.done}\n</code></pre>"},{"location":"build-graphs/graphify/#subgraph-reuse-optional","title":"Subgraph reuse (optional)","text":"<p>You can register graphs and call them as nodes (advanced). For most cases, compose <code>@tool</code>s directly inside <code>@graphify</code>.</p>"},{"location":"build-graphs/graphify/#when-to-use-graphify","title":"When to use <code>@graphify</code>","text":"<ul> <li>You need parallelism (map) or aggregation (reduce).</li> <li>You need ordering without data flow (<code>_after</code>/barriers).</li> <li>You want a reusable / inspectable DAG (e.g., schedule in a UI).</li> </ul> <p>See also: graph_fn.md, tool.md, choosing.md.</p>"},{"location":"build-graphs/tool/","title":"<code>@tool</code> \u2014 Turn any function into a graph node","text":"<p>Make a plain function a typed, reusable node with explicit inputs/outputs. Works in both <code>@graph_fn</code> (immediate run with visible steps) and <code>@graphify</code> (graph build).</p>"},{"location":"build-graphs/tool/#decorator","title":"Decorator","text":"<pre><code>@tool(outputs: list[str], inputs: list[str] | None = None, *, name: str | None = None, version: str = \"0.1.0\")\ndef fn(...): ...\n</code></pre> <ul> <li>outputs (list[str]) \u2014 Output field names this tool produces.</li> <li>inputs (list[str], optional) \u2014 Input names; inferred from signature if omitted.</li> <li>name (str, optional) \u2014 Registry name (defaults to function name).</li> <li>version (str) \u2014 SemVer for registry/lineage.</li> </ul>"},{"location":"build-graphs/tool/#return-normalization","title":"Return normalization","text":"<ul> <li><code>None</code> -&gt; <code>{}</code></li> <li><code>dict</code> -&gt; as-is</li> <li><code>tuple</code> -&gt; <code>{ \"out0\": v0, \"out1\": v1, ... }</code></li> <li>single value -&gt; <code>{ \"result\": value }</code></li> </ul> <p>Contract check: Declared <code>outputs</code> must be present in the normalized return, otherwise a <code>ValueError</code> is raised.</p>"},{"location":"build-graphs/tool/#two-modes-same-decorator","title":"Two modes (same decorator)","text":"Where called from Behavior Outside any graph Runs immediately and returns a dict. Inside <code>@graph_fn</code> Creates a node handle you can expose. Inside <code>@graphify</code> Adds a node to the DAG (honors control kw). <p>Control kwargs (graph build only): - <code>_after</code> (NodeHandle | list) \u2014 add control-edge dependency. - <code>_alias</code> / <code>_id</code> \u2014 override node id / alias. - <code>_labels</code> (list[str]) \u2014 annotate node for UI/search. - <code>_name</code> \u2014 display name hint.</p>"},{"location":"build-graphs/tool/#minimal-example","title":"Minimal example","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"y\"])\ndef square(x:int) -&gt; dict:\n    return {\"y\": x*x}\n</code></pre> <p>Use in <code>graph_fn</code> or <code>@graphify</code> as shown in their pages.</p>"},{"location":"channel-setup/console-setup/","title":"Console Channel Overview","text":"<p>The console channel is the simplest built-in channel in AetherGraph. It prints messages to your terminal and, when possible, reads replies directly from standard input.</p> <p>It is the default channel used when you don\u2019t specify any other channel.</p>"},{"location":"channel-setup/console-setup/#channel-key","title":"Channel key","text":"<ul> <li> <p>Default channel key: <code>console:stdin</code></p> </li> <li> <p>If you call:</p> </li> </ul> <pre><code>await context.channel().send_text(\"Hello from AetherGraph\")\n</code></pre> <p>and you haven\u2019t overridden the default channel, AetherGraph will use the console channel (i.e., <code>console:stdin</code>).</p> <ul> <li>You can also reference it explicitly:</li> </ul> <pre><code>chan = context.channel(\"console:stdin\")\nawait chan.send_text(\"This goes to the terminal\")\n</code></pre>"},{"location":"channel-setup/console-setup/#capabilities","title":"Capabilities","text":"<p>The console channel supports:</p> <ul> <li>Text output \u2013 plain messages printed to the terminal.</li> <li>Input \u2013 reading user input from standard input for <code>ask_*</code> methods.</li> <li>Buttons/approvals \u2013 <code>ask_approval</code> prompts are rendered as numbered options; you reply by typing a number or label.</li> </ul> <p>Internally, the adapter declares:</p> <pre><code>capabilities = {\"text\", \"input\", \"buttons\"}\n</code></pre>"},{"location":"channel-setup/console-setup/#how-send_text-behaves","title":"How <code>send_text</code> behaves","text":"<p>When you call:</p> <pre><code>await context.channel().send_text(\"Hello \ud83d\udc4b\")\n</code></pre> <p>AetherGraph:</p> <ul> <li>Prints a line like:</li> </ul> <pre><code>[console] agent.message :: Hello \ud83d\udc4b\n</code></pre> <ul> <li>Returns immediately; there is no waiting or continuation involved.</li> </ul> <p>This makes the console channel ideal for quick debugging, demos, and local experiments.</p>"},{"location":"channel-setup/console-setup/#how-ask_-behaves","title":"How <code>ask_*</code> behaves","text":"<p>When you call <code>ask_text</code> or <code>ask_approval</code> on the console channel, AetherGraph will:</p> <ul> <li>Print a prompt to the terminal.</li> <li>Block briefly while it reads a line from <code>stdin</code> (via <code>sys.stdin.readline</code> in a background executor).</li> <li>Use that line as the response and resume your graph inline.</li> </ul> <p>For example:</p> <pre><code>name = await context.channel().ask_text(\"What is your name?\")\nawait context.channel().send_text(f\"Nice to meet you, {name}!\")\n</code></pre> <p>This typically behaves with no asynchronous wait in the sense of external channels: everything happens in your local terminal session.</p> <p>Note: In non-interactive environments (e.g., no <code>stdin</code>, or CI), the console adapter may not be able to capture input. In that case, it will fall back to persisting a real continuation so the run can be resumed later. For normal local use in a terminal, <code>ask_*</code> works inline and does not require any external channel setup.</p>"},{"location":"channel-setup/file-setup/","title":"File Channel Overview","text":"<p>The file channel is a simple, one-way output channel that writes messages from AetherGraph directly to files on disk.</p>"},{"location":"channel-setup/file-setup/#when-to-use-the-file-channel","title":"When to use the file channel","text":"<p>Use the file channel when you want to:</p> <ul> <li>Keep a persistent log of what a graph did (steps, status, results).</li> <li>Save run transcripts for later inspection, papers, or debugging.</li> <li>Capture output in a way that can be easily opened with any text editor.</li> </ul> <p>It\u2019s ideal for individual researchers who want lightweight, local logging without setting up external services.</p>"},{"location":"channel-setup/file-setup/#where-files-are-written","title":"Where files are written","text":"<p>In this setup, the file channel writes under:</p> <pre><code>&lt;workspace&gt;/channel_files\n</code></pre> <ul> <li><code>&lt;workspace&gt;</code> is the root directory you configured for AetherGraph.</li> <li>Inside <code>channel_files</code>, the rest of the path is taken from your channel key.</li> </ul> <p>For example, if your workspace is <code>./aethergraph_data</code> and you use:</p> <pre><code>chan = context.channel(\"file:runs/demo_run.log\")\nawait chan.send_text(\"Demo run started\")\n</code></pre> <p>The message will be appended to:</p> <pre><code>./aethergraph_data/channel_files/runs/demo_run.log\n</code></pre> <p>You can define any relative path after <code>file:</code> (e.g., <code>file:logs/experiment_01.txt</code>), and AetherGraph will create the parent directories if needed.</p> <p>This makes it easy to organize logs by project, experiment, or date while keeping everything local to your workspace.</p>"},{"location":"channel-setup/introduction/","title":"Channels Overview","text":"<p>AetherGraph supports multiple channels for delivering messages and interacting with users or tools. This page summarizes:</p> <ul> <li>What channels exist and what they\u2019re good for.</li> <li>Which capabilities each channel supports.</li> <li>How <code>ask_*</code> works vs inform-only channels.</li> <li>Notes on persistence, concurrency, and security.</li> </ul> <p>Note on OSS channels and scope</p> <p>The channel system in the open-source AetherGraph is designed for personal, local, and small-team use, with a focus on convenience and easy integration into your own frontends. It is not intended as a high-scale, multi-tenant messaging infrastructure, and you should not try to \u201cproduction-scale\u201d the current adapters as-is. In particular, for safety concern, many integrations rely on local polling and simple wiring rather than hardened, multi-region webhook setups. For anything exposed publicly or handling sensitive data, you should treat these channels as experimental building blocks: keep them behind trusted networks, review security settings carefully, and expect that a future, more rigorous release will provide safer patterns for large-scale and public deployments.</p>"},{"location":"channel-setup/introduction/#1-channels-and-when-to-use-them","title":"1. Channels and when to use them","text":""},{"location":"channel-setup/introduction/#console-console","title":"Console (<code>console:</code>)","text":"<ul> <li>Default channel when you don\u2019t specify anything (key: <code>console:stdin</code>).</li> <li> <p>Best for:</p> </li> <li> <p>Local development and debugging.</p> </li> <li>Simple CLI-style interaction.</li> <li>Messages are printed to the terminal, and <code>ask_*</code> reads from standard input.</li> <li>No external setup required.</li> </ul>"},{"location":"channel-setup/introduction/#slack-slack","title":"Slack (<code>slack:</code>)","text":"<ul> <li>Two-way, rich chat channel (bot-based).</li> <li> <p>Best for:</p> </li> <li> <p>Team workflows.</p> </li> <li>Rich notifications and approvals in Slack.</li> <li><code>ask_*</code> prompts that can resume even if the Python process restarts.</li> <li>Requires a Slack app + bot token.</li> </ul>"},{"location":"channel-setup/introduction/#telegram-tg","title":"Telegram (<code>tg:</code>)","text":"<ul> <li>Two-way, rich chat channel via the Telegram Bot API.</li> <li> <p>Best for:</p> </li> <li> <p>Individual researchers who prefer Telegram.</p> </li> <li>Mobile-friendly notifications and prompts.</li> <li>Uses polling (local/dev) or webhook (advanced) to receive messages.</li> <li>Support for <code>ask_*</code> with resumable waits, similar to Slack (experimental).</li> </ul>"},{"location":"channel-setup/introduction/#file-file","title":"File (<code>file:</code>)","text":"<ul> <li>Inform-only channel that writes messages to disk.</li> <li> <p>Best for:</p> </li> <li> <p>Persistent logs and run transcripts.</p> </li> <li>Keeping a local record under <code>workspace/channel_files/...</code>.</li> <li>No user input; <code>ask_*</code> is not supported.</li> </ul>"},{"location":"channel-setup/introduction/#webhook-webhook","title":"Webhook (<code>webhook:</code>)","text":"<ul> <li>Generic inform-only channel that POSTs JSON to a URL.</li> <li> <p>Best for:</p> </li> <li> <p>Sending notifications to services with incoming webhooks (Slack, Discord, etc.).</p> </li> <li>Triggering automation tools like Zapier/Make.</li> <li>Integrating with custom backends.</li> <li>Outbound only; no <code>ask_*</code>.</li> </ul>"},{"location":"channel-setup/introduction/#2-capability-comparison","title":"2. Capability comparison","text":"<p>Legend:</p> <ul> <li>\u2705 Supported in a first-class way.</li> <li>\ud83d\udcdd Logged/forwarded only (no interaction).</li> <li>\u2716\ufe0f Not supported.</li> </ul> Channel Key prefix / example Default? Text Input / <code>ask_*</code> Buttons / approval Image File Stream / edit Inbound resume? Console <code>console:stdin</code> \u2705 \u2705 \u2705 (inline) \u2705 (numbered) \u2716\ufe0f \u2716\ufe0f \u2716\ufe0f \u2716\ufe0f (no resume) Slack <code>slack:team/T:chan/C[:thread/TS]</code> \u2716\ufe0f \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Telegram <code>tg:chat/&lt;id&gt;[:topic/&lt;thread_id&gt;]</code> \u2716\ufe0f \u2705 \u2705 (experimental) \u2705 \u2705 \u2705 \u2705 \u2705 (experimental) File <code>file:logs/default.log</code> \u2716\ufe0f \u2705 \u2716\ufe0f \ud83d\udcdd (logged only) \ud83d\udcdd \ud83d\udcdd \ud83d\udcdd N/A Webhook <code>webhook:https://...</code> \u2716\ufe0f \u2705 \u2716\ufe0f \ud83d\udcdd (payload only) \ud83d\udcdd \ud83d\udcdd \ud83d\udcdd N/A <p>Notes:</p> <ul> <li>Console: simple and local; great for debugging but not meant for long-term, resumable conversations.</li> <li>Slack / Telegram: full chat channels; support richer content and resumable <code>ask_*</code> flows.</li> <li>File / Webhook: inform-only; they record/deliver messages but cannot accept replies.</li> </ul>"},{"location":"channel-setup/introduction/#3-ask_-vs-inform-only-channels","title":"3. <code>ask_*</code> vs inform-only channels","text":""},{"location":"channel-setup/introduction/#ask_-interactive-channels","title":"<code>ask_*</code> (interactive channels)","text":"<ul> <li> <p><code>ask_text</code>, <code>ask_approval</code>, etc. are supported on:</p> </li> <li> <p>console (inline input via stdin),</p> </li> <li>Slack,</li> <li> <p>Telegram (experimental).</p> </li> <li> <p>For Slack/Telegram:</p> <ul> <li>When you call <code>await context.channel().ask_text(...)</code>, AetherGraph creates a Continuation and stores it.</li> <li>The Python process can safely wait, sleep, or even be restarted.</li> <li>When a user replies in Slack/Telegram, the inbound adapter maps that message to the stored Continuation and calls <code>resume_router.resume(...)</code>.</li> <li>The graph then continues from where it left off.</li> </ul> </li> </ul>"},{"location":"channel-setup/introduction/#inform-only-channels","title":"Inform-only channels","text":"<ul> <li> <p>File and Webhook are strictly push-only:</p> <ul> <li><code>send_text</code> works (log/write/POST),</li> <li><code>ask_*</code> is not supported and should not be used with these channels.</li> </ul> </li> </ul> <p>They\u2019re ideal for notifications, logs, and integrations where you don\u2019t need a conversational back-and-forth.</p>"},{"location":"channel-setup/introduction/#4-console-and-resuming-waits","title":"4. Console and resuming waits","text":"<p>The console channel is designed for simple, inline interaction:</p> <ul> <li><code>send_text</code> prints directly to the terminal.</li> <li><code>ask_*</code> methods read from <code>stdin</code> synchronously (via a background executor) and then immediately resume the graph.</li> </ul> <p>Because of this design:</p> <ul> <li>There is no durable, cross-process resume for console-based <code>ask_*</code>.</li> <li>If the Python process dies while waiting for console input, that wait is not meant to be resumed later.</li> </ul> <p>This is a conscious tradeoff:</p> <ul> <li>The console channel stays extremely simple and reliable for local interactive runs.</li> <li>For resumable, long-lived conversations, prefer Slack or Telegram.</li> </ul>"},{"location":"channel-setup/introduction/#5-concurrency-and-multiple-channels","title":"5. Concurrency and multiple channels","text":"<p>AetherGraph\u2019s channel layer is designed so you can:</p> <ul> <li> <p>Send multiple messages concurrently on the same channel:</p> <ul> <li>e.g., several <code>send_text</code> calls to Slack or file within the same graph.</li> </ul> </li> <li> <p>Use multiple channels in one graph:</p> <ul> <li>e.g., log to file, notify via webhook, and talk to Slack in the same run.</li> </ul> </li> </ul> <p>Each <code>ChannelSession</code> and <code>OutEvent</code> is independent; the underlying adapters handle ordering and delivery.</p> <p>Example pattern:</p> <pre><code>async def run_with_notifications(context):\n    # Console debug\n    await context.channel(\"console:stdin\").send_text(\"Run started\")\n\n    # Log to file\n    await context.channel(\"file:runs/exp_01.log\").send_text(\"Run started\")\n\n    # Notify Slack\n    await context.channel(\"slack:team/T:chan/C\").send_text(\"Run started in Slack\")\n\n    # Notify external system via webhook\n    await context.channel(\"webhook:https://hooks.zapier.com/hooks/catch/.../\").send_text(\n        \"Run started in AetherGraph\"\n    )\n</code></pre>"},{"location":"channel-setup/introduction/#6-security-notes-for-webhook-channels","title":"6. Security notes for webhook channels","text":"<p>Webhook channels POST data to arbitrary URLs. Keep these points in mind:</p> <ul> <li> <p>Treat webhook URLs as secrets:</p> <ul> <li>Many platforms use long, random webhook URLs as the only authentication.</li> <li>Do not commit them to public repos or print them in logs.</li> </ul> </li> <li> <p>Use HTTPS whenever possible:</p> <ul> <li>Always prefer <code>https://</code> URLs so notifications are encrypted in transit.</li> </ul> </li> <li> <p>Optional shared secrets:</p> <ul> <li>If you control the receiving endpoint, you can configure the webhook adapter to send an extra header (e.g., <code>X-AetherGraph-Secret</code>) and verify it server-side.</li> </ul> </li> <li> <p>Least privilege:</p> <ul> <li>Only enable webhook targets that you actually need.</li> </ul> </li> </ul> <p>For sensitive or production workflows, consider routing webhook events through a trusted intermediary (e.g., Zapier/Make or your own backend) rather than posting directly to critical systems.</p>"},{"location":"channel-setup/slack-setup/","title":"Slack Integration Setup (Socket Mode)","text":"<p>This guide walks you through connecting Slack to AetherGraph using Socket Mode. This works great for local / individual use \u2014 no public URL or ngrok required.</p> <p>We\u2019ll do it in three steps:</p> <ol> <li>Create and configure a Slack app using a manifest.</li> <li>Configure AetherGraph via <code>.env</code>.</li> <li>Choose a default Slack channel and <code>channel_key</code> (with optional aliases).</li> </ol>"},{"location":"channel-setup/slack-setup/#1-create-a-slack-app-with-manifest","title":"1. Create a Slack App (with manifest)","text":"<ol> <li>Go to https://api.slack.com/apps and click \u201cCreate New App\u201d \u2192 \u201cFrom an app manifest\u201d.</li> <li>Choose the workspace where you want to use AetherGraph.</li> <li>Paste the following manifest (adjust the <code>display_information.name</code> if you like):</li> </ol> <pre><code>_display_information:\n  name: AetherGraph\nfeatures:\n  bot_user:\n    display_name: AetherGraph\n    always_online: true\noauth_config:\n  scopes:\n    bot:\n      - chat:write\n      - chat:write.public\n      - channels:history\n      - groups:history\n      - im:history\n      - mpim:history\nsettings:\n  interactivity:\n    is_enabled: true\n  event_subscriptions:\n    bot_events:\n      - message.channels\n      - message.im\n      - app_mention\n</code></pre> <p>Note: When using Socket Mode, you do not need to configure an HTTP Request URL for Events or Interactivity.</p> <ol> <li>Click Create.</li> <li>Go to OAuth &amp; Permissions \u2192 Install App to Workspace and complete the install.</li> </ol>"},{"location":"channel-setup/slack-setup/#enable-socket-mode-and-get-tokens","title":"Enable Socket Mode and get tokens","text":"<ol> <li>In your Slack app, go to Socket Mode in the left sidebar.</li> <li>Toggle Enable Socket Mode \u2192 ON.</li> <li> <p>Click \u201cGenerate App-Level Token\u201d:</p> </li> <li> <p>Name it something like <code>aethergraph-app-token</code>.</p> </li> <li>Grant it the <code>connections:write</code> scope.</li> <li>Copy the resulting token (it starts with <code>xapp-\u2026</code>).</li> <li>Go to OAuth &amp; Permissions and copy the Bot User OAuth Token (starts with <code>xoxb-\u2026</code>).</li> </ol> <p>You now have:</p> <ul> <li>Bot token (e.g. <code>xoxb-...</code>)</li> <li>App token (e.g. <code>xapp-...</code>)</li> <li>Optional: Signing secret (used later for webhook mode)</li> </ul> <p>You can find the signing secret under Basic Information \u2192 App Credentials \u2192 Signing Secret.</p>"},{"location":"channel-setup/slack-setup/#2-configure-env-for-aethergraph","title":"2. Configure <code>.env</code> for AetherGraph","text":"<p>AetherGraph reads Slack settings from <code>AETHERGRAPH_SLACK__*</code> entries in your <code>.env</code>.</p> <p>Add the following variables:</p> <pre><code># Turn Slack on\nAETHERGRAPH_SLACK__ENABLED=true\n\n# Tokens from Slack\nAETHERGRAPH_SLACK__BOT_TOKEN=xoxb-your-bot-token-here\nAETHERGRAPH_SLACK__APP_TOKEN=xapp-your-app-token-here\n\n# Optional but recommended to set now (used for future webhook mode)\nAETHERGRAPH_SLACK__SIGNING_SECRET=your-signing-secret-here\n\n# Transport mode (recommended defaults for local / individual use)\nAETHERGRAPH_SLACK__SOCKET_MODE_ENABLED=true\nAETHERGRAPH_SLACK__WEBHOOK_ENABLED=false\n</code></pre> <p>With this configuration:</p> <ul> <li>The AetherGraph sidecar opens a WebSocket connection to Slack (Socket Mode).</li> <li>You do not need a public URL, ngrok, or any <code>/slack/events</code> endpoint.</li> </ul> <p>Restart your AetherGraph sidecar after editing <code>.env</code> so the new settings take effect.</p>"},{"location":"channel-setup/slack-setup/#3-choosing-a-default-slack-channel-and-channel_key","title":"3. Choosing a default Slack channel and <code>channel_key</code>","text":"<p>AetherGraph uses a channel key to know where to send messages. For Slack, the canonical format is:</p> <pre><code>slack:team/&lt;TEAM_ID&gt;:chan/&lt;CHANNEL_ID&gt;\n</code></pre> <p>You can set a default channel in two ways:</p>"},{"location":"channel-setup/slack-setup/#31-using-default_team_id-default_channel_id","title":"3.1. Using <code>DEFAULT_TEAM_ID</code> / <code>DEFAULT_CHANNEL_ID</code>","text":"<ol> <li> <p>In Slack, find your team (workspace) ID and channel ID:</p> </li> <li> <p>Team ID: often visible in the URL or via a quick Slack API call.</p> </li> <li>Channel ID: right-click a channel \u2192 Copy link and grab the <code>C...</code> part.</li> <li>Add these to your <code>.env</code>:</li> </ol> <pre><code>AETHERGRAPH_SLACK__DEFAULT_TEAM_ID=T0123456789\nAETHERGRAPH_SLACK__DEFAULT_CHANNEL_ID=C0123456789\n</code></pre> <p>AetherGraph will then derive the canonical channel key:</p> <pre><code>slack:team/T0123456789:chan/C0123456789\n</code></pre> <p>When you call:</p> <pre><code>chan = context.channel()\nawait chan.send_text(\"Hello from AetherGraph \ud83d\udc4b\")\n</code></pre> <p>it will use this default Slack channel.</p>"},{"location":"channel-setup/slack-setup/#32-using-default_channel_key-directly","title":"3.2. Using <code>DEFAULT_CHANNEL_KEY</code> directly","text":"<p>If you prefer, you can set the full key yourself:</p> <pre><code>AETHERGRAPH_SLACK__DEFAULT_CHANNEL_KEY=slack:team/T0123456789:chan/C0123456789\n</code></pre> <p>This is the most explicit and is what AetherGraph uses internally.</p>"},{"location":"channel-setup/slack-setup/#4-using-channel-keys-for-multiple-slack-channels","title":"4. Using channel keys for multiple Slack channels","text":"<p>The canonical Slack <code>channel_key</code> used by AetherGraph has the form:</p> <pre><code>slack:team/&lt;TEAM_ID&gt;:chan/&lt;CHANNEL_ID&gt;\n</code></pre> <p>You can use this in two main ways:</p> <ol> <li> <p>Set a default channel for most runs (recommended)</p> </li> <li> <p>Use <code>AETHERGRAPH_SLACK__DEFAULT_TEAM_ID</code> and <code>AETHERGRAPH_SLACK__DEFAULT_CHANNEL_ID</code> in your <code>.env</code>.</p> </li> <li>AetherGraph will derive the canonical key internally.</li> <li> <p>When you call <code>context.channel()</code> without arguments, it will use that default Slack channel.</p> </li> <li> <p>Select a channel explicitly in code</p> </li> <li> <p>If you have multiple Slack channels you want to talk to, you can always pass a <code>channel_key</code> directly:</p> </li> </ol> <pre><code>chan = context.channel(\"slack:team/T0123456789:chan/C0123456789\")\nawait chan.send_text(\"Sending to this specific channel\")\n</code></pre> <p>This bypasses any default and sends to the channel you specify.</p> <p>Note: you don\u2019t have to set a default Slack channel. If no default is configured, AetherGraph falls back to <code>console:stdin</code> for <code>context.channel()</code>; you can still target Slack explicitly by passing a full <code>channel_key</code> when you need it.</p>"},{"location":"channel-setup/slack-setup/#5-quick-test","title":"5. Quick test","text":"<p>Once everything is configured:</p> <ol> <li>Ensure your <code>.env</code> has the Slack values and the sidecar is running.</li> <li> <p>Run a small test graph:</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"hello_slack\")\nasync def hello_slack(*, context: NodeContext):\n    chan = context.channel()  # uses default Slack channel\n    await chan.send_text(\"Hello from AetherGraph \ud83d\udc4b\")\n</code></pre> </li> <li> <p>Execute this graph and confirm that the message appears in your chosen Slack channel.</p> </li> </ol> <p>If the message arrives, your Slack Socket Mode integration is working.</p>"},{"location":"channel-setup/slack-setup/#note-local-only-pattern","title":"Note: local-only pattern","text":"<p>This Socket Mode setup is intended for local / personal use: AetherGraph runs on your machine or a dev box, and connects out to Slack over a secure WebSocket. We do not support production webhook with the initial release.</p> <p>Do not expose your local sidecar directly to the public internet!</p>"},{"location":"channel-setup/telegram-setup/","title":"Telegram Integration Setup (Local, Experimental)","text":"<p>This guide shows how to connect a Telegram bot to AetherGraph for local / personal use.</p> <p>\u26a0\ufe0f Status: Telegram support is currently experimental. In practice it often works very well, but sometimes:</p> <ul> <li>the polling loop can take a while to pick up the first message, or</li> <li>you may need to restart the sidecar if the connection gets stuck.</li> </ul> <p>It\u2019s great for demos and local exploration, but not recommended yet for production-critical usage.</p> <p>We\u2019ll keep it simple and mirror the Slack setup:</p> <ol> <li>Create a Telegram bot with BotFather.</li> <li>Paste your bot token into the AetherGraph <code>.env</code>.</li> <li>(Optional) Choose a default Telegram chat / <code>channel_key</code>.</li> <li>Run a quick test.</li> </ol>"},{"location":"channel-setup/telegram-setup/#1-create-a-telegram-bot-botfather","title":"1. Create a Telegram bot (BotFather)","text":"<ol> <li> <p>Open Telegram and start a chat with @BotFather.</p> </li> <li> <p>Send the command <code>/newbot</code> and follow the prompts:</p> </li> <li> <p>Choose a name (e.g. <code>AetherGraph Telegram Bot</code>).</p> </li> <li> <p>Choose a username (must end with <code>bot</code>, e.g. <code>aethergraph_dev_bot</code>).</p> </li> <li> <p>BotFather will reply with a message that includes your bot\u2019s token, e.g.: <code>8....:hweCnwe...</code></p> </li> <li> <p>Copy that token \u2014 you\u2019ll paste it into your AetherGraph <code>.env</code> file.</p> </li> </ol> <p>That\u2019s all you need on the Telegram side for local polling.</p>"},{"location":"channel-setup/telegram-setup/#2-add-telegram-settings-to-env","title":"2. Add Telegram settings to <code>.env</code>","text":"<p>AetherGraph reads Telegram configuration from <code>AETHERGRAPH_TELEGRAM__*</code> variables.</p> <p>For local usage with polling (no public URL, no webhook), add something like this to your <code>.env</code>:</p> <pre><code># Turn Telegram integration on\nAETHERGRAPH_TELEGRAM__ENABLED=true\n\n# Bot token from BotFather\nAETHERGRAPH_TELEGRAM__BOT_TOKEN=...\n\n# Local/dev polling mode (recommended)\nAETHERGRAPH_TELEGRAM__POLLING_ENABLED=true\nAETHERGRAPH_TELEGRAM__WEBHOOK_ENABLED=false\n\n# Optional: default chat ID for context.channel()\nAETHERGRAPH_TELEGRAM__DEFAULT_CHAT_ID=...\n</code></pre> <p>Notes:</p> <ul> <li><code>BOT_TOKEN</code> is the token from BotFather.</li> <li><code>POLLING_ENABLED=true</code> tells AetherGraph to use Telegram\u2019s <code>getUpdates</code> API (no public IP required).</li> <li><code>WEBHOOK_ENABLED=false</code> keeps webhook mode off for now.</li> <li><code>DEFAULT_CHAT_ID</code> is optional but convenient \u2014 AetherGraph will use it as the default Telegram chat when you call <code>context.channel()</code> with no arguments.</li> </ul> <p>After editing <code>.env</code>, restart the AetherGraph sidecar so it picks up the new settings.</p> <p>\ud83d\udd0e If you previously used webhooks with this bot: run <code>deleteWebhook</code> once so polling can receive updates:</p> <pre><code>curl \"https://api.telegram.org/bot&lt;YOUR_BOT_TOKEN&gt;/deleteWebhook\"\n</code></pre> <p>You only need to do this once per bot.</p>"},{"location":"channel-setup/telegram-setup/#3-default-chat-and-channel_key","title":"3. Default chat and <code>channel_key</code>","text":"<p>Internally, AetherGraph uses a channel key for each target. For Telegram, the canonical form is:</p> <pre><code>tg:chat/&lt;CHAT_ID&gt;\n</code></pre> <p>For basic 1:1 chats or simple groups, that\u2019s all you need.</p>"},{"location":"channel-setup/telegram-setup/#31-using-default_chat_id-recommended","title":"3.1. Using <code>DEFAULT_CHAT_ID</code> (recommended)","text":"<p>If you set:</p> <pre><code>AETHERGRAPH_TELEGRAM__DEFAULT_CHAT_ID=7663982940\n</code></pre> <p>then AetherGraph will treat:</p> <pre><code>tg:chat/7663982940\n</code></pre> <p>as the default Telegram <code>channel_key</code>.</p> <p>In your graph code, you can simply do:</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"hello_telegram\")\nasync def hello_telegram(*, context: NodeContext):\n    chan = context.channel()  # uses default Telegram chat\n    await chan.send_text(\"Hello from AetherGraph via Telegram \ud83d\udc4b\")\n</code></pre>"},{"location":"channel-setup/telegram-setup/#32-selecting-a-chat-explicitly","title":"3.2. Selecting a chat explicitly","text":"<p>If you want to target a specific chat (for example, while experimenting with multiple chat IDs), you can pass the <code>channel_key</code> directly:</p> <pre><code>chan = context.channel(\"tg:chat/7663982940\")\nawait chan.send_text(\"Sending to this specific Telegram chat\")\n</code></pre> <p>This works the same way in both polling and webhook modes (even though webhook is not recommended for local use right now).</p>"},{"location":"channel-setup/telegram-setup/#4-quick-test","title":"4. Quick test","text":"<p>Once your <code>.env</code> is set and the sidecar is running:</p> <ol> <li>Make sure <code>AETHERGRAPH_TELEGRAM__ENABLED=true</code> and <code>POLLING_ENABLED=true</code>.</li> <li>Send a message to your bot in Telegram (e.g. <code>/start</code>).</li> <li>Run a small test graph:</li> </ol> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"ping_telegram\")\nasync def ping_telegram(*, context: NodeContext):\n    chan = context.channel()  # uses default Telegram chat if configured\n    await chan.send_text(\"Ping from AetherGraph \ud83d\udef0\ufe0f\")\n</code></pre> <ol> <li>Execute this graph and check that the message appears in your Telegram chat.</li> </ol> <p>If the message arrives, the basic Telegram polling integration is working.</p> <p>\ud83e\uddea Experimental note:</p> <ul> <li>On some networks, the first polling call may take a while to pick up messages.</li> <li>If it seems stuck, try restarting the sidecar and sending a fresh message.</li> <li>For now, treat Telegram as a best-effort local channel; for more robust integrations, prefer Slack or your internal web UI until Telegram support stabilizes.</li> </ul>"},{"location":"channel-setup/webhook-setup/","title":"Webhook Channel Setup &amp; Usage","text":"<p>The webhook channel lets AetherGraph send events to almost any external service that accepts an HTTP POST.</p> <p>\ud83d\udd14 One-way only: webhook channels are inform-only. They push messages out from AetherGraph, but cannot receive replies or drive <code>ask_*</code> prompts.</p> <p>This makes them perfect for notifications, logs, and status updates.</p> <p>We\u2019ll cover:</p> <ol> <li>What the webhook channel is and when to use it.</li> <li>Supported platforms (tested examples).</li> <li>How to use <code>webhook:</code> channels in your graphs.</li> </ol>"},{"location":"channel-setup/webhook-setup/#1-what-is-the-webhook-channel","title":"1. What is the webhook channel?","text":"<p>A webhook channel is a generic one-way output channel:</p> <ul> <li>When your graph calls:</li> </ul> <pre><code>await context.channel(\"webhook:&lt;URL&gt;\").send_text(\"Run finished \u2705\")\n</code></pre> <ul> <li>AetherGraph\u2019s webhook adapter will <code>POST</code> a small JSON payload to <code>&lt;URL&gt;</code> with fields like:</li> </ul> <pre><code>{\n  \"type\": \"agent.message\",\n  \"channel\": \"webhook:&lt;URL&gt;\",\n  \"text\": \"Run finished \u2705\",\n  \"content\": \"Run finished \u2705\",\n  \"meta\": {},\n  \"timestamp\": \"...\"\n}\n</code></pre> <ul> <li>The external service (Slack, Discord, Zapier, etc.) receives that payload and does something with it.</li> </ul> <p>Key properties:</p> <ul> <li>Inform-only: no replies, no continuations, no <code>ask_*</code>.</li> <li>Outbound-only: AetherGraph only makes HTTP requests; no need to expose a public server for this.</li> <li>Very flexible: one adapter can talk to many tools, depending on the URL you plug in.</li> </ul> <p>Use webhooks when you want to:</p> <ul> <li>Get notifications in Slack/Discord when a run finishes.</li> <li>Pipe run updates into automation tools like Zapier.</li> <li>Trigger external workflows (email, Notion, issue tracking, etc.) without building a custom adapter.</li> </ul>"},{"location":"channel-setup/webhook-setup/#2-supported-platforms-tested-examples","title":"2. Supported platforms (tested examples)","text":"<p>The webhook channel is generic, but a few platforms have been tested and work well:</p>"},{"location":"channel-setup/webhook-setup/#21-slack-incoming-webhook","title":"2.1. Slack (Incoming Webhook)","text":"<p>Slack has a built-in Incoming Webhook feature:</p> <ol> <li>In Slack, go to Apps \u2192 Manage Apps \u2192 Search for \u201cIncoming Webhooks\u201d.</li> <li>Add the Incoming Webhooks app to your workspace.</li> <li>Create a new webhook for a specific channel.</li> <li>Slack will give you a URL like:</li> </ol> <pre><code>https://hooks.slack.com/services/XXX/YYY/ZZZ\n</code></pre> <p>You can use this URL directly in a <code>webhook:</code> channel.</p>"},{"location":"channel-setup/webhook-setup/#22-discord-webhook","title":"2.2. Discord (Webhook)","text":"<p>Discord servers support webhooks on a per-channel basis:</p> <ol> <li>In your Discord server, go to Server Settings \u2192 Integrations \u2192 Webhooks.</li> <li>Create a New Webhook, choose a target channel.</li> <li>Copy the Webhook URL, which looks like:</li> </ol> <pre><code>https://discord.com/api/webhooks/1234567890/ABCDEF...\n</code></pre> <p>You can use this URL directly in a <code>webhook:</code> channel.</p>"},{"location":"channel-setup/webhook-setup/#23-zapier-catch-hook","title":"2.3. Zapier (Catch Hook)","text":"<p>Zapier\u2019s Webhooks by Zapier \u2192 Catch Hook trigger works perfectly with AetherGraph\u2019s webhook payload.</p> <ol> <li> <p>In Zapier, create a new Zap.</p> </li> <li> <p>Set the Trigger to \u201cWebhooks by Zapier \u2192 Catch Hook\u201d.</p> </li> <li> <p>Zapier gives you a URL like:</p> </li> </ol> <pre><code>https://hooks.zapier.com/hooks/catch/123456/abcdef/\n</code></pre> <ol> <li> <p>Use this URL in a <code>webhook:</code> channel.</p> </li> <li> <p>In Zapier, map the incoming <code>text</code>/<code>content</code> fields to whatever action you like (Slack message, email, Notion page, etc.).</p> </li> </ol> <p>These three platforms cover a huge range of use cases. Anything that can accept an HTTP POST can likely be wired in the same way.</p>"},{"location":"channel-setup/webhook-setup/#3-how-to-use-the-webhook-channel","title":"3. How to use the webhook channel","text":"<p>The webhook channel uses a simple key format:</p> <pre><code>webhook:&lt;WEBHOOK_URL&gt;\n</code></pre> <p>Where <code>&lt;WEBHOOK_URL&gt;</code> is the full URL provided by Slack, Discord, Zapier, etc.</p>"},{"location":"channel-setup/webhook-setup/#31-example-discord-notification","title":"3.1. Example: Discord notification","text":"<p>Suppose Discord gives you this URL:</p> <pre><code>https://discord.com/api/webhooks/1437303232475959398/UFD16h-JMadbKfTbigsTYTRQ7F_rcaAe2-4ZIpVP8tNGhJgbhTm_peQRWU0V86qi39Yx\n</code></pre> <p>You can send messages to that channel with:</p> <pre><code>from aethergraph import graph_fn, NodeContext, run\n\nDISCORD_WEBHOOK_URL = \"https://discord.com/api/webhooks/1437303232475959398/UFD16h-JMadbKfTbigsTYTRQ7F_rcaAe2-4ZIpVP8tNGhJgbhTm_peQRWU0V86qi39Yx\"\n\n@graph_fn(name=\"webhook_discord_demo\")\nasync def webhook_discord_demo(*, context: NodeContext):\n    channel_key = f\"webhook:{DISCORD_WEBHOOK_URL}\"\n    chan = context.channel(channel_key)\n\n    await chan.send_text(\"AetherGraph run finished \ud83c\udf89\")\n    await chan.send_text(\"Model: demo-model-v1\\nAccuracy: 0.93\\nLoss: 0.12\")\n\n    return {\"notified\": True}\n</code></pre> <p>When this graph runs, the messages appear in your Discord channel via the webhook.</p>"},{"location":"channel-setup/webhook-setup/#32-example-zapier-catch-hook","title":"3.2. Example: Zapier Catch Hook","text":"<p>If Zapier gives you a URL like:</p> <pre><code>https://hooks.zapier.com/hooks/catch/123456/abcdef/\n</code></pre> <p>You can send events to it with:</p> <pre><code>ZAP_URL = \"https://hooks.zapier.com/hooks/catch/123456/abcdef/\"\n\n@graph_fn(name=\"webhook_zapier_demo\")\nasync def webhook_zapier_demo(*, context: NodeContext):\n    chan = context.channel(f\"webhook:{ZAP_URL}\")\n\n    await chan.send_text(\"AetherGraph just completed an experiment \ud83e\uddea\")\n    await chan.send_text(\"You can map this 'text' field in Zapier to Slack, email, or Notion.\")\n\n    return {\"notified\": True}\n</code></pre> <p>In Zapier, you can then:</p> <ul> <li>Use the Catch Hook trigger to receive the JSON payload.</li> <li>Map the <code>text</code> or <code>content</code> field to downstream actions (Slack, Gmail, Notion, etc.).</li> </ul>"},{"location":"channel-setup/webhook-setup/#33-example-slack-incoming-webhook","title":"3.3. Example: Slack Incoming Webhook","text":"<p>If Slack gives you a webhook URL like:</p> <pre><code>https://hooks.slack.com/services/XXX/YYY/ZZZ\n</code></pre> <p>You can send notifications with:</p> <pre><code>SLACK_WEBHOOK_URL = \"https://hooks.slack.com/services/XXX/YYY/ZZZ\"\n\n@graph_fn(name=\"webhook_slack_demo\")\nasync def webhook_slack_demo(*, context: NodeContext):\n    chan = context.channel(f\"webhook:{SLACK_WEBHOOK_URL}\")\n\n    await chan.send_text(\"AetherGraph run completed \u2705\")\n\n    return {\"notified\": True}\n</code></pre> <p>Slack receives the JSON payload and posts the message into the configured channel.</p> <p>You can also use this pattern with other tools that provide incoming webhooks, such as Microsoft Teams, Google Chat, Mattermost, Rocket.Chat, or Zulip. These aren\u2019t officially tested yet in AetherGraph, but as long as you have a webhook URL and the service accepts JSON via HTTP POST, you can usually connect it either directly via <code>webhook:&lt;URL&gt;</code> or by running the payload through an intermediate tool like Zapier/Make to adapt the JSON shape.</p>"},{"location":"channel-setup/webhook-setup/#4-notes-best-practices","title":"4. Notes &amp; best practices","text":"<ul> <li>The webhook channel is one-way: it does not support <code>ask_*</code> or replies.</li> <li>It is ideal for notifications, logging, and triggering external workflows.</li> <li>You can safely combine it with other channels (console, Slack adapter, Telegram, file) in the same graph.</li> <li> <p>Error handling is best-effort:</p> </li> <li> <p>If the target URL returns a 4xx/5xx or times out, the adapter logs a warning but does not fail your graph.</p> </li> </ul> <p>As long as you have a webhook URL from an external service, you can plug it into <code>webhook:&lt;URL&gt;</code> and let AetherGraph deliver your messages there.</p>"},{"location":"examples/1-chat-with-memory/","title":"Example: First Steps with Memory, Channel, LLM &amp; Artifacts","text":"<p>Who is this for? Folks who have never used AetherGraph (AG). We\u2019ll build a tiny chat agent that remembers what was said, responds using an LLM, and saves a session summary \u2014 all step\u2011by\u2011step.</p>"},{"location":"examples/1-chat-with-memory/#what-youll-build","title":"What you\u2019ll build","text":"<p>A stateful chat agent that:</p> <ol> <li>Stores each message as a <code>chat_turn</code> event in AG Memory.</li> <li>Reads prior events on startup to preload context.</li> <li>Chats with you via <code>context.channel()</code> (console/Slack/Web supported).</li> <li>Uses <code>context.llm()</code> to reply and later summarize the session.</li> <li>Saves the transcript + summary as an artifact (a file you can inspect).</li> </ol> <p>This mirrors real apps: assistants, experiment logs, ops runbooks, or optimization loops that need persistent context.</p>"},{"location":"examples/1-chat-with-memory/#prerequisites-2-minutes","title":"Prerequisites (2 minutes)","text":"<ul> <li>Python 3.10+</li> <li>Install AG (adjust to your package source):</li> </ul> <p><pre><code>pip install aethergraph\n</code></pre> * LLM key (e.g., OpenAI) in your environment or <code>.env</code>:</p> <p><pre><code>export OPENAI_API_KEY=sk-...\n</code></pre> * (Optional) Slack/Web UI: Not required for this tutorial; we use the console channel. You can switch to Slack/Web later without changing the agent code.</p>"},{"location":"examples/1-chat-with-memory/#glossary-ag-in-60-seconds","title":"Glossary (AG in 60 seconds)","text":"<ul> <li><code>@graph_fn</code>: A Python function that AG can schedule/run (think: a task node with helpful runtime wiring).</li> <li> <p><code>NodeContext</code>: Passed into your <code>@graph_fn</code>; gives you services:</p> </li> <li> <p><code>context.memory()</code> \u2013 record &amp; query events (structured log).</p> </li> <li><code>context.channel()</code> \u2013 input/output with the user (console/Slack/Web).</li> <li><code>context.llm()</code> \u2013 talk to an LLM provider using a unified API.</li> <li><code>context.artifacts()</code> \u2013 save/load files, JSON, blobs.</li> <li>Events (Memory): Append\u2011only records with fields like <code>kind</code>, <code>text</code>, <code>metrics</code>, <code>tags</code>, <code>stage</code>, <code>severity</code>. You choose the schema; AG stores and fetches them for you.</li> </ul> <p>Key idea: keep your logic in plain Python; treat services (memory, channel, llm, artifacts) as pluggable I/O.</p>"},{"location":"examples/1-chat-with-memory/#step-0-minimal-run-harness","title":"Step 0 \u2014 Minimal run harness","text":"<p>We\u2019ll run two functions in one process: one to seed memory, one to chat.</p> <pre><code># run_harness.py\nif __name__ == \"__main__\":\n    import asyncio\n    from aethergraph.runner import run_async\n    from aethergraph import start_server\n\n    # Start the sidecar: enables interactive I/O and resumable waits\n    url = start_server(port=8000, log_level=\"warning\")\n    print(\"Sidecar:\", url)\n\n    async def main():\n        # Same run_id so they share the same memory namespace\n        await run_async(seed_chat_memory_demo, inputs={}, run_id=\"demo_chat_with_memory\")\n        result = await run_async(chat_agent_with_memory, inputs={}, run_id=\"demo_chat_with_memory\")\n        print(\"Result:\", result)\n\n    asyncio.run(main())\n</code></pre> <p>Why a sidecar? For console/Slack/Web prompts (<code>ask_*</code>) AG uses event\u2011driven waits that the sidecar hosts. Pure compute graphs can run without it.</p>"},{"location":"examples/1-chat-with-memory/#step-1-seed-memory-why-how","title":"Step 1 \u2014 Seed memory (why &amp; how)","text":"<p>Why: Many real agents need past context on first run (previous chats, last experiment settings, etc.). We\u2019ll preload two <code>chat_turn</code> events so the agent \u201cremembers\u201d something before you type.</p> <pre><code># seed.py\nfrom aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"seed_chat_memory_demo\")\nasync def seed_chat_memory_demo(*, context: NodeContext):\n    mem = context.memory()\n    logger = context.logger()\n\n    # Each record() call writes an Event. `data` is JSON\u2011encoded into Event.text.\n    await mem.record(\n        kind=\"chat_turn\",\n        data={\"role\": \"user\", \"text\": \"We talked about integrating AetherGraph into my project.\"},\n        tags=[\"chat\", \"user\", \"seed\"], severity=2, stage=\"observe\",\n    )\n    await mem.record(\n        kind=\"chat_turn\",\n        data={\"role\": \"assistant\", \"text\": \"Start with a simple graph_fn and add services later.\"},\n        tags=[\"chat\", \"assistant\", \"seed\"], severity=2, stage=\"act\",\n    )\n\n    logger.info(\"Seeded two chat turns.\")\n    return {\"seeded\": True}\n</code></pre> <p>Design tip: Use <code>kind</code> consistently (<code>\"chat_turn\"</code>) so you can query exactly what you need later. <code>tags</code>, <code>stage</code>, and <code>severity</code> help with reporting and filtering.</p>"},{"location":"examples/1-chat-with-memory/#step-2-load-prior-events-make-memory-useful","title":"Step 2 \u2014 Load prior events (make memory useful)","text":"<p>Goal: On startup, fetch recent <code>chat_turn</code> events, decode them, and prime the chat history.</p> <pre><code># load_history.py (excerpt inside the agent)\nprevious_turns = await mem.recent_data(kinds=[\"chat_turn\"], limit=50)\nconversation = []\nfor d in previous_turns:\n    if isinstance(d, dict) and d.get(\"role\") in (\"user\", \"assistant\") and d.get(\"text\"):\n        conversation.append({\"role\": d[\"role\"], \"text\": d[\"text\"]})\n\nif conversation:\n    await chan.send_text(f\"\ud83e\udde0 I loaded {len(conversation)} prior turns. I\u2019ll use them as context.\")\nelse:\n    await chan.send_text(\"\ud83d\udc4b New session. I\u2019ll remember as we go.\")\n</code></pre> <p>Why not read raw events? <code>recent_data()</code> returns the decoded JSON payloads you originally wrote via <code>data=...</code>. It\u2019s the fastest way to get back to your domain objects.</p>"},{"location":"examples/1-chat-with-memory/#step-3-talk-to-the-user-channel-101","title":"Step 3 \u2014 Talk to the user (Channel 101)","text":"<p>Goal: Use <code>channel.ask_text()</code> to get input and <code>channel.send_text()</code> to reply. This works the same in console, Slack, or a web adapter.</p> <pre><code># channel_loop.py (excerpt inside the agent)\nwhile True:\n    user = await chan.ask_text(\"You:\")\n    if not user:\n        continue\n    if user.strip().lower() in (\"quit\", \"exit\"):\n        await chan.send_text(\"\ud83d\udc4b Ending. Let me summarize...\")\n        break\n\n    # Store the user turn in memory *and* in our local transcript\n    conversation.append({\"role\": \"user\", \"text\": user})\n    await mem.record(kind=\"chat_turn\", data={\"role\": \"user\", \"text\": user},\n                     tags=[\"chat\", \"user\"], severity=2, stage=\"observe\")\n</code></pre> <p>Why Channel? It abstracts the transport. Your agent code stays the same whether you test locally or ship to Slack/Web.</p>"},{"location":"examples/1-chat-with-memory/#step-4-call-the-llm-compact-history","title":"Step 4 \u2014 Call the LLM (compact history)","text":"<p>Goal: Build a small window from the transcript (e.g., last 10 turns) and call <code>llm.chat()</code>.</p> <pre><code># llm_reply.py (excerpt inside the agent)\nhistory_tail = conversation[-10:]\nmessages = ([{\"role\": \"system\", \"content\": \"You are a helpful, concise assistant.\"}] +\n            [{\"role\": t[\"role\"], \"content\": t[\"text\"]} for t in history_tail])\n\nreply, _usage = await llm.chat(messages=messages)\nconversation.append({\"role\": \"assistant\", \"text\": reply})\nawait mem.record(kind=\"chat_turn\", data={\"role\": \"assistant\", \"text\": reply},\n                 tags=[\"chat\", \"assistant\"], severity=2, stage=\"act\")\nawait chan.send_text(reply)\n</code></pre> <p>Why only last 10 turns? Keep prompts cheap. Memory retains all history; your prompt includes a smart slice.</p>"},{"location":"examples/1-chat-with-memory/#step-5-summarize-save-artifacts","title":"Step 5 \u2014 Summarize &amp; Save (Artifacts)","text":"<p>Goal: Generate a session summary, then persist both transcript and summary for later inspection.</p> <pre><code># summarize_and_save.py (excerpt inside the agent)\nhist_text = \"\n\".join(f\"{t['role']}: {t['text']}\" for t in conversation[-20:])\nsummary_text, _ = await llm.chat(messages=[\n    {\"role\": \"system\", \"content\": \"You write clear, concise summaries.\"},\n    {\"role\": \"user\", \"content\": \"Summarize the conversation, focusing on main topics and TODOs.\n\n\" + hist_text},\n])\nawait chan.send_text(\"\ud83d\udccc Session summary:\n\" + summary_text)\n\npayload = {\"conversation\": conversation, \"summary\": summary_text}\ntry:\n    saved = await artifacts.save_json(payload, suggested_uri=\"./chat_session_with_memory.json\")\nexcept Exception:\n    saved = None\n</code></pre> <p>Artifacts act like a project filesystem managed by AG. Save JSON, images, binaries \u2014 and load them from other runs.</p>"},{"location":"examples/1-chat-with-memory/#step-6-put-it-together-the-agent","title":"Step 6 \u2014 Put it together: the agent","text":"<p>Below is the full <code>@graph_fn</code> combining Steps 2\u20135. (Utility imports omitted for brevity.)</p> <pre><code>from typing import Any, Dict, List\nfrom aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"chat_agent_with_memory\")\nasync def chat_agent_with_memory(*, context: NodeContext):\n    logger = context.logger()\n    chan = context.channel()\n    mem = context.memory()\n    artifacts = context.artifacts()\n    llm = context.llm()\n\n    conversation: List[Dict[str, Any]] = []\n\n    # Load prior history\n    try:\n        previous_turns = await mem.recent_data(kinds=[\"chat_turn\"], limit=50)\n    except Exception:\n        previous_turns = []\n    for d in previous_turns:\n        if isinstance(d, dict) and d.get(\"role\") in (\"user\", \"assistant\") and d.get(\"text\"):\n            conversation.append({\"role\": d[\"role\"], \"text\": d[\"text\"]})\n\n    await chan.send_text(\n        f\"\ud83e\udde0 I loaded {len(conversation)} prior turns. Type 'quit' to end.\"\n        if conversation else \"\ud83d\udc4b New session. Type 'quit' to end.\"\n    )\n\n    # Chat loop\n    while True:\n        user = await chan.ask_text(\"You:\")\n        if not user:\n            continue\n        if user.strip().lower() in (\"quit\", \"exit\"):\n            await chan.send_text(\"\ud83d\udc4b Ending. Let me summarize...\")\n            break\n\n        conversation.append({\"role\": \"user\", \"text\": user})\n        await mem.record(kind=\"chat_turn\", data={\"role\": \"user\", \"text\": user},\n                         tags=[\"chat\", \"user\"], severity=2, stage=\"observe\")\n\n        history_tail = conversation[-10:]\n        messages = ([{\"role\": \"system\", \"content\": \"You are a helpful, concise assistant.\"}] +\n                    [{\"role\": t[\"role\"], \"content\": t[\"text\"]} for t in history_tail])\n        reply, _ = await llm.chat(messages=messages)\n\n        conversation.append({\"role\": \"assistant\", \"text\": reply})\n        await mem.record(kind=\"chat_turn\", data={\"role\": \"assistant\", \"text\": reply},\n                         tags=[\"chat\", \"assistant\"], severity=2, stage=\"act\")\n        await chan.send_text(reply)\n\n    # Summarize &amp; save\n    hist_text = \"\n\".join(f\"{t['role']}: {t['text']}\" for t in conversation[-20:])\n    summary_text, _ = await llm.chat(messages=[\n        {\"role\": \"system\", \"content\": \"You write clear, concise summaries.\"},\n        {\"role\": \"user\", \"content\": \"Summarize the conversation with decisions/TODOs.\n\n\" + hist_text},\n    ])\n    await chan.send_text(\"\ud83d\udccc Session summary:\n\" + summary_text)\n\n    try:\n        await artifacts.save_json({\"conversation\": conversation, \"summary\": summary_text},\n                                  suggested_uri=\"./chat_session_with_memory.json\")\n    except Exception:\n        pass\n\n    return {\"turns\": len(conversation), \"summary\": summary_text}\n</code></pre>"},{"location":"examples/1-chat-with-memory/#step-7-run-it","title":"Step 7 \u2014 Run it","text":"<p>From your shell:</p> <pre><code>python run_harness.py\n</code></pre> <p>You\u2019ll see the sidecar URL and then a prompt:</p> <pre><code>You: hello\n... assistant replies ...\nYou: quit\n</code></pre> <p>A <code>chat_session_with_memory.json</code> artifact will be saved. Rerun \u2014 the agent will preload what was said last time.</p>"},{"location":"examples/1-chat-with-memory/#variations-next-steps","title":"Variations &amp; next steps","text":"<ul> <li>Filter by time/kind/tags: <code>recent_data(kinds=[...], limit=..., since=...)</code>.</li> <li>Track metrics: add <code>metrics={...}</code> to <code>record()</code> and chart them later.</li> <li>Multiple channels: same agent works with Slack/Web by switching adapters.</li> <li>Long\u2011term summaries: store occasional <code>kind=\"session_summary\"</code> events.</li> <li>Privacy/retention: implement deletion or redaction policies per <code>tags</code> or <code>stage</code>.</li> </ul>"},{"location":"examples/1-chat-with-memory/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>No replies? Check your LLM key and provider quota.</li> <li>Ask/answer stuck? Ensure the sidecar started (see console log for URL).</li> <li>Artifact not saved? Verify write permissions in the working directory.</li> </ul>"},{"location":"examples/1-chat-with-memory/#summary-full-example-link","title":"Summary &amp; Full Example Link","text":"<p>You built a stateful chat agent without any AG prior knowledge: you logged events, recalled them, chatted via Channel, used LLM to reply/summarize, and persisted results with Artifacts.</p> <p>Full example in repo: TBD (link placeholder)</p>"},{"location":"examples/channel/","title":"Channels \u2013 Practical Usage Cheatsheet","text":"<p>This page is a lightweight guide, not a full API reference. It shows how to use channels in everyday graphs and what the most common operations look like.</p>"},{"location":"examples/channel/#1-getting-a-channel","title":"1. Getting a channel","text":"<p>In any graph, you start from the <code>NodeContext</code>:</p> <pre><code>chan = context.channel()                # default channel (often console or a default chat)\nchan_slack = context.channel(\"slack:team/T:chan/C\")\nchan_file  = context.channel(\"file:runs/demo.log\")\n</code></pre> <p>Use cases:</p> <ul> <li>No argument \u2192 send to whatever default channel you configured.</li> <li>Explicit key \u2192 target a specific channel (Slack, Telegram, file, webhook, etc.).</li> </ul> <p>You then call convenient methods on <code>chan</code>.</p>"},{"location":"examples/channel/#2-sending-messages-send_text","title":"2. Sending messages (<code>send_text</code>)","text":"<pre><code>await chan.send_text(\"Run started \ud83d\ude80\")\n</code></pre> <p>When to use:</p> <ul> <li> <p>Any time you want to tell the user something:</p> </li> <li> <p>status updates (\"loading data\u2026\"),</p> </li> <li>final results (\"accuracy = 0.93\"),</li> <li>errors, tips, or links.</li> </ul> <p>Where it goes depends on the channel:</p> <ul> <li>console \u2192 prints to the terminal.</li> <li>Slack / Telegram \u2192 sends a chat message.</li> <li>file \u2192 appends a line to a log file.</li> <li>webhook \u2192 POSTs JSON to the external URL.</li> </ul> <p>You usually don\u2019t need to worry about the return value; it\u2019s handled internally.</p>"},{"location":"examples/channel/#3-asking-for-input-ask_text","title":"3. Asking for input (<code>ask_text</code>)","text":"<pre><code>name = await context.channel().ask_text(\"What is your name?\")\nawait context.channel().send_text(f\"Nice to meet you, {name}!\")\n</code></pre> <p>When to use:</p> <ul> <li> <p>You need free-form input from a human:</p> </li> <li> <p>names, descriptions, small pieces of text,</p> </li> <li>short commands (\"yes/no\", \"option A\", etc.).</li> </ul> <p>Supported channels:</p> <ul> <li>console \u2192 reads from stdin inline.</li> <li>Slack / Telegram \u2192 sends a prompt and waits for a reply.</li> </ul> <p>Not supported / not meaningful on:</p> <ul> <li>file and webhook (those are inform-only channels).</li> </ul> <p>Behind the scenes, Slack/Telegram use continuations, so the run can be resumed when a reply arrives.</p>"},{"location":"examples/channel/#4-approvals-choices-ask_approval","title":"4. Approvals &amp; choices (<code>ask_approval</code>)","text":"<pre><code>res = await context.channel().ask_approval(\n    \"Deploy the model to production?\",\n    options=[\"Approve\", \"Reject\"],\n)\n\nif res[\"approved\"]:\n    await context.channel().send_text(\"Deployment approved \u2705\")\nelse:\n    await context.channel().send_text(\"Deployment cancelled \u274c\")\n</code></pre> <p>When to use:</p> <ul> <li> <p>You want a simple decision from the user:</p> </li> <li> <p>approve/reject,</p> </li> <li>pick from a short list of options.</li> </ul> <p>Channel behavior:</p> <ul> <li>console \u2192 shows numbered options and waits for a number/label.</li> <li>Slack / Telegram \u2192 renders real buttons; user clicks, continuation resumes.</li> </ul> <p>Again, this is not meant for file/webhook channels.</p>"},{"location":"examples/channel/#5-files-uploads-high-level","title":"5. Files &amp; uploads (high level)","text":"<p>For interactive channels (Slack, Telegram, console) and some tools, you\u2019ll often work with files directly, not just links.</p> <p>Typical high-level patterns:</p>"},{"location":"examples/channel/#51-sending-a-file-send_file","title":"5.1. Sending a file (<code>send_file</code>)","text":"<pre><code># e.g. you just generated a local report\nreport_path = \"./outputs/report.pdf\"\n\nchan = context.channel()  # default chat (Slack/Telegram/console)\nawait chan.send_file(report_path, caption=\"Here is your report \ud83d\udcce\")\n</code></pre> <p>When to use:</p> <ul> <li>You want the user to receive the actual file in their chat or UI.</li> <li>Slack/Telegram will show the file as an attachment; console/file/webhook channels may log or reference it instead, depending on implementation.</li> </ul>"},{"location":"examples/channel/#52-asking-the-user-to-upload-a-file-ask_file","title":"5.2. Asking the user to upload a file (<code>ask_file</code>)","text":"<pre><code>files = await context.channel().ask_file(\"Please upload a CSV file with your data.\")\n\n# `files` is typically a list of file references with\n# fields like name, uri, mimetype, etc.\nfor f in files:\n    await context.channel().send_text(f\"Got file: {f['name']}\")\n</code></pre> <p>When to use:</p> <ul> <li>You need the user to provide input as a file (datasets, configs, documents).</li> <li>Works best on Slack/Telegram or a web UI that supports uploads.</li> </ul>"},{"location":"examples/channel/#53-sending-buttons-for-links","title":"5.3. Sending buttons for links","text":"<p>Buttons are a convenient way to surface links (e.g. to artifacts, dashboards) instead of pasting raw URLs.</p> <p>Conceptually, you can build a message with one or more buttons that open URLs:</p> <pre><code>from aethergraph.contracts.services.channel import Button\n\nreport_url = \"https://example.com/artifacts/runs/123/report.pdf\"\n\nbuttons = {\n    \"open_report\": Button(label=\"Open report\", url=report_url),\n}\n\nchan = context.channel()\nawait chan.send_buttons(\"Your report is ready:\", buttons=buttons)\n</code></pre> <p>On rich channels (Slack/Telegram/web UI) this can render as clickable buttons; on simpler channels, it may fall back to plain text.</p> <p>Use file and button helpers when you want the user to act on artifacts directly (download, open, inspect) rather than just reading text.</p>"},{"location":"examples/channel/#6-streaming-progress-high-level","title":"6. Streaming &amp; progress (high level)","text":"<p>Some adapters (Slack, Telegram, web UI) support streaming and progress updates, so the user sees things evolve in place instead of a single final message.</p> <p>A common pattern is to use a progress helper that periodically updates a status message while your work runs:</p> <pre><code>@graph_fn(name=\"train_with_progress\")\nasync def train_with_progress(*, context: NodeContext):\n    await context.channel().send_text(\"Training started \u23f3\")\n\n    total_steps = 5\n    for step in range(1, total_steps + 1):\n        # Do some work here...\n        await context.channel().send_text(f\"Progress: {step}/{total_steps}\")\n\n    await context.channel().send_text(\"Training finished \u2705\")\n</code></pre> <p>On rich channels (Slack/Telegram/web UI), the framework can render more advanced streaming or progress UIs using specialized helpers; the core idea is the same: send updates frequently so the user sees the task moving forward.</p>"},{"location":"examples/channel/#7-putting-it-together-a-small-example","title":"7. Putting it together \u2013 a small example","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"simple_run\")\nasync def simple_run(*, context: NodeContext):\n    # 1) Notify in the default channel\n    await context.channel().send_text(\"Run started \ud83d\ude80\")\n\n    # 2) Ask the user a question (console/Slack/Telegram)\n    name = await context.channel().ask_text(\"What should we name this experiment?\")\n\n    # 3) Log the name to a file channel\n    await context.channel(\"file:runs/experiment_names.log\").send_text(name)\n\n    # 4) Optionally notify an external system via webhook\n    # await context.channel(\"webhook:https://hooks.zapier.com/hooks/catch/.../\").send_text(\n    #     f\"New experiment: {name}\"\n    # )\n\n    await context.channel().send_text(f\"All set, {name} \u2705\")\n    return {\"experiment_name\": name}\n</code></pre> <p>Use this cheatsheet as a mental model for channels:</p> <ul> <li><code>channel()</code> \u2192 pick where messages go.</li> <li><code>send_text</code> \u2192 tell the user or an external system something.</li> <li><code>ask_*</code> \u2192 only on interactive channels (console/Slack/Telegram), when you need input.</li> <li>Let higher-level tooling take care of files and streaming; the channel adapters handle the transport details.</li> </ul>"},{"location":"examples/example-list/","title":"Examples of Aethergraph Usage","text":"<p>This page is your guided tour to AetherGraph (AG) through runnable, copy\u2011pasteable examples. Start with the Hero Demos to experience a complete flow in minutes (console + LLM only). Then explore two complementary pattern pools:</p> <ul> <li>Method Showcase \u2014 small, focused snippets for individual <code>context.*</code> capabilities and services.</li> <li>Concrete Example Patterns \u2014 larger recipes that combine multiple features for real\u2011world tasks.</li> </ul> <p>This catalog evolves over time. We periodically add new demos, refine existing ones, and may adjust paths as the repo grows. The tables below always reflect the current recommended entry points.</p> <p>All the examples are in a separate example repo. Please clone the repo and set up <code>.env</code> to run the examples.</p>"},{"location":"examples/example-list/#hero-demos","title":"Hero Demos","text":"<p>Short, polished demos that tell a clear story. Each fits in a single file and returns observable outputs (messages, artifacts, or both).</p> # Demo What you\u2019ll see (story) Path 1 Chat Agent with Memory Seeds or loads chat memory \u2192 you chat \u2192 it summarizes the whole session and saves transcript + summary as artifacts. <code>demo_examples/1_chat_with_memory.py</code> 2 Channel Wizard A one\u2011function wizard that collects config using <code>ask_*</code>, shows a recap, lets you Confirm/Restart/Cancel, then saves <code>run_config.json</code>. <code>demo_examples/2_channel_wizard_interactive_workflow.py</code> 3 Optimization Loop A tiny gradient\u2011descent run that logs step metrics to Memory, writes periodic checkpoints as artifacts, and returns final params. <code>demo_examples/3_optimization_loop_with_artifacts.py</code> 4 Extend Prompt Services Agents use <code>context.prompt_store()</code> to fetch prompts and <code>context.llm_observer()</code> to log calls\u2014clean agent code, centralized control. <code>servdemo_examplesices/4_external_service_prompt_store.py</code> 5 Simple Copilot (Tool Router) A console copilot routes each query to a calculator, a summarizer, or a direct answer using a tiny LLM classifier; replies inline. <code>demo_examples/5_simple_copilot_tool_using_router.py</code> 6 Resume a Static Graph (graphify) A static graph with a slow, checkpointed node crashes mid\u2011run; re\u2011running with the same <code>run_id</code> resumes from the checkpoint and completes. <code>demo_examples/6_crash_resume_static_graph.py</code>"},{"location":"examples/example-list/#method-showcase-patterns-by-feature","title":"Method Showcase \u2014 Patterns by Feature","text":"<p>Concise examples organized by capability. Use these as copy\u2011paste starting points.</p>"},{"location":"examples/example-list/#a-interaction-channel","title":"A) Interaction (Channel)","text":"<ul> <li>A.1 <code>send_text</code> \u2014 Minimal \"hello\" send.   Path: <code>method_showcase/1_channels/1_channel_send_text.py</code></li> <li>A.2 <code>ask_text</code> \u2014 Prompt \u2192 await user reply \u2192 echo.   Path: <code>method_showcase/1_channels/2_channel_ask_text.py</code></li> <li>A.3 <code>ask_approval</code> \u2014 Present options, branch on choice.   Path: <code>method_showcase/1_channels/3_channel_ask_approval.py</code></li> <li>A.4 Channel setup \u2014 Configure Slack / Telegram / Webhook adapters and key conventions.   Path: <code>method_showcase/1_channels/4_channel_setup.py</code></li> <li>A.5 Method walk\u2011through \u2014 Linear tour of all channel methods using a small \u201cportfolio\u201d demo.   Path: <code>method_showcase/1_channels/5_channel_method_walkthrough.py</code></li> <li>A.6 File channel \u2014 Ask for files, read them, return attachments/links.   Path: <code>method_showcase/1_channels/6_file_channel_example.py</code></li> </ul>"},{"location":"examples/example-list/#b-memory-artifacts","title":"B) Memory &amp; Artifacts","text":"<ul> <li>B.1 Memory \u2014 record &amp; query \u2014 Append events and fetch recent history.   Path: <code>method_showcase/2_artifacts_memory/1_memory_record.py</code></li> <li>B.2 Memory \u2014 typed results \u2014 Store structured tool results for fast retrieval.   Path: <code>method_showcase/2_artifacts_memory/2_memory_write_result.py</code></li> <li>B.3 Artifacts \u2014 save text/JSON \u2014 Persist text/JSON and auto\u2011index.   Path: <code>method_showcase/2_artifacts_memory/3_artifacts_save_txt_json.py</code></li> <li>B.4 Artifacts \u2014 save &amp; search files \u2014 Save arbitrary files and rank/search across agents.   Path: <code>method_showcase/2_artifacts_memory/4_artifacts_save_search_files.py</code></li> <li>B.5 Memory \u2192 RAG \u2014 Promote memory events into a vector index and answer with citations.   Path: <code>method_showcase/2_artifacts_memory/5_memory_rag.py</code></li> </ul>"},{"location":"examples/example-list/#c-logger-kv","title":"C) Logger &amp; KV","text":"<ul> <li>C.1 Logger \u2014 Structured logs, levels, and tracing.   Path: <code>method_showcase/3_logger_kv/1_logger_usage.py</code></li> <li>C.2 KV \u2014 Store and fetch run\u2011level globals.   Path: <code>method_showcase/3_logger_kv/2_kv_usage.py</code></li> </ul>"},{"location":"examples/example-list/#d-llm","title":"D) LLM","text":"<ul> <li>D.1 Chat \u2014 One\u2011shot and multi\u2011turn <code>context.llm().chat(...)</code>.   Path: <code>method_showcase/4_llm/1_llm_chat.py</code></li> <li>D.2 Multiple profiles \u2014 Configure and use multiple LLM clients.   Path: <code>method_showcase/4_llm/2_setup_multiple_llm_profiles.py</code></li> <li>D.3 Inline profile \u2014 Set keys/models at runtime (no preregistration).   Path: <code>method_showcase/4_llm/3_inline_llm_setup.py</code></li> <li>D.4 Raw API \u2014 Use <code>.raw()</code> to pass advanced payloads directly.   Path: <code>method_showcase/4_llm/4_passing_raw_api.py</code></li> </ul>"},{"location":"examples/example-list/#e-rag","title":"E) RAG","text":"<ul> <li>E.1 Ingest files \u2014 Upsert docs into a vector index.   Path: <code>method_showcase/5_rag/ingest_files.py</code></li> <li>E.2 Inspect corpora \u2014 List/inspect existing corpora.   Path: <code>method_showcase/5_rag/list_inspect_corpora.py</code></li> <li>E.3 Search &amp; answer \u2014 Retrieve \u2192 synthesize with citations.   Path: <code>method_showcase/5_rag/search_retrieve_answer.py</code></li> </ul>"},{"location":"examples/example-list/#f-extending-services","title":"F) Extending Services","text":"<ul> <li>F.1 Materials DB \u2014 Register a materials property service for quick lookups.   Path: <code>method_showcase/6_extending_services/1_material_db.py</code></li> <li>F.2 HuggingFace model \u2014 Expose an external model as a service.   Path: <code>method_showcase/6_extending_services/2_huggingface_model.py</code></li> <li>F.3 Rate limiting \u2014 Token\u2011bucket wrapper with retries/backoff.   Path: <code>method_showcase/6_extending_services/3_rate_limit.py</code></li> <li>F.4 Access NodeContext in a service \u2014 Patterns for using <code>context</code> safely inside services.   Path: <code>method_showcase/6_extending_services/4_access_ctx_in_service.py</code></li> <li>F.5 Critical sections / mutex \u2014 Design a thread\u2011safe service API.   Path: <code>method_showcase/6_extending_services/5_critical_mutex_usage.py</code></li> </ul>"},{"location":"examples/example-list/#g-concurrency","title":"G) Concurrency","text":"<ul> <li>G.1 <code>graphify</code> map\u2011reduce \u2014 Fan\u2011out + fan\u2011in with static graphs.   Path: <code>method_showcase/7_concurrency/graphify_map_reduce.py</code></li> <li>G.2 <code>graph_fn</code> concurrency \u2014 Launch concurrent tasks with a concurrency cap.   Path: <code>method_showcase/7_concurrency/graph_fn_concurrency.py</code></li> </ul>"},{"location":"examples/example-list/#concrete-example-patterns-advanced-recipes","title":"Concrete Example Patterns \u2014 Advanced Recipes","text":"<p>Bigger compositions that mirror real\u2011world tasks.</p>"},{"location":"examples/example-list/#a-state-resumption","title":"A) State &amp; Resumption","text":"<ul> <li>A.1 Crash &amp; Resume (static graph) \u2014 Design a static graph so a long node can checkpoint and resume indefinitely using the same <code>run_id</code>.   Path: <code>pattern_examples/1_state_resumption/1_resume_external_waits.py</code> Run: <code>python -m examples.resumption.crash_and_resume</code></li> <li>A.2 Long Job Monitor \u2014 Submit to <code>job_manager</code>, poll with backoff, surface failures via channel, let the user Retry/Abort.   Path: <code>pattern_examples/1_state_resumption/2_long_job_monitor.py</code></li> </ul>"},{"location":"examples/example-list/#b-agent-patterns","title":"B) Agent Patterns","text":"<ul> <li>B.1 Chain\u2011of\u2011Thought Agent \u2014 Two\u2011stage flow: CoT reasoning trace \u2192 final concise answer (optionally store traces).   Path: <code>pattern_examples/2_agent_patterns/1_chain_of_thought.py</code></li> <li>B.2 ReAct Agent \u2014 Thought \u2192 Action (tool) \u2192 Observation loop until \u201cFinish\u201d, with a compact history state.   Path: <code>pattern_examples/agents/2_simple_react.py</code></li> <li>B.3 RL Policy as <code>graph_fn</code> \u2014 Treat a graph as a policy: observation in, action out; log trajectories via Memory/Artifacts.   Path: <code>pattern_examples/agents/3_reinforcement_learnining_policy.py</code></li> </ul>"},{"location":"examples/example-list/#c-applied-endtoend","title":"C) Applied End\u2011to\u2011End","text":"<ul> <li>C.1 CSV Analyzer (interactive) \u2014 Ask for a CSV, summarize shape (rows/cols), headers, and simple stats; return findings and artifacts.   Path: <code>pattern_examples/3_e2e_patterns/1_csv_analyzer.py</code></li> <li>C.2 Paper \u2192 Implementation Sketch (interactive) \u2014 Ask for a text/PDF, sketch a Python implementation, run sandboxed, return logs/files as artifacts.   Path: <code>pattern_examples/3_e2e_patterns/2_paper_implementation_sketch.py</code></li> <li>C.3 Deep Research Agent \u2014 Use <code>graphify</code> concurrency to parallelize retrieval/summarization and synthesize findings.   Path: <code>pattern_examples/3_e2e_patterns/3_deep_research_agent.py</code></li> </ul>"},{"location":"examples/llm/","title":"<code>context.llm()</code> Mini Tutorial","text":"<p>This is a quick guide to the <code>NodeContext.llm()</code> helper in Aethergraph: how it\u2019s wired, how to use profiles, how to configure providers in <code>.env</code>, and how embeddings + RAG fit in.</p>"},{"location":"examples/llm/#1-what-contextllm-is-and-supported-providers","title":"1. What <code>context.llm()</code> is (and supported providers)","text":"<p><code>context.llm()</code> is a convenience accessor that gives you a ready-to-use LLM client for the current run.</p> <p>Under the hood it returns a <code>GenericLLMClient</code>, which implements:</p> <ul> <li><code>await client.chat(messages, **kw)</code>  \u2013 text (and multimodal) chat</li> <li><code>await client.embed(texts, **kw)</code>    \u2013 embeddings</li> </ul> <p>The client is created from config (<code>LLMSettings</code>) and currently supports these providers:</p> <ul> <li><code>openai</code>    \u2013 OpenAI-compatible API (GPT\u20114o, GPT\u20115, etc.)</li> <li><code>anthropic</code> \u2013 Claude 3.x models via <code>/v1/messages</code></li> <li><code>google</code>    \u2013 Gemini models via <code>generateContent</code> / <code>embedContent</code></li> <li><code>lmstudio</code>  \u2013 LM Studio\u2019s OpenAI-compatible local server</li> <li><code>ollama</code>    \u2013 Ollama\u2019s OpenAI-compatible local server</li> </ul> <p>(Plus <code>azure</code> and <code>openrouter</code> if you configure them.)</p> <p>Important: This layer is meant as a lightweight helper. It is not exhaustively tested across every model and provider variant. If you hit an edge case or a model with a quirky API, you can:</p> <ul> <li>Call the HTTP APIs yourself with plain Python, or</li> <li>Register your own extended service and bypass <code>context.llm()</code> for that use case.</li> </ul>"},{"location":"examples/llm/#2-what-is-a-profile","title":"2. What is a \u201cprofile\u201d?","text":"<p>A profile is a named LLM configuration: provider + model + optional base URL, timeout, and secrets.</p> <p>Structurally (from config):</p> <pre><code>class LLMProfile(BaseModel):\n    provider: Provider = \"openai\"      # e.g. \"openai\", \"anthropic\", \"google\", \"lmstudio\", \"ollama\"\n    model: str = \"gpt-4o-mini\"         # chat / reasoning model\n    embed_model: str | None = None      # optional embedding model\n    base_url: str | None = None\n    timeout: float = 60.0\n    azure_deployment: str | None = None\n    api_key: SecretStr | None = None\n    api_key_ref: str | None = None\n\nclass LLMSettings(BaseModel):\n    enabled: bool = True\n    default: LLMProfile = LLMProfile()\n    profiles: Dict[str, LLMProfile] = Field(default_factory=dict)\n</code></pre> <p>At runtime, these become <code>GenericLLMClient</code> instances keyed by profile name:</p> <ul> <li><code>\"default\"</code> \u2013 always present</li> <li><code>\"gemini\"</code>, <code>\"anthropic\"</code>, <code>\"local\"</code>, etc. \u2013 optional extra profiles</li> </ul>"},{"location":"examples/llm/#3-using-contextllm-inside-a-node","title":"3. Using <code>context.llm()</code> inside a node","text":""},{"location":"examples/llm/#31-basic-usage-default-profile","title":"3.1 Basic usage (default profile)","text":"<pre><code>async def hello_world(*, context: NodeContext, input_text: str):\n    llm = context.llm()  # same as context.llm(\"default\")\n\n    text, usage = await llm.chat([\n        {\"role\": \"system\", \"content\": \"Be brief.\"},\n        {\"role\": \"user\", \"content\": f\"Say hi back to: {input_text}\"},\n    ])\n\n    return {\"reply\": text, \"usage\": usage}\n</code></pre>"},{"location":"examples/llm/#32-using-a-named-profile","title":"3.2 Using a named profile","text":"<pre><code>async def multi_vendor_demo(*, context: NodeContext, query: str):\n    openai_client   = context.llm(\"default\")   # e.g. OpenAI\n    gemini_client   = context.llm(\"gemini\")    # Google Gemini profile\n    anthropic_client = context.llm(\"anthropic\") # Claude profile\n\n    o_text, _ = await openai_client.chat([\n        {\"role\": \"user\", \"content\": f\"OpenAI: {query}\"},\n    ])\n\n    g_text, _ = await gemini_client.chat([\n        {\"role\": \"user\", \"content\": f\"Gemini: {query}\"},\n    ])\n\n    a_text, _ = await anthropic_client.chat([\n        {\"role\": \"user\", \"content\": f\"Claude: {query}\"},\n    ])\n\n    return {\"openai\": o_text, \"gemini\": g_text, \"anthropic\": a_text}\n</code></pre>"},{"location":"examples/llm/#33-embeddings-via-embed","title":"3.3 Embeddings via <code>embed()</code>","text":"<pre><code>async def embed_example(*, context: NodeContext, texts: list[str]):\n    # Use default profile\u2019s embedding model (see .env config section below)\n    client = context.llm()\n\n    vectors = await client.embed(texts)\n    # vectors: List[List[float]]\n\n    return {\"embeddings\": vectors}\n</code></pre>"},{"location":"examples/llm/#34-reasoning-effort-for-gpt5-openai","title":"3.4 Reasoning effort for GPT\u20115 (OpenAI)","text":"<p>For OpenAI GPT\u20115-family models (e.g. <code>gpt-5-nano</code>, <code>gpt-5-mini</code>, etc.), you can pass an optional <code>reasoning_effort</code> kwarg:</p> <pre><code>async def gpt5_reasoning_example(*, context: NodeContext):\n    client = context.llm(\"default\")  # configured with a gpt-5-* model\n\n    text, usage = await client.chat(\n        [\n            {\"role\": \"system\", \"content\": \"Think step-by-step.\"},\n            {\"role\": \"user\", \"content\": \"Explain why 2 + 2 = 4.\"},\n        ],\n        reasoning_effort=\"high\",  # \"low\" | \"medium\" | \"high\" (OpenAI GPT\u20115 only)\n    )\n\n    return {\"answer\": text, \"usage\": usage}\n</code></pre> <p>For non\u2011GPT\u20115 models or non\u2011OpenAI providers, <code>reasoning_effort</code> is ignored.</p>"},{"location":"examples/llm/#4-configuring-profiles-via-env","title":"4. Configuring profiles via <code>.env</code>","text":""},{"location":"examples/llm/#41-default-profile","title":"4.1 Default profile","text":"<p><code>AppSettings</code> uses:</p> <ul> <li><code>env_prefix=\"AETHERGRAPH_\"</code></li> <li><code>env_nested_delimiter=\"__\"</code></li> </ul> <p>So the default LLM profile is configured by env vars like:</p> <pre><code># Turn LLM on globally\nAETHERGRAPH_LLM__ENABLED=true\n\n# Default profile (\"default\")\nAETHERGRAPH_LLM__DEFAULT__PROVIDER=openai\nAETHERGRAPH_LLM__DEFAULT__MODEL=gpt-4o-mini\nAETHERGRAPH_LLM__DEFAULT__EMBED_MODEL=text-embedding-3-small\nAETHERGRAPH_LLM__DEFAULT__BASE_URL=https://api.openai.com/v1\nAETHERGRAPH_LLM__DEFAULT__TIMEOUT=60\nAETHERGRAPH_LLM__DEFAULT__API_KEY=sk-proj-...\n</code></pre>"},{"location":"examples/llm/#42-additional-named-profiles","title":"4.2 Additional named profiles","text":"<p>Profiles live under <code>llm.profiles[\"NAME\"]</code>, which maps to env like:</p> <pre><code># Gemini profile\nAETHERGRAPH_LLM__PROFILES__GEMINI__PROVIDER=google\nAETHERGRAPH_LLM__PROFILES__GEMINI__MODEL=gemini-1.5-pro-latest\nAETHERGRAPH_LLM__PROFILES__GEMINI__EMBED_MODEL=text-embedding-004\nAETHERGRAPH_LLM__PROFILES__GEMINI__TIMEOUT=60\nAETHERGRAPH_LLM__PROFILES__GEMINI__API_KEY=AIzaSy...\n\n# Anthropic profile\nAETHERGRAPH_LLM__PROFILES__ANTHROPIC__PROVIDER=anthropic\nAETHERGRAPH_LLM__PROFILES__ANTHROPIC__MODEL=claude-3-7-sonnet-20250219\nAETHERGRAPH_LLM__PROFILES__ANTHROPIC__TIMEOUT=60\nAETHERGRAPH_LLM__PROFILES__ANTHROPIC__API_KEY=ant-...\n\n# LM Studio local profile\nAETHERGRAPH_LLM__PROFILES__LOCAL__PROVIDER=lmstudio\nAETHERGRAPH_LLM__PROFILES__LOCAL__MODEL=your-lmstudio-model-id\nAETHERGRAPH_LLM__PROFILES__LOCAL__BASE_URL=http://localhost:1234/v1\nAETHERGRAPH_LLM__PROFILES__LOCAL__TIMEOUT=60\n</code></pre> <p>Then you can access them via:</p> <pre><code>context.llm()               # default\ncontext.llm(\"gemini\")      # Gemini\ncontext.llm(\"anthropic\")   # Claude\ncontext.llm(\"local\")       # LM Studio\n</code></pre> <p>Note: <code>embed_model</code> is optional. If omitted, <code>embed()</code> will fall back to <code>EMBED_MODEL</code> env or a sensible default (e.g. <code>text-embedding-3-small</code>).</p>"},{"location":"examples/llm/#5-embeddings-rag-default-behavior","title":"5. Embeddings &amp; RAG default behavior","text":"<p>For RAG and other embedding-heavy workflows, Aethergraph\u2019s helpers (e.g. index / vector store integration) will typically use the default profile\u2019s embedding configuration, i.e.:</p> <ul> <li><code>AETHERGRAPH_LLM__DEFAULT__EMBED_MODEL</code> if set,</li> <li>otherwise <code>EMBED_MODEL</code> env var,</li> <li>otherwise a built\u2011in default like <code>text-embedding-3-small</code>.</li> </ul> <p>So if you want to control which embedding model is used for global RAG, set:</p> <pre><code>AETHERGRAPH_LLM__DEFAULT__EMBED_MODEL=text-embedding-3-small\n</code></pre> <p>or override per call:</p> <pre><code>vectors = await context.llm().embed(texts, model=\"text-embedding-3-large\")\n</code></pre>"},{"location":"examples/llm/#6-adding-overriding-profiles-at-runtime","title":"6. Adding / overriding profiles at runtime","text":"<p>You don\u2019t have to declare everything in <code>.env</code>. You can also create or update profiles in code at runtime.</p>"},{"location":"examples/llm/#61-quick-runtime-profile-with-llm_set_key","title":"6.1 Quick runtime profile with <code>llm_set_key</code>","text":"<p><code>NodeContext.llm_set_key()</code> is a convenience for creating or updating a profile in memory:</p> <pre><code>async def runtime_profile_demo(*, context: NodeContext):\n    # Create/update profile \"runtime-openai\" on the fly\n    context.llm_set_key(\n        provider=\"openai\",\n        model=\"gpt-4o-mini\",               # NEW: model included for convenience\n        api_key=\"sk-proj-...\",            # in-memory only\n        profile=\"runtime-openai\",\n    )\n\n    client = context.llm(\"runtime-openai\")\n\n    text, _ = await client.chat([\n        {\"role\": \"user\", \"content\": \"Hello from runtime profile!\"},\n    ])\n\n    return {\"reply\": text}\n</code></pre> <p>This does not persist anything to disk or secrets store; it\u2019s only for the current process.</p>"},{"location":"examples/llm/#62-fully-configuring-a-profile-via-llm","title":"6.2 Fully configuring a profile via <code>llm()</code>","text":"<p>You can also configure a profile in one shot using <code>context.llm()</code> with overrides:</p> <pre><code>async def llm_inline_config_demo(*, context: NodeContext):\n    client = context.llm(\n        profile=\"lab\",\n        provider=\"google\",\n        model=\"gemini-1.5-pro-latest\",\n        api_key=\"AIzaSy...\",\n        base_url=\"https://generativelanguage.googleapis.com\",\n        timeout=60.0,\n    )\n\n    text, _ = await client.chat([\n        {\"role\": \"user\", \"content\": \"Hi from the lab profile!\"},\n    ])\n\n    return {\"reply\": text}\n</code></pre> <p>If the profile doesn\u2019t exist, it will be created. If it exists, it will be updated in place.</p>"},{"location":"examples/llm/#7-provider-specific-config-notes","title":"7. Provider-specific config notes","text":""},{"location":"examples/llm/#71-openai-provideropenai","title":"7.1 OpenAI (<code>provider=\"openai\"</code>)","text":"<p>Required:</p> <ul> <li><code>AETHERGRAPH_LLM__...__API_KEY</code> \u2013 or <code>OPENAI_API_KEY</code> if using env-based fallback.</li> </ul> <p>Optional / defaults:</p> <ul> <li><code>base_url</code> \u2013 defaults to <code>https://api.openai.com/v1</code>.</li> <li><code>timeout</code> \u2013 defaults to <code>60</code> seconds.</li> <li><code>model</code> \u2013 any chat/vision/reasoning model (e.g. <code>gpt-4o-mini</code>, <code>gpt-4o</code>, <code>gpt-5-nano</code>).</li> <li><code>embed_model</code> \u2013 e.g. <code>text-embedding-3-small</code>.</li> </ul>"},{"location":"examples/llm/#72-anthropic-provideranthropic","title":"7.2 Anthropic (<code>provider=\"anthropic\"</code>)","text":"<p>Required:</p> <ul> <li><code>api_key</code> \u2013 <code>ANTHROPIC_API_KEY</code> or <code>AETHERGRAPH_LLM__...__API_KEY</code>.</li> <li><code>model</code> \u2013 e.g. <code>claude-3-7-sonnet-20250219</code>.</li> </ul> <p>Optional / defaults:</p> <ul> <li><code>base_url</code> \u2013 defaults to <code>https://api.anthropic.com</code>.</li> <li><code>timeout</code> \u2013 defaults to <code>60</code>.</li> </ul> <p>Anthropic does not support embeddings via this client. <code>embed()</code> will raise <code>NotImplementedError</code> for <code>provider=\"anthropic\"</code>.</p>"},{"location":"examples/llm/#73-google-gemini-providergoogle","title":"7.3 Google / Gemini (<code>provider=\"google\"</code>)","text":"<p>Required:</p> <ul> <li><code>api_key</code> \u2013 <code>AETHERGRAPH_LLM__...__API_KEY</code>.</li> <li><code>model</code> \u2013 e.g. <code>gemini-1.5-pro-latest</code> for chat.</li> </ul> <p>Optional / defaults:</p> <ul> <li><code>base_url</code> \u2013 defaults to <code>https://generativelanguage.googleapis.com</code>.</li> <li><code>embed_model</code> \u2013 e.g. <code>text-embedding-004</code>.</li> </ul> <p>Endpoints used:</p> <ul> <li>Chat: <code>POST /v1/models/{model}:generateContent</code>.</li> <li>Embeddings: <code>POST /v1/models/{embed_model}:embedContent</code>.</li> </ul>"},{"location":"examples/llm/#74-lm-studio-providerlmstudio","title":"7.4 LM Studio (<code>provider=\"lmstudio\"</code>)","text":"<p>LM Studio exposes an OpenAI-compatible server.</p> <p>Required:</p> <ul> <li><code>base_url</code> \u2013 usually <code>http://localhost:1234/v1</code> (or whatever the LM Studio UI shows).</li> <li><code>model</code> \u2013 the LM Studio model ID (shown in the UI).</li> </ul> <p>Optional:</p> <ul> <li>No API key is required by default.</li> </ul> <p>Endpoints used:</p> <ul> <li>Chat: <code>POST {base_url}/chat/completions</code>.</li> <li>Embeddings: <code>POST {base_url}/embeddings</code>.</li> </ul>"},{"location":"examples/llm/#75-ollama-providerollama","title":"7.5 Ollama (<code>provider=\"ollama\"</code>)","text":"<p>Ollama also provides an OpenAI-compatible mode.</p> <p>Required/Defaults:</p> <ul> <li><code>base_url</code> \u2013 defaults to <code>http://localhost:11434/v1</code> if not set.</li> <li><code>model</code> \u2013 an Ollama model name (e.g. <code>llama3</code>, <code>mistral</code>, etc. configured in Ollama).</li> </ul> <p>Optional:</p> <ul> <li>Usually no API key.</li> </ul> <p>Endpoints used:</p> <ul> <li>Chat: <code>POST {base_url}/chat/completions</code>.</li> <li>Embeddings: <code>POST {base_url}/embeddings</code>.</li> </ul>"},{"location":"examples/llm/#8-when-not-to-use-contextllm","title":"8. When not to use <code>context.llm()</code>","text":"<p><code>context.llm()</code> is intentionally thin and opinionated. You might want to bypass it when:</p> <ul> <li>You need cutting-edge / experimental API features that aren\u2019t wired yet.</li> <li>You want a very custom request/response shape.</li> <li>You\u2019re targeting a provider that isn\u2019t in the built-in list.</li> </ul> <p>In those cases you can:</p> <ul> <li>Use <code>httpx</code> (or the vendor\u2019s official SDK) directly inside your node, or</li> <li>Wrap your own client as a separate service and inject it into <code>NodeServices</code>.</li> </ul> <p>The built-in <code>llm()</code> helper gives you a fast \u201chappy path\u201d for common providers and models, without preventing you from going lower-level when you need to.</p>"},{"location":"examples/memory-artifact/","title":"Memory &amp; Artifact Mini Examples","text":"<p>These snippets assume you\u2019re inside an async function where you already have:</p> <pre><code>mem: MemoryFacade = context.memory()\narts: ArtifactFacade = context.artifacts()\n</code></pre> <p>They\u2019re meant as supplementary examples, not main docs.</p>"},{"location":"examples/memory-artifact/#1-events-101-what-is-saved-and-how-to-read-it","title":"1. Events 101 \u2013 what is saved and how to read it","text":""},{"location":"examples/memory-artifact/#11-recording-a-simple-event","title":"1.1 Recording a simple event","text":"<pre><code># Record a simple user message as an event.\nevt = await mem.record(\n    kind=\"user_msg\",\n    data={\"role\": \"user\", \"text\": \"How do I use AetherGraph?\"},\n    tags=[\"chat\", \"demo\"],\n    severity=2,\n    stage=\"observe\",\n)\n\nprint(\"Event ID:\", evt.event_id)\nprint(\"Kind:\", evt.kind)\nprint(\"Text payload (JSON string):\", evt.text)\n</code></pre> <p>What happens:</p> <ul> <li><code>record(...)</code> JSON-serializes <code>data</code> and stores it in <code>evt.text</code>.</li> <li>Adds scope fields like <code>session_id</code>, <code>run_id</code>, <code>graph_id</code>, <code>node_id</code>, <code>agent_id</code>.</li> <li>Appends the <code>Event</code> to HotLog (recent buffer) and Persistence (JSONL).</li> </ul> <p>Conceptually, an Event is:</p> <p>\u201cSomething happened (tool call, chat turn, metric, etc.), scoped to this session/run.\u201d</p>"},{"location":"examples/memory-artifact/#12-reading-raw-events-vs-decoded-data","title":"1.2 Reading raw events vs decoded data","text":"<pre><code># Raw events (Event objects)\nfrom aethergraph.contracts.services.memory import Event\n\nevents: list[Event] = await mem.recent(kinds=[\"user_msg\"], limit=10)\nfor e in events:\n    print(\"Raw event kind:\", e.kind, \"text:\", e.text)\n\n# Decoded data (whatever you passed as data=...)\ndata_items = await mem.recent_data(kinds=[\"user_msg\"], limit=10)\nfor d in data_items:\n    print(\"Decoded data:\", d)   # dict: {\"role\": \"...\", \"text\": \"...\"}\n</code></pre> <ul> <li><code>recent(...)</code> \u2192 <code>list[Event]</code> (full event objects).</li> <li><code>recent_data(...)</code> \u2192 <code>list[Any]</code> using the JSON-in-<code>text</code> convention of <code>record()</code>.</li> </ul> <p>Users who just want \u201cthe thing I logged\u201d should use <code>recent_data</code>.</p>"},{"location":"examples/memory-artifact/#2-write_result-indices-structured-toolagent-outputs","title":"2. <code>write_result</code> &amp; indices \u2013 structured tool/agent outputs","text":"<p><code>write_result</code> is a convenience for logging structured outputs from a tool/agent and updating indices so you can ask things like:</p> <ul> <li>\u201cWhat was the last value named <code>result</code>?\u201d</li> <li>\u201cWhat are the latest outputs for <code>tool.calculator</code>?\u201d</li> </ul>"},{"location":"examples/memory-artifact/#21-recording-a-tool-result","title":"2.1 Recording a tool result","text":"<pre><code># Imagine a tiny calculator tool\ninputs = [\n    {\"name\": \"expression\", \"kind\": \"text\", \"value\": \"1 + 2 * 3\"},\n]\noutputs = [\n    {\"name\": \"result\", \"kind\": \"number\", \"value\": 7},\n]\n\nevt = await mem.write_result(\n    topic=\"tool.calculator\",      # identifier for this tool/agent\n    inputs=inputs,\n    outputs=outputs,\n    tags=[\"tool\", \"calculator\"],\n    metrics={\"latency_ms\": 12.3},\n    message=\"Evaluated 1 + 2 * 3\",\n)\n\nprint(\"tool_result event_id:\", evt.event_id)\nprint(\"Kind:\", evt.kind)   # \"tool_result\"\nprint(\"Tool:\", evt.tool)   # \"tool.calculator\"\n</code></pre>"},{"location":"examples/memory-artifact/#22-reading-via-indices","title":"2.2 Reading via indices","text":"<pre><code># 1) Last output value by name (fast)\nlast_result = await mem.get_last_value(\"result\")\nprint(\"get_last_value('result') -&gt;\", last_result)\n# e.g. {\"name\":\"result\",\"kind\":\"number\",\"value\":7}\n\n# 2) Latest reference outputs by kind (e.g. \"number\", \"json\", \"uri\")\nnumber_refs = await mem.get_latest_values_by_kind(\"number\", limit=5)\nprint(\"get_latest_values_by_kind('number'):\", number_refs)\n# e.g. [{\"name\":\"result\",\"kind\":\"number\",\"value\":7}, ...]\n\n# 3) Last outputs for a given topic (tool/agent)\ncalc_outputs = await mem.get_last_outputs_for_topic(\"tool.calculator\")\nprint(\"get_last_outputs_for_topic('tool.calculator'):\", calc_outputs)\n# e.g. {\"result\": 7, \"latency_ms\": 12.3, ...} (depends on index impl)\n</code></pre> <p>Purpose of <code>write_result</code>:</p> <ul> <li>Normalizes tool/agent outputs into a <code>tool_result</code> event.</li> <li>Keeps HotLog + Persistence in sync.</li> <li>Updates indices so other code can quickly answer questions about latest outputs.</li> </ul>"},{"location":"examples/memory-artifact/#3-artifacts-101-save-list-search-best","title":"3. Artifacts 101 \u2013 save, list, search, best","text":"<p>An Artifact is an immutable asset:</p> <ul> <li>Models, reports, checkpoints, metrics files, directories, etc.</li> <li>Stored via an <code>AsyncArtifactStore</code> and indexed via <code>AsyncArtifactIndex</code>.</li> </ul> <p>In agents, you normally access them through ArtifactFacade via <code>context.artifacts()</code>.</p>"},{"location":"examples/memory-artifact/#31-save-small-text-json-artifacts","title":"3.1 Save small text &amp; JSON artifacts","text":"<pre><code># Save a plain-text log\nlog_art = await arts.save_text(\n    \"This is a tiny experiment log.\",\n    suggested_uri=\"./logs/experiment_001.txt\",\n)\nprint(\"Log artifact URI:\", log_art.uri)\n\n# Save structured metrics as JSON\nmetrics_art = await arts.save_json(\n    {\"epoch\": 3, \"train_loss\": 0.42, \"val_loss\": 0.55},\n    suggested_uri=\"./metrics/exp001_epoch3.json\",\n)\nprint(\"Metrics artifact URI:\", metrics_art.uri)\n</code></pre>"},{"location":"examples/memory-artifact/#32-save-a-file-with-kindlabelsmetrics","title":"3.2 Save a file with kind/labels/metrics","text":"<pre><code>checkpoint_path = \"./checkpoints/exp001_step100.pt\"\n\nckpt_art = await arts.save(\n    checkpoint_path,\n    kind=\"model_checkpoint\",\n    labels={\"experiment\": \"exp001\", \"step\": \"100\"},\n    metrics={\"val_loss\": 0.55},\n    suggested_uri=\"./checkpoints/exp001_step100.pt\",\n    pin=True,   # mark as important/keep\n)\n\nprint(\"Checkpoint id:\", ckpt_art.id)\nprint(\"Checkpoint kind:\", ckpt_art.kind)\nprint(\"Checkpoint labels:\", ckpt_art.labels)\nprint(\"Checkpoint metrics:\", ckpt_art.metrics)\n</code></pre>"},{"location":"examples/memory-artifact/#4-listing-searching-artifacts-scope-labels-metrics","title":"4. Listing &amp; searching artifacts (scope, labels, metrics)","text":""},{"location":"examples/memory-artifact/#41-list-all-artifacts-for-this-run","title":"4.1 List all artifacts for this run","text":"<pre><code>arts_in_run = await arts.list(scope=\"run\")\nprint(\"Artifacts in this run:\", [a.uri for a in arts_in_run])\n</code></pre>"},{"location":"examples/memory-artifact/#42-search-by-kind-label","title":"4.2 Search by kind + label","text":"<pre><code>exp_ckpts = await arts.search(\n    kind=\"model_checkpoint\",\n    labels={\"experiment\": \"exp001\"},\n    scope=\"run\",\n)\nprint(\"Exp001 checkpoints:\", [a.uri for a in exp_ckpts])\n</code></pre> <ul> <li><code>kind</code> narrows by artifact type.</li> <li><code>labels</code> filters by label key/value.</li> <li> <p><code>scope</code> controls implicit filters:</p> </li> <li> <p><code>\"run\"</code> = current run only (default).</p> </li> <li><code>\"graph\"</code> / <code>\"node\"</code> = more specific.</li> <li><code>\"project\"</code> / <code>\"all\"</code> = wider.</li> </ul>"},{"location":"examples/memory-artifact/#43-selecting-the-best-artifact-by-metric","title":"4.3 Selecting the \"best\" artifact by metric","text":"<pre><code>best_ckpt = await arts.best(\n    kind=\"model_checkpoint\",\n    metric=\"val_loss\",\n    mode=\"min\",                # minimize validation loss\n    scope=\"run\",\n    filters={\"experiment\": \"exp001\"},\n)\n\nif best_ckpt:\n    print(\"Best checkpoint (by val_loss):\", best_ckpt.uri, best_ckpt.metrics)\nelse:\n    print(\"No checkpoint found.\")\n</code></pre> <p>Here, <code>best(...)</code> asks the index to:</p> <ul> <li>Filter by <code>kind</code> + <code>filters</code> (labels).</li> <li>Select the artifact with min or max on the given <code>metric</code>.</li> </ul>"},{"location":"examples/memory-artifact/#5-loading-artifacts-and-turning-uris-into-paths","title":"5. Loading artifacts and turning URIs into paths","text":""},{"location":"examples/memory-artifact/#51-load-payload-back-from-the-store","title":"5.1 Load payload back from the store","text":"<pre><code># If the artifact payload is JSON\nmetrics = await arts.load_artifact(metrics_art.uri)\nprint(\"Loaded metrics json:\", metrics)\n\n# If it's bytes (e.g., a binary checkpoint)\nckpt_bytes = await arts.load_artifact_bytes(ckpt_art.uri)\nprint(\"Loaded checkpoint size:\", len(ckpt_bytes))\n</code></pre>"},{"location":"examples/memory-artifact/#52-convert-artifact-uri-to-local-filesystem-path","title":"5.2 Convert artifact URI to local filesystem path","text":"<pre><code># Turn an artifact URI into a local file path (for external libs)\nlocal_ckpt_path = arts.to_local_file(ckpt_art)\nprint(\"Local checkpoint path:\", local_ckpt_path)\n\n# Same idea for directories:\n# local_dir = arts.to_local_dir(dir_artifact)\n</code></pre> <p>These helpers are handy when your artifacts are tracked as <code>file://...</code> URIs but some library expects a plain <code>str</code> path.</p>"},{"location":"examples/memory-artifact/#6-combining-memory-artifacts","title":"6. Combining memory + artifacts","text":"<p>Typical pattern:</p> <ol> <li>Agent runs a job.</li> <li>Saves results as artifacts.</li> <li>Logs a <code>tool_result</code> event with artifact URIs in outputs.</li> <li>Later, uses memory indices + artifact search/load to inspect results.</li> </ol> <pre><code># 1) Save metrics as an artifact\nmetrics_art = await arts.save_json(\n    {\"epoch\": 10, \"train_loss\": 0.21, \"val_loss\": 0.24},\n    suggested_uri=\"./metrics/exp002_epoch10.json\",\n)\n\n# 2) Log a structured result referencing the artifact\nawait mem.write_result(\n    topic=\"trainer.exp002\",\n    outputs=[\n        {\"name\": \"final_val_loss\", \"kind\": \"number\", \"value\": 0.24},\n        {\"name\": \"metrics_uri\", \"kind\": \"uri\", \"value\": metrics_art.uri},\n    ],\n    tags=[\"training\", \"exp002\"],\n    message=\"Training finished for exp002\",\n    metrics={\"epoch\": 10},\n)\n\n# 3) Later: quickly get last trainer outputs via indices\ntrainer_outs = await mem.get_last_outputs_for_topic(\"trainer.exp002\")\nprint(\"Trainer last outputs:\", trainer_outs)\n# e.g. {\"final_val_loss\": 0.24, \"metrics_uri\": \"file://.../metrics/exp002_epoch10.json\"}\n\n# 4) Load the metrics artifact via the recorded URI\nloaded_metrics = await arts.load_artifact(trainer_outs[\"metrics_uri\"])\nprint(\"Loaded metrics from artifact:\", loaded_metrics)\n</code></pre> <p>This example shows how memory (events + indices) and artifacts work together:</p> <ul> <li>Memory tells you what happened last and which artifact URIs matter.</li> <li>ArtifactFacade lets you search, rank, and then actually load those files.</li> </ul>"},{"location":"examples/memory-artifact/#7-rag-memory-turning-events-into-searchable-knowledge","title":"7. RAG + Memory \u2013 turning events into searchable knowledge","text":"<p>RAG (Retrieval-Augmented Generation) here is wired through MemoryFacade to let you:</p> <ol> <li>Create or bind to a corpus (a logical collection of documents/chunks).</li> <li>Promote events (e.g., tool results, chat summaries) into that corpus.</li> <li>Search / answer questions over it using an LLM.</li> <li>Optionally snapshot or compact the corpus over time.</li> </ol> <p>These examples assume <code>mem: MemoryFacade</code> is configured with a <code>RAGFacade</code>.</p>"},{"location":"examples/memory-artifact/#71-binding-to-a-corpus-projectsessionrun","title":"7.1 Binding to a corpus (project/session/run)","text":"<pre><code># Common pattern: bind to a project-level corpus.\ncorpus_id = await mem.rag_bind(scope=\"project\")\nprint(\"Using corpus:\", corpus_id)\n\n# Or a session-specific corpus\nsession_corpus = await mem.rag_bind(scope=\"session\")\nprint(\"Session corpus:\", session_corpus)\n\n# Or an explicitly named key (stable across runs if you reuse it)\nteam_corpus = await mem.rag_bind(scope=\"project\", key=\"team-alpha-notes\")\nprint(\"Team corpus:\", team_corpus)\n</code></pre> <ul> <li> <p><code>scope</code> controls how the corpus is keyed:</p> </li> <li> <p><code>\"project\"</code> \u2192 tied to workspace/project.</p> </li> <li><code>\"session\"</code> \u2192 tied to session_id.</li> <li><code>\"run\"</code> \u2192 tied to particular run (more ephemeral).</li> <li>You can override with <code>corpus_id=</code> directly if you already know the ID.</li> </ul>"},{"location":"examples/memory-artifact/#72-promoting-events-into-rag-event-doc","title":"7.2 Promoting events into RAG (event \u2192 doc)","text":"<p>You can convert existing memory events into RAG documents with <code>rag_promote_events()</code>.</p> <pre><code>corpus_id = await mem.rag_bind(scope=\"project\")\n\n# Promote recent high-signal tool_result events into the corpus\nstats = await mem.rag_promote_events(\n    corpus_id=corpus_id,\n    where={\n        \"kinds\": [\"tool_result\"],   # filter by Event.kind\n        \"limit\": 200,\n    },\n    policy={\n        \"min_signal\": 0.3,           # ignore low-signal noise\n        \"chunk\": {\"size\": 800, \"overlap\": 120},  # (if your RAG index supports chunking)\n    },\n)\n\nprint(\"Promoted events stats:\", stats)\n# e.g. {\"added\": 12, \"chunks\": 48, \"index\": \"SomeIndexImpl\"}\n</code></pre> <p>What happens:</p> <ul> <li><code>rag_promote_events</code> pulls events (via <code>recent</code> or your provided <code>events</code> list).</li> <li> <p>For each event, it builds a document:</p> </li> <li> <p><code>text</code> from <code>Event.text</code> (or a JSON of inputs/outputs/metrics).</p> </li> <li><code>title</code> + <code>labels</code> derived from kind/tool/stage/tags.</li> <li>Upserts docs into the RAG index.</li> <li>Logs a <code>tool_result</code> under topic <code>rag.promote.&lt;corpus_id&gt;</code> for traceability.</li> </ul> <p>You can also pass <code>events=</code> yourself if you already filtered them manually.</p>"},{"location":"examples/memory-artifact/#73-direct-upsert-of-custom-docs-bypassing-events","title":"7.3 Direct upsert of custom docs (bypassing events)","text":"<p>If you just have ad-hoc docs (e.g., notes, specs), you can call <code>rag_upsert</code>:</p> <pre><code>corpus_id = await mem.rag_bind(scope=\"project\")\n\ndocs = [\n    {\n        \"text\": \"AetherGraph is a framework for building agentic graphs.\",\n        \"title\": \"AG overview\",\n        \"labels\": {\"topic\": \"overview\", \"source\": \"docs\"},\n    },\n    {\n        \"text\": \"MemoryFacade coordinates HotLog, Persistence, Indices, and optional RAG.\",\n        \"title\": \"MemoryFacade design\",\n        \"labels\": {\"topic\": \"memory\", \"source\": \"notes\"},\n    },\n]\n\nupsert_stats = await mem.rag_upsert(corpus_id=corpus_id, docs=docs)\nprint(\"RAG upsert stats:\", upsert_stats)\n</code></pre> <p>This bypasses events entirely and goes straight to documents.</p>"},{"location":"examples/memory-artifact/#74-searching-the-corpus","title":"7.4 Searching the corpus","text":"<p>Once docs are in the corpus, you can run semantic/hybrid search:</p> <pre><code>corpus_id = await mem.rag_bind(scope=\"project\")\n\nhits = await mem.rag_search(\n    corpus_id=corpus_id,\n    query=\"How does the memory system work?\",\n    k=5,\n    filters={\"topic\": \"memory\"},  # optional label filter\n    mode=\"hybrid\",                 # or \"dense\"\n)\n\nfor h in hits:\n    print(\"Score:\", h[\"score\"])\n    print(\"Title:\", h[\"meta\"].get(\"title\"))\n    print(\"Text snippet:\", h[\"text\"][:120], \"...\")\n    print(\"Labels:\", h[\"meta\"].get(\"labels\"))\n    print(\"---\")\n</code></pre> <p><code>rag_search</code> returns a list of serializable dicts:</p> <ul> <li><code>text</code>  \u2013 chunk text.</li> <li><code>meta</code>  \u2013 metadata (labels, title, etc.).</li> <li><code>score</code> \u2013 similarity/relevance score.</li> </ul>"},{"location":"examples/memory-artifact/#75-rag-answer-retrieval-llm-citations","title":"7.5 RAG answer \u2013 retrieval + LLM + citations","text":"<p>For \u201cask a question over everything in the corpus\u201d you use <code>rag_answer</code>:</p> <pre><code>corpus_id = await mem.rag_bind(scope=\"project\")\n\nanswer = await mem.rag_answer(\n    corpus_id=corpus_id,\n    question=\"Summarize how MemoryFacade and ArtifactFacade work together.\",\n    style=\"concise\",          # or \"detailed\"\n    with_citations=True,\n    k=6,\n)\n\nprint(\"Answer:\\n\", answer.get(\"answer\"))\nprint(\"Citations:\")\nfor c in answer.get(\"resolved_citations\", []):\n    print(\"- From doc:\", c.get(\"doc_id\"), \"score=\", c.get(\"score\"))\n</code></pre> <p><code>rag_answer</code> will:</p> <ol> <li>Run retrieval over the corpus.</li> <li>Call the LLM with retrieved chunks.</li> <li>Return an <code>answer</code> plus <code>resolved_citations</code>.</li> <li>Log a <code>tool_result</code> under topic <code>rag.answer.&lt;corpus_id&gt;</code> with outputs    and usage metrics (via <code>write_result</code>).</li> </ol> <p>These helpers are optional, but they show how the RAG integration fits the same pattern as memory + artifacts:</p> <ul> <li>Memory: events + indices for \u201cwhat happened and when?\u201d.</li> <li>Artifacts: big immutable assets (files, bundles) with labels/metrics.</li> <li>RAG: a semantic index over the content of your events/docs, used by   your agents via standard tools (<code>rag_search</code>, <code>rag_answer</code>, etc.).</li> </ul>"},{"location":"key-concepts/agent-via-graph-fn/","title":"Agents via <code>@graph_fn</code>","text":"<p>This chapter introduces agents in AetherGraph through the <code>@graph_fn</code> decorator. You\u2019ll learn how <code>@tool</code> functions become nodes on the fly, when and why to use async functions, and how to chain or nest them to form structured yet reactive agentic workflows.</p>"},{"location":"key-concepts/agent-via-graph-fn/#1-what-is-a-graph_fn","title":"1. What is a <code>graph_fn</code>?","text":"<p>A <code>graph_fn</code> turns a plain Python function into an agent with access to rich context services\u2014channel, memory, artifacts, logger, and more. It runs in the normal Python runtime by default; no DAG is captured automatically when you invoke it. For most interactive or agentic workflows, this lightweight mode is ideal: you get an ergonomic async function with context utilities for I/O, persistence, and orchestration without committing to graph capture.</p>"},{"location":"key-concepts/agent-via-graph-fn/#function-shape","title":"Function shape","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"example\")\nasync def example(x: int, *, context: NodeContext):\n    # Access runtime services from the context\n    await context.channel().send_text(f\"x={x}\")\n    return {\"y\": x + 1}\n</code></pre> <ul> <li>Define your own API through standard parameters.</li> <li>Include <code>*, context</code> to access the <code>NodeContext</code>; if omitted, nothing is injected.</li> </ul> <p>Minimal example:</p> <pre><code>@graph_fn(name=\"hello_agent\")\nasync def hello_agent(name: str = \"world\", *, context: NodeContext):\n    await context.channel().send_text(f\"\ud83d\udc4b Hello, {name}!\")\n    context.memory().record(kind=\"usr_data\", data={\"name\": name})\n    context.logger().info(\"Greeted user\", extra={\"name\": name})\n    return {\"message\": f\"Hello, {name}\"}\n</code></pre> <p>Key idea: <code>@graph_fn</code> provides a reactive agent interface\u2014async execution with contextual power\u2014while keeping runtime overhead minimal. Nodes are only added when you explicitly use <code>@tool</code> or call other graphs.</p>"},{"location":"key-concepts/agent-via-graph-fn/#2-tools-nodes-on-the-fly","title":"2. Tools: nodes on the fly","text":"<p>The <code>@tool</code> decorator marks a Python function as a tool node. When called inside a <code>graph_fn</code>, the runtime creates a node on the fly and records its inputs/outputs for provenance, inspection, or future resumptions.</p> <p>Rule of thumb: for exploratory, reactive development, call regular Python functions freely. Reach for <code>@tool</code> when you need traceable state, durability, or resume checkpoints.</p>"},{"location":"key-concepts/agent-via-graph-fn/#example-a-simple-sum-tool","title":"Example: a simple sum tool","text":"<pre><code>from typing import List\nfrom aethergraph import tool\n\n@tool(outputs=[\"total\"])\ndef sum_vec(xs: List[float]) -&gt; dict:\n    return {\"total\": float(sum(xs))}\n</code></pre> <p>Use inside a <code>graph_fn</code>:</p> <pre><code>@graph_fn(name=\"tool_demo\")\nasync def tool_demo(values: list[float], *, context: NodeContext):\n    stats = {\"n\": len(values)}             # executed inline\n    out = sum_vec(values)                  # \u2190 captured as a node\n    await context.channel().send_text(f\"n={stats['n']}, sum={out['total']}\")\n    return {\"total\": out[\"total\"]}\n</code></pre> <p>You can mix normal Python code and <code>@tool</code> calls seamlessly. Only <code>@tool</code> calls create nodes.</p> <p>To inspect the implicit graph created during execution, call <code>graph_fn.last_graph</code> \u2014 it returns the captured <code>TaskGraph</code> for visualization or reuse.</p>"},{"location":"key-concepts/agent-via-graph-fn/#3-async-first-chaining-nesting-and-concurrency","title":"3. Async-first: chaining, nesting, and concurrency","text":"<p>AetherGraph adopts async-first design because agents often:</p> <ul> <li>Wait for user input (<code>ask_text</code>, <code>ask_approval</code>)</li> <li>Perform I/O (HTTP, file writes, DB queries)</li> <li>Launch parallel sub-tasks</li> </ul>"},{"location":"key-concepts/agent-via-graph-fn/#chaining-and-nesting-graph_fns","title":"Chaining and nesting <code>graph_fn</code>s","text":"<p>You can call one <code>graph_fn</code> from another. Each call creates a child subgraph node:</p> <pre><code>@graph_fn(name=\"step1\")\nasync def step1(x: int, *, context: NodeContext) -&gt; dict:\n    return {\"y\": x + 1}\n\n@graph_fn(name=\"step2\")\nasync def step2(y: int, *, context: NodeContext) -&gt; dict:\n    return {\"z\": y * 2}\n\n@graph_fn(name=\"pipeline\")\nasync def pipeline(x: int, *, context: NodeContext) -&gt; dict:\n    a = await step1(x)       # \u2192 child node\n    b = await step2(a[\"y\"]) # \u2192 child node\n    return {\"z\": b[\"z\"]}\n</code></pre>"},{"location":"key-concepts/agent-via-graph-fn/#fan-out-concurrency","title":"Fan-out concurrency","text":"<p>Launch multiple subgraphs concurrently with <code>asyncio.gather</code>:</p> <pre><code>import asyncio\n\n@graph_fn(name=\"concurrent_steps\")\nasync def concurrent_steps(a: int, b: int, *, context: NodeContext) -&gt; dict:\n    r1, r2 = await asyncio.gather(step1(a), step2(b))\n    return {\"r1\": r1[\"y\"], \"r2\": r2[\"z\"]}\n</code></pre> <p>This pattern enables natural fan-out/fan-in parallelism within a single reactive agent.</p>"},{"location":"key-concepts/agent-via-graph-fn/#4-running-a-graph_fn","title":"4. Running a <code>graph_fn</code>","text":"<p>You can execute a <code>graph_fn</code> directly from async code or through the provided runners.</p>"},{"location":"key-concepts/agent-via-graph-fn/#option-a-direct-await","title":"Option A \u2013 Direct await","text":"<pre><code># In an async function\nresult = await pipeline(3)\n</code></pre>"},{"location":"key-concepts/agent-via-graph-fn/#option-b-synchronous-helper","title":"Option B \u2013 Synchronous helper","text":"<p><pre><code>from aethergraph.runner import run\nfinal = run(pipeline, inputs={\"x\": 3})\n</code></pre> This is preferred in Jupyter Notebook. </p>"},{"location":"key-concepts/agent-via-graph-fn/#option-c-explicit-async-runner","title":"Option C \u2013 Explicit async runner","text":"<pre><code>from aethergraph.runner import run_async\n# In an async function\nresult = await run_async(pipeline, inputs={\"x\": 3})\n</code></pre> <p>The <code>run_*</code> helpers drive the event loop and normalize execution for both reactive and static graphs.</p>"},{"location":"key-concepts/agent-via-graph-fn/#5-summary","title":"5. Summary","text":"<ul> <li><code>@graph_fn</code> wraps a Python function into an async agent with an injected <code>NodeContext</code> exposing rich runtime services.</li> <li>Execution stays in normal Python until you invoke <code>@tool</code> or another <code>graph_fn</code>\u2014only those create nodes.</li> <li><code>@tool</code> functions let you capture intermediate steps for provenance and durability.</li> <li>Agents are composable: call one <code>graph_fn</code> from another or fan out with <code>asyncio.gather</code>.</li> <li>Use <code>run()</code> or <code>run_async()</code> for simple orchestration; prefer plain calls + context for lightweight workflows.</li> </ul> <p>AetherGraph\u2019s agent model combines Pythonic simplicity with event-driven introspection\u2014reactive first, deterministic when needed.</p>"},{"location":"key-concepts/artifacts-memory/","title":"Artifacts and Memory","text":"<p>This chapter covers two foundational pillars of AetherGraph\u2019s runtime: Artifacts and Memory. Together, they form the provenance backbone \u2014 making every result, file, and intermediate step traceable, reproducible, and retrievable long after execution.</p> <p>Mental model: Artifacts capture what was produced; Memory captures what happened (events, results, metrics) and why (context, summaries, links).</p>"},{"location":"key-concepts/artifacts-memory/#1-why-artifacts-memory-exist","title":"1. Why Artifacts &amp; Memory Exist","text":"<p>Most Python workflows scatter outputs across temp folders and logs with no consistent linkage. AetherGraph fixes this by binding everything to the active run/graph/node and exposing consistent, high\u2011level APIs for saving and recalling state.</p> Concern Manual management With AetherGraph Provenance Files &amp; logs scattered; hard to link Every record stamped with <code>{run_id, graph_id, node_id}</code> + tool metadata Reproducibility Filenames drift; env unknown Content\u2011addressed + typed records \u2192 deterministic recall Discoverability Grep and guess Query by <code>kind</code>, <code>labels</code>, <code>metrics</code>, scope; ask \u201cbest by metric\u201d Durability Ad\u2011hoc paths; stale temp dirs CAS store + index; pins; export/replay Collaboration Tribal conventions Shared schema (URIs/records) + searchable index <p>Takeaway: Use artifacts for durable assets; use memory for structured, queryable history. Both are scoped to your execution so you can reconstruct the story of a run.</p>"},{"location":"key-concepts/artifacts-memory/#2-artifacts-persistent-assets","title":"2. Artifacts \u2014 Persistent Assets","text":"<p>Artifacts are immutable, content\u2011addressed assets (CAS) produced or consumed by agents/tools: files, directories, JSON payloads, or serialized objects.</p>"},{"location":"key-concepts/artifacts-memory/#why-artifacts-vs-manual-files","title":"Why Artifacts (vs. manual files)?","text":"<ul> <li>Content\u2011addressed: the URI reflects the content (CAS) \u2014 no silent overwrites, no need for manual naming.</li> <li>Typed + labeled: add <code>kind</code>, <code>labels</code>, and <code>metrics</code> to organize results.</li> <li>Indexed: query by scope/labels or rank by metric. </li> <li>Provenance\u2011stamped: <code>{run_id, graph_id, node_id, tool_name, tool_version}</code> baked in.</li> <li>Portable: <code>to_local_path(uri)</code> resolves for local or remote stores.</li> </ul>"},{"location":"key-concepts/artifacts-memory/#architecture","title":"Architecture","text":"<pre><code>[ Your Agent / Tool ]\n          \u2502   (context)\n          \u25bc\n[ NodeContext ]\n          \u2502\n          \u25bc\n[ context.artifacts() \u2014 Artifact Facade ]\n     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502               \u2502                \u2502                 \u2502\n     \u2502 save / writer \u2502 stage / ingest \u2502 list / search   \u2502 best / pin\n     \u25bc               \u25bc                \u25bc                 \u25bc\n[ Artifact Store ]  [ Staging Area ]  [ Artifact Index ]  [ Retention ]\n     (CAS/FS)          (tmp)             (SQLite/KV)        (pins)\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#core-api","title":"Core API","text":"Method Purpose <code>stage()</code> / <code>stage_dir()</code> Reserve a temp path for producing files/dirs safely. <code>save()</code> Save an existing path and index it. Returns an artifact with <code>uri</code>. <code>save_text()</code> Store small text payloads. <code>save_json()</code> Store a JSON payload. <code>writer()</code> Context manager to stream\u2011write binary content; atomically indexes on close. <code>list()</code> / <code>search()</code> / <code>best()</code> Query and rank artifacts by descriptors or metrics. <code>pin()</code> Mark as retained (skip cleanup policies). <code>to_local_path()</code> Resolve a CAS URI to a local filesystem path."},{"location":"key-concepts/artifacts-memory/#examples","title":"Examples","text":"<p>Save a file</p> <pre><code>@graph_fn(name=\"produce_artifact\", outputs=[\"report_uri\"])\nasync def produce_artifact(*, context):\n    art = await context.artifacts().save(\n        path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\": \"A\"}\n    )\n    return {\"report_uri\": art.uri}\n</code></pre> <p>Search a past artfiact</p> <pre><code>@graph_fn(name=\"search_reports\", outputs=[\"top_uri\"])\nasync def search_reports(*, context):\n    results = await context.artifacts().search(\n        kind=\"report\", labels={\"exp\": \"A\"}\n    )\n    return {\"top_uri\": results[0].uri if results else None}\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#3-memory-structured-event-result-log","title":"3. Memory \u2014 Structured Event &amp; Result Log","text":"<p>Memory is a unified fa\u00e7ade for recording, persisting, and querying events during an agent\u2019s lifetime: raw logs, typed results, metrics, and their relationships with artifacts. </p>"},{"location":"key-concepts/artifacts-memory/#why-memory-design-intent","title":"Why Memory (design intent)","text":"<ul> <li>Contextual recall: agents can react based on recent or historical state.</li> <li>Typed outputs: <code>write_result</code> records semantic outputs with names/kinds/values.</li> <li>RAG\u2011ready: promote events to a vector index for retrieval\u2011augmented answers.</li> <li>Analytics: retrieve last actions for logical connection, track trends, or export for traceability.</li> </ul>"},{"location":"key-concepts/artifacts-memory/#architecture_1","title":"Architecture","text":"<pre><code>[ Your Agent / Tool ]\n          \u2502   (context)\n          \u25bc\n[ NodeContext ]\n          \u2502\n          \u25bc\n[ context.memory() \u2014 Memory Facade ]\n        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502               \u2502                  \u2502               \u2502              \u2502\n        \u25bc               \u25bc                  \u25bc               \u25bc              \u25bc\n   [ HotLog ]      [ Persistence ]     [ Indices ]     [ Summaries ]  [ RAG ]\n (ephemeral KV)   (JSONL append-only)   (SQLite/KV)     (LLM-based)   (FAISS)\n        \u2502               \u2502                  \u2502               \u2502              \u2502\n   recent()/tail   replay/export      last_by_name     distill_*      search/answer\n                                      last_outputs                     promote_events\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#core-api_1","title":"Core API","text":"Method Purpose <code>record_raw()</code> Append a low\u2011level event. <code>record()</code> Convenience structured logging. <code>write_result()</code> Log a typed output; updates indices. <code>recent()</code> / <code>recent_data()</code> Fetch most recent events / event data <code>last_by_name()</code> Get the latest output value by name. <code>rag_bind()</code> / <code>rag_promote_events()</code> / <code>rag_answer()</code> RAG lifecycle helpers (requires LLM)."},{"location":"key-concepts/artifacts-memory/#examples_1","title":"Examples","text":"<p>Record an event</p> <pre><code>@graph_fn(name=\"remember_output\", outputs=[\"y\"])\nasync def remember_output(x: int, *, context):\n    y = x + 1\n    await context.memory().record(kind=\"cal.result\", data={\"y\": y})\n    return {\"y\": y}\n</code></pre> <p>Recall + summarization</p> <pre><code>recent = await context.memory().recent(limit=10) # return list of events\n</code></pre> <p>Promote to RAG</p> <pre><code>corpus = await context.memory().rag_bind()\nawait context.memory().rag_promote_events(corpus_id=corpus)\nans = await context.memory().rag_answer(corpus_id=corpus, question=\"What was the best run?\")\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#4-artifacts-memory-better-together","title":"4. Artifacts \u00d7 Memory \u2014 Better Together","text":"<p>Artifacts and Memory reference each other: results and metrics point to artifact URIs; artifact metadata references the node that produced them. This bi\u2011directional linking enables:</p> <ul> <li>Reconstructing the full story of a result (inputs \u2192 tools \u2192 outputs \u2192 files).</li> <li>Ranking/searching results across runs/experiments.</li> <li>Efficient clean\u2011up strategies (e.g., keep pinned/best; GC the rest).</li> </ul>"},{"location":"key-concepts/artifacts-memory/#5-extensibility-external-systems","title":"5. Extensibility &amp; External Systems","text":"<p>AetherGraph\u2019s built\u2011ins for Artifacts and Memory are part of the OSS core runtime and are not swappable in place. That is intentional: we rely on their stable semantics for provenance, lineage, and tooling. If you need custom memory or storage systems (local or cloud), see Extending Context Services for <code>Service</code> APIs.</p>"},{"location":"key-concepts/artifacts-memory/#summary","title":"Summary","text":"<ul> <li>Artifacts make outputs durable, searchable, and reproducible with CAS URIs and rich indexing.</li> <li>Memory records the event stream and typed results for contextual recall, analytics, and RAG.</li> <li>Together they provide end\u2011to\u2011end provenance and effortless \u201ctime travel\u201d across runs.</li> </ul> <p>See also: <code>context.artifacts()</code> \u00b7 <code>context.memory()</code> \u00b7 <code>context.rag()</code> \u00b7 External Context Services</p>"},{"location":"key-concepts/channels-interaction/","title":"Channels and Interaction","text":"<p>A channel is how an agent communicates with the outside world \u2014 Slack, Telegram, Console, Web, or any other adapter. The <code>context.channel()</code> method returns a ChannelSession, a lightweight helper that provides a consistent Python API for sending and receiving messages, buttons, files, streams, and progress updates \u2014 regardless of which adapter you use.</p> <p>Default behavior: If no adapters are configured, AetherGraph automatically uses the console (<code>\"console:stdin\"</code>) as the default channel for input/output. To target Slack, Telegram, or Web, see Channel Setup section; your agent code remains unchanged in all channels.</p> <p>In short: Switch communication targets freely. The agent logic stays identical.</p>"},{"location":"key-concepts/channels-interaction/#1-what-is-a-channel","title":"1. What Is a Channel?","text":"<p>A channel is a routing target for interaction. It allows you to interact with an Agent inside a Python function. </p> <p>You can specify a channel key or alias (e.g., <code>\"slack:#research\"</code>) or rely on the system default. See Channel Setup for non-console key setup. </p>"},{"location":"key-concepts/channels-interaction/#resolution-order","title":"Resolution Order","text":"<ol> <li>Per-call override: <code>await context.channel().send_text(\"hi\", channel=\"slack:#alerts\")</code></li> <li>Bound session key: <code>ch = context.channel(\"slack:#research\"); await ch.send_text(\"hi\")</code></li> <li>Bus default: taken from <code>services.channels.get_default_channel_key()</code></li> <li>Fallback: <code>console:stdin</code></li> </ol>"},{"location":"key-concepts/channels-interaction/#2-quick-start","title":"2. Quick Start","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"channel_demo\")\nasync def channel_demo(*, context):\n    ch = context.channel(\"slack:#research\")\n    await ch.send_text(\"Starting experiment\u2026\")\n    resp = await ch.ask_approval(\"Proceed?\", options=[\"Yes\", \"No\"])\n    if resp[\"approved\"]:\n        await ch.send_text(\"\u2705 Launching run.\")\n</code></pre> <p>Channels handle both output and input asynchronously \u2014 messages, approvals, file uploads, and more \u2014 using cooperative waits under the hood.</p>"},{"location":"key-concepts/channels-interaction/#3-core-methods","title":"3. Core Methods","text":"<p>Availability depends on the adapter\u2019s capabilities (e.g., file uploads are not supported in the console channel).</p> Method Purpose <code>send_text()</code>  / <code>ask_text()</code> Send/ask a plain text message. <code>send_file()</code> / <code>ask_file()</code> Upload/ask a file. <code>ask_approval()</code> Request approval or a choice. <code>send_buttons()</code> Send buttons to UI with links <code>stream()</code> Open a streaming session for incremental updates. <code>progress()</code> Stream a progress bar <code>get_last_uploads()</code> Fetch uploaded files from UI at anytime <p>All <code>ask_*</code> methods use event-driven continuations, ensuring replies are properly correlated to their originating node.</p> <p>For exact usage, refer to <code>context.channel()</code> API. </p>"},{"location":"key-concepts/channels-interaction/#4-concurrency-and-fan-out","title":"4. Concurrency and Fan-Out","text":"<p>You can launch multiple concurrent asks in the same bound channel session and correlate the results:</p> <pre><code>import asyncio\n\n@graph_fn(name=\"concurrent_asks\")\nasync def concurrent_asks(*, context):\n    ch = context.channel(\"slack:#research\")\n\n    async def one(tag):\n        name = await ch.ask_text(f\"[{tag}] What\u2019s your name?\")\n        await ch.send_text(f\"[{tag}] Thanks, {name}!\")\n        return {tag: name}\n\n    a, b = await asyncio.gather(one(\"A\"), one(\"B\"))\n    return {\"names\": a | b}\n</code></pre>"},{"location":"key-concepts/channels-interaction/#5-extensibility","title":"5. Extensibility","text":"<p>The channel interface can be extended to support any platform with a compatible API (HTTP, WebSocket, SDK). In practice, the inbound method for resuming interactions depends heavily on the target platform\u2019s event model.</p> <ul> <li>For notification-only channels, the API is straightforward \u2014 send events, no continuations.</li> <li>For interactive channels (e.g., Slack, Telegram, Web), resumptions rely on correlation IDs and continuation stores.</li> </ul> <p>In the OSS edition, AetherGraph currently includes built-in support for Console, Slack, Telegram, and generic Webhooks. We will release adapter protocal API for extension and support for additional adapters in future releases.</p>"},{"location":"key-concepts/channels-interaction/#summary","title":"Summary","text":"<ul> <li>Channels unify all interaction patterns (text, files, approvals, progress, and streaming) under one async API.</li> <li>Default channel is console; others (Slack, Telegram, Web) are pluggable.</li> <li>All <code>ask_*</code> methods suspend execution via event-driven continuations, resuming seamlessly upon reply.</li> <li>Channels are adapter-agnostic and fully extensible \u2014 swap backends, not code.</li> </ul> <p>Write once, interact anywhere \u2014 your agents stay Pythonic, event\u2011driven, and platform\u2011neutral.</p>"},{"location":"key-concepts/concurrency-orchestration/","title":"Concurrency, Fan\u2011In/Fan\u2011Out &amp; Graph\u2011Level Orchestration","text":"<p>AetherGraph provides Python\u2011first concurrency that works from reactive agents to scheduled DAGs. You can orchestrate parallelism naturally in Python, while the runtime enforces safe scheduling and per\u2011run concurrency caps.</p>"},{"location":"key-concepts/concurrency-orchestration/#1-graph_fn-pythonic-concurrency-for-reactive-agents","title":"1. <code>@graph_fn</code> \u2014 Pythonic Concurrency for Reactive Agents","text":"<p><code>@graph_fn</code> functions execute through normal Python async semantics. Plain Python awaits run directly on the event loop, while any <code>@tool</code> calls inside a <code>@graph_fn</code> become implicit nodes managed by the agent\u2019s internal scheduler.</p> <p>Example: bounded fan\u2011out using a semaphore</p> <pre><code>import asyncio\nfrom aethergraph import graph_fn\n\nsem = asyncio.Semaphore(4)  # cap concurrent jobs (user-managed)\n\nasync def run_capped(fn, **kw):\n    async with sem:\n        return await fn(**kw)\n\n@graph_fn(name=\"batch_agent\")\nasync def batch_agent(items: list[str], *, context):\n    async def one(x):\n        await context.channel().send_text(f\"processing {x}\")\n        return {\"y\": x.upper()}\n\n    # fan\u2011out with manual cap\n    tasks = [run_capped(one, x=v) for v in items]\n    results = await asyncio.gather(*tasks)\n\n    # fan\u2011in\n    return {\"ys\": [r[\"y\"] for r in results]}\n</code></pre> <p>Notes:</p> <ul> <li>Plain Python steps execute immediately \u2014 not capped by the scheduler.</li> <li><code>@tool</code> calls are scheduled and counted toward the agent\u2019s concurrency cap through <code>max_concurrency</code> (default = 4).</li> <li>You can override per\u2011run limits by passing <code>max_concurrency=&lt;int&gt;</code> to <code>run()</code> or <code>run_async()</code> or use <code>graph_fn(.., max_concurrency=&lt;int&gt;)</code>.</li> <li>For nested or composed agents, effective concurrency multiplies; use semaphores or pools to control load.</li> <li>Ideal for reactive, exploratory agents or mixed I/O + compute logic.</li> </ul>"},{"location":"key-concepts/concurrency-orchestration/#2-graphify-schedulercontrolled-static-dags","title":"2. <code>@graphify</code> \u2014 Scheduler\u2011Controlled Static DAGs","text":"<p>In static DAGs built with <code>@graphify</code>, every <code>@tool</code> call becomes a node in a TaskGraph. Concurrency is automatically managed by the runtime scheduler, respecting per\u2011run limits.</p> <p>Minimal fan\u2011in/fan\u2011out example:</p> <pre><code>from aethergraph import graphify, tool\n\n@tool(outputs=[\"result\"])\nasync def pick(items: list[int], index: int):\n    return {\"result\": items[index]}\n\n@tool(outputs=[\"out\"])\nasync def work(x: int):\n    print(f\"Working on {x}...\")\n    return {\"out\": x * 2}\n\n@tool(outputs=[\"sum\"])\nasync def reduce_sum(xs: list[int]):\n    return {\"sum\": sum(xs)}\n\n@graphify(name=\"map_reduce\", inputs=[\"vals\"], outputs=[\"sum\"])\ndef map_reduce(vals):\n    results = [pick(items=vals, index=i) for i in range(len(vals))]  # We need use a tool to extract values as vals is a ref not a list \n    outs = [work(x=v.result) for v in results]       # fan\u2011out\n    total = reduce_sum(xs=[o.out for o in outs])     # fan\u2011in\n    return {\"sum\": total.sum}\n</code></pre> <p>Key points:</p> <ul> <li>The scheduler enforces <code>max_concurrency</code> automatically (default = 4).</li> <li>You can override per\u2011run limits by passing <code>max_concurrency=&lt;int&gt;</code> to <code>run()</code>, or <code>run_async()</code>.</li> <li>Static DAG concurrency is global and consistent across all tool nodes.</li> <li>Each node runs once dependencies resolve; no explicit <code>await</code> is required.</li> </ul>"},{"location":"key-concepts/concurrency-orchestration/#3-graphlevel-orchestration-patterns","title":"3. Graph\u2011Level Orchestration Patterns","text":"<p>All orchestration in AetherGraph is just Python. You can run sequentially or concurrently using standard async primitives.</p>"},{"location":"key-concepts/concurrency-orchestration/#a-sequential-orchestration-plain-python","title":"A) Sequential orchestration (plain Python)","text":"<pre><code>res1 = await graph_fn1(a=1, max_concurrency=N) # graph-level concurrency\nres2 = await graph_fn2(b=2, max_concurrency=N)\n</code></pre>"},{"location":"key-concepts/concurrency-orchestration/#b-concurrent-graph_fn-runs-asyncfriendly","title":"B) Concurrent <code>graph_fn</code> runs (async\u2011friendly)","text":"<pre><code>res1, res2 = await asyncio.gather(\n    graph_fn1(a=1, max_concurrency=N),\n    graph_fn2(b=2, max_concurrency=N),\n)\n</code></pre>"},{"location":"key-concepts/concurrency-orchestration/#c-concurrent-graph-runner-works-for-both-graph_fn-and-graphify","title":"C) Concurrent graph runner (works for both <code>graph_fn</code> and <code>graphify</code>)","text":"<pre><code>from aethergraph.runner import run_async\n\nres1, res2 = await asyncio.gather(\n    run_async(graph1, inputs={\"a\": 1}, max_concurrency=8),\n    run_async(graph2, inputs={\"b\": 2}, max_concurrency=2),\n)\n</code></pre> <p>Default concurrency for each graph is 4, but you can override it per call with <code>max_concurrency</code> in either <code>run()</code> or <code>run_async()</code>. Becareful of global concurrency limit. Use semaphores or pools to control load.  Do not use <code>runner.run()</code> for concurrent graph runs.</p>"},{"location":"key-concepts/concurrency-orchestration/#4-concurrency-comparison","title":"4. Concurrency Comparison","text":"Aspect <code>@graph_fn</code> (Reactive) <code>@graphify</code> (Static) Concurrency Control Automatic via scheduler (<code>max_concurrency</code>) Automatic via scheduler (<code>max_concurrency</code>) Default Limit Default 4 per run, multiply with nested calls Default 4 per run Plain Python Awaitables Run immediately, outside scheduler Not applicable (only tool nodes) Nested Calls Supported Not yet supported Failure Behavior Caught at runtime; user decides Scheduler stops on first error (configurable) Use Case Agents, exploration, hybrid control Pipelines, batch workflows, reproducible DAGs"},{"location":"key-concepts/concurrency-orchestration/#takeaways","title":"Takeaways","text":"<ul> <li>Reactive vs Deterministic: <code>graph_fn</code> for interactive exploration; <code>graphify</code> for reproducible pipelines.</li> <li>Fan\u2011In/Fan\u2011Out: Async patterns in <code>graph_fn</code>; data edges in <code>graphify</code>.</li> <li>Concurrency Control: Default cap = 4; override per run with <code>max_concurrency</code>.</li> <li>Scalability: Local schedulers per agent; a global scheduler orchestrates multiple runs.</li> <li>Everything is Python: The runtime extends standard async execution into persistent, inspectable DAG scheduling.</li> </ul>"},{"location":"key-concepts/context-services/","title":"Context Services Overview","text":"<p>Context is the lightweight runtime handle that every agent and tool receives during execution. It represents the active run, graph, and node scope and exposes AetherGraph\u2019s built-in runtime services\u2014channels, memory, artifacts, logs, and more\u2014through a clean, Pythonic interface.</p> <p>In short: Context is what makes an AetherGraph program \u201calive.\u201d It bridges your pure Python logic with interactive I/O, persistence, orchestration, and AI-powered capabilities\u2014without introducing a new DSL or framework-specific syntax.</p>"},{"location":"key-concepts/context-services/#1-why-context-matters","title":"1. Why Context Matters","text":"<p>AetherGraph\u2019s guiding principle is Python-first orchestration. The context system makes that possible by providing a unified way to connect logic and infrastructure.</p> <p>Core benefits:</p> <ul> <li>Decoupled logic: Agents and tools can call <code>context.&lt;service&gt;()</code> without worrying about back-end details or deployment environment.</li> <li>Automatic provenance: Each call carries its <code>run_id</code>, <code>graph_id</code>, and <code>node_id</code>, ensuring full traceability.</li> <li>Zero-friction orchestration: Handles message passing, persistence, and coordination transparently.</li> <li>Optional intelligence: Attach LLMs, RAG corpora, or MCP servers only when needed\u2014no dependencies until configured.</li> </ul> <p>In practice, <code>NodeContext</code> turns plain async functions into interactive, stateful agents that can communicate, remember, reason, and orchestrate\u2014all from Python.</p>"},{"location":"key-concepts/context-services/#2-quick-start","title":"2. Quick Start","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"hello_context\")\nasync def hello_context(*, context):\n    await context.channel().send_text(\"Hello from AetherGraph!\")\n    await context.memory().record(\n        kind=\"chat_data\",\n        data={\"message\": \"Hello from AetherGraph!\"},  \n    ) # remember key events\n    context.logger().info(\"finished\", extra={\"stage\": \"done\"})\n    return {\"ok\": True}\n</code></pre> <p>Each call operates within a specific node scope. The runtime automatically provides <code>run_id</code>, <code>graph_id</code>, and <code>node_id</code> to maintain context and provenance.</p>"},{"location":"key-concepts/context-services/#3-context-structure","title":"3. Context Structure","text":"<p>Each <code>NodeContext</code> carries stable identifiers and bound service references.</p> <pre><code>@dataclass\nclass NodeContext:\n    run_id: str\n    graph_id: str\n    node_id: str\n    services: NodeServices  # all bound runtime services\n</code></pre> <p>Identifiers</p> <ul> <li>run_id \u2014 unique per execution run.</li> <li>graph_id \u2014 identifies which graph the node belongs to.</li> <li>node_id \u2014 unique ID for the node invocation.</li> </ul>"},{"location":"key-concepts/context-services/#4-context-services","title":"4. Context Services","text":"<p>AetherGraph organizes its context services into core, optional, and utility layers.</p>"},{"location":"key-concepts/context-services/#core-services","title":"Core Services","text":"Method Purpose <code>context.channel()</code> Message and interaction bus \u2014 send/receive text/approval/files, show progress, or streaming events. <code>context.memory()</code> Memory fa\u00e7ade \u2014 record events, write typed results, query history, or manage RAG-ready logs. <code>context.artifacts()</code> Artifact store fa\u00e7ade \u2014 save/retrieve files, track outputs, and query files by labels/metrics artifacts. <code>context.kv()</code> Lightweight key\u2013value store for ephemeral coordination and small caches. <code>context.logger()</code> Structured logger with <code>{run_id, graph_id, node_id}</code> metadata automatically included."},{"location":"key-concepts/context-services/#optional-services-config-dependent","title":"Optional Services (config-dependent)","text":"<p>Optional services require API keys or runtime configuration. They are injected dynamically when available.</p> Method Purpose <code>context.llm()</code> Access an LLM client for chat, embeddings, or raw apis access (OpenAI, Anthropic, Google, or local backends, etc.). <code>context.rag()</code> Retrieval-augmented generation fa\u00e7ade \u2014 build corpora, upsert documents, search, and answer queries. <code>context.mcp()</code> Connect to external MCP tool servers via stdio, WebSocket, or HTTP."},{"location":"key-concepts/context-services/#utility-helpers","title":"Utility Helpers","text":"Method Purpose <code>context.clock()</code> Clock utilities for timestamps, delays, and scheduling. <code>context.continuations()</code> Access continuation store; used internally for dual-stage waits (<code>ask_text</code>, <code>ask_approval</code>). <p>If a service is unavailable, its accessor raises a clear runtime error (e.g., <code>LLMService not available</code>). Configure them globally or per-environment to enable.</p>"},{"location":"key-concepts/context-services/#5-typical-patterns","title":"5. Typical Patterns","text":""},{"location":"key-concepts/context-services/#1-ask-wait-resume","title":"1 Ask \u2192 Wait \u2192 Resume","text":"<pre><code>text = await context.channel().ask_text(\"Provide a dataset path\") \n# wait for external input and resume when done\nawait context.channel().send(f\"you provided the dataset path {text}\")\n</code></pre>"},{"location":"key-concepts/context-services/#2-artifacts-memory","title":"2 Artifacts + Memory","text":"<pre><code># save and return an artifact \nart = await context.artifacts().save(path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\": \"A\"}) \n# save and return an event  \nevt = await context.memory().record(kind=\"checkpoint\", data={\"info\": \"experiment A saved\"}) \n\n# In later stage or other agent:\n# list all previous saved art with kind == \"report\"\narts = await context.artifacts().search(kind=\"report\")      \n# list past 100 memory with kinds include \"checkpoint\" \nevts = await context.memory().recent(kinds=[\"checkpoint\"], limit=100)    \n</code></pre>"},{"location":"key-concepts/context-services/#4-rag-llm-answers","title":"4 RAG + LLM Answers","text":"<pre><code># ingest data into vector DB\ncorpus_id = \"notes\"\n_ = await context.rag().upsert_docs(corpus_id, docs)  # your documentations in a list \n\n# later search/answer\nhits = await context.rag().search(corpus_id, query=\"Tool used in experiment #A2?\", k=5)\nans = await context.rag().answer(corpus_id, question=\"What is the best iteration and what is the loss?\", style=\"concise\")\nawait context.channel().send_text(ans[\"answer\"])\n</code></pre>"},{"location":"key-concepts/context-services/#5-external-tools-via-mcp","title":"5 External Tools via MCP","text":"<pre><code>res = await context.mcp(\"ws\").call(\"search\", {\"q\": \"tolerance analysis\", \"k\": 5})\n</code></pre>"},{"location":"key-concepts/context-services/#6-custom-context-services","title":"6. Custom Context Services","text":"<p>The context system is fully extensible. You can define your own service and expose it via <code>context.&lt;name&gt;()</code> using <code>register_context_service()</code>.</p> <p>Use cases:</p> <ul> <li>Add domain-specific APIs (e.g., simulation, materials DB, experiment tracking).</li> <li>Provide custom persistence or distributed coordination layers.</li> <li>Implement bridges between external systems (e.g., job schedulers, cloud storage, or lab devices).</li> </ul> <p>See External Context Services for API details and examples.</p>"},{"location":"key-concepts/context-services/#7-design-philosophy","title":"7. Design Philosophy","text":"<ul> <li>Python-first: use direct calls, not DSL syntax.</li> <li>Minimal surface: each service follows a small, composable API.</li> <li>Composable orchestration: mix local and remote services freely.</li> <li>Swappable backends: replace LLM, KV, or artifact backends without touching agent logic.</li> </ul>"},{"location":"key-concepts/context-services/#see-also","title":"See Also","text":"<ul> <li>[<code>context.channel()</code>] \u2014 cooperative waits, streaming, progress updates</li> <li>[<code>context.memory()</code>] \u2014 event log, typed results, summaries, and RAG helpers</li> <li>[<code>context.artifacts()</code>] \u2014 content-addressable storage and retrieval</li> <li>[<code>context.llm()</code>] \u2014 chat, completion, and embeddings</li> <li>[<code>context.rag()</code>] \u2014 corpus creation and QA retrieval</li> <li>[<code>context.mcp()</code>] \u2014 bridges to external tool servers</li> <li>[<code>context.kv()</code>] \u2014 transient coordination and state passing</li> <li>[<code>context.logger()</code>] \u2014 structured</li> </ul>"},{"location":"key-concepts/event-driven-waits/","title":"Event\u2011Driven Waits: Cooperative vs Dual\u2011Stage","text":"<p>AetherGraph agents are event\u2011driven: they can pause mid\u2011flow and safely resume when a reply, upload, or callback arrives. There are two complementary wait modes, and you can use them flexibly in both <code>@graph_fn</code> and <code>@graphify</code>\u2011built graphs.</p> <ul> <li>Cooperative waits \u2014 via <code>context.channel().ask_*</code>. Simplest way to prompt + wait in reactive agents.</li> <li>Dual\u2011stage waits \u2014 via <code>@tool</code> nodes that split into Stage A (prompt/setup) and Stage B (resume/produce). Best for static graphs and reliable orchestration.</li> </ul> <p>Flexibility: <code>context.*</code> methods are available inside <code>@tool</code> nodes (therefore inside <code>@graphify</code>). Dual\u2011stage tools can also be <code>await</code>\u2011ed directly inside <code>@graph_fn</code>. In either case, they form a node and persist a continuation.</p>"},{"location":"key-concepts/event-driven-waits/#1-cooperative-waits-channelfirst","title":"1 Cooperative Waits (Channel\u2011first)","text":"<p>What: <code>context.channel().ask_text / ask_approval / ask_files</code> send a prompt and yield until a reply or timeout. The runtime persists a continuation token so the run can resume after restarts.</p> <p>Where: Primarily inside <code>@graph_fn</code>. Can also be called from within a <code>@tool</code> if you want cooperative logic inside a node.</p> <p>Example</p> <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"cooperative_demo\", outputs=[\"msg\"]) \nasync def cooperative_demo(*, context):\n    name = await context.channel().ask_text(\"Your name?\")\n    await context.channel().send_text(f\"Hi, {name}!\")\n    return {\"msg\": f\"greeted:{name}\"}\n</code></pre> <p>Properties</p> <ul> <li>Minimal code, great for exploratory, chat\u2011style agents.</li> <li>Thread/channel\u2011aware correlation.</li> <li>Durable continuations; survives restarts.</li> </ul>"},{"location":"key-concepts/event-driven-waits/#2-dualstage-waits-toolfirst","title":"2 Dual\u2011Stage Waits (Tool\u2011first)","text":"<p>What: A node splits into two stages: A emits the prompt/sets up state, B resumes once the event arrives and produces outputs. Maps cleanly to static DAGs and lets the global scheduler manage resumptions and retries.</p> <p>Use in both places:</p> <ul> <li>In <code>@graphify</code> as standard tool nodes.</li> <li>In <code>@graph_fn</code> with <code>await</code> for immediate use \u2014 they still become nodes under the hood.</li> </ul> <p>Built\u2011in channel tools</p> <pre><code># Use these in either style:\nfrom aethergraph.tools import ask_text, ask_approval, ask_files\n\n# A) Inside a static graph\nfrom aethergraph import graphify\n\n@graphify(name=\"collect_input\", inputs=[], outputs=[\"greeting\"]) \ndef collect_input():\n    name = ask_text(prompt=\"Your name?\")      # node yields \u2192 resumes on reply\n    return {\"greeting\": name.text}\n\n# B) Await directly in a graph_fn\nfrom aethergraph import graph_fn\n\n@graph_fn(name=\"dualstage_in_fn\", outputs=[\"choice\"]) \nasync def dualstage_in_fn(*, context):\n    res = await ask_approval(prompt=\"Proceed?\", options=(\"Yes\",\"No\"))\n    return {\"choice\": res[\"choice\"]}\n</code></pre> <p>Properties</p> <ul> <li>Node\u2011level persistence, retries, and metrics.</li> <li>Works seamlessly with global scheduling (centralized control, resumptions at scale).</li> <li>Great for UI + pipeline hybrids (prompt in Stage A, compute in Stage B).</li> </ul>"},{"location":"key-concepts/event-driven-waits/#3-using-context-inside-graphify","title":"3 Using <code>context.*</code> inside <code>@graphify</code>","text":"<p><code>context</code> methods (channels, memory, artifacts, kv, logger, etc.) are available inside <code>@tool</code> nodes. This means your static graphs can still interact, log, and persist during node execution while retaining DAG inspectability.</p> <pre><code>from aethergraph import tool\n\n@tool(outputs=[\"ok\"]) \nasync def notify_and_tag(*, context):\n    await context.channel().send_text(\"Started node\u2026\")\n    await context.memory().record(kind=\"status\", data={\"stage\":\"start\"})\n    return {\"ok\": True}\n</code></pre>"},{"location":"key-concepts/event-driven-waits/#4-comparison-cooperative-vs-dualstage-vs-manual-checkpoints","title":"4 Comparison: Cooperative vs Dual\u2011Stage vs Manual Checkpoints","text":"Aspect Cooperative (<code>context.channel().ask_*</code>) Dual-Stage (<code>@tool</code> ask_*) Manual checkpoints Authoring style Inline, minimal Explicit node with A/B stages N/A in AG (not built-in) Resumability Hard \u2014 stateless unless save the state to memory manually Native continuations per node (resumeable after restart) Possible but manual/fragile Retry / Idempotency Coarse (re-invoke the whole function) Fine (node-level retry, idempotent resumes) Manual Scale Great for interactive sessions, small graphs Excellent for large runs / thousands of waits Limited by implementation CPU load (waiting) Keeps process / event loop alive; lightweight but not zero Zero CPU \u2014 node is dormant until resumed Depends on checkpointing backend Memory footprint Held in local task heap (light) Released after serialization; only metadata retained Depends on snapshot granularity Disk usage Optional if memory writes used Tiny (~1\u201310 KB per node) \u2014 correlator + inputs serialized Potentially heavy (full state dump) Latency to resume Instant within current process Slightly higher (resume event \u2192 lookup \u2192 dispatch) Potentially high (manual restore) <p>Why Dual\u2011Stage scales</p> <ul> <li>Node\u2011granular control: retries, backoff, and resumption are local to the waiting node.</li> <li>Central orchestration: the global scheduler can queue, shard, or migrate blocked nodes.</li> <li>Observability: each wait is a first\u2011class node with metrics and logs.</li> <li>Determinism: Stage boundaries clarify side\u2011effects and make runs reproducible.</li> </ul> <p>Manual checkpoints (framework\u2011agnostic snapshots) aren\u2019t part of AetherGraph. Dual\u2011stage nodes cover the same reliability space with less boilerplate and better provenance.</p>"},{"location":"key-concepts/event-driven-waits/#5-extending-dualstage-tools","title":"5 Extending Dual\u2011Stage Tools","text":"<p>You can author custom dual\u2011stage nodes with <code>DualStageTool</code> to model your own A/B waits (e.g., submit job \u2192 wait \u2192 collect). Some examples of the usage include </p> <ul> <li>custom channel waits</li> <li>submit/run long simualtion on cloud</li> <li>data/model training pipeline on external systems</li> <li>external API Polling that reports a compleltion asynchronously</li> </ul> <p>A compact public API for this is planned; detailed docs will ship soon.</p>"},{"location":"key-concepts/event-driven-waits/#6-takeaways","title":"6 Takeaways","text":"<ul> <li>All <code>context.channel().ask_*</code> calls are cooperative waits by default.</li> <li>Dual\u2011stage tools work in both <code>@graphify</code> and <code>@graph_fn</code> (awaitable) and always materialize as nodes.</li> <li>For large, reliable systems: prefer dual\u2011stage for node\u2011level retries, metrics, and scheduler control.</li> <li><code>context.*</code> is available inside <code>@tool</code> nodes, so static graphs can still interact, log, and persist cleanly.</li> <li>Manual checkpointing isn\u2019t needed; dual\u2011stage nodes give better reliability with less boilerplate.</li> </ul>"},{"location":"key-concepts/extending-context/","title":"Extending Context Services","text":"<p>AetherGraph lets you extend the runtime by adding your own <code>context.&lt;name&gt;</code> methods. These external context services live alongside built\u2011ins like <code>channel</code>, <code>memory</code>, and <code>artifacts</code>, and provide reusable, lifecycle\u2011aware helpers for clients, caches, orchestration, or domain APIs \u2014 without changing your agent code.</p> <p>Key idea: keep agent logic pure\u2011Python; move integration glue and shared state into services that the runtime injects per node.</p>"},{"location":"key-concepts/extending-context/#1-what-is-an-external-context-service","title":"1. What is an External Context Service?","text":"<p>A context service is a registered Python object bound into every <code>NodeContext</code>. After registration, you can use it anywhere inside a graph or tool:</p> <pre><code>@graph_fn(name=\"demo\")\nasync def demo(*, context):\n    info = await context.trainer().inspect_job(job_id=\"abc123\")\n    return {\"status\": info[\"status\"]}\n</code></pre>"},{"location":"key-concepts/extending-context/#whywhen-to-use","title":"Why/When to Use","text":"<ul> <li>Reusable helpers \u2014 share clients (e.g., HPC, S3, DB, solver), connection pools, or token buckets.</li> <li>Shared state \u2014 memoize expensive lookups; coordinate across nodes within a run.</li> <li>Centralized config \u2014 keep API keys, timeouts, routing, or policies in one place.</li> <li>Per\u2011node awareness \u2014 access to built-in services through <code>self.ctx()</code> for provenance or multi\u2011tenancy.</li> </ul> <p>Use a service for long\u2011lived instances or cross\u2011node coordination. For tiny stateless helpers, plain imports are fine.</p>"},{"location":"key-concepts/extending-context/#2-naming-boundaries-important","title":"2. Naming &amp; Boundaries (Important)","text":"<p>Built\u2011ins (<code>context.artifacts()</code>, <code>context.memory()</code>, etc.) are not swappable in OSS. To extend the system, register new services with new names (e.g., <code>context.trainer()</code>, <code>context.datasets()</code>, <code>context.lineage_store()</code>).</p> <ul> <li>Keep agent code explicit about which storage or API it\u2019s using.</li> <li>If mirroring/exporting, record links (artifact URIs, memory event IDs) inside your external system for provenance.</li> </ul>"},{"location":"key-concepts/extending-context/#3-minimal-service-instancebased","title":"3. Minimal Service (Instance\u2011based)","text":""},{"location":"key-concepts/extending-context/#define-a-service-and-use-selfctx","title":"Define a service and use <code>self.ctx()</code>","text":"<pre><code>from aethergraph import Service\n\nclass Trainer(Service):\n    async def submit(self, spec: dict) -&gt; str:\n        # Submit a training job to your HPC/cluster\n        ... \n        return job_id\n\n    async def inspect_job(self, job_id: str) -&gt; dict:\n        # Inspect the job status\n        ...\n        return {\"job_id\": job_id, \"status\": status}\n</code></pre>"},{"location":"key-concepts/extending-context/#register-at-startup-pass-an-instance","title":"Register at startup (pass an instance)","text":"<pre><code>from aethergraph.runtime import register_context_service\nfrom aethergraph import start_server()\n\n# register after server is started \nstart_server() \nregister_context_service(\"trainer\",  Trainer())\n</code></pre> <p>After this, <code>context.trainer()</code> is available everywhere in the runtime.</p>"},{"location":"key-concepts/extending-context/#4-usage-patterns","title":"4. Usage Patterns","text":""},{"location":"key-concepts/extending-context/#a-submit-training-link-artifacts","title":"A) Submit training &amp; link artifacts","text":"<pre><code>@graph_fn(name=\"train_model\", outputs=[\"job_id\", \"ckpt_uri\"]) \nasync def train_model(spec: dict, *, context):\n    #  Submit to your cluster via the custom service\n    job_id = await context.trainer().submit(spec)\n    return {\"job_id\": job_id, \"ckpt_uri\": ckpt.uri}\n</code></pre>"},{"location":"key-concepts/extending-context/#b-inspect-status-in-another-nodetool","title":"B) Inspect status in another node/tool","text":"<pre><code>@tool(name=\"wait_for_training\", outputs=[\"ready\"]) \nasync def wait_for_training(job_id: str, *, context) -&gt; dict:\n    # Inspect you job through your service\n    info = await context.trainer().inspect_job(job_id)\n    return {\"ready\": info[\"status\"] == \"COMPLETED\"}\n</code></pre>"},{"location":"key-concepts/extending-context/#5-concurrency-lifecycle","title":"5. Concurrency &amp; Lifecycle","text":"<p>If you expect your services are accessed by multiple agents concurrently, consider the designs: </p> <ul> <li>Lifecycle hooks: <code>start()</code> / <code>close()</code> are optional; call them from your app/server bootstrap.</li> <li>Shared access: use <code>self.critical()</code> to protect mutable shared state. Design your own mutex when scaling up. </li> <li>Per\u2011node context: call <code>self.ctx()</code> whenever you need <code>{run_id, graph_id, node_id}</code>.</li> <li>Async native: expose async APIs; if integrating queues, consider <code>asyncio.Queue</code>.</li> </ul>"},{"location":"key-concepts/extending-context/#6-common-service-patterns-examples","title":"6. Common Service Patterns (Examples)","text":"Scenario Suggested accessor What it abstracts Typical operations HPC / Training orchestration <code>context.trainer()</code> Submit/track jobs on Slurm/K8s/Ray <code>submit(spec)</code>, <code>inspect_job(id)</code>, <code>cancel(id)</code> External object storage <code>context.storage()</code> S3/GCS/MinIO buckets &amp; signed URLs <code>put(path)</code>, <code>get(uri)</code>, <code>sign(uri)</code>, <code>list(prefix)</code> Vendor API client <code>context.apiclient()</code> Rate\u2011limited, retried HTTP SDK <code>get/put/post</code>, <code>batch()</code>, <code>retry/backoff</code> In\u2011house AI models <code>context.models()</code> Local inference endpoints <code>embed(texts)</code>, <code>generate(prompt)</code> Materials DB / domain registry <code>context.materials()</code> Domain lookups &amp; cached tables <code>get_index(name)</code>, <code>search(filters)</code> Lineage export <code>context.lineage_store()</code> Mirror core provenance to BI/warehouse <code>export_run(run_id)</code>, <code>push(events)</code> <p>Pick names that are explicit in your org (e.g., <code>context.k8s_jobs()</code>, <code>context.minio()</code>). Avoid names that shadow built\u2011ins.</p>"},{"location":"key-concepts/extending-context/#summary","title":"Summary","text":"<ul> <li>External services add named capabilities to <code>context</code> without changing agent code.</li> <li>Built\u2011ins remain stable; extend via new names (no in\u2011place swaps).</li> <li>Register instances, not factories; services run on the main event loop.</li> <li>Use <code>self.ctx()</code> to fetch per\u2011node provenance on demand; protect shared state with <code>critical()</code> or your own lock design.</li> </ul>"},{"location":"key-concepts/introduction/","title":"AetherGraph \u2014 Key Concepts Overview","text":"<p>AetherGraph rethinks how agentic systems are built: graphs are agents, context is the runtime fabric, and execution is event\u2011driven. This document lays out the philosophy and the essential architecture so you can quickly reason about how it differs from typical frameworks\u2014and how to use it effectively.</p>"},{"location":"key-concepts/introduction/#1-introduction-graphs-context","title":"1. Introduction: Graphs + Context","text":"<p>AetherGraph departs from most agent frameworks in two fundamental ways:</p> <ol> <li>Every agent is a graph. You model behavior as a directed acyclic graph (DAG) with nodes that can expand dynamically (reactive) or be planned statically.</li> <li>Every node runs inside a <code>NodeContext</code>. The context is your per\u2011node control plane that exposes rich, injectable services.</li> </ol> <p>NodeContext services include (built\u2011ins plus anything you add):</p> <ul> <li>Channels (Slack, Console/Web, \u2026) for I/O and interaction</li> <li>Artifacts (blob store) for large files and generated assets</li> <li>Memory (history + summaries + optional RAG)</li> <li>KV &amp; Logger for quick state and observability</li> <li>LLM / MCP / RAG bridges for model calls and tool use</li> <li>Your custom services registered at runtime</li> </ul> <p>The system is pythonic, reactive, and extensible: author graphs directly in Python, then let context services handle communication, persistence, and orchestration details.</p>"},{"location":"key-concepts/introduction/#2-graphs-as-agents","title":"2. Graphs as Agents","text":"<p>AetherGraph unifies two modes under one mental model:</p>"},{"location":"key-concepts/introduction/#reactive-agent-graph_fn","title":"Reactive agent \u2014 <code>@graph_fn</code>","text":"<ul> <li>Executes immediately when called (behaves like an async Python function).</li> <li>Expands into an implicit DAG as tools run.</li> <li>Perfect for interactive, service\u2011rich, or conversational workflows.</li> </ul>"},{"location":"key-concepts/introduction/#static-agent-graphify","title":"Static agent \u2014 <code>@graphify</code>","text":"<ul> <li>First builds a concrete <code>TaskGraph</code>, then executes it via the scheduler.</li> <li>Ideal for stable, deterministic, or large\u2011scale workflows.</li> </ul> <p>One model, two tempos. Iterate fast with <code>@graph_fn</code>; snap to repeatable DAGs with <code>@graphify</code>.</p>"},{"location":"key-concepts/introduction/#3-context-as-the-runtime-fabric","title":"3. Context as the Runtime Fabric","text":"<p>Every node (and each <code>graph_fn</code> invocation) receives a NodeContext. Think of it as a scoped service locator with lifecycle hooks.</p> <ul> <li>Per\u2011node scoping \u2192 independent logging, persistence, messaging.</li> <li>Injectable services \u2192 bind built\u2011ins or your own with simple registration.</li> <li>Sidecar\u2011friendly \u2192 channel adapters and stores can run out\u2011of\u2011process.</li> </ul> <p>Common patterns:</p> <ul> <li><code>context.channel().ask_text()</code> to collect input from a user/UI.</li> <li><code>context.artifacts().save(...)</code> to persist files and structured outputs.</li> <li><code>context.memory().record(...)</code> to capture provenance and summaries.</li> <li><code>context.llm().chat(...)</code> to call your configured model provider.</li> </ul>"},{"location":"key-concepts/introduction/#4-eventdriven-execution-waits","title":"4. Event\u2011Driven Execution &amp; Waits","text":"<p>AetherGraph is event\u2011driven (not polling). Nodes suspend when waiting for input or an external event; the runtime persists and resumes continuations on demand so suspended nodes consume little to no CPU/RAM. Two primary wait styles are exposed:</p> <ul> <li>Cooperative waits \u2014 inline <code>await</code> for short, interactive flows where the coroutine remains in memory for low\u2011latency response.</li> <li>Dual\u2011stage waits \u2014 split a node into setup and resume phases; the continuation is durably stored so the process can free resources until an event triggers resume.</li> </ul> Mode Description CPU/RAM Ideal for Cooperative waits Inline <code>await</code> (e.g., <code>await context.channel().ask_text()</code>); coroutine stays in memory Minimal Small reactive loops, interactive sessions Dual\u2011stage waits Split node into setup/resume; continuation stored in a durable store Zero when paused Large DAGs, long\u2011lived pipelines Manual checkpoints User\u2011managed state snapshot/restore User\u2011managed Legacy integration, custom control <p>Dual\u2011stage waits + event routing enable massive concurrency with tiny footprint.</p>"},{"location":"key-concepts/introduction/#5-execution-scheduling","title":"5. Execution &amp; Scheduling","text":"<p>Each graph runs under a scheduler that selects ready nodes and respects your concurrency caps. Within a graph you get async DAG orchestration by default.</p>"},{"location":"key-concepts/introduction/#crossgraph-orchestration","title":"Cross\u2011graph orchestration","text":"<p>AetherGraph intentionally avoids a heavyweight orchestrator in the OSS core. For concurrent runs across multiple agents/graphs, use idiomatic Python:</p> <pre><code># Concurrent reactive runs\nawait asyncio.gather(\n    graph_fn_agent_a(...),   # a @graph_fn with async __call__\n    graph_fn_agent_b(...),\n)\n\n# Sequential orchestration\nawait graph_fn_agent_a(...)\nawait graph_fn_agent_b(...)\n</code></pre>"},{"location":"key-concepts/introduction/#6-extensibility-everywhere","title":"6. Extensibility Everywhere","text":"<p>Pure Python\u2014no DSLs, no codegen.</p> <ul> <li>Tools: <code>@tool</code> to define reusable, typed primitives.</li> <li>Graphs: <code>@graphify</code> to materialize a DAG for repeatable runs.</li> <li>Services: <code>register_context_service()</code> to inject your own capabilities.</li> <li>Adapters: extend ChannelBus to new transports (Slack, Web, PyQt, \u2026).</li> </ul> <p>Example (sketch)</p> <p>Define a <code>@tool</code>: <pre><code>@tool(name=\"analyze\", outputs=[\"val\"])\ndef analyze(x: int) -&gt; int:\n    return {\"val\": x * 2}\n</code></pre></p> <p>Define a <code>@graph_fn</code> (<code>@tool</code> is optional) <pre><code>@graph_fn(name=\"demo\")\nasync def demo(*, context):\n    y = await analyze(21)\n    await context.channel().send_text(f\"Answer: {y}\")\n</code></pre></p>"},{"location":"key-concepts/introduction/#7-how-aethergraph-differs","title":"7. How AetherGraph Differs","text":"Area AetherGraph Typical frameworks Runtime Unified Python, event\u2011driven Split control planes, mixed models Waits Zero\u2011CPU dual\u2011stage waits Blocking awaits, threads, polling Context Rich per\u2011node runtime fabric Globals, hidden singletons Composition Dynamic (<code>graph_fn</code>) + Static (<code>@graphify</code>) Usually one or the other Scheduling Async DAG scheduling per graph Step\u2011based loops, less reactive Extensibility Decorators + Python APIs Fixed plugin systems Provenance Built\u2011in memory/artifacts External or ad\u2011hoc"},{"location":"key-concepts/introduction/#8-takeaways","title":"8. Takeaways","text":"<p>AetherGraph bridges interactive research and deterministic automation:</p> <ul> <li>React first, then structure for production.</li> <li>Keep state, context, and scheduling where they belong\u2014with the nodes.</li> <li>Use event\u2011driven waits to scale without idle compute.</li> </ul>"},{"location":"key-concepts/other-services-overview/","title":"Other Services Overview","text":"<p>AetherGraph\u2019s context exposes a set of lightweight, composable runtime services that complement Channels, Artifacts, and Memory. Use them when your agent needs coordination, observability, or intelligent reasoning \u2014 while keeping your core logic pure Python.</p> <p>Philosophy: keep code idiomatic; reach for <code>context.&lt;service&gt;()</code> only when you need I/O, coordination, or intelligence.</p>"},{"location":"key-concepts/other-services-overview/#1-kv-ephemeral-coordination","title":"1. KV \u2014 Ephemeral Coordination","text":"<p>A minimal key\u2013value store for transient state, synchronization, and quick signals between nodes/agents.</p> <pre><code>@graph_fn(name=\"kv_demo\", outputs=[\"ok\"])\nasync def kv_demo(*, context):\n    kv = context.kv()\n    await kv.set(\"stage\", \"preflight\", ttl_s=300)\n    stage = await kv.get(\"stage\")  # \u2192 \"preflight\"\n    return {\"ok\": stage == \"preflight\"}\n</code></pre> <p>Why/When: feature flags, locks/counters, short-lived coordination.</p> <p>Default backend: in-memory KV. </p>"},{"location":"key-concepts/other-services-overview/#2-logger-structured-logs-with-provenance","title":"2. Logger \u2014 Structured Logs with Provenance","text":"<p>Structured logging with <code>{run_id, graph_id, node_id}</code> automatically injected.</p> <pre><code>@graph_fn(name=\"log_demo\", outputs=[\"done\"])\nasync def log_demo(*, context):\n    log = context.logger()\n    log.info(\"starting\", extra={\"component\": \"ingest\"})\n    try:\n        ...\n        log.info(\"finished\", extra={\"component\": \"ingest\"})\n        return {\"done\": True}\n    except Exception:\n        log.exception(\"ingest failed\")\n        return {\"done\": False}\n</code></pre> <p>Why/When: lifecycle traces, metrics, error reporting.</p> <p>Default backend: Python <code>logging</code>.</p>"},{"location":"key-concepts/other-services-overview/#3-llm-unified-chat-embeddings-optional","title":"3. LLM \u2014 Unified Chat &amp; Embeddings (optional)","text":"<p>Provider-agnostic interface for chat/completions and embeddings. Requires configuration (API keys, profile).</p> <pre><code>@graph_fn(name=\"llm_demo\", outputs=[\"reply\"])\nasync def llm_demo(prompt: str, *, context):\n    llm = context.llm(profile=\"default\")\n    msg = await llm.chat([{\"role\": \"user\", \"content\": prompt}])\n    return {\"reply\": msg[\"content\"]}\n</code></pre> <p>Why/When: summarization, drafting, tool-use planning, embeddings.</p> <p>Backends: OpenAI, Anthropic, local, etc.</p>"},{"location":"key-concepts/other-services-overview/#4-rag-longterm-semantic-recall-optional","title":"4. RAG \u2014 Long\u2011Term Semantic Recall (optional)","text":"<p>Build searchable corpora from events/docs; retrieve or answer with citations. Requires an LLM for answering.</p> <pre><code>@graph_fn(name=\"rag_demo\", outputs=[\"answer\"])\nasync def rag_demo(q: str, *, context):\n    corpus_id: str = \"demo-corpus-inline\",\n    rag = context.rag()\n    docs = [\n        ... # you list of docs (text, markdown, or pdf path)\n    ]\n    result = await rag.upsert_docs(corpus_id, docs) # this will create and ingest into the corpus\n    ans = await rag.answer(corpus_id=corpus, question=q)\n    return {\"answer\": ans[\"answer\"]}\n</code></pre> <p>Why/When: semantic search, project recall, retrieval\u2011augmented QA.</p> <p>Default backend: Sqlite Vector DB, switchable to FAISS. </p>"},{"location":"key-concepts/other-services-overview/#5-mcp-external-tool-bridges-optional","title":"5. MCP \u2014 External Tool Bridges (optional)","text":"<p>Connect to external tool servers over stdio/WebSocket/HTTP via Model Context Protocol (MCP).</p> <pre><code>@graph_fn(name=\"mcp_demo\", outputs=[\"hits\"])\nasync def mcp_demo(*, context):\n    ws = context.mcp(\"ws\")  # adapter name\n    res = await ws.call(\"search\", {\"q\": \"tolerance analysis\", \"k\": 3})\n    return {\"hits\": res.get(\"items\", [])}\n</code></pre> <p>Why/When: integrate non-Python tools or remote services with structured contracts.</p>"},{"location":"key-concepts/other-services-overview/#takeaways","title":"Takeaways","text":"<ul> <li>Access everything through <code>context.&lt;service&gt;()</code> \u2014 no globals or custom wiring.</li> <li>KV and Logger work out of the box; LLM/RAG/MCP are optional and enabled by config.</li> <li>Backends are pluggable; you can move from local to managed services without changing agent code.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/","title":"Server (Sidecar) Overview","text":"<p>The AetherGraph sidecar is a lightweight process that boots your runtime services and exposes a tiny HTTP/WebSocket surface for adapters (Slack, Web, Telegram, \u2026) and continuations (event\u2011driven waits). With the server, you can: </p> <ul> <li>Real interactions: <code>ask_text/approval/files</code> from Slack/Web/Telegram and resume the run</li> <li>Centralized service wiring: artifacts, memory, kv, llm, rag, mcp, logger</li> <li>A shared control plane: health, upload hooks, progress streams, basic inspect</li> </ul> <p>In short: keep your agents plain Python; start the sidecar for I/O, resumability, and shared services. Always start the server before registering services and running agents</p>"},{"location":"key-concepts/server-start-sidecar/#quick-start","title":"Quick Start","text":"<pre><code>from aethergraph import start_server, stop_server\n\nurl = start_server(host=\"127.0.0.1\", port=0)  # FastAPI + Uvicorn in a background thread\nprint(\"sidecar:\", url)\n\n# ... run @graph_fn / @graphify normally ...\n\nstop_server()  # optional (handy in tests/CI)\n</code></pre> <p>Tips</p> <ul> <li>Use <code>port=0</code> to pick a free port automatically. </li> <li>Start it once per process; reuse the base URL across adapters/UI.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/#what-start_server-does","title":"What <code>start_server()</code> Does","text":"<ol> <li>Load config &amp; workspace \u2014 resolve paths, secrets, and profiles; make the workspace if needed.</li> <li>Build &amp; register services \u2014 channels, artifacts (store/index), memory (hotlog/persistence/indices), kv, llm, rag, mcp, logger.</li> <li> <p>Expose endpoints \u2014</p> <ul> <li>Continuations: resume callbacks for <code>ask_text / ask_approval / ask_files</code></li> <li>Adapters: chat/events, uploads, progress streams</li> </ul> </li> <li> <p>Launch Uvicorn \u2014 run the app in a background thread and return the base URL.</p> </li> </ol> <p>It is safe to use <code>start_server()</code> in Jupyter notebook</p>"},{"location":"key-concepts/server-start-sidecar/#minimal-api","title":"Minimal API","text":"<p><code>start_server(host=\"127.0.0.1\", port=8000, ...) -&gt; str</code></p> <p>Starts the sidecar in\u2011process and returns the base URL.</p> <p><code>start-server_async(...) -&gt; str</code></p> <p>Async\u2011friendly variant (still hosts the server in a thread), convenient inside async apps/tests.</p> <p><code>stop_server() -&gt; None</code></p> <p>Stops the background server. Useful for teardown in tests/CI.</p>"},{"location":"key-concepts/server-start-sidecar/#common-issues-fixes","title":"Common Issues &amp; Fixes","text":"<ul> <li>No reply after <code>ask_text()</code> \u2192 The adapter isn\u2019t posting resume events to the sidecar. Verify the sidecar base URL and token in the adapter config.</li> <li>CORS blocked in web UI \u2192 Allow your UI origin in sidecar settings (CORS <code>allow_origins</code>).</li> <li>Port busy \u2192 Use <code>port=0</code> or pick an open port.</li> <li>Service not available (e.g., LLM/RAG) \u2192 Ensure your <code>create_app()</code> wires those services or provide the required credentials.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/#notes-on-architecture","title":"Notes on Architecture","text":"<ul> <li>The sidecar runs its own event loop/thread; your agents/tools run on the main loop. They communicate via the ChannelBus/HTTP hooks.</li> <li>External context services you register as instances run on the main loop, so <code>asyncio</code> locks work as expected.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/#takeaways","title":"Takeaways","text":"<ul> <li>The sidecar is your local control plane: services + continuations + adapters.</li> <li>Start it with <code>start()</code> when you need interactions, persistence, or shared wiring.</li> <li>Your agent code stays plain Python either way; the sidecar simply adds I/O and resumability.</li> </ul>"},{"location":"key-concepts/static-graph-agent/","title":"Static Graphs with <code>@graphify</code>","text":"<p><code>@graphify</code> transforms a plain Python function into a graph builder. Instead of running immediately (like <code>@graph_fn</code>), it constructs a TaskGraph from <code>@tool</code> calls \u2014 a reusable, explicit DAG that you can run later.</p> <p>In short:</p> <ul> <li><code>@graph_fn</code> \u2192 executes now (reactive, dynamic)</li> <li><code>@graphify</code> \u2192 builds first, runs later (deterministic DAG)</li> </ul>"},{"location":"key-concepts/static-graph-agent/#1-what-is-a-static-graph","title":"1. What Is a Static Graph?","text":"<p>A static graph is a declarative DAG of tool nodes and dependencies. Each node is a <code>@tool</code> call; edges represent data flow or forced ordering. You build it once, then you can inspect, persist, visualize, and run it repeatedly.</p> <p>Why static? Repeatability, inspectability, resumability, and clear fan\u2011in/fan\u2011out. Static graphs shine for pipelines and reproducible experiments where determinism and analysis matter.</p>"},{"location":"key-concepts/static-graph-agent/#2-graphify-vs-graph_fn","title":"2. <code>@graphify</code> vs <code>@graph_fn</code>","text":"Aspect <code>@graph_fn</code> (Reactive) <code>@graphify</code> (Static) Execution Runs immediately when called Builds a DAG first; run later Composition Mix plain Python + <code>@tool</code> (implicit nodes) Only <code>@tool</code> nodes are valid steps Context usage Rich <code>context.*</code> available inline Need to wrap <code>context.*</code> in a tool to access it Inspectability Inspect implicit graph via <code>graph_fn.last_graph()</code> Full spec via <code>.io()</code>, <code>.spec()</code>, <code>TaskGraph.pretty()</code> Resumability Graph cannot be resumed (use memory to resume sementically) Graph execution can resume to last node without running from the start Best for Interactive agents, quick iteration Pipelines, reproducible runs, analytics <p>Note: Nested static\u2011graph calls are not supported at the moment (no calling one <code>@graphify</code> from another as a node). Compose via tools or run graphs separately.</p>"},{"location":"key-concepts/static-graph-agent/#3-define-and-build-a-graph","title":"3. Define and Build a Graph","text":"<pre><code>from aethergraph import graphify, tool\n\n@tool(outputs=[\"rows\"])       \ndef load_csv(path: str): ...\n\n@tool(outputs=[\"clean\"])      \ndef clean(rows): ...\n\n@tool(outputs=[\"model\"])      \ndef train(data): ...\n\n@tool(outputs=[\"uri\"])        \ndef report(model): ...\n\n@graphify(name=\"etl_train_report\", inputs=[\"csv_path\"], outputs=[\"uri\"]) \ndef etl_train_report(csv_path):\n    raw  = load_csv(path=csv_path)\n    tidy = clean(rows=raw.rows)\n    mod  = train(data=tidy.clean)\n    rep  = report(model=mod.model)\n    return {\"uri\": rep.uri}\n\nG = etl_train_report.build()     # \u2192 TaskGraph\n</code></pre> <p>All the input and output in the definition of a graph builder are <code>graph_ref</code> and <code>NodeHandler</code>, respectively. Accessing them in the graph builder will not display the actual value (e.g. you cannot access <code>raw[0]</code> or <code>raw.rows[0]</code> and pass it to the next tool). Always use a <code>@tool</code> to pack/uppack variables or integrate multiple small steps in a <code>@tool</code>. </p>"},{"location":"key-concepts/static-graph-agent/#control-ordering-without-data-edges","title":"Control ordering without data edges","text":"<p>Use <code>_after</code> to enforce sequence when there\u2019s no data dependency:</p> <pre><code>@tool(outputs=[\"ok\"])    \ndef fetch(): ...\n\n@tool(outputs=[\"done\"])  \ndef train(): ...\n\n@graphify(name=\"seq\", inputs=[], outputs=[\"done\"]) \ndef seq():\n    a = fetch()\n    b = train(_after=a)        # run `train` after `fetch`\n    return {\"done\": b.done}\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#referencing-tool-outputs-dot-vs-key","title":"Referencing Tool Outputs (dot vs. key)","text":"<p>Each <code>@tool</code> must declare its outputs. AetherGraph wraps the call in a handle whose fields mirror those names, so you can access them either as attributes or dict keys \u2014 both are equivalent.</p> <pre><code>@tool(outputs=[\"rows\"])\ndef load_csv(path: str):\n# must return a dict matching declared outputs\nreturn {\"rows\": parse_csv(path)}\n\n\n@tool(outputs=[\"clean\"])\ndef clean(rows):\nreturn {\"clean\": tidy(rows)}\n\n\n@graphify(name=\"etl\", inputs=[\"csv_path\"], outputs=[\"clean\"])\ndef etl(csv_path):\nraw = load_csv(path=csv_path)\n# Access either way; these are equivalent\ntidy1 = clean(rows=raw.rows) # dot access\ntidy2 = clean(rows=raw[\"rows\"]) # key access\nreturn {\"clean\": tidy1.clean}\n</code></pre> <p>Consistency matters: declared output names (e.g.,<code>outputs=[\"rows\"]</code>) must match the keys you return from the tool (e.g., <code>{\"rows\": ...}</code>). Mismatches raise clear build/runtime errors.</p> <p>Multiple outputs <pre><code>@tool(outputs=[\"mean\", \"std\"])\ndef stats(xs: list[float]):\nreturn {\"mean\": avg(xs), \"std\": stdev(xs)}\n\n\n@graphify(name=\"use_stats\", inputs=[\"xs\"], outputs=[\"m\"])\ndef use_stats(xs):\ns = stats(xs=xs)\nreturn {\"m\": s.mean} # or s[\"mean\"]\n</code></pre></p> <p>Think of tool calls as typed nodes whose declared outputs become fields on the node handle.</p>"},{"location":"key-concepts/static-graph-agent/#4-fanin-fanout-patterns","title":"4. Fan\u2011in / Fan\u2011out Patterns","text":"<pre><code>@tool(outputs=[\"v\"]) \ndef step(x: int): ...\n\n@tool(outputs=[\"z\"]) \ndef join(a, b): ...\n\n@graphify(name=\"fan\", inputs=[\"x1\", \"x2\"], outputs=[\"z\"]) \ndef fan(x1, x2):\n    a = step(x=\"x1\")  # fan\u2011out 1\n    b = step(x=\"x2\")  # fan\u2011out 2\n    j = join(a=a.v, b=b.v)  # fan\u2011in\n    return {\"z\": j.z}\n</code></pre> <p>Tips: you can use for loop to create fan-in and fan-out</p>"},{"location":"key-concepts/static-graph-agent/#5-run-a-built-graph","title":"5. Run a Built Graph","text":"<p>Run the materialized DAG with the runner (sync or async):</p> <pre><code>from aethergraph.runner import run, run_async\n\nresult = run(G, inputs={\"csv_path\": \"data/train.csv\"})\n# \u2192 {\"uri\": \"file://...\"}\n\n# Async form (e.g., inside another async function)\nfinal = await run_async(G, inputs={\"csv_path\": \"data/train.csv\"})\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#6-inspect-and-explore","title":"6. Inspect and Explore","text":"<p><code>@graphify</code> builders expose helpers for IO/signature and full spec:</p> <pre><code>sig  = etl_train_report.io()     # inputs/outputs signature\nspec = etl_train_report.spec()   # GraphSpec (nodes, edges, metadata)\n</code></pre> <p>Runtime helpers on <code>TaskGraph</code>:</p> <pre><code>print(G.pretty())                # human\u2011friendly table\nprint(G.ascii_overview())        # compact overview\n\n# Select / find nodes\nids     = G.list_nodes()                         # visible node_ids\nfirst_c = G.find_by_logic(\"clean\", first=True)  # by tool/logic name\nsome    = G.find_by_label(\"train\")              # by label\nsel     = G.select(\"@my_alias\")                 # mini\u2011DSL (@alias, #label, logic:, name:, id:, /regex/)\n\n# Topology &amp; subgraphs\norder   = G.topological_order()                  # raises if cycles\nup      = G.get_upstream_nodes(first_c)          # dependency closure\nsub     = G.get_subgraph_nodes(first_c)          # downstream closure\n</code></pre> <p>Export / visualize</p> <pre><code>dot = G.to_dot()                 # Graphviz DOT\n# G.visualize()                  # if enabled: render to file/viewer\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#key-points","title":"Key Points","text":"<ul> <li><code>@graphify</code> builds a DAG from <code>@tool</code> calls; you run it later.</li> <li>Use <code>_after</code> to force ordering without data edges.</li> <li>Fan\u2011out/fan\u2011in is natural with multiple <code>@tool</code> calls and a later join.</li> <li>Inspect via <code>.io()</code>, <code>.spec()</code>, <code>TaskGraph.pretty()</code>, <code>ascii_overview()</code>, <code>to_dot()</code>.</li> <li>No nested static graphs currently (don\u2019t call one <code>@graphify</code> from another as a node).</li> <li>For reactive agents, stick with <code>@graph_fn</code>; for pipelines, prefer <code>@graphify</code>.</li> </ul>"},{"location":"recipes/data-analysis-loop/","title":"Recipe: Iterative Data Analysis","text":"<ul> <li>User asks for analysis \u2192 generate code</li> <li>Run code; store figures &amp; tables in <code>artifacts()</code></li> <li>Record metrics in <code>memory()</code> and summarize at the end</li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"recipes/job-submit-poll/","title":"Recipe: Submit + Poll + Notify","text":"<ul> <li>Submit a long-running job</li> <li>Poll status and send progress via <code>channel()</code></li> <li>On failure, ask user to retry or stop; save logs to <code>artifacts()</code></li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"recipes/memory-rag-mini/","title":"Recipe: Mini Memory\u2011RAG","text":"<ul> <li>Ingest notes as <code>memory().record(kind=\"note\", data=...)</code></li> <li>Simple retrieval via <code>memory().recent()/query()</code> into <code>llm().chat()</code></li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"reference/config/","title":"Config Keys","text":"<ul> <li><code>AETH_PORT</code> \u2014 server port</li> <li><code>LOG_LEVEL</code> \u2014 logging level (e.g., INFO)</li> <li><code>MODEL_NAME</code> \u2014 default LLM</li> <li><code>SLACK_*</code> \u2014 Slack integration (bot token, signing secret)</li> </ul> <p>(Expand with your actual config schema.)</p>"},{"location":"reference/context-artifacts/","title":"AetherGraph \u2014 <code>context.artifacts()</code> Reference","text":"<p>This page documents the ArtifactFacade methods returned by <code>context.artifacts()</code> in a concise format: signature, brief description, parameters, and returns \u2014 plus examples for <code>writer()</code> and scoped search.</p>"},{"location":"reference/context-artifacts/#overview","title":"Overview","text":"<p><code>context.artifacts()</code> returns an ArtifactFacade bound to the current <code>run_id</code>, <code>graph_id</code>, and <code>node_id</code>. It wraps an <code>AsyncArtifactStore</code> for persistence and an <code>AsyncArtifactIndex</code> for search/pinning/metrics. Most mutating ops auto\u2011index and record an occurrence.</p> <p>Typical flow</p> <ol> <li> <p>Stage (optional) \u2192 write \u2192 ingest</p> </li> <li> <p>Or directly save an existing file path</p> </li> <li> <p>Or use the built\u2011in <code>writer()</code> context manager to stream bytes and auto\u2011index</p> </li> </ol>"},{"location":"reference/context-artifacts/#artifactsstage","title":"artifacts.stage","text":"<p><pre><code>stage(ext: str = \"\") -&gt; str\n</code></pre> Plan a staging file path (temporary path) with an optional extension.</p> <p>Parameters</p> <ul> <li>ext (str, optional) \u2013 Suggested extension (e.g., \".png\", \".csv\").</li> </ul> <p>Returns str \u2013 Staging file path.</p>"},{"location":"reference/context-artifacts/#artifactsingest","title":"artifacts.ingest","text":"<p><pre><code>ingest(staged_path: str, *, kind: str, labels=None, metrics=None, suggested_uri: str | None = None, pin: bool = False) -&gt; Artifact\n</code></pre> Ingest a previously staged file into the store, attach metadata, and auto\u2011index.</p> <p>Parameters</p> <ul> <li> <p>staged_path (str) \u2013 Path returned by <code>stage()</code> or <code>stage_dir()</code>.</p> </li> <li> <p>kind (str) \u2013 Logical artifact kind (e.g., \"image\", \"table\", \"model\").</p> </li> <li> <p>labels (dict, optional) \u2013 Arbitrary labels; merged into index filters.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics used for <code>best()</code> queries.</p> </li> <li> <p>suggested_uri (str, optional) \u2013 Hint for final URI; store may ignore.</p> </li> <li> <p>pin (bool) \u2013 Mark artifact as pinned in the index.</p> </li> </ul> <p>Returns Artifact \u2013 Indexed artifact record.</p>"},{"location":"reference/context-artifacts/#artifactssave","title":"artifacts.save","text":"<p><pre><code>save(path: str, *, kind: str, labels=None, metrics=None, suggested_uri: str | None = None, pin: bool = False) -&gt; Artifact\n</code></pre> Save an existing on\u2011disk file to the store with metadata; auto\u2011index and record occurrence. Sets <code>last_artifact</code>.</p> <p>Parameters</p> <ul> <li> <p>path (str) \u2013 Existing file path to persist.</p> </li> <li> <p>kind (str) \u2013 Logical artifact kind.</p> </li> <li> <p>labels (dict, optional) \u2013 Arbitrary labels.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics.</p> </li> <li> <p>suggested_uri (str, optional) \u2013 Hint for final URI; store may ignore.</p> </li> <li> <p>pin (bool) \u2013 Mark artifact as pinned.</p> </li> </ul> <p>Returns Artifact \u2013 Indexed artifact record.</p>"},{"location":"reference/context-artifacts/#artifactswriter","title":"artifacts.writer","text":"<p><pre><code>writer(*, kind: str, planned_ext: str | None = None, pin: bool = False) -&gt; AsyncContextManager[Writer]\n</code></pre> Open a binary writer context that persists bytes as an artifact; auto\u2011indexes on exit. Sets <code>last_artifact</code>.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Logical artifact kind.</p> </li> <li> <p>planned_ext (str, optional) \u2013 Extension hint for underlying temp file.</p> </li> <li> <p>pin (bool) \u2013 Mark resulting artifact as pinned.</p> </li> </ul> <p>Yields Writer \u2013 File\u2011like object; write bytes and close by exiting the context.</p> <p>Example <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"make_png\")\nasync def make_png(*, context):\n    import PIL.Image as Image\n    img = Image.new(\"RGB\", (128, 128), (255, 122, 26))\n    async with context.artifacts().writer(kind=\"image\", planned_ext=\".png\") as w:\n        # writer exposes a real file handle underneath\n        img.save(w, format=\"PNG\")\n    art = context.artifacts().last_artifact\n    await context.channel().send_image(url=art.uri, title=\"Generated PNG\")\n    return {\"uri\": art.uri}\n</code></pre></p>"},{"location":"reference/context-artifacts/#artifactsstage_dir","title":"artifacts.stage_dir","text":"<p><pre><code>stage_dir(suffix: str = \"\") -&gt; str\n</code></pre> Plan a staging directory for multi\u2011file artifacts.</p> <p>Parameters</p> <ul> <li>suffix (str, optional) \u2013 Optional directory suffix.</li> </ul> <p>Returns str \u2013 Staging directory path.</p>"},{"location":"reference/context-artifacts/#artifactsingest_dir","title":"artifacts.ingest_dir","text":"<p><pre><code>ingest_dir(staged_dir: str, **kw) -&gt; Artifact\n</code></pre> Ingest a directory of files as a single logical artifact; forwards extra keyword args to the store.</p> <p>Parameters</p> <ul> <li> <p>staged_dir (str) \u2013 Directory created by <code>stage_dir()</code>.</p> </li> <li> <p>kw \u2013 Store\u2011specific options (e.g., kind/labels/metrics/pin).</p> </li> </ul> <p>Returns Artifact \u2013 Indexed artifact record.</p>"},{"location":"reference/context-artifacts/#artifactstmp_path","title":"artifacts.tmp_path","text":"<p><pre><code>tmp_path(suffix: str = \"\") -&gt; str\n</code></pre> Alias of <code>stage()</code> for convenience.</p> <p>Parameters</p> <ul> <li>suffix (str, optional) \u2013 Extension or suffix.</li> </ul> <p>Returns str \u2013 Staging file path.</p>"},{"location":"reference/context-artifacts/#artifactsload_artifact","title":"artifacts.load_artifact","text":"<p><pre><code>load_artifact(uri: str) -&gt; Any\n</code></pre> Load a previously saved artifact by URI, using the store\u2019s type\u2011specific loader.</p> <p>Parameters</p> <ul> <li>uri (str) \u2013 Artifact URI.</li> </ul> <p>Returns Any \u2013 Decoded object (depends on store &amp; artifact type).</p>"},{"location":"reference/context-artifacts/#artifactsload_artifact_bytes","title":"artifacts.load_artifact_bytes","text":"<p><pre><code>load_artifact_bytes(uri: str) -&gt; bytes\n</code></pre> Load raw bytes for a previously saved artifact by URI.</p> <p>Parameters</p> <ul> <li>uri (str) \u2013 Artifact URI.</li> </ul> <p>Returns bytes \u2013 Artifact content.</p>"},{"location":"reference/context-artifacts/#artifactslist","title":"artifacts.list","text":"<p><pre><code>list(*, scope: Literal[\"node\",\"run\",\"graph\",\"project\",\"all\"] = \"run\") -&gt; list[Artifact]\n</code></pre> Quick listing with implicit scoping (defaults to the current run). Under the hood, this uses the index with reasonable filters for the given scope.</p> <p>Parameters</p> <ul> <li> <p>scope (str) \u2013 One of:</p> </li> <li> <p>\"node\" \u2013 filter by (run_id, graph_id, node_id) </p> </li> <li> <p>\"graph\" \u2013 filter by (run_id, graph_id) </p> </li> <li> <p>\"run\" \u2013 filter by (run_id) (default) </p> </li> <li> <p>\"project\" \u2013 filter by project/org if tracked in labels  </p> </li> <li> <p>\"all\" \u2013 no implicit filters (use sparingly)</p> </li> </ul> <p>Returns list[Artifact] \u2013 Matching artifacts.</p>"},{"location":"reference/context-artifacts/#artifactssearch","title":"artifacts.search","text":"<p><pre><code>search(*, kind: str | None = None, labels: dict | None = None, metric: str | None = None, mode: Literal[\"max\",\"min\"] | None = None, scope: Scope = \"run\", extra_scope_labels: dict | None = None) -&gt; list[Artifact]\n</code></pre> Index search with automatic scoping. Merges your <code>labels</code> with scope\u2011derived labels.</p> <p>Parameters</p> <ul> <li> <p>kind (str, optional) \u2013 Filter by artifact kind.</p> </li> <li> <p>labels (dict, optional) \u2013 Arbitrary label filters.</p> </li> <li> <p>metric (str, optional) \u2013 Metric name for ranking.</p> </li> <li> <p>mode ({\"max\",\"min\"}, optional) \u2013 Ranking direction.</p> </li> <li> <p>scope (Scope) \u2013 Implicit scope (default: \"run\").</p> </li> <li> <p>extra_scope_labels (dict, optional) \u2013 Additional scope labels to merge.</p> </li> </ul> <p>Returns list[Artifact] \u2013 Search results.</p> <p>Example (scoped search) <pre><code>best_imgs = await context.artifacts().search(kind=\"image\", scope=\"graph\")\n</code></pre></p>"},{"location":"reference/context-artifacts/#artifactsbest","title":"artifacts.best","text":"<p><pre><code>best(*, kind: str, metric: str, mode: Literal[\"max\",\"min\"], scope: Scope = \"run\", filters: dict | None = None) -&gt; Artifact | None\n</code></pre> Return the best artifact by a metric, with optional filters and implicit scope.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Artifact kind.</p> </li> <li> <p>metric (str) \u2013 Metric key.</p> </li> <li> <p>mode ({\"max\",\"min\"}) \u2013 Ranking direction.</p> </li> <li> <p>scope (Scope) \u2013 Implicit scope (default: \"run\").</p> </li> <li> <p>filters (dict, optional) \u2013 Additional label filters.</p> </li> </ul> <p>Returns Artifact | None \u2013 Best match or <code>None</code> if not found.</p>"},{"location":"reference/context-artifacts/#artifactspin","title":"artifacts.pin","text":"<p><pre><code>pin(artifact_id: str, pinned: bool = True) -&gt; None\n</code></pre> Pin or unpin an artifact in the index.</p> <p>Parameters</p> <ul> <li> <p>artifact_id (str) \u2013 ID of the artifact to (un)pin.</p> </li> <li> <p>pinned (bool) \u2013 <code>True</code> to pin; <code>False</code> to unpin.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-artifacts/#scoping-details","title":"Scoping details","text":"<p>The facade enriches queries with labels depending on the <code>scope</code> argument:</p> <ul> <li> <p>node \u2192 <code>{ graph_id, node_id }</code> </p> </li> <li> <p>graph \u2192 <code>{ graph_id }</code> </p> </li> <li> <p>project \u2192 <code>{ project_id }</code> (if tracked)  </p> </li> <li> <p>run \u2192 uses <code>list_for_run(run_id)</code> </p> </li> <li> <p>all \u2192 passes through to index with no implicit labels</p> </li> </ul>"},{"location":"reference/context-artifacts/#practical-examples","title":"Practical examples","text":"<p>1) Direct save <pre><code>uri = \"/tmp/plot.png\"\n# ... generate image to uri ...\nart = await context.artifacts().save(uri, kind=\"image\", labels={\"task\":\"eval\"}, metrics={\"psnr\": 31.2})\n</code></pre></p> <p>2) Stage \u2192 write \u2192 ingest <pre><code>staged = await context.artifacts().stage(\".csv\")\nwith open(staged, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"x,y\\n1,2\\n3,4\\n\")\nart = await context.artifacts().ingest(staged_path=staged, kind=\"table\", labels={\"split\":\"val\"})\n</code></pre></p> <p>3) Search best <pre><code>winner = await context.artifacts().best(kind=\"model\", metric=\"val_acc\", mode=\"max\", scope=\"run\")\nif winner:\n    await context.channel().send_text(f\"Best model: {winner.uri} acc={winner.metrics['val_acc']:.3f}\")\n</code></pre></p> <p>4) Multi\u2011file directory <pre><code>dir_path = await context.artifacts().stage_dir(\"_report\")\n# ... write several files to dir_path ...\nart = await context.artifacts().ingest_dir(dir_path, kind=\"report\", labels={\"format\":\"html\"})\n</code></pre></p> <p>5) Pin <pre><code>await context.artifacts().pin(artifact_id=art.id, pinned=True)\n</code></pre></p>"},{"location":"reference/context-channel/","title":"AetherGraph \u2014 <code>context.channel()</code> Reference","text":"<p>This page documents the ChannelSession methods returned by <code>context.channel()</code> in a concise, PyTorch\u2011style format: signature, brief description, parameters, and returns.</p>"},{"location":"reference/context-channel/#overview-choosing-a-channel","title":"Overview \u2014 Choosing a channel","text":"<p>Use <code>context.channel(&lt;key&gt;)</code> to bind a ChannelSession to a specific destination for all subsequent calls from that session. You can also override per call with the <code>channel=</code> keyword.</p> <p>Common forms - <code>slack:#research</code> \u2014 a Slack channel by name</p> <ul> <li> <p><code>slack:@alice</code> \u2014 a Slack DM</p> </li> <li> <p><code>telegram:@mychannel</code> \u2014 a Telegram channel</p> </li> <li> <p><code>console:stdin</code> \u2014 console fallback (default if nothing is configured)</p> </li> </ul> <p>Resolution order (what channel is used?) 1. Per\u2011call override: <code>await context.channel().send_text(\"hi\", channel=\"slack:#alerts\")</code></p> <ol> <li> <p>Bound session key: <code>ch = context.channel(\"slack:#research\"); await ch.send_text(\"hi\")</code></p> </li> <li> <p>Bus default: whatever <code>services.channels.get_default_channel_key()</code> returns</p> </li> <li> <p>Fallback: <code>console:stdin</code></p> </li> </ol> <p>Examples <pre><code># Bind a session to #research for many messages\nch = context.channel(\"slack:#research\")\nawait ch.send_text(\"Starting the run\u2026\")\nawait ch.send_text(\"Progress will be posted here.\")\n\n# One\u2011off override to a different channel\nawait context.channel().send_text(\"Heads\u2011up in #alerts\", channel=\"slack:#alerts\")\n\n# Stream to #research explicitly\nasync with context.channel().stream(channel=\"slack:#research\") as s:\n    await s.delta(\"Parsing\u2026 \")\n    await s.delta(\"OK\")\n    await s.end(\"Done\")\n\n# Progress bar to the default (no key passed)\nasync with context.channel().progress(title=\"Crunching\", total=100) as bar:\n    await bar.update(current=30, eta_seconds=90)\n    await bar.end(subtitle=\"All set!\")\n</code></pre></p>"},{"location":"reference/context-channel/#channelsend_text","title":"channel.send_text","text":"<p><pre><code>send_text(text, *, meta: dict | None = None, channel: str | None = None)\n</code></pre> Send a plain text message to a channel.</p> <p>Parameters</p> <ul> <li> <p>text (str) \u2013 Message body to send.</p> </li> <li> <p>meta (dict, optional) \u2013 Arbitrary metadata for adapters/analytics.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override (e.g., <code>\"slack:#research\"</code>).</p> </li> </ul> <p>Returns </p> <p><code>None</code></p>"},{"location":"reference/context-channel/#channelsend_image","title":"channel.send_image","text":"<p><pre><code>send_image(url: str | None = None, *, alt: str = \"image\", title: str | None = None, channel: str | None = None)\n</code></pre> Post an image by URL with <code>alt</code>/<code>title</code> text.</p> <p>Parameters</p> <ul> <li> <p>url (str, optional) \u2013 Image URL. Use <code>send_file</code> for file bytes.</p> </li> <li> <p>alt (str) \u2013 Alt text (default: <code>\"image\"</code>).</p> </li> <li> <p>title (str, optional) \u2013 Optional title/caption.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p><code>None</code></p>"},{"location":"reference/context-channel/#channelsend_file","title":"channel.send_file","text":"<p><pre><code>send_file(url: str | None = None, *, file_bytes: bytes | None = None, filename: str = \"file.bin\", title: str | None = None, channel: str | None = None)\n</code></pre> Upload or link a file to the channel. Provide either <code>url</code> or <code>file_bytes</code>.</p> <p>Parameters</p> <ul> <li> <p>url (str, optional) \u2013 Remote file URL to attach.</p> </li> <li> <p>file_bytes (bytes, optional) \u2013 Raw bytes to upload.</p> </li> <li> <p>filename (str) \u2013 Display filename (default: <code>\"file.bin\"</code>).</p> </li> <li> <p>title (str, optional) \u2013 Optional caption/label.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p><code>None</code></p>"},{"location":"reference/context-channel/#channelsend_buttons","title":"channel.send_buttons","text":"<p><pre><code>send_buttons(text: str, buttons: list[Button], *, meta: dict | None = None, channel: str | None = None)\n</code></pre> Send a short message with interactive buttons (links or postbacks depending on adapter).</p> <p>Parameters</p> <ul> <li> <p>text (str) \u2013 Leading text.</p> </li> <li> <p>buttons (list[Button]) \u2013 Button list; at minimum a <code>label</code> per button.</p> </li> <li> <p>meta (dict, optional) \u2013 Arbitrary metadata.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-channel/#channelask_text","title":"channel.ask_text","text":"<p><pre><code>ask_text(prompt: str, *, timeout_s: int = 3600, silent: bool = False, channel: str | None = None)\n</code></pre> Ask the user for free\u2011text using cooperative wait/continuations.</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Prompt text shown to the user. (Ignored if <code>silent=True</code>.)</p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>silent (bool) \u2013 If <code>True</code>, binds to current thread/channel without posting a prompt.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>str \u2013 The user\u2019s text (empty string if none).</p>"},{"location":"reference/context-channel/#channelwait_text","title":"channel.wait_text","text":"<p><pre><code>wait_text(*, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Wait for the next text reply in the current thread/channel without sending a prompt.</p> <p>Parameters</p> <ul> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>str \u2013 The user\u2019s text.</p>"},{"location":"reference/context-channel/#channelask_approval","title":"channel.ask_approval","text":"<p><pre><code>ask_approval(prompt: str, options: Iterable[str] = (\"Approve\", \"Reject\"), *, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Ask the user to approve or pick an option.</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Title or question.</p> </li> <li> <p>options (Iterable[str]) \u2013 Button labels (default: <code>(\"Approve\",\"Reject\")</code>).</p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>dict \u2013 <code>{ \"approved\": bool, \"choice\": str }</code>.</p>"},{"location":"reference/context-channel/#channelget_latest_uploads","title":"channel.get_latest_uploads","text":"<p><pre><code>get_latest_uploads(*, clear: bool = True)\n</code></pre> Fetch latest uploaded files for this channel (Ephemeral KV required).</p> <p>Parameters</p> <ul> <li>clear (bool) \u2013 If <code>True</code>, consume and clear the inbox (default: <code>True</code>).</li> </ul> <p>Returns </p> <p>list[FileRef] \u2013 Recent file references.</p> <p>Raises </p> <p><code>RuntimeError</code> \u2013 if KV is not available.</p>"},{"location":"reference/context-channel/#channelask_files","title":"channel.ask_files","text":"<p><pre><code>ask_files(*, prompt: str, accept: list[str] | None = None, multiple: bool = True, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Ask the user to upload file(s) with optional text input.</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Prompt text.</p> </li> <li> <p>accept (list[str], optional) \u2013 MIME types or extensions (adapter\u2011hint only).</p> </li> <li> <p>multiple (bool) \u2013 Allow selecting multiple files (default: <code>True</code>). </p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>dict \u2013 <code>{ \"text\": str, \"files\": list[FileRef] }</code>.</p>"},{"location":"reference/context-channel/#channelask_text_or_files","title":"channel.ask_text_or_files","text":"<p><pre><code>ask_text_or_files(*, prompt: str, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Let the user respond with either text or file(s).</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Prompt text.</p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>dict \u2013 <code>{ \"text\": str, \"files\": list[FileRef] }</code>.</p>"},{"location":"reference/context-channel/#channelstream","title":"channel.stream","text":"<p><pre><code>stream(channel: str | None = None)  # async context manager\n</code></pre> Create a stream for incremental message updates (token/delta style). Within the context, use <code>s.delta()</code> to append text and <code>s.end()</code> to finalize.</p> <p>Parameters</p> <ul> <li>channel (str, optional) \u2013 Per\u2011stream channel override.</li> </ul> <p>Yields StreamSender \u2013 with methods:</p> <ul> <li> <p><code>start()</code> \u2013 explicitly start the stream (optional; auto on first delta).</p> </li> <li> <p><code>delta(text_piece: str)</code> \u2013 append a delta (adapter receives upsert with full text).</p> </li> <li> <p><code>end(full_text: str | None = None)</code> \u2013 finalize; optionally set final text.</p> </li> </ul> <p>Example <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"stream_demo\")\nasync def stream_demo(*, context):\n    async with context.channel().stream() as s:\n        for chunk in [\"Hello\", \" \", \"world\", \"\u2026\"]:\n            await s.delta(chunk)\n        await s.end(\"Hello world!\")\n</code></pre></p>"},{"location":"reference/context-channel/#channelprogress","title":"channel.progress","text":"<p><pre><code>progress(*, title: str = \"Working...\", total: int | None = None, key_suffix: str = \"progress\", channel: str | None = None)  # async context manager\n</code></pre> Create a progress reporter (start/update/end) bound to the current run/node.</p> <p>Parameters</p> <ul> <li> <p>title (str) \u2013 Progress title (default: <code>\"Working...\"</code>).</p> </li> <li> <p>total (int, optional) \u2013 If set, progress is shown as <code>current/total</code>; allows <code>percent</code> updates.</p> </li> <li> <p>key_suffix (str) \u2013 Included in the internal upsert key (default: <code>\"progress\"</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011progress channel override.</p> </li> </ul> <p>Yields ProgressSender \u2013 with methods:</p> <ul> <li> <p><code>start(subtitle: str | None = None)</code> \u2013 start (auto on first update).</p> </li> <li> <p><code>update(current: int | None = None, inc: int | None = None, subtitle: str | None = None, percent: float | None = None, eta_seconds: float | None = None)</code></p> </li> <li> <p><code>end(subtitle: str | None = \"Done.\", success: bool = True)</code></p> </li> </ul> <p>Example <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"progress_demo\")\nasync def progress_demo(*, context):\n    async with context.channel().progress(title=\"Crunching\", total=5) as bar:\n        for i in range(5):\n            await bar.update(current=i+1, eta_seconds=(4-i)*0.5, subtitle=f\"step {i+1}/5\")\n        await bar.end(subtitle=\"All set!\")\n</code></pre></p>"},{"location":"reference/context-channel/#channel-resolution-notes","title":"Channel resolution notes","text":"<ul> <li> <p>Per\u2011call <code>channel=</code> overrides everything.</p> </li> <li> <p>Otherwise, the session\u2019s bound key (from <code>context.channel(bound_key)</code>) is used.</p> </li> <li> <p>Else, the bus default via <code>services.channels.get_default_channel_key()</code>.</p> </li> <li> <p>Else, fallback <code>\"console:stdin\"</code>.</p> </li> </ul>"},{"location":"reference/context-channel/#guarantees","title":"Guarantees","text":"<ul> <li> <p>Streams/progress use idempotent upsert keys derived from <code>(run_id, node_id, suffix)</code>.</p> </li> <li> <p>Ask methods bind correlators at both message and thread level to capture replies.</p> </li> </ul>"},{"location":"reference/context-kv/","title":"AetherGraph \u2014 <code>context.kv()</code> Reference","text":"<p>This page documents the Key\u2013Value API available via <code>context.kv()</code> in a concise format. The KV store is process\u2011local and transient \u2014 ideal for coordination, small caches, inboxes, and short\u2011lived lists. Not intended for large blobs or durability.</p>"},{"location":"reference/context-kv/#overview","title":"Overview","text":"<ul> <li>Keys are simple strings; values can be any JSON\u2011serializable Python object (adapters may allow arbitrary picklables, but keep it small).</li> <li>Most methods support TTL (time\u2011to\u2011live in seconds). Expired entries are pruned lazily or via <code>purge_expired()</code>.</li> <li>For namespacing, prefer prefixes like <code>\"run:&lt;id&gt;:...\"</code>, <code>\"inbox:&lt;channel&gt;\"</code>, etc.</li> </ul>"},{"location":"reference/context-kv/#kvget","title":"kv.get","text":"<p><pre><code>get(key: str, default: Any = None) -&gt; Any\n</code></pre> Fetch a value by key; returns <code>default</code> if missing or expired.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 Lookup key.</p> </li> <li> <p>default (Any, optional) \u2013 Value to return when absent/expired (default <code>None</code>).</p> </li> </ul> <p>Returns Any \u2013 Stored value or <code>default</code>.</p>"},{"location":"reference/context-kv/#kvset","title":"kv.set","text":"<p><pre><code>set(key: str, value: Any, *, ttl_s: int | None = None) -&gt; None\n</code></pre> Set a key to a value with optional TTL.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 Key to write.</p> </li> <li> <p>value (Any) \u2013 Value to store.</p> </li> <li> <p>ttl_s (int, optional) \u2013 Expiration in seconds.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvdelete","title":"kv.delete","text":"<p><pre><code>delete(key: str) -&gt; None\n</code></pre> Remove a key if present.</p> <p>Parameters</p> <ul> <li>key (str) \u2013 Key to delete.</li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvlist_append_unique","title":"kv.list_append_unique","text":"<p><pre><code>list_append_unique(key: str, items: list[dict], *, id_key: str = \"id\", ttl_s: int | None = None) -&gt; list[dict]\n</code></pre> Append unique dict items to a list value under <code>key</code>. Uniqueness is determined by <code>item[id_key]</code>.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 List container key.</p> </li> <li> <p>items (list[dict]) \u2013 Items to append.</p> </li> <li> <p>id_key (str) \u2013 Field name used for uniqueness (default: <code>\"id\"</code>).</p> </li> <li> <p>ttl_s (int, optional) \u2013 Reset TTL for the list.</p> </li> </ul> <p>Returns list[dict] \u2013 Updated list.</p>"},{"location":"reference/context-kv/#kvlist_pop_all","title":"kv.list_pop_all","text":"<p><pre><code>list_pop_all(key: str) -&gt; list\n</code></pre> Pop and return the entire list stored at <code>key</code>. Empties the container.</p> <p>Parameters</p> <ul> <li>key (str) \u2013 List container key.</li> </ul> <p>Returns list \u2013 Previous list content (empty list if none or not a list).</p>"},{"location":"reference/context-kv/#kvmget","title":"kv.mget","text":"<p><pre><code>mget(keys: list[str]) -&gt; list[Any]\n</code></pre> Batch get multiple keys.</p> <p>Parameters</p> <ul> <li>keys (list[str]) \u2013 Keys to read.</li> </ul> <p>Returns list[Any] \u2013 Values in the same order as <code>keys</code>.</p>"},{"location":"reference/context-kv/#kvmset","title":"kv.mset","text":"<p><pre><code>mset(kv: dict[str, Any], *, ttl_s: int | None = None) -&gt; None\n</code></pre> Batch set multiple keys with an optional shared TTL.</p> <p>Parameters</p> <ul> <li> <p>kv (dict[str, Any]) \u2013 Key\u2013value pairs.</p> </li> <li> <p>ttl_s (int, optional) \u2013 TTL to apply to all entries.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvexpire","title":"kv.expire","text":"<p><pre><code>expire(key: str, ttl_s: int) -&gt; None\n</code></pre> Update/assign a TTL for an existing key.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 Key to expire.</p> </li> <li> <p>ttl_s (int) \u2013 Time\u2011to\u2011live in seconds from now.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvpurge_expired","title":"kv.purge_expired","text":"<p><pre><code>purge_expired(limit: int = 1000) -&gt; int\n</code></pre> Remove up to <code>limit</code> expired keys.</p> <p>Parameters</p> <ul> <li>limit (int) \u2013 Maximum removals per call (default: 1000).</li> </ul> <p>Returns int \u2013 Number of keys purged.</p>"},{"location":"reference/context-kv/#practical-examples","title":"Practical examples","text":"<p>1) Channel inbox (files/messages) <pre><code># Adapter pushes uploads into an inbox list\nawait context.kv().list_append_unique(\n    key=f\"inbox:{channel_key}\",\n    items=[{\"id\": file_id, \"filename\": name, \"url\": url}],\n    ttl_s=3600,\n)\n\n# Agent consumes the inbox later\nfiles = await context.kv().list_pop_all(f\"inbox:{channel_key}\")\nif files:\n    await context.channel().send_text(f\"Received {len(files)} file(s)\")\n</code></pre></p> <p>2) Short\u2011lived cache with TTL <pre><code>k = f\"run:{context.run_id}:spec\"\nspec = await context.kv().get(k)\nif spec is None:\n    spec = await expensive_fetch()\n    await context.kv().set(k, spec, ttl_s=300)  # cache for 5 minutes\n</code></pre></p> <p>3) Batch write/read <pre><code>await context.kv().mset({\n    f\"run:{context.run_id}:step\": 42,\n    f\"run:{context.run_id}:eta\": 120,\n}, ttl_s=600)\n\nvals = await context.kv().mget([\n    f\"run:{context.run_id}:step\",\n    f\"run:{context.run_id}:eta\",\n])\nstep, eta = vals\n</code></pre></p> <p>4) Update TTL <pre><code>await context.kv().expire(f\"run:{context.run_id}:spec\", ttl_s=900)\n</code></pre></p>"},{"location":"reference/context-kv/#notes-behaviors","title":"Notes &amp; behaviors","text":"<ul> <li> <p>Transient: data is in\u2011process only; it disappears on restart.</p> </li> <li> <p>Small values: do not store large binaries; use <code>context.artifacts()</code> for blobs.</p> </li> <li> <p>Concurrency: the implementation uses an internal lock; operations are atomic per call.</p> </li> <li> <p>TTL semantics: reads lazily drop expired entries; use <code>purge_expired()</code> to actively clean.</p> </li> </ul>"},{"location":"reference/context-llm/","title":"AetherGraph \u2014 <code>context.llm()</code> Reference","text":"<p>This page documents the LLM client retrieved via <code>context.llm(profile=\"default\")</code>, in a concise format. The client implements two core calls:</p> <ul> <li><code>chat(messages, **kwargs) -&gt; (text: str, usage: dict)</code></li> <li><code>embed(texts, **kwargs) -&gt; list[list[float]]</code></li> </ul> <p>Profiles are managed by an <code>LLMService</code> that holds one or more configured clients (\"default\", \"azure\", \"local\", etc.).</p>"},{"location":"reference/context-llm/#quick-start","title":"Quick start","text":"<pre><code># 1) Use the default LLM profile\nllm = context.llm()                # == context.llm(\"default\")\ntext, usage = await llm.chat([\n    {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n    {\"role\":\"user\",   \"content\":\"Summarize AetherGraph in one sentence.\"},\n])\n\n# 2) Switch/set a key at runtime (in\u2011memory only)\ncontext.llm_set_key(provider=\"openai\", api_key=\"sk-...\", profile=\"default\")\n\n# 3) Use a named profile (must exist in LLMService)\nllm = context.llm(\"azure\")\n</code></pre>"},{"location":"reference/context-llm/#supported-providers-via-genericllmclient","title":"Supported providers (via GenericLLMClient)","text":"<p><code>{\"openai\",\"azure\",\"anthropic\",\"google\",\"openrouter\",\"lmstudio\",\"ollama\"}</code></p> <p>Credentials and endpoints are read from environment by default, but can be provided at construction time. Runtime key overrides are allowed via <code>context.llm_set_key(...)</code>.</p> <p>Common env vars:</p> <ul> <li><code>OPENAI_API_KEY</code>, <code>OPENAI_BASE_URL</code></li> <li><code>AZURE_OPENAI_KEY</code>, <code>AZURE_OPENAI_ENDPOINT</code>, <code>AZURE_OPENAI_DEPLOYMENT</code></li> <li><code>ANTHROPIC_API_KEY</code></li> <li><code>GOOGLE_API_KEY</code></li> <li><code>OPENROUTER_API_KEY</code></li> <li><code>LMSTUDIO_BASE_URL</code> (default <code>http://localhost:1234/v1</code>)</li> <li><code>OLLAMA_BASE_URL</code>   (default <code>http://localhost:11434/v1</code>)</li> </ul>"},{"location":"reference/context-llm/#llmchat","title":"llm.chat","text":"<p><pre><code>chat(messages: list[dict], **kw) -&gt; tuple[str, dict]\n</code></pre> Send a chat completion request and return <code>(text, usage)</code>.</p> <p>Parameters</p> <ul> <li> <p>messages (list[dict]) \u2013 OpenAI\u2011style conversation turns, e.g. <code>{\"role\":\"system\"|\"user\"|\"assistant\", \"content\": str}</code>.</p> </li> <li> <p>kw \u2013 Common knobs (provider\u2011dependent):  </p> </li> <li> <p>model (str, optional) \u2013 Model name (defaults to client\u2019s configured model).  </p> </li> <li> <p>temperature (float, optional) \u2013 Sampling temperature (default <code>0.5</code>).  </p> </li> <li> <p>top_p (float, optional) \u2013 Nucleus sampling (default <code>1.0</code>).  </p> </li> <li> <p>max_tokens (int, optional) \u2013 Max tokens (Anthropic/Azure/Google paths).  </p> </li> </ul> <p>Returns tuple[str, dict] \u2013 Generated <code>text</code> and a <code>usage</code> dict (token counts where supported).</p> <p>Example <pre><code>sys = {\"role\":\"system\",\"content\":\"Be concise.\"}\nusr = {\"role\":\"user\",\"content\":\"What is AetherGraph?\"}\ntext, usage = await context.llm().chat([sys, usr], temperature=0.2)\nawait context.channel().send_text(text)\n</code></pre></p>"},{"location":"reference/context-llm/#llmembed","title":"llm.embed","text":"<p><pre><code>embed(texts: list[str], **kw) -&gt; list[list[float]]\n</code></pre> Return embeddings for a list of strings.</p> <p>Parameters</p> <ul> <li> <p>texts (list[str]) \u2013 Text strings to embed.</p> </li> <li> <p>kw \u2013 Common knobs (provider\u2011dependent):  </p> </li> <li> <p>model (str, optional) \u2013 Embedding model name (default <code>text-embedding-3-small</code> for OpenAI\u2011like providers).</p> </li> </ul> <p>Returns list[list[float]] \u2013 Embedding vectors.</p> <p>Example <pre><code>vecs = await context.llm().embed([\"lens design\", \"holography basics\"])  # [[...], [...]]\n</code></pre></p>"},{"location":"reference/context-llm/#profiles-and-keys","title":"Profiles and keys","text":""},{"location":"reference/context-llm/#contextllmprofile-str-default-llmclient","title":"<code>context.llm(profile: str = \"default\") -&gt; LLMClient</code>","text":"<p>Retrieve the configured LLM client for a named profile. Raises if <code>LLMService</code> is not bound or profile missing.</p>"},{"location":"reference/context-llm/#contextllm_set_keyprovider-str-api_key-str-profile-str-default-none","title":"<code>context.llm_set_key(provider: str, api_key: str, profile: str = \"default\") -&gt; None</code>","text":"<p>Override an API key in memory for the given profile (good for demos/notebooks). Does not persist.</p> <p>Example <pre><code># Switch the default profile to use a local LM Studio server at runtime\ncontext.llm_set_key(provider=\"lmstudio\", api_key=\"sk-ignore\", profile=\"default\")\ntext, _ = await context.llm().chat([\n    {\"role\":\"user\",\"content\":\"Say hi from LM Studio\"}\n])\n</code></pre></p> <p>For long\u2011lived storage, use your project\u2019s Secrets provider and <code>LLMService.persist_key(secret_name, api_key)</code> if available.</p>"},{"location":"reference/context-llm/#providerspecific-notes","title":"Provider\u2011specific notes","text":"<ul> <li> <p>OpenAI / OpenRouter / LM Studio / Ollama \u2013 uses OpenAI\u2011style <code>/chat/completions</code> and <code>/embeddings</code> routes. <code>usage</code> is included where supported.</p> </li> <li> <p>Azure OpenAI \u2013 requires <code>AZURE_OPENAI_ENDPOINT</code> and <code>AZURE_OPENAI_DEPLOYMENT</code>; uses Azure routes.</p> </li> <li> <p>Anthropic (Claude) \u2013 converts OpenAI\u2011style messages to Anthropic\u2019s message format; returns concatenated text blocks.</p> </li> <li> <p>Google (Gemini) \u2013 uses <code>:generateContent</code> and <code>:embedContent</code>; <code>usage</code> shape differs and may be empty.</p> </li> <li> <p>Embeddings \u2013 not supported for Anthropic in this client.</p> </li> </ul>"},{"location":"reference/context-llm/#error-handling-retries","title":"Error handling &amp; retries","text":"<p>The client wraps calls with exponential backoff (<code>_Retry</code>) for transient HTTP errors (<code>ReadTimeout</code>, <code>ConnectError</code>, <code>HTTPStatusError</code>). You may still want to catch and surface provider\u2011specific errors around quota/keys.</p> <p>Example <pre><code>try:\n    text, usage = await context.llm().chat([{ \"role\":\"user\", \"content\":\"ping\" }])\nexcept Exception as e:\n    await context.channel().send_text(f\"LLM error: {e}\")\n</code></pre></p>"},{"location":"reference/context-llm/#patterns-with-context","title":"Patterns with Context","text":"<p>Router\u2011then\u2011Act <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"router_then_act\")\nasync def router_then_act(*, context):\n    sys = {\"role\":\"system\",\"content\":\"Route to 'summarize' or 'plot'\"}\n    usr = {\"role\":\"user\",\"content\":\"Summarize this research log.\"}\n    decision, _ = await context.llm().chat([sys, usr], temperature=0.0)\n    if \"summarize\" in decision.lower():\n        # Call downstream tool and write a result\n        await context.memory().write_result(\n            topic=\"router\",\n            outputs=[{\"name\":\"route\",\"kind\":\"text\",\"value\":\"summarize\"}],\n        )\n        await context.channel().send_text(\"Routing \u2192 summarize\")\n</code></pre></p> <p>RAG: retrieve \u2192 answer <pre><code>@graph_fn(name=\"rag_answer\")\nasync def rag_answer(*, context, q: str):\n    hits = await context.memory().rag_search(corpus_id=\"notes\", query=q, k=6)\n    prompt = [{\"role\":\"system\",\"content\":\"Answer using the provided notes.\"},\n              {\"role\":\"user\",\"content\":\"\\n\\n\".join(h.get(\"text\",\"\") for h in hits) + \"\\n\\nQ: \" + q}]\n    text, usage = await context.llm().chat(prompt, temperature=0.2, model=\"gpt-4o-mini\")\n    return {\"answer\": text, \"tokens\": usage.get(\"total_tokens\")}\n</code></pre></p>"},{"location":"reference/context-llm/#summary","title":"Summary","text":"<ul> <li>Use <code>context.llm()</code> to get a ready\u2011to\u2011use client for the current profile.</li> <li><code>chat()</code> returns <code>(text, usage)</code>; <code>embed()</code> returns vectors.</li> <li>Switch keys ad\u2011hoc with <code>context.llm_set_key(...)</code>; persist via your Secrets provider when available.</li> </ul>"},{"location":"reference/context-logger/","title":"AetherGraph \u2014 <code>context.logger()</code> Quick Reference","text":"<p><code>context.logger()</code> returns a pre\u2011scoped Python <code>logging.Logger</code> bound to the current run/graph/node via the project\u2019s <code>StdLoggerService</code>.</p> <ul> <li>Namespace: <code>node.&lt;node_id&gt;</code></li> <li>Extra context on every record: <code>{run_id, graph_id, node_id}</code></li> <li>Outputs: console (text) + rotating file (<code>$LOG_DIR/aethergraph.log</code>), optional JSON, optional async QueueHandler</li> </ul>"},{"location":"reference/context-logger/#basics","title":"Basics","text":"<pre><code>log = context.logger()\nlog.info(\"starting step\")\nlog.debug(\"inputs\", extra={\"shape\": [n, d]})\nlog.warning(\"retrying\", extra={\"attempt\": i})\ntry:\n    ...\nexcept Exception:\n    log.exception(\"failed tool call\")  # includes traceback\n</code></pre> <p>Returns <code>logging.Logger</code> \u2014 fully configured for the current node/run.</p>"},{"location":"reference/context-logger/#formatting-levels-service-defaults","title":"Formatting &amp; levels (service defaults)","text":"<p>Configured by <code>StdLoggerService.build(cfg)</code> and <code>LoggingConfig</code>:</p> <ul> <li>Global level: <code>cfg.level</code> (e.g., <code>INFO</code>) with optional per\u2011namespace overrides</li> <li>Console formatter: <code>cfg.console_pattern</code></li> <li>File formatter: text (<code>cfg.file_pattern</code>) or JSON (<code>cfg.use_json = True</code>)</li> <li>File rotation: <code>cfg.max_bytes</code>, <code>cfg.backup_count</code></li> <li>Non\u2011blocking file I/O: <code>cfg.enable_queue = True</code> (QueueHandler + Listener)</li> </ul> <p>You can rebuild the service on server start to apply new settings.</p>"},{"location":"reference/context-logger/#structured-fields","title":"Structured fields","text":"<p>Every call accepts <code>extra={...}</code> for structured, searchable fields. The service injects <code>{run_id, graph_id, node_id}</code> automatically. <pre><code>log.info(\"optimizer step\", extra={\"lr\": 3e-4, \"batch\": 64, \"phase\": \"warmup\"})\n</code></pre></p>"},{"location":"reference/context-logger/#good-practices","title":"Good practices","text":"<ul> <li>Use <code>debug</code> for noisy internals; rely on <code>INFO</code> for milestone breadcrumbs.</li> <li>Prefer <code>extra={...}</code> over string concatenation for metrics/values.</li> <li>Use <code>exception()</code> within <code>except</code> blocks to capture tracebacks.</li> <li>Log artifact URIs (<code>extra={\"artifact\": uri}</code>) instead of large payloads.</li> </ul>"},{"location":"reference/context-logger/#oneliner-pattern-in-tools","title":"One\u2011liner pattern in tools","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"demo\")\nasync def demo(*, context):\n    log = context.logger()\n    log.info(\"hello\", extra={\"stage\": \"start\"})\n    # ... work ...\n    log.info(\"done\", extra={\"duration_ms\": 123})\n</code></pre>"},{"location":"reference/context-logger/#summary","title":"Summary","text":"<ul> <li>Call <code>context.logger()</code> inside graph/tools for a scoped logger.</li> <li>Structured fields available via <code>extra={...}</code>; run/graph/node auto\u2011injected.</li> </ul>"},{"location":"reference/context-mcp/","title":"AetherGraph \u2014 <code>context.mcp()</code> Reference","text":"<p>This page documents the Model Context Protocol (MCP) client you obtain with <code>context.mcp(name)</code>. Use it to call tools, list resources, or read resources exposed by a remote/local MCP server over stdio, WebSocket, or HTTP.</p> <p>Import surface (for examples below): <pre><code>from aethergraph.services.mcp import (\n    MCPService,\n    StdioMCPClient,\n    WsMCPClient,\n    HttpMCPClient,\n)\n</code></pre></p>"},{"location":"reference/context-mcp/#concepts","title":"Concepts","text":"<ul> <li>MCPService: registry of named MCP clients (e.g., <code>\"local\"</code>, <code>\"ws\"</code>, <code>\"http\"</code>), handles lazy open/close and convenience calls.</li> <li>MCPClientProtocol: transport\u2011specific client implementing <code>open()</code>, <code>close()</code>, <code>call(tool, params)</code>, <code>list_tools()</code>, <code>list_resources()</code>, <code>read_resource(uri)</code>.</li> <li>Tools: remote RPCs exposed by the MCP server (e.g., <code>readFile</code>, <code>search</code>, <code>stat</code>).</li> <li>Resources: server\u2011advertised URIs you can <code>read_resource()</code> (e.g., <code>file://\u2026</code>, <code>repo://\u2026</code>).</li> </ul> <p><code>context.mcp(name)</code> returns the client registered under <code>name</code> via your process\u2011global <code>MCPService</code>.</p>"},{"location":"reference/context-mcp/#mcpservice-registry","title":"MCPService (registry)","text":""},{"location":"reference/context-mcp/#register","title":"register","text":"<p><pre><code>register(name: str, client: MCPClientProtocol) -&gt; None\n</code></pre> Register a client under a name.</p>"},{"location":"reference/context-mcp/#remove","title":"remove","text":"<p><pre><code>remove(name: str) -&gt; None\n</code></pre> Unregister a client.</p>"},{"location":"reference/context-mcp/#has-names-get","title":"has / names / get","text":"<p><pre><code>has(name: str) -&gt; bool\nnames() -&gt; list[str]\nget(name: str = \"default\") -&gt; MCPClientProtocol\n</code></pre> Query and retrieve clients by name.</p>"},{"location":"reference/context-mcp/#open-close","title":"open / close","text":"<p><pre><code>open(name: str) -&gt; None\nclose(name: str) -&gt; None\nopen_all() -&gt; None\nclose_all() -&gt; None\n</code></pre> Manage client lifecycles. <code>call()/list_*()</code> implicitly <code>open()</code> on first use.</p>"},{"location":"reference/context-mcp/#call-helpers","title":"call helpers","text":"<p><pre><code>call(name: str, tool: str, params: dict | None = None) -&gt; dict\nlist_tools(name: str) -&gt; list[MCPTool]\nlist_resources(name: str) -&gt; list[MCPResource]\nread_resource(name: str, uri: str) -&gt; dict\n</code></pre> Thin wrappers to keep call sites small; auto\u2011open if needed.</p>"},{"location":"reference/context-mcp/#optional-secretsruntime-headers","title":"optional secrets/runtime headers","text":"<p><pre><code>set_header(name: str, key: str, value: str) -&gt; None\npersist_secret(secret_name: str, value: str) -&gt; None\n</code></pre> <code>set_header()</code> is handy for WS/HTTP auth tokens at runtime. <code>persist_secret()</code> stores a credential via your Secrets provider (if writable).</p>"},{"location":"reference/context-mcp/#transport-clients","title":"Transport clients","text":""},{"location":"reference/context-mcp/#stdiomcpclient","title":"StdioMCPClient","text":"<p><pre><code>StdioMCPClient(cmd: list[str], env: dict[str,str] | None = None, timeout: float = 60.0)\n</code></pre> Spawn a subprocess and speak JSON\u2011RPC over stdio.</p>"},{"location":"reference/context-mcp/#wsmcpclient","title":"WsMCPClient","text":"<p><pre><code>WsMCPClient(url: str, *, headers: dict[str,str] | None = None, timeout: float = 60.0, ping_interval: float = 20.0, ping_timeout: float = 10.0)\n</code></pre> Connect to an MCP server over WebSocket.</p>"},{"location":"reference/context-mcp/#httpmcpclient","title":"HttpMCPClient","text":"<p><pre><code>HttpMCPClient(base_url: str, *, headers: dict[str,str] | None = None, timeout: float = 60.0)\n</code></pre> Call an MCP server over HTTP (JSON).</p>"},{"location":"reference/context-mcp/#contextmcpname","title":"context.mcp(name)","text":"<p><pre><code>context.mcp(name: str) -&gt; MCPClientProtocol\n</code></pre> Return the named client. Typically you register names like <code>\"local\"</code>, <code>\"ws\"</code>, <code>\"http\"</code> during app startup, then retrieve them inside tools/agents.</p> <p>Example <pre><code>client = context.mcp(\"ws\")\nout = await client.call(\"search\", {\"q\": \"holography\", \"k\": 5})\n</code></pre></p>"},{"location":"reference/context-mcp/#calling-tools","title":"Calling tools","text":"<p><pre><code>client.call(tool: str, params: dict | None = None) -&gt; dict\n</code></pre> Invoke a remote tool by name with JSON\u2011serializable params.</p> <p>Parameters - tool (str) \u2013 Tool name (server\u2011defined). - params (dict, optional) \u2013 Arguments for the tool.</p> <p>Returns dict \u2013 Tool result payload (shape defined by the server).</p> <p>Example <pre><code># Filesystem\u2011like server\nres = await context.mcp(\"local\").call(\"readFile\", {\"path\": \"/data/notes.txt\"})\ntext = res.get(\"text\") or res.get(\"content\") or \"\"\nawait context.channel().send_text(f\"len={len(text)}\")\n</code></pre></p>"},{"location":"reference/context-mcp/#listing-tools-resources","title":"Listing tools &amp; resources","text":"<p><pre><code>client.list_tools() -&gt; list[MCPTool]\nclient.list_resources() -&gt; list[MCPResource]\nclient.read_resource(uri: str) -&gt; dict\n</code></pre> Enumerate server capabilities and read advertised resources.</p> <p>Example <pre><code># Tool discovery\nfor t in await context.mcp(\"http\").list_tools():\n    await context.channel().send_text(f\"tool: {t.name} \u2014 {t.description}\")\n\n# Resource fetch\nfor r in await context.mcp(\"ws\").list_resources():\n    if r.uri.startswith(\"file://\"):\n        blob = await context.mcp(\"ws\").read_resource(r.uri)\n        await context.channel().send_text(f\"read {r.uri} \u2192 {len(blob.get('text',''))} chars\")\n</code></pre></p>"},{"location":"reference/context-mcp/#endtoend-setup-startup","title":"End\u2011to\u2011end setup (startup)","text":"<pre><code>from aethergraph.services.mcp import MCPService, StdioMCPClient, WsMCPClient, HttpMCPClient\nfrom aethergraph.v3.core.runtime.runtime_services import set_mcp_service\nimport os, sys\n\nDEMO_HTTP_TOKEN = os.environ.setdefault(\"DEMO_HTTP_TOKEN\", \"demo_token_123\")\n\nmcp = MCPService()\nmcp.register(\"local\", StdioMCPClient(cmd=[sys.executable, \"-m\", \"aethergraph.plugins.mcp.fs_server\"]))\nmcp.register(\"ws\", WsMCPClient(url=\"ws://localhost:8765\", headers={\"Authorization\": \"Bearer demo_token_123\"}))\nmcp.register(\"http\", HttpMCPClient(\"http://127.0.0.1:8769\", headers={\"Authorization\": f\"Bearer {DEMO_HTTP_TOKEN}\"}))\n\nset_mcp_service(mcp)  # make available to NodeContext\n</code></pre>"},{"location":"reference/context-mcp/#using-inside-a-graph-function","title":"Using inside a graph function","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"mcp_search_demo\", inputs=[\"q\"], outputs=[\"text\"], version=\"0.1.0\")\nasync def mcp_search_demo(q: str, *, context):\n    out = await context.mcp(\"ws\").call(\"search\", {\"q\": q, \"k\": 5})\n    text = out.get(\"text\") or out.get(\"content\") or \"\"\n    await context.channel().send_text(text[:200] + (\"\u2026\" if len(text) &gt; 200 else \"\"))\n    return {\"text\": text}\n</code></pre>"},{"location":"reference/context-mcp/#choosing-a-transport","title":"Choosing a transport","text":"<ul> <li>stdio: best when you ship or control the server process (local tools, file system, Git, CLI wrappers). Minimal latency, simple auth via env.</li> <li>WebSocket: interactive servers that push events, need long\u2011lived sessions, or custom headers/tokens.</li> <li>HTTP: stateless request/response, easy to deploy behind gateways; good fit for cloud MCP services.</li> </ul> <p>Tip: You can register multiple transports to the same logical backend under different names (<code>\"fs-local\"</code>, <code>\"fs-ws\"</code>) and switch per call.</p>"},{"location":"reference/context-mcp/#auth-headers","title":"Auth &amp; headers","text":"<ul> <li>Pass headers at client construction (<code>headers={\"Authorization\": \"Bearer \u2026\"}</code>).</li> <li>Update at runtime via <code>MCPService.set_header(name, key, value)</code> for WS/HTTP clients.</li> <li>Persist tokens via <code>MCPService.persist_secret(...)</code> when your Secrets provider supports writes.</li> </ul>"},{"location":"reference/context-mcp/#error-handling","title":"Error handling","text":"<p>Wrap calls to surface clear messages back to the user. <pre><code>try:\n    res = await context.mcp(\"http\").call(\"search\", {\"q\": \"mtf\"})\nexcept KeyError:\n    await context.channel().send_text(\"Unknown MCP profile. Did you register it?\")\nexcept Exception as e:\n    await context.channel().send_text(f\"MCP error: {e}\")\n</code></pre></p>"},{"location":"reference/context-mcp/#summary","title":"Summary","text":"<ul> <li>Register your clients at startup with <code>MCPService.register()</code> and wire the service into runtime so <code>context.mcp(name)</code> can retrieve them.</li> <li>Use <code>.call()</code> for tools, <code>.list_tools()/.list_resources()</code> for discovery, and <code>.read_resource()</code> to fetch URIs.</li> <li>Choose stdio/WS/HTTP based on deployment and interaction needs; manage auth via headers/</li> </ul>"},{"location":"reference/context-memory/","title":"AetherGraph \u2014 <code>context.memory()</code> Reference","text":"<p>This page documents the MemoryFacade returned by <code>context.memory()</code> in a concise format: signature, brief description, parameters, returns, and practical examples. The facade coordinates three core components \u2014 HotLog (recent, transient), Persistence (durable JSONL/appends), and Indices (fast derived views) \u2014 with optional ArtifactStore, RAG, and LLM services.</p>"},{"location":"reference/context-memory/#overview","title":"Overview","text":"<p><code>context.memory()</code> is bound to your current runtime scope (<code>session_id</code>, <code>run_id</code>, <code>graph_id</code>, <code>node_id</code>, <code>agent_id</code>). Typical operations:</p> <ol> <li> <p>Record events (raw or typed results)</p> </li> <li> <p>Query recent/last/by\u2011kind outputs via indices/hotlog</p> </li> <li> <p>Distill (rolling summaries, episode summaries)</p> </li> <li> <p>RAG (optional): upsert, search, answer using a configured RAG + LLM</p> </li> </ol>"},{"location":"reference/context-memory/#memoryrecord_raw","title":"memory.record_raw","text":"<p><pre><code>record_raw(*, base: dict, text: str | None = None, metrics: dict | None = None, sources: list[str] | None = None) -&gt; Event\n</code></pre> Append a normalized event to HotLog (fast) and Persistence (durable). Computes a stable <code>event_id</code> and a lightweight <code>signal</code> if absent.</p> <p>Parameters</p> <ul> <li> <p>base (dict) \u2013 Canonical fields describing the event (e.g., <code>kind</code>, <code>stage</code>, <code>severity</code>, <code>tool</code>, <code>tags</code>, <code>entities</code>, <code>inputs</code>, <code>outputs</code>, \u2026). Missing scope keys are filled from the bound context.</p> </li> <li> <p>text (str, optional) \u2013 Human\u2011readable message/body.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics (latency, token counts, costs, etc.).</p> </li> <li> <p>sources (list[str], optional) \u2013 Event IDs this event summarizes/derives from.</p> </li> </ul> <p>Returns Event \u2013 The appended event.</p> <p>Notes Does not update <code>indices</code> automatically. Use <code>write_result()</code> when you want indices updated for typed outputs.</p>"},{"location":"reference/context-memory/#memoryrecord","title":"memory.record","text":"<p><pre><code>record(kind, data, tags=None, entities=None, severity=2, stage=None, inputs_ref=None, outputs_ref=None, metrics=None, sources=None, signal=None) -&gt; Event\n</code></pre> Convenience wrapper around <code>record_raw()</code> for common fields; stringifies <code>data</code> if needed.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Event kind (e.g., <code>\"user_msg\"</code>, <code>\"tool_call\"</code>).</p> </li> <li> <p>data (Any) \u2013 JSON\u2011serializable payload; will be stringified for <code>text</code>.</p> </li> <li> <p>tags (list[str], optional) \u2013 Tag list.</p> </li> <li> <p>entities (list[str], optional) \u2013 Entity IDs.</p> </li> <li> <p>severity (int) \u2013 1\u20135 scale (default 2).</p> </li> <li> <p>stage (str, optional) \u2013 Phase label (e.g., <code>\"observe\"</code>, <code>\"act\"</code>).</p> </li> <li> <p>inputs_ref (list[dict], optional) \u2013 Typed input references (Value[]).</p> </li> <li> <p>outputs_ref (list[dict], optional) \u2013 Typed output references (Value[]).</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics.</p> </li> <li> <p>sources (list[str], optional) \u2013 Upstream event IDs.</p> </li> <li> <p>signal (float, optional) \u2013 0.0\u20131.0; if omitted, computed heuristically.</p> </li> </ul> <p>Returns Event \u2013 The appended event.</p>"},{"location":"reference/context-memory/#memorywrite_result","title":"memory.write_result","text":"<p><pre><code>write_result(*, topic: str, inputs: list[dict] | None = None, outputs: list[dict] | None = None, tags: list[str] | None = None, metrics: dict | None = None, message: str | None = None, severity: int = 3) -&gt; Event\n</code></pre> Record a typed result (tool/agent/flow) and update indices for quick retrieval.</p> <p>Parameters</p> <ul> <li> <p>topic (str) \u2013 Tool/agent/flow identifier (used by <code>indices.last_outputs_by_topic</code>).</p> </li> <li> <p>inputs (list[dict], optional) \u2013 Typed inputs (Value[]).</p> </li> <li> <p>outputs (list[dict], optional) \u2013 Typed outputs (Value[]). Indices derive from these.</p> </li> <li> <p>tags (list[str], optional) \u2013 Tag list.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics.</p> </li> <li> <p>message (str, optional) \u2013 Human\u2011readable summary.</p> </li> <li> <p>severity (int) \u2013 Default 3.</p> </li> </ul> <p>Returns Event \u2013 The normalized <code>tool_result</code> event.</p> <p>Effect Auto\u2011appends to HotLog &amp; Persistence and calls <code>indices.update(session_id, evt)</code>.</p>"},{"location":"reference/context-memory/#memoryrecent","title":"memory.recent","text":"<p><pre><code>recent(*, kinds: list[str] | None = None, limit: int = 50) -&gt; list[Event]\n</code></pre> Return recent events from HotLog (most recent last), optionally filtering by <code>kinds</code>.</p> <p>Parameters</p> <ul> <li> <p>kinds (list[str], optional) \u2013 Filter kinds.</p> </li> <li> <p>limit (int) \u2013 Max events (default 50).</p> </li> </ul> <p>Returns list[Event] \u2013 Recent events.</p>"},{"location":"reference/context-memory/#memorylast_by_name","title":"memory.last_by_name","text":"<p><pre><code>last_by_name(name: str)\n</code></pre> Return the last output value by <code>name</code> from Indices (fast path).</p> <p>Parameters</p> <ul> <li>name (str) \u2013 Output name.</li> </ul> <p>Returns Any \u2013 The stored value for that name (adapter\u2011dependent) or <code>None</code>.</p>"},{"location":"reference/context-memory/#memorylatest_refs_by_kind","title":"memory.latest_refs_by_kind","text":"<p><pre><code>latest_refs_by_kind(kind: str, *, limit: int = 50)\n</code></pre> Return latest ref outputs by <code>ref.kind</code> (fast path, KV\u2011backed) from Indices.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Reference kind.</p> </li> <li> <p>limit (int) \u2013 Max items (default 50).</p> </li> </ul> <p>Returns list[Any] \u2013 Recent references.</p>"},{"location":"reference/context-memory/#memorylast_outputs_by_topic","title":"memory.last_outputs_by_topic","text":"<p><pre><code>last_outputs_by_topic(topic: str)\n</code></pre> Return the last output map for a given topic (tool/flow/agent) from Indices.</p> <p>Parameters</p> <ul> <li>topic (str) \u2013 Topic identifier.</li> </ul> <p>Returns dict | None \u2013 Latest outputs or <code>None</code> if absent.</p>"},{"location":"reference/context-memory/#memorydistill_rolling_chat","title":"memory.distill_rolling_chat","text":"<p><pre><code>distill_rolling_chat(*, max_turns: int = 20, min_signal: float | None = None) -&gt; dict\n</code></pre> Build a rolling chat summary from recent user/assistant turns (reads HotLog; typically writes a JSON summary via Persistence).</p> <p>Parameters</p> <ul> <li> <p>max_turns (int) \u2013 Window of turns to include (default 20).</p> </li> <li> <p>min_signal (float, optional) \u2013 Signal threshold; uses facade default if omitted.</p> </li> </ul> <p>Returns dict \u2013 Descriptor (e.g., <code>{ \"uri\": ..., \"sources\": [...] }</code>).</p>"},{"location":"reference/context-memory/#memorydistill_episode","title":"memory.distill_episode","text":"<p><pre><code>distill_episode(*, tool: str, run_id: str, include_metrics: bool = True) -&gt; dict\n</code></pre> Summarize a tool/agent episode (all events for a given <code>run_id</code> + <code>tool</code>). Reads HotLog/Persistence; writes back a summary JSON (and optionally CAS bundle).</p> <p>Parameters</p> <ul> <li> <p>tool (str) \u2013 Tool/agent identifier.</p> </li> <li> <p>run_id (str) \u2013 Run to summarize.</p> </li> <li> <p>include_metrics (bool) \u2013 Include metrics in the summary (default True).</p> </li> </ul> <p>Returns dict \u2013 Descriptor (e.g., <code>{ \"uri\": ..., \"sources\": [...], \"metrics\": {...} }</code>).</p>"},{"location":"reference/context-memory/#rag-helpers-optional","title":"RAG helpers (optional)","text":""},{"location":"reference/context-memory/#memoryrag_upsert","title":"memory.rag_upsert","text":"<p><pre><code>rag_upsert(*, corpus_id: str, docs: Sequence[dict], topic: str | None = None) -&gt; dict\n</code></pre> Upsert documents into a RAG corpus via the configured RAG facade.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>docs (Sequence[dict]) \u2013 Documents/chunks with text and metadata.</p> </li> <li> <p>topic (str, optional) \u2013 Optional topic name to attribute the upsert.</p> </li> </ul> <p>Returns dict \u2013 Upsert stats (shape adapter\u2011specific).</p> <p>Raises <code>RuntimeError</code> \u2013 if RAG facade is not configured.</p>"},{"location":"reference/context-memory/#memoryrag_search","title":"memory.rag_search","text":"<p><pre><code>rag_search(*, corpus_id: str, query: str, k: int = 8) -&gt; list[dict]\n</code></pre> Retrieve best\u2011matching chunks for a query.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>query (str) \u2013 Natural language query.</p> </li> <li> <p>k (int) \u2013 Max results (default 8), reranked.</p> </li> </ul> <p>Returns list[dict] \u2013 Ranked hits.</p> <p>Raises <code>RuntimeError</code> \u2013 if RAG facade is not configured.</p>"},{"location":"reference/context-memory/#memoryrag_answer","title":"memory.rag_answer","text":"<p><pre><code>rag_answer(*, corpus_id: str, question: str, style: str = \"concise\", k: int = 6, llm_profile: str = \"default\") -&gt; dict\n</code></pre> Answer a question using RAG + LLM (both must be configured).</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>question (str) \u2013 User question.</p> </li> <li> <p>style (str) \u2013 Answering style (e.g., <code>\"concise\"</code>).</p> </li> <li> <p>k (int) \u2013 Max retrieved chunks (default 6).</p> </li> <li> <p>llm_profile (str) \u2013 Profile name to select an LLM client.</p> </li> </ul> <p>Returns dict \u2013 Answer payload (adapter\u2011specific).</p> <p>Raises <code>RuntimeError</code> \u2013 if RAG or LLM is not configured.</p>"},{"location":"reference/context-memory/#practical-examples","title":"Practical examples","text":"<p>1) Record + recent <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"mem_record_recent\")\nasync def mem_record_recent(*, context):\n    evt = await context.memory().record(\n        kind=\"user_msg\",\n        data={\"text\":\"hello world\",\"lang\":\"en\"},\n        tags=[\"demo\",\"quickstart\"],\n        severity=2,\n    )\n    recent = await context.memory().recent(kinds=[\"user_msg\"], limit=5)\n    await context.channel().send_text(f\"recent user_msg count={len(recent)}\")\n    return {\"event_id\": evt.event_id, \"recent_count\": len(recent)}\n</code></pre></p> <p>2) Write a typed result and fetch last outputs <pre><code>@graph_fn(name=\"mem_write_result\")\nasync def mem_write_result(*, context):\n    await context.memory().write_result(\n        topic=\"eval.step\",\n        outputs=[{\"name\":\"acc\",\"kind\":\"number\",\"value\":0.912}],\n        metrics={\"latency_ms\": 120},\n        message=\"evaluation complete\",\n    )\n    last = await context.memory().last_outputs_by_topic(\"eval.step\")\n    await context.channel().send_text(f\"last acc={last['acc']:.3f}\")\n</code></pre></p> <p>3) Rolling chat summary <pre><code>@graph_fn(name=\"mem_rolling\")\nasync def mem_rolling(*, context):\n    summary = await context.memory().distill_rolling_chat(max_turns=16)\n    await context.channel().send_text(f\"rolling summary uri: {summary.get('uri','&lt;none&gt;')}\")\n</code></pre></p> <p>4) Episode summary <pre><code>@graph_fn(name=\"mem_episode\")\nasync def mem_episode(*, context, run_id: str, tool: str):\n    desc = await context.memory().distill_episode(tool=tool, run_id=run_id)\n    await context.channel().send_text(f\"episode summary: {desc.get('uri','&lt;none&gt;')}\")\n</code></pre></p> <p>5) RAG (if configured) <pre><code>@graph_fn(name=\"mem_rag\")\nasync def mem_rag(*, context):\n    # Upsert a few docs\n    await context.memory().rag_upsert(\n        corpus_id=\"notes\",\n        docs=[{\"id\":\"1\",\"text\":\"Optics basics: Snell's law\"}],\n    )\n    # Search\n    hits = await context.memory().rag_search(corpus_id=\"notes\", query=\"Snell\")\n    # Answer\n    ans = await context.memory().rag_answer(corpus_id=\"notes\", question=\"What is Snell's law?\", style=\"concise\")\n    await context.channel().send_text(ans.get(\"answer\",\"&lt;no answer&gt;\"))\n</code></pre></p>"},{"location":"reference/context-memory/#notes-behaviors","title":"Notes &amp; behaviors","text":"<ul> <li> <p>Signal heuristic: if not provided, <code>record(_raw)</code> computes a 0.0\u20131.0 <code>signal</code> from severity + presence/length of text + metrics.</p> </li> <li> <p>Durability: every <code>record_raw</code> &amp; <code>write_result</code> appends to Persistence; <code>recent()</code> reads from HotLog.</p> </li> <li> <p>Indices: <code>write_result()</code> updates fast views used by <code>last_by_name</code>, <code>latest_refs_by_kind</code>, <code>last_outputs_by_topic</code>.</p> </li> <li> <p>Artifacts: distillers may produce CAS artifacts when an <code>ArtifactStore</code> is provided.</p> </li> <li> <p>Performance: methods are async; backends should avoid blocking the event loop (use <code>asyncio.to_thread</code> for heavy IO).</p> </li> </ul>"},{"location":"reference/context-rag/","title":"AetherGraph \u2014 <code>context.rag()</code> Reference","text":"<p>This page documents the RAGFacade returned by <code>context.rag()</code> in a concise format: signature, brief description, parameters, returns, and practical examples.</p> <p>The facade covers: corpus management, document ingestion (upsert), retrieval (search/retrieve), and question answering with optional citation resolution.</p>"},{"location":"reference/context-rag/#overview","title":"Overview","text":"<p><code>context.rag()</code> provides high\u2011level helpers backed by:</p> <ul> <li>an Artifact Store (for persisted doc assets),</li> <li>an Embedding client (e.g., <code>context.llm().embed()</code>),</li> <li>a Vector index backend (add/search),</li> <li>a TextSplitter (chunking before embedding), and</li> <li>an optional LLM client for QA.</li> </ul>"},{"location":"reference/context-rag/#ragadd_corpus","title":"rag.add_corpus","text":"<p><pre><code>add_corpus(corpus_id: str, meta: dict | None = None) -&gt; None\n</code></pre> Create a new corpus directory with metadata if it does not exist.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Unique identifier for the corpus.</p> </li> <li> <p>meta (dict, optional) \u2013 Arbitrary metadata stored alongside the corpus.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-rag/#ragupsert_docs","title":"rag.upsert_docs","text":"<p><pre><code>upsert_docs(corpus_id: str, docs: list[dict]) -&gt; dict\n</code></pre> Ingest and index a list of documents (file\u2011based or inline text). Handles artifact persistence, chunking, embedding, and index add.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>docs (list[dict]) \u2013 Each doc is either:</p> </li> <li> <p>File doc: <code>{ \"path\": \"/path/to/file.pdf\", \"labels\": {...}, \"title\": \"Optional\" }</code></p> </li> <li> <p>Inline text doc: <code>{ \"text\": \"...\", \"title\": \"Optional\", \"labels\": {...} }</code></p> </li> </ul> <p>Returns dict \u2013 Summary like <code>{ \"added\": int, \"chunks\": int, \"index\": \"BackendName\" }</code>.</p> <p>Notes - PDFs and Markdown are parsed with built\u2011in extractors; other files default to text.</p> <ul> <li>Each doc and chunk is assigned a stable SHA\u2011derived ID and recorded in <code>docs.jsonl</code> / <code>chunks.jsonl</code> under the corpus folder.</li> </ul>"},{"location":"reference/context-rag/#ragsearch","title":"rag.search","text":"<p><pre><code>search(corpus_id: str, query: str, k: int = 8, filters: dict | None = None, mode: str = \"hybrid\") -&gt; list[SearchHit]\n</code></pre> Hybrid retrieval: dense vector search with optional lexical fusion, returning the top\u2011k chunks.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>query (str) \u2013 Natural language query.</p> </li> <li> <p>k (int) \u2013 Number of results (default 8).</p> </li> <li> <p>filters (dict, optional) \u2013 Reserved for metadata filtering (adapter\u2011specific).</p> </li> <li> <p>mode ({\"dense\",\"hybrid\"}) \u2013 Retrieval mode (default <code>\"hybrid\"</code>).</p> </li> </ul> <p>Returns list[SearchHit] \u2013 Ranked hits with <code>chunk_id</code>, <code>doc_id</code>, <code>corpus_id</code>, <code>score</code>, <code>text</code>, <code>meta</code>.</p>"},{"location":"reference/context-rag/#ragretrieve","title":"rag.retrieve","text":"<p><pre><code>retrieve(corpus_id: str, query: str, k: int = 6, rerank: bool = True) -&gt; list[SearchHit]\n</code></pre> Convenience wrapper over <code>search(..., mode=\"hybrid\")</code> for top\u2011k retrieval.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>query (str) \u2013 Natural language query.</p> </li> <li> <p>k (int) \u2013 Number of results (default 6).</p> </li> <li> <p>rerank (bool) \u2013 Currently ignored (hybrid already fuses scores).</p> </li> </ul> <p>Returns list[SearchHit] \u2013 Ranked hits.</p>"},{"location":"reference/context-rag/#raganswer","title":"rag.answer","text":"<p><pre><code>answer(corpus_id: str, question: str, *, llm: GenericLLMClient | None = None, style: str = \"concise\", with_citations: bool = True, k: int = 6) -&gt; dict\n</code></pre> Answer a question using retrieved context and an LLM.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>question (str) \u2013 End\u2011user question.</p> </li> <li> <p>llm (GenericLLMClient, optional) \u2013 LLM to use; defaults to the facade\u2019s configured client.</p> </li> <li> <p>style ({\"concise\",\"detailed\"}) \u2013 Answer verbosity/style.</p> </li> <li> <p>with_citations (bool) \u2013 Whether to include resolved citations.</p> </li> <li> <p>k (int) \u2013 Retrieval depth (default 6).</p> </li> </ul> <p>Returns dict \u2013 <code>{ \"answer\": str, \"citations\": [...], \"usage\": {...}, \"resolved_citations\": [...]? }</code>.</p> <p>Behavior - Builds a context block from top\u2011k chunks (numbered <code>[1]</code>, <code>[2]</code>, ...).</p> <ul> <li>Prompts the LLM to answer only from the provided context and cite chunk numbers.</li> </ul>"},{"location":"reference/context-rag/#ragresolve_citations","title":"rag.resolve_citations","text":"<p><pre><code>resolve_citations(corpus_id: str, citations: list[dict]) -&gt; list[dict]\n</code></pre> Resolve citation metadata for display/download.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>citations (list[dict]) \u2013 Items like <code>{ \"chunk_id\", \"doc_id\", \"rank\" }</code>.</p> </li> </ul> <p>Returns list[dict] \u2013 Sorted by <code>rank</code>, each <code>{ rank, doc_id, title, uri, chunk_id, snippet }</code>.</p>"},{"location":"reference/context-rag/#practical-examples","title":"Practical examples","text":"<p>1) Create a corpus and ingest docs <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"rag_ingest\")\nasync def rag_ingest(*, context):\n    await context.rag().add_corpus(\"notes\")\n    stats = await context.rag().upsert_docs(\n        corpus_id=\"notes\",\n        docs=[\n            {\"text\": \"Optics basics: Snell's law relates angles of incidence and refraction.\" , \"title\": \"optics\"},\n            {\"path\": \"/data/papers/holography.md\", \"labels\": {\"topic\": \"holography\"}},\n        ],\n    )\n    await context.channel().send_text(f\"RAG upsert: {stats}\")\n</code></pre></p> <p>2) Search and preview hits <pre><code>@graph_fn(name=\"rag_search_preview\")\nasync def rag_search_preview(*, context, q: str):\n    hits = await context.rag().search(corpus_id=\"notes\", query=q, k=5)\n    for i, h in enumerate(hits, 1):\n        await context.channel().send_text(f\"[{i}] score={h.score:.3f}  doc={h.doc_id}\\n{h.text[:200]}\")\n</code></pre></p> <p>3) Answer with citations <pre><code>@graph_fn(name=\"rag_answer_with_citations\")\nasync def rag_answer_with_citations(*, context, q: str):\n    out = await context.rag().answer(corpus_id=\"notes\", question=q, style=\"concise\", k=6)\n    ans = out.get(\"answer\", \"\")\n    cites = out.get(\"resolved_citations\", [])\n    await context.channel().send_text(ans)\n    for c in cites[:3]:\n        await context.channel().send_text(f\"[#{c['rank']}] {c['title']} \u2014 {c['snippet']}\")\n</code></pre></p>"},{"location":"reference/context-rag/#notes-behaviors","title":"Notes &amp; behaviors","text":"<ul> <li> <p>Chunking &amp; Embedding: Documents are split via <code>TextSplitter</code> then embedded in batch; the index stores <code>(chunk_id, vector, meta)</code>.</p> </li> <li> <p>Artifacts: File docs and inline text are persisted to the Artifact Store; returned URIs appear in doc metadata and resolved citations.</p> </li> <li> <p>IDs: <code>doc_id</code> and <code>chunk_id</code> are stable SHA\u2011derived IDs; re\u2011ingesting the same content usually yields the same IDs (subject to meta changes).</p> </li> <li> <p>Filters: <code>filters</code> is reserved for future adapter support (label\u2011based narrowing).</p> </li> <li> <p>LLM &amp; Usage: <code>answer()</code> returns provider usage where available; some providers may omit it.</p> </li> </ul>"},{"location":"reference/decorators/","title":"Decorator API \u2014 <code>@graph_fn</code>, <code>@graphify</code>, <code>@tool</code>","text":"<p>A single reference page for the three core decorators you\u2019ll use to build with AetherGraph.</p>"},{"location":"reference/decorators/#quick-chooser","title":"Quick chooser","text":"Use this when\u2026 Pick Why You want the quickest way to make a Python function runnable as a graph entrypoint and get a <code>context</code> for services <code>@graph_fn</code> Small, ergonomic, ideal for tutorials, notebooks, single\u2011entry tools/agents You need to expose reusable steps with typed I/O that can run standalone or as graph nodes <code>@tool</code> Dual\u2011mode decorator; gives you fine control of inputs/outputs; portable and composable Your function body is mostly tool wiring (fan\u2011in/fan\u2011out) and you want a static graph spec from Python syntax <code>@graphify</code> Author graphs declaratively; returns a <code>TaskGraph</code> factory; great for orchestration patterns"},{"location":"reference/decorators/#graph_fn","title":"<code>@graph_fn</code>","text":"<p>Wrap a normal async function into a runnable graph with optional <code>context</code> injection.</p>"},{"location":"reference/decorators/#signature","title":"Signature","text":"<pre><code>@graph_fn(name: str, *, inputs: list[str] | None = None, outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\nasync def my_fn(..., *, context: NodeContext): ...\n</code></pre>"},{"location":"reference/decorators/#description","title":"Description","text":"<ul> <li>Builds a fresh <code>TaskGraph</code> under the hood and executes it immediately.</li> <li>If your function signature includes <code>context: NodeContext</code>, AetherGraph injects a <code>NodeContext</code> so you can call <code>context.channel()</code>, <code>context.memory()</code>, <code>context.artifacts()</code>, <code>context.llm()</code>, etc.</li> <li>Ideal for single\u2011file demos, CLI/notebook usage, and simple agents.</li> </ul>"},{"location":"reference/decorators/#parameters","title":"Parameters","text":"<ul> <li>name (str, required) \u2014 Graph ID and human\u2011readable name.</li> <li>inputs (list[str], optional) \u2014 Declared input keys. Purely declarative; your function still gets normal Python args.</li> <li>outputs (list[str], optional) \u2014 Declared output keys. If you return a single literal, declare exactly one.</li> <li>version (str, optional) \u2014 Semantic version for registry.</li> <li>agent (str, optional) \u2014 If provided, registers this graph function as an agent under the given name.</li> </ul>"},{"location":"reference/decorators/#returns","title":"Returns","text":"<ul> <li>The decorator returns a <code>GraphFunction</code> object. Calling/awaiting it executes the graph and returns a <code>dict</code> of outputs keyed by <code>outputs</code>.</li> </ul>"},{"location":"reference/decorators/#minimal-example","title":"Minimal example","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"hello.world\", inputs=[\"name\"], outputs=[\"greeting\"], version=\"0.1.0\")\nasync def hello_world(name: str, *, context: NodeContext):\n    await context.channel().send_text(f\"\ud83d\udc4b Hello {name}\")\n    return {\"greeting\": f\"Hello, {name}!\"}\n\n# Run (async)\nres = await hello_world(name=\"Aether\")\nprint(res[\"greeting\"])  # \u2192 \"Hello, Aether!\"\n</code></pre>"},{"location":"reference/decorators/#tips","title":"Tips","text":"<ul> <li>Return a <code>dict</code> where keys match <code>outputs</code>. If you return a single literal, declare one output.</li> <li>You can call <code>@tool</code> functions inside a <code>@graph_fn</code> (they\u2019ll run immediately, not build nodes). Use this for small, fast helper steps.</li> <li>For complex orchestration (fan\u2011in/fan\u2011out), prefer <code>@graphify</code> so <code>@tool</code> calls become nodes.</li> </ul>"},{"location":"reference/decorators/#tool","title":"<code>@tool</code>","text":"<p>Dual\u2011mode decorator for reusable steps with explicit inputs/outputs.</p>"},{"location":"reference/decorators/#signature_1","title":"Signature","text":"<pre><code>@tool(outputs: list[str], *, inputs: list[str] | None = None, name: str | None = None, version: str = \"0.1.0\")\ndef/async def my_tool(...): ...\n</code></pre>"},{"location":"reference/decorators/#description_1","title":"Description","text":"<ul> <li>Immediate mode (no builder/interpreter active): calling the function executes it now and returns a <code>dict</code> of outputs.</li> <li>Graph mode (inside a <code>graph(...)</code> / <code>@graphify</code> body or during <code>@graph_fn</code> build): calling the proxy adds a node to the current graph and returns a <code>NodeHandle</code> with typed outputs.</li> <li>Registers the underlying implementation in the runtime registry for portability.</li> </ul>"},{"location":"reference/decorators/#parameters_1","title":"Parameters","text":"<ul> <li>outputs (list[str], required) \u2014 Names of output values (e.g., <code>[\"result\"]</code>, <code>[\"image\", \"stats\"]</code>).</li> <li>inputs (list[str], optional) \u2014 Input names (auto\u2011inferred from signature if omitted).</li> <li>name (str, optional) \u2014 Registry/display name; defaults to function name.</li> <li>version (str, optional) \u2014 Semantic version for registry.</li> </ul>"},{"location":"reference/decorators/#returns_1","title":"Returns","text":"<ul> <li>In immediate mode: <code>dict</code> of outputs.</li> <li>In graph mode: <code>NodeHandle</code> with <code>.out_key</code> attributes (e.g., <code>node.result</code>).</li> </ul>"},{"location":"reference/decorators/#example-reusable-step","title":"Example \u2014 reusable step","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"sum\", \"count\"])\ndef aggregate(xs: list[int]):\n    return {\"sum\": sum(xs), \"count\": len(xs)}\n\n# Immediate mode\nprint(aggregate([1,2,3]))  # {\"sum\": 6, \"count\": 3}\n</code></pre>"},{"location":"reference/decorators/#example-using-tool-inside-graph_fn-immediate-execution","title":"Example \u2014 using <code>@tool</code> inside <code>@graph_fn</code> (immediate execution)","text":"<pre><code>from aethergraph import graph_fn, tool, NodeContext\n\n@tool(outputs=[\"sum\"])  \ndef add(x: int, y: int):\n    return {\"sum\": x + y}\n\n@graph_fn(name=\"calc.pipeline\", inputs=[\"a\",\"b\"], outputs=[\"total\"])\nasync def calc(a: int, b: int, *, context: NodeContext):\n    out = add(a, b)                 # immediate mode here\n    await context.channel().send_text(f\"sum = {out['sum']}\")\n    return {\"total\": out[\"sum\"]}\n</code></pre>"},{"location":"reference/decorators/#tips_1","title":"Tips","text":"<ul> <li>Use <code>@tool</code> to make steps portable and inspectable (typed I/O makes graphs predictable).</li> <li>In <code>@graph_fn</code> the <code>@tool</code> call executes immediately; in <code>@graphify</code> the same call becomes a graph node.</li> <li>Control\u2011flow knobs like <code>_after</code>, <code>_id</code>, <code>_alias</code> apply only in graph\u2011building contexts (e.g., <code>@graphify</code>), not in <code>@graph_fn</code> bodies.</li> </ul>"},{"location":"reference/decorators/#graphify","title":"<code>@graphify</code>","text":"<p>Author a static TaskGraph by writing normal Python that calls <code>@tool</code>s. The function body executes during build to register nodes and edges; returned node handles/literals define graph outputs.</p>"},{"location":"reference/decorators/#signature_2","title":"Signature","text":"<pre><code>@graphify(*, name: str = \"default_graph\", inputs: Iterable[str] | dict = (), outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\ndef my_graph(...):\n    ...  # body calls @tool proxies (graph mode)\n    return {...}  # NodeHandle(s) and/or literal refs\n</code></pre>"},{"location":"reference/decorators/#description_2","title":"Description","text":"<ul> <li>The decorated function becomes a factory: calling <code>my_graph.build()</code> returns a <code>TaskGraph</code> spec.</li> <li>When the body runs under the builder, calls to <code>@tool</code> proxies add nodes to the graph and return <code>NodeHandle</code>s.</li> <li>Perfect for fan\u2011out (parallel branches) and fan\u2011in (join/aggregate) patterns.</li> </ul>"},{"location":"reference/decorators/#parameters_2","title":"Parameters","text":"<ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (iterable[str] or dict) \u2014 Required/optional input names. If dict, keys are optional names with defaults in the body.</li> <li>outputs (list[str], optional) \u2014 Names of exposed boundary outputs. If body returns a single literal, declare exactly one.</li> <li>version (str) \u2014 Semantic version.</li> <li>agent (str, optional) \u2014 Register this graph as an agent (factory registered).</li> </ul>"},{"location":"reference/decorators/#returns_2","title":"Returns","text":"<ul> <li> <p>The decorator returns a builder function with:</p> </li> <li> <p><code>.build() -&gt; TaskGraph</code></p> </li> <li><code>.spec() -&gt; TaskGraphSpec</code></li> <li><code>.io() -&gt; IO signature</code></li> </ul>"},{"location":"reference/decorators/#example-fanout-fanin","title":"Example \u2014 fan\u2011out + fan\u2011in","text":"<pre><code>from aethergraph import graphify, tool\n\n@tool(outputs=[\"y\"])\ndef f(x: int):\n    return {\"y\": x * x}\n\n@tool(outputs=[\"z\"])\ndef g(x: int):\n    return {\"z\": x + 1}\n\n@tool(outputs=[\"sum\"])  \ndef add(a: int, b: int):\n    return {\"sum\": a + b}\n\n@graphify(name=\"fan_in_out\", inputs=[\"x\"], outputs=[\"total\"]) \ndef pipe(x):\n    a = f(x=x)          # node A (graph mode)  \u2510\n    b = g(x=x)          # node B (graph mode)  \u2518  \u2190 fan\u2011out\n    c = add(a=a.y, b=b.z)   # node C depends on A,B \u2190 fan\u2011in\n    return {\"total\": c.sum}\n\nG = pipe.build()\n</code></pre>"},{"location":"reference/decorators/#example-ordering-with-_after-and-aliasing","title":"Example \u2014 ordering with <code>_after</code> and aliasing","text":"<pre><code>@tool(outputs=[\"out\"]) \ndef step(name: str):\n    return {\"out\": name}\n\n@graphify(name=\"ordered\", inputs=[]) \ndef ordered():\n    a = step(name=\"A\", _alias=\"first\")\n    b = step(name=\"B\", _after=a)\n    c = step(name=\"C\", _after=[a, b], _id=\"third\")\n    return {\"final\": c.out}\n\nG = ordered.build()\n</code></pre>"},{"location":"reference/decorators/#using-tool-inside-graph_fn-vs-graphify","title":"Using <code>@tool</code> inside <code>@graph_fn</code> vs <code>@graphify</code>","text":"<ul> <li>Inside <code>@graph_fn</code>: <code>@tool</code> calls execute immediately (no <code>_after</code>/alias). Great for quick helpers.</li> <li>Inside <code>@graphify</code>: <code>@tool</code> calls define nodes (support <code>_after</code>, <code>_alias</code>, <code>_id</code>, <code>_labels</code>). Ideal for orchestration.</li> </ul>"},{"location":"reference/decorators/#interop-best-practices","title":"Interop &amp; best practices","text":"<ol> <li>Start simple with <code>@graph_fn</code> \u2014 it\u2019s the easiest way to get <code>context</code> and ship a working demo.</li> <li>Extract reusable steps with <code>@tool</code> \u2014 typed I/O makes debugging, tracing, and promotion to graphs trivial.</li> <li> <p>Promote to <code>@graphify</code> when you need:</p> </li> <li> <p>Parallel branches (fan\u2011out), joins (fan\u2011in)</p> </li> <li>Explicit ordering with <code>_after</code></li> <li>Reuse via <code>NodeHandle</code> composition and aliasing</li> <li> <p>Context access:</p> </li> <li> <p><code>@graph_fn</code> gives you <code>context: NodeContext</code> directly.</p> </li> <li>In <code>@graphify</code>, nodes don\u2019t get <code>context</code>; tools run with context at execution time when the graph is interpreted. Use <code>@tool</code> implementations to call <code>context.*</code>.</li> <li>Outputs discipline \u2014 keep outputs small and typed (e.g., <code>{ \"image\": ref, \"metrics\": {\u2026} }</code>).</li> <li>Registry \u2014 all three decorators register artifacts (graph fn as runnable, tool impls, graph factories) so you can call by name later.</li> </ol>"},{"location":"reference/decorators/#see-also","title":"See also","text":"<ul> <li>Quick Start: install, start server, first <code>@graph_fn</code>.</li> <li>**Contex</li> </ul>"},{"location":"reference/rest-api/","title":"REST API","text":"<ul> <li><code>GET /health</code> \u2192 200 OK</li> <li><code>POST /execute</code> \u2192 Execute a graph function</li> <li><code>GET/PUT /artifacts/*</code> \u2192 Retrieve/store artifacts</li> </ul> <p>(Add OpenAPI/Redoc when ready.)</p>"},{"location":"reference/tools-facade/","title":"Tools Facade","text":""},{"location":"reference/tools-facade/#registerfunc-namenone-inputsnone-outputsnone-str","title":"register(func, *, name=None, inputs=None, outputs=None) \u2192 str","text":"<p>Registers a tool and returns its name/id.</p>"},{"location":"reference/tools-facade/#callname-args-dict-dict","title":"call(name, args: dict) \u2192 dict","text":"<p>Invokes a tool by name with validated args.</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/","title":"Tutorial 1: Build Your First <code>graph_fn</code>","text":"<p>This tutorial walks you through the core API of AetherGraph and helps you build your first reactive agent using the <code>@graph_fn</code> decorator.</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#overview","title":"\ud83d\ude80 Overview","text":"<p>AetherGraph introduces a Python\u2011native way to create agents that can think, wait, and talk \u2014 all inside a normal Python function. With the <code>@graph_fn</code> decorator, you can:</p> <ul> <li>Add context\u2011aware I/O (<code>context.channel()</code>, <code>context.llm()</code>, <code>context.memory()</code>)</li> <li>Run interactively or headlessly</li> <li>Chain, nest, and resume computations without defining a custom graph DSL</li> </ul> <p>In this tutorial, you will:</p> <ol> <li>Start the AetherGraph server (the sidecar)</li> <li>Define your first <code>graph_fn</code></li> <li>Call an LLM and send messages through the channel</li> <li>Run it synchronously and see the result</li> </ol>"},{"location":"tutorials/t1-build-your-first-graph-fn/#1-boot-the-sidecar","title":"1. Boot the Sidecar","text":"<p>Before you run any agent, you must start the sidecar server, which wires up the runtime services such as channel communication, artifact storage, memory, and resumptions.</p> <pre><code>from aethergraph import start_server\n\nurl = start_server()  # launches a lightweight FastAPI server in the background\nprint(\"AetherGraph sidecar server started at:\", url)\n</code></pre> <p>The sidecar is safe to start anywhere \u2014 even in Jupyter or interactive shells. It sets up a workspace under <code>./aethergraph_data</code> by default. Your data, including artifacts, memory, resumption files, will all be exported to workspace for persist access.</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#2-define-a-minimal-agent","title":"2. Define a Minimal Agent","text":"<p>A <code>graph_fn</code> is a context\u2011injected async function that represents a reactive node or agent.</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"hello_world\")\nasync def hello_world(input_text: str, *, context: NodeContext):\n    context.logger().info(\"hello_world started\")\n\n    # Send a message via the default channel (console)\n    await context.channel().send_text(f\"\ud83d\udc4b Hello! You sent: {input_text}\")\n\n    # Optional: Call an LLM directly from the context\n    llm_text, _usage = await context.llm().chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"Be brief.\"},\n            {\"role\": \"user\", \"content\": f\"Say hi back to: {input_text}\"},\n        ]\n    )\n    await context.channel().send_text(f\"LLM replied: {llm_text}\")\n\n    output = input_text.upper()\n    context.logger().info(\"hello_world finished\")\n    return {\"final_output\": output}\n</code></pre> <p>Return value: Although you can return any data type in a dictionary, it is suggested to return a dictionary of JSON-serializable results (e.g. <code>{\"result\": value}</code>). For large data or binary files, save them via <code>context.artifacts().write(...)</code> and return the artifact path/uri instead for later reuse. </p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#key-concepts","title":"Key Concepts","text":"Concept Description <code>@graph_fn</code> Turns a plain async Python function into a context\u2011aware agent. <code>NodeContext</code> Injected automatically. Gives access to channels, memory, LLMs, and logging. <code>context.channel()</code> Sends and receives messages (console, Slack, web UI, etc.). <code>context.llm()</code> Unified interface to language models via environment configuration. <code>context.logger()</code> Node\u2011aware structured logging. <p>See the full API of <code>graph_fn</code> at Graph Function API</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#3-run-the-agent","title":"3. Run the Agent","text":"<p>You can run your <code>graph_fn</code> directly or use helper runners.</p> <pre><code>from aethergraph import run\n\nif __name__ == \"__main__\":\n    result = run(hello_world, inputs={\"input_text\": \"hello world\"})\n    print(\"Result:\", result)\n</code></pre> <p>Output example:</p> <pre><code>[AetherGraph] \ud83d\udc4b Hello! You sent: hello world\n[AetherGraph] LLM replied: Hi there!\nResult: {'final_output': 'HELLO WORLD'}\n</code></pre> <p>You can also <code>await hello_world(...)</code> in any async context \u2014 all <code>graph_fn</code>s are awaitable by design.</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#4-why-this-is-different","title":"4. Why This is Different","text":"<p>AetherGraph\u2019s <code>@graph_fn</code> model is Pythonic yet agentic. Unlike traditional workflow frameworks that require static DAG definitions, AetherGraph lets you:</p> <ul> <li>Run without pre\u2011declaring a graph \u2013 it dynamically builds one as you go.</li> <li>Access unified runtime services \u2013 channels, memory, artifacts, LLMs, and schedulers are all injected via context.</li> <li>Compose natively \u2013 you can <code>await</code> another <code>graph_fn</code>, mix <code>@tool</code>s, or parallelize with <code>asyncio.gather</code>.</li> <li>Stay resumable \u2013 everything you run is automatically backed by a persistent runtime; you can resume mid\u2011flow later.</li> </ul> <p>These traits make AetherGraph unique among Python agent frameworks \u2014 designed not only for chatbots, but also for scientific, engineering, and simulation workflows.</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#5-next-steps","title":"5. Next Steps","text":"<p>In the next tutorial, you\u2019ll learn how to turn these reactive functions into static DAGs using <code>graphify()</code>, enabling resumable and inspectable computation graphs.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/","title":"Tutorial 2: Static Graphs with <code>@graphify</code>","text":"<p><code>@graphify</code> turns a plain Python function into a graph builder. Instead of executing immediately (like <code>@graph_fn</code>), it builds a deterministic TaskGraph from <code>@tool</code> calls \u2014 a DAG you can inspect, persist, and run later.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#mental-model","title":"\ud83e\udded Mental Model","text":"<p><code>@graph_fn</code> \u2192 executes now (reactive, context\u2011rich)</p> <p><code>@graphify</code> \u2192 builds first, runs later (deterministic DAG)</p> <ul> <li>Each <code>@tool</code> call becomes a node in the DAG.</li> <li>Edges are formed by data flow and optional ordering via <code>_after=[\u2026]</code>.</li> <li>You get reproducibility, inspectability, and clean fan\u2011in/fan\u2011out.</li> </ul> <p>Note: Access runtime services (<code>channel</code>, <code>llm</code>, <code>memory</code>) through tools in static graphs. If you need direct <code>context.*</code> calls inline, use <code>@graph_fn</code>.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#1-key-rules-short","title":"1. Key Rules (short)","text":"<ul> <li>Only <code>@tool</code> calls are allowed as steps in a <code>@graphify</code> builder. Use plain Python only to wire values or format the graph (no side\u2011effects); such code will not appear as nodes.</li> <li>Build \u2260 Run. Calling a <code>@graphify</code> function returns a TaskGraph. Use a runner to execute it.</li> <li>Async supported. Tools can be sync or async; the runner provides both sync and async entry points.</li> <li>Resumption requires stable IDs. Give important nodes a fixed <code>*_id</code> and reuse the same <code>run_id</code> when resuming.</li> <li>Outputs: Return a dict of JSON\u2011serializable values for resumption. Large/binary data \u2192 save via <code>artifacts()</code> and return a reference. (Full rules live in the API page.)</li> </ul> <p>Related: <code>@graph_fn</code> can also emit an implicit graph when you call <code>@tool</code>s inside it. Use <code>_after</code> to enforce ordering there too, and inspect the last run\u2019s captured graph with <code>graph_fn.last_graph</code>.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#2-shapes-tools-graphify","title":"2. Shapes (tools &amp; graphify)","text":""},{"location":"tutorials/t2-static-graph-with-graphify/#tool-shape-suggested","title":"<code>@tool</code> shape (suggested)","text":"<pre><code>from aethergraph import tool\n\n@tool(name=\"load_csv\", outputs=[\"rows\"])            # names become handle fields. Name is optional running locally\ndef load_csv(path: str) -&gt; dict:                    # return dict matching outputs\n    # ... load and parse ...\n    return {\"rows\": rows}\n</code></pre> <ul> <li><code>@tool</code> can be sync or async, they all run as async internally. </li> <li>Declare <code>outputs=[...]</code>. Returned dict must contain those keys.</li> </ul>"},{"location":"tutorials/t2-static-graph-with-graphify/#graphify-shape-suggested","title":"<code>@graphify</code> shape (suggested)","text":"<pre><code>from aethergraph import graphify\n\n@graphify(name=\"etl\", inputs=[\"csv_path\"], outputs=[\"nrows\"])  # declarative I/O\ndef etl(csv_path: str):\n    raw = load_csv(path=csv_path)         # node\n    # ... add more tool calls ...\n    return {\"nrows\": len(raw.rows)}      # JSON-serializable outputs\n</code></pre> <ul> <li><code>graphify</code> is always a sync function. No <code>await</code> allowed inside the builder.</li> <li>Use <code>_after=...</code> to force ordering when no data edge exists.</li> <li>Calling <code>etl()</code> builds a <code>TaskGraph</code>; it does not run.</li> <li>Run using <code>run(...)</code> / <code>run_async(...)</code> with <code>inputs={...}</code>.</li> </ul>"},{"location":"tutorials/t2-static-graph-with-graphify/#3-minimal-example-build-run","title":"3. Minimal Example \u2014 Build \u2192 Run","text":"<pre><code>from aethergraph import graphify, tool\nfrom aethergraph.runner import run  \n\n@tool(outputs=[\"doubled\"])  \ndef double(x: int) -&gt; dict:\n    return {\"doubled\": x * 2}\n\n@tool(outputs=[\"shifted\"])  \ndef add_ten(x: int) -&gt; dict:\n    return {\"shifted\": x + 10}\n\n@graphify(name=\"tiny_pipeline\", inputs=[\"x\"], outputs=[\"y\"])\ndef tiny_pipeline(x: int):\n    a = double(x=x)                   # node A\n    b = add_ten(x=a.doubled)         # node B depends on A via data edge\n    return {\"y\": b.shifted}\n\n# Build (no execution yet)\nG = tiny_pipeline()                   # \u2192 TaskGraph\n\n# Run (sync helper, useful in Jupyter notebook)\nresult = run(G, inputs={\"x\": 7})\nprint(result)  # {'y': 24}\n</code></pre> <p>Try <code>max_concurrency=1</code> vs <code>&gt;1</code> in the runner if your tools are async and parallelizable.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#4-ordering-without-data-edges-_after","title":"4. Ordering Without Data Edges \u2014 <code>_after</code>","text":"<pre><code>@tool(outputs=[\"ok\"])  \ndef fetch() -&gt; dict: ...\n\n@tool(outputs=[\"done\"])\ndef train() -&gt; dict: ...\n\n@graphify(name=\"seq\", inputs=[], outputs=[\"done\"])\ndef seq():\n    a = fetch()\n    b = train(_after=a)               # force run-after without wiring data\n    return {\"done\": b.done}\n</code></pre> <ul> <li>Use a single node or a list <code>_after=[a, b]</code>.</li> </ul>"},{"location":"tutorials/t2-static-graph-with-graphify/#5-resume-a-run-stable-_id-run_id","title":"5. Resume a Run \u2014 Stable <code>_id</code> + <code>run_id</code>","text":"<p>Resumption lets you continue a partially-completed graph without redoing finished nodes. This is useful for flaky I/O or long pipelines.</p> <ul> <li>Assign deterministic IDs to nodes with <code>_id=\"...\"</code> in your tools.</li> <li>Reuse the same <code>run_id</code> when invoking the runner again.</li> <li>Indefinite waits (e.g., human input) are supported via dedicated wait tools and are covered in the Channels &amp; Wait Models tutorial\u2014this section uses a non\u2011channel example.</li> </ul> <pre><code>from aethergraph import graphify, tool\nfrom aethergraph.core.runtime.graph_runner import run_async\nimport random\n\n@tool(outputs=[\"ok\"])  \ndef prepare() -&gt; dict:\n    # Pretend to set up workspace/artifacts\n    return {\"ok\": True}\n\n@tool(outputs=[\"value\"])  \ndef flaky_compute(x: int) -&gt; dict:\n    # Simulate a transient failure half the time\n    if random.random() &lt; 0.5:\n        raise RuntimeError(\"transient error \u2014 try resuming\")\n    return {\"value\": x * 2}\n\n@tool(outputs=[\"ok\"])  \ndef finalize(v: int) -&gt; dict:\n    # Commit final result (e.g., write an artifact)\n    return {\"ok\": True}\n\n@graphify(name=\"resumable_pipeline\", inputs=[\"x\"], outputs=[\"y\"]) \ndef resumable_pipeline(x: int):\n    s1 = prepare(_id=\"prepare_1\")\n    s2 = flaky_compute(x=x, _after=s1, _id=\"flaky_2\")  # may fail on first run\n    s3 = finalize(v=s2.value, _after=s2, _id=\"finalize_3\")\n    return {\"y\": s2.value}\n\n# First run may fail while computing 'flaky_2'...\n# await run_async(resumable_pipeline(), inputs={\"x\": 21}, run_id=\"run-abc\")\n\n# Re-run with the SAME run_id to resume from the failed node (prepare_1 is skipped):\n# await run_async(resumable_pipeline(), inputs={\"x\": 21}, run_id=\"run-abc\")\n</code></pre> <p>Keep <code>_id</code>s stable to allow the engine to match nodes. If a node fails or is interrupted, resuming with the same <code>run_id</code> will continue from the last successful checkpoint.</p> <p>Use json-serializable output in <code>@tool</code> so that Aethergraph can reload previous outputs; otherwise resumption may fail.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#6-inspect-beforeafter-running","title":"6. Inspect Before/After Running","text":"<p>Once you have a <code>TaskGraph</code> (e.g., <code>G = tiny_pipeline()</code>), you can:</p> <pre><code>print(G.pretty())           # readable node table\nprint(G.ascii_overview())   # compact topology\nprint(G.topological_order())\n\n# Graph metadata\nsig  = tiny_pipeline.io()   # declared inputs/outputs\nspec = tiny_pipeline.spec() # full GraphSpec (nodes, edges, metadata)\n\n# Export (if enabled)\ndot = G.to_dot()            # Graphviz DOT text\n# G.visualize()             # render to image if your env supports it\n</code></pre>"},{"location":"tutorials/t2-static-graph-with-graphify/#7-practical-tips","title":"7. Practical Tips","text":"<ul> <li>Keep nodes small and typed: expose clear outputs (e.g., <code>outputs=[\"clean\"]</code>).</li> <li>Use JSON\u2011serializable returns; store big/binary as artifacts.</li> <li>Prefer <code>_after</code> for control edges instead of fake data plumb\u2011through.</li> <li>No nested static graphs (don\u2019t call one <code>@graphify</code> from another). Use tools or run graphs separately.</li> <li>Async tools + <code>max_concurrency</code> unlock parallel speedups.</li> </ul>"},{"location":"tutorials/t2-static-graph-with-graphify/#8-summary","title":"8. Summary","text":"<ul> <li><code>@graphify</code> materializes a static DAG from <code>@tool</code> calls.</li> <li>Build with the function call; run with the runner (sync or async).</li> <li>For resumption, use stable <code>_id</code> per node and replay with the same <code>run_id</code>.</li> <li>Inspect graphs via <code>pretty()</code>, <code>ascii_overview()</code>, <code>.io()</code>, <code>.spec()</code>, and <code>to_dot()</code>.</li> </ul> <p>Use <code>@graphify</code> for pipelines and reproducible experiments; stick with <code>@graph_fn</code> for interactive, context\u2011heavy agents.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/","title":"Tutorial 3: Talk to Your Graph \u2014 Channels and Waits","text":"<p>This tutorial explains how your graph talks back \u2014 how agents communicate with the outside world and how different kinds of waits work under the hood. You\u2019ll learn the difference between cooperative waits (for live, stateless agents) and dual-stage waits (for resumable workflows), and how to use each effectively.</p> <p>Goal: Understand how channels unify I/O, and why only <code>@graphify</code> with dual-stage waits can resume safely after a crash.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#1-what-is-a-channel","title":"1. What Is a Channel?","text":"<p>A channel is your agent\u2019s communication route \u2014 Slack, Telegram, Web UI, or Console. It lets your code send messages, request input, and stream updates through a consistent API.</p> <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"greet\")\nasync def greet(*, context):\n    ch = context.channel(\"slack:#research\")   # pick a target channel\n    await ch.send_text(\"Starting demo\u2026\")\n    name = await ch.ask_text(\"Your name?\")    # cooperative wait\n    await ch.send_text(f\"Nice to meet you, {name}!\")\n    return {\"user\": name}\n</code></pre> <ul> <li>The <code>context.channel()</code> method returns a <code>ChannelSession</code> helper with async methods like <code>send_text</code>, <code>ask_text</code>, <code>ask_approval</code>, <code>ask_files</code>, <code>stream</code>, and <code>progress</code>.</li> <li>If no channel is configured, it falls back to the console (<code>console:stdin</code>).</li> </ul> <p>\ud83d\udca1 Channel setup and adapter configuration (Slack, Telegram, Web) are covered in Channel Setup.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#2-the-two-wait-models","title":"2. The Two Wait Models","text":"<p>AetherGraph supports two wait mechanisms \u2014 cooperative and dual-stage \u2014 both built on the continuation system but with very different lifecycles.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#cooperative-waits-via-contextchannel-methods","title":"Cooperative waits \u2014 via <code>context.channel()</code> methods","text":"<ul> <li>Implemented by <code>ChannelSession</code> (<code>ask_text</code>, <code>ask_approval</code>, <code>ask_files</code>, etc.).</li> <li>Work inside a running process \u2014 the node suspends, then resumes when the reply arrives.</li> <li>These waits are stateful for inspection, but not resumable; if the process dies, the session is lost.</li> <li>Used mainly in <code>@graph_fn</code> agents, which execute immediately and stay alive.</li> </ul>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#dual-stage-waits-via-built-in-channel-tools","title":"Dual-stage waits \u2014 via built-in channel tools","text":"<ul> <li>Implemented as @tool nodes in <code>aethergraph.tools</code> (<code>ask_text</code>, <code>send_text</code>, etc.).</li> <li>Each wait becomes a graph node stored in the runtime snapshot.</li> <li>Can pause indefinitely and resume after restarts using <code>run_id</code> in <code>@graphify</code>.</li> <li>Used in <code>@graphify</code> graphs, which are strictly persisted and versioned.</li> </ul> <p>\u26a0\ufe0f All built-in dual-stage methods are <code>@tool</code>s \u2014 do not call them inside another tool. They are meant for use in graphify or top-level graph_fn logic, not nested nodes.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#3-lifecycle-and-persistence","title":"3. Lifecycle and Persistence","text":"Concept <code>@graph_fn</code> + Cooperative Waits <code>@graphify</code> + Dual-Stage Waits Execution Runs immediately (reactive) Builds DAG, runs with scheduler State Stateful for in-process waits Snapshot persisted to disk or DB Wait behavior Cooperative (in-process) Dual-stage (resumable) Resume after crash \u274c Lost, consider saving progress in memory and sementic recovery \u2705 Recoverable with <code>run_id</code>  and stable <code>node_id</code>; set up <code>_id</code> when building the graph <p>You can also use the <code>context.channel()</code> method in <code>@graphify</code> for convenience within a <code>@tool</code>, or use dual-stage wait tools in <code>graph_fn</code>. However, these approaches cannot guarantee resumption due to the stateful nature of the method or graph. Caveat for console dual-stage tools: Console input is handled differently, and dual-stage waits do not support resumption for console channels. However, it is rare for a local process using the console to terminate unexpectedly.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#4-cooperative-wait-example","title":"4. Cooperative Wait Example","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"cooperative_wait\")\nasync def cooperative_wait(*, context):\n    ch = context.channel()\n    await ch.send_text(\"Processing...\")\n    ans = await ch.ask_approval(\"Continue?\", options=[\"Yes\", \"No\"])\n    if ans[\"approved\"]:\n        await ch.send_text(\"\u2705 Proceeding.\")\n    else:\n        await ch.send_text(\"\u274c Stopped.\")\n    return {\"ok\": ans[\"approved\"]}\n</code></pre> <ul> <li>Perfect for short-lived interactive runs.</li> <li>Not resumable if interrupted; all state is lost when the process exits.</li> <li>Consider saving states to memory for sementic recovery for non-critical tasks.</li> </ul>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#5-dual-stage-wait-example-resumable","title":"5. Dual-Stage Wait Example (Resumable)","text":"<pre><code>from aethergraph import graphify\nfrom aethergraph.tools import send_text, ask_text # built-in `@tool`. Do not use them in anohter `@tool`\n\n@graphify(name=\"dual_stage_greet\", inputs=[\"channel\"], outputs=[\"greeting\"])\ndef dual_stage_greet(channel: str):\n    a = send_text(text=\"Hello!\", channel=channel, _id=\"start\")\n    b = ask_text(prompt=\"What's your name?\", channel=channel, _after=a, _id=\"wait_name\")\n    c = send_text(text=f\"Hi {b.text}!\", channel=channel, _after=b, _id=\"reply\")\n    return {\"greeting\": c.text}\n</code></pre> <ul> <li>Each step is a tool node with a unique <code>_id</code>.</li> <li>If the process stops after <code>ask_text</code>, simply rerun with the same <code>run_id</code> to resume.</li> <li>The system restores from the last persisted snapshot.</li> </ul>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#key-takeaways","title":"Key Takeaways","text":"<ul> <li><code>context.channel()</code> methods implement cooperative waits \u2014 great for live agents.</li> <li>Built-in channel tools (<code>ask_text</code>, <code>send_text</code>, etc.) implement dual-stage waits \u2014 required for resumable graphs.</li> <li><code>graph_fn</code> is stateless, inspectable via <code>.last_graph</code> but not recoverable.</li> <li><code>graphify</code> uses snapshots to persist progress and enable recovery with <code>run_id</code>.</li> <li>Dual-stage tools are <code>@tool</code> nodes \u2014 never call them inside another tool.</li> </ul> <p>Channels make your graph talk. Wait models decide how long it remembers the conversation.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/","title":"Tutorial 4: Store &amp; Recall \u2014 Artifacts and Memory","text":"<p>This tutorial focus on the how Aethergraph can memorize what happened before. Learn how to save outputs, log results/events, and recall them later using AetherGraph\u2019s two persistence pillars:</p> <ul> <li>Artifacts \u2014 durable assets (files/dirs/JSON/text) stored by content address (CAS URI) with labels &amp; metrics for ranking and search.</li> <li>Memory \u2014 a structured event &amp; result log with fast \u201cwhat\u2019s the latest?\u201d recall (e.g., <code>last_by_name</code>), plus simple recent\u2011history queries.</li> </ul> <p>We\u2019ll build this up step\u2011by\u2011step with short, copy\u2011ready snippets.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#0-what-youll-use","title":"0. What you\u2019ll use","text":"<pre><code># Access services from your NodeContext\narts = context.artifacts()   # artifact store\nmem  = context.memory()      # event &amp; result log\n</code></pre> <p>Mental model: Artifacts hold large, immutable outputs. Memory records what happened and the small named values you need to recall quickly.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#1-save-something-get-a-uri-open-it-later","title":"1. Save something \u2192 get a URI \u2192 open it later","text":""},{"location":"tutorials/t4-store-recall-artifacts-memory/#a-ingest-an-existing-file","title":"A. Ingest an existing file","text":"<pre><code>art = await arts.save(\n    path=\"/tmp/report.pdf\",\n    kind=\"report\",                 # a short noun; you\u2019ll filter/rank by this\n    labels={\"exp\": \"A\"},          # 1\u20133 filters you actually plan to query\n    # metrics={\"bleu\": 31.2},      # optional if you\u2019ll rank later\n)\nuri = art.uri                       # stable CAS URI\n\n# When you need a real path again:\npath = arts.to_local_path(uri)\n</code></pre> <p>Why CAS? It prevents accidental overwrites and gives you a stable handle you can pass around (in Memory, dashboards, etc.).</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#b-streamwrite-no-temp-file-atomically","title":"B. Stream\u2011write (no temp file), atomically","text":"<pre><code>async with arts.writer(kind=\"plot\", planned_ext=\".png\") as w:\n    w.write(png_bytes)\n# on exit \u2192 the artifact is committed and indexed\n</code></pre> <p>Tip: Prefer <code>writer(...)</code> for programmatically produced bytes.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#2-record-results-youll-want-to-recall-fast","title":"2. Record results you\u2019ll want to recall fast","text":"<p>Use Memory for structured results and lightweight logs.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#a-record-a-typed-result-fast-recall-by-name","title":"A. Record a typed result (fast recall by name)","text":"<pre><code>await mem.write_result(\n    topic=\"train.step\",\n    outputs=[\n        {\"name\": \"val_acc\",  \"kind\": \"number\", \"value\": 0.912},\n        {\"name\": \"ckpt_uri\", \"kind\": \"uri\",    \"value\": uri},\n    ],\n)\n\nlast_acc = await mem.last_by_name(\"val_acc\")\n</code></pre> <p><code>write_result</code> indexes named values so <code>last_by_name(\"val_acc\")</code> is O(1) to fetch the latest.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#b-log-arbitrary-events-structured-but-lightweight","title":"B. Log arbitrary events (structured but lightweight)","text":"<pre><code>await mem.record(\n    kind=\"train_log\",\n    data={\"epoch\": 1, \"loss\": 0.25, \"acc\": 0.91},\n)\n\nrecent = await mem.recent(kinds=[\"train_log\"], limit=3)\n</code></pre> <p>Need only the decoded payloads?</p> <pre><code>logs = await mem.recent_data(kinds=[\"train_log\"], limit=3)\n</code></pre> <p>Use <code>record</code> for progress/trace breadcrumbs. Use <code>write_result</code> for small named values you\u2019ll query later.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#3-search-filter-and-rank-artifacts","title":"3. Search, filter, and rank artifacts","text":""},{"location":"tutorials/t4-store-recall-artifacts-memory/#a-search-by-labels-you-saved-earlier","title":"A. Search by labels you saved earlier","text":"<pre><code>hits = await arts.search(\n    kind=\"report\",\n    labels={\"exp\": \"A\"},    # exact\u2011match filter across indexed labels\n)\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#b-pick-best-so-far-by-a-metric","title":"B. Pick \u201cbest so far\u201d by a metric","text":"<pre><code>best = await arts.best(\n    kind=\"checkpoint\",\n    metric=\"val_acc\",   # must exist in artifact.metrics\n    mode=\"max\",         # or \"min\"\n    scope=\"run\",        # limit to current run | graph | node\n)\nif best:\n    best_path = arts.to_local_path(best.uri)\n</code></pre> <p>Attach <code>metrics={\"val_acc\": ...}</code> when saving to enable ranking later.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#4-endtoend-save-log-and-recall","title":"4. End\u2011to\u2011end: save, log, and recall","text":"<pre><code>@graph_fn(name=\"train_epoch\", outputs=[\"ckpt_uri\"])\nasync def train_epoch(epoch: int, *, context):\n    arts = context.artifacts()\n    mem  = context.memory()\n\n    # 1) Export a checkpoint to a temp path you control\n    tmp_path = \"/tmp/ckpt.bin\"\n\n    # 2) Ingest it as an Artifact\n    ckpt = await arts.save(\n        path=tmp_path,\n        kind=\"checkpoint\",\n        labels={\"epoch\": epoch},\n        # metrics={\"val_acc\": val_acc},\n    )\n\n    # 3) Record the important values for quick recall\n    await mem.write_result(\n        topic=\"train.epoch\",\n        outputs=[\n            {\"name\": \"epoch\",    \"kind\": \"number\", \"value\": epoch},\n            {\"name\": \"ckpt_uri\", \"kind\": \"uri\",    \"value\": ckpt.uri},\n        ],\n    )\n\n    return {\"ckpt_uri\": ckpt.uri}\n</code></pre> <p>Now, any later node can do:</p> <pre><code>latest_uri = await context.memory().last_by_name(\"ckpt_uri\")\npath = context.artifacts().to_local_path(latest_uri)\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#5-practical-recipes","title":"5. Practical recipes","text":""},{"location":"tutorials/t4-store-recall-artifacts-memory/#a-save-small-jsontext-directly","title":"A. Save small JSON/Text directly","text":"<pre><code>cfg_art = await arts.save_json({\"lr\": 1e-3, \"batch\": 64})\nlog_art = await arts.save_text(\"training finished ok\")\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#b-browse-everything-produced-in-this-run","title":"B. Browse everything produced in this run","text":"<pre><code>all_run_outputs = await arts.list(scope=\"run\")\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#c-pin-something-to-keep-forever","title":"C. Pin something to keep forever","text":"<pre><code>await arts.pin(artifact_id=cfg_art.artifact_id)\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#d-keep-memory-lean-but-persistent","title":"D. Keep Memory lean but persistent","text":"<ul> <li>Memory acts like a fixed\u2011length hot queue for fast recall (<code>last_by_name</code>, <code>recent</code>).</li> <li>All events are persisted for later inspection, but only a rolling window stays hot in KV for speed.</li> </ul>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#6-minimal-reference-schemas-helpers","title":"6. Minimal reference (schemas &amp; helpers)","text":"<p>You rarely need all fields. Here are the useful bits to recognize in code and logs.</p> <pre><code>@dataclass\nclass Artifact:\n    artifact_id: str\n    uri: str           # CAS URI\n    kind: str          # short noun (e.g., \"checkpoint\", \"report\")\n    labels: dict[str, Any]\n    metrics: dict[str, Any]\n    preview_uri: str | None = None\n    pinned: bool = False\n</code></pre> <pre><code>@dataclass\nclass Event:\n    event_id: str\n    ts: str\n    kind: str          # e.g., \"tool_result\", \"train_log\"\n    topic: str | None = None\n    inputs: list[Value] | None = None\n    outputs: list[Value] | None = None\n    metrics: dict[str, float] | None = None\n    text: str | None = None   # JSON string or message text\n    version: int = 1\n</code></pre> <p>Helper (already built\u2011in) that returns decoded payloads from <code>recent</code>:</p> <pre><code>async def recent_data(*, kinds: list[str], limit: int = 50) -&gt; list[Any]\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#7-when-to-use-what","title":"7. When to use what","text":"Need Use Why Keep a file/dir for later <code>artifacts.save(...)</code> / <code>writer(...)</code> Durable, deduped, indexed Store small JSON/Text <code>artifacts.save_json/text</code> Convenience, still indexed Quick recall of named values <code>memory.write_result</code> \u2192 <code>last_by_name</code> O(1) latest lookups Log structured progress/events <code>memory.record</code> \u2192 <code>recent</code>/<code>recent_data</code> Lightweight trace Pick the best checkpoint/report <code>artifacts.best(kind, metric, mode)</code> Built\u2011in ranking List everything from current run <code>artifacts.list(scope=\"run\")</code> One\u2011liner browse"},{"location":"tutorials/t4-store-recall-artifacts-memory/#8-rag-turning-history-into-answers-optional","title":"8. RAG: turning history into answers (optional)","text":"<p>Memory ships with a RAG facade so you can promote events/results into a searchable corpus.</p> <pre><code>@graph_fn(name=\"make_rag_corpus\", outputs=[\"answer\"])\nasync def make_rag_corpus(question: str, *, context):\n    mem = context.memory()\n\n    corpus = await mem.rag_bind(scope=\"project\")\n    await mem.rag_promote_events(\n        corpus_id=corpus,\n        where={\"kinds\": [\"tool_result\"], \"limit\": 200},\n    )\n    ans = await mem.rag_answer(corpus_id=corpus, question=question)\n    snap = await mem.rag_snapshot(corpus_id=corpus, title=\"Weekly knowledge snapshot\")\n    return {\"answer\": ans.get(\"answer\", \"\"), \"snapshot_uri\": snap.get(\"uri\")}\n</code></pre> <p>Use this if you want citations and cross\u2011run Q&amp;A on top of your logs.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#9-troubleshooting-tips","title":"9. Troubleshooting &amp; tips","text":"<ul> <li>I saved without labels/metrics \u2014 can I still search? Yes. You can list by scope and filter in Python. Add labels/metrics next time for richer queries.</li> <li>URIs vs paths? Always store/share URIs. Resolve to a path only when you need to read the bytes: <code>arts.to_local_path(uri)</code>.</li> <li>Performance: Keep Memory results tiny and focused (a few named values). Put large blobs in Artifacts.</li> <li>Naming: Re\u2011use a small stable set of <code>kind</code> values (e.g., <code>checkpoint</code>, <code>report</code>, <code>plot</code>). It pays off in search and dashboards.</li> </ul>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#summary","title":"Summary","text":"<ul> <li>Artifacts: durable results with CAS URIs, plus labels/metrics for search &amp; ranking.</li> <li>Memory: structured results &amp; events for instant recall (<code>last_by_name</code>, <code>recent</code>).</li> <li>Use them together: save outputs as Artifacts, then record the important URIs and numbers with <code>write_result</code> for fast retrieval.</li> <li>Optional: promote Memory into RAG to get searchable, cited answers across runs.</li> </ul> <p>See also: <code>context.artifacts()</code> \u00b7 <code>context.memory()</code> \u00b7 RAG helpers (<code>rag_bind</code>, <code>rag_promote_events</code>, <code>rag_answer</code>, <code>rag_snapshot</code>)</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/","title":"Tutorial 5: Add Intelligence \u2014 LLM &amp; RAG","text":"<p>This tutorial adds language models and retrieval\u2011augmented generation (RAG) to your agents. You\u2019ll:</p> <ol> <li>set up an LLM profile</li> <li>chat from a graph function</li> <li>build a searchable RAG corpus from your files/memory</li> <li>answer questions grounded by retrieved context (with optional citations)</li> </ol> <p>Works with OpenAI, Anthropic, Google (Gemini), OpenRouter, LM Studio, and Ollama via a unified GenericLLMClient.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#0-mental-model","title":"0. Mental model","text":"<ul> <li>LLM: a provider\u2011agnostic client you access via <code>context.llm(...)</code> for chat and embeddings.</li> <li>RAG: a corpus of documents (from files and/or Memory events) that are chunked, embedded, and retrieved to ground LLM answers.</li> </ul> <pre><code>llm = context.llm(profile=\"default\")   # chat &amp; embed\nrag = context.rag()                     # corpora, upsert, search, answer\n</code></pre>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>API keys for the providers you want (e.g., OpenAI, Anthropic, Gemini, OpenRouter).</li> <li>If using local models: LM Studio or Ollama running locally and a base URL.</li> </ul>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#2-configure-llms-profiles","title":"2. Configure LLMs (Profiles)","text":"<p>You can configure profiles in environment variables (recommended) or at runtime.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#a-env-profiles-recommended","title":"A) <code>.env</code> profiles (recommended)","text":"<p>Profiles are named by the section after <code>LLM__</code>. Example: a profile called <code>MY_OPENAI</code>:</p> <pre><code>AETHERGRAPH_LLM__MY_OPENAI__PROVIDER=openai\nAETHERGRAPH_LLM__MY_OPENAI__MODEL=gpt-4o-mini\nAETHERGRAPH_LLM__MY_OPENAI__TIMEOUT=60\nAETHERGRAPH_LLM__MY_OPENAI__API_KEY=sk-...\nAETHERGRAPH_LLM__MY_OPENAI__EMBED_MODEL=text-embedding-3-small  # needed for llm().embed() or RAG\n</code></pre> <p>Then in code:</p> <pre><code>llm = context.llm(profile=\"my_openai\")\ntext, usage = await llm.chat([...])\n</code></pre> <p>The default profile comes from your container config. Use profiles when you want to switch providers/models per node or per run.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#b-register-at-runtime-programmatic","title":"B) Register at runtime (programmatic)","text":"<p>Useful for notebooks/demos or dynamically wiring services:</p> <pre><code>from aethergraph.llm import register_llm_client, set_rag_llm_client\n\nclient = register_llm_client(\n    profile=\"runtime_openai\",\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    api_key=\"sk-...\",\n)\n\n# RAG can use a dedicated LLM (for embedding + answering). If not set, it uses the default profile.\nset_rag_llm_client(client=client)\n</code></pre> <p>You can also pass parameters directly to <code>set_rag_llm_client(provider=..., model=..., embed_model=..., api_key=...)</code>.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#c-oneoff-key-injection","title":"C) One\u2011off key injection","text":"<p>If you just need to override a key in memory for a demo:</p> <pre><code>context.llm_set_key(provider=\"openai\", api_key=\"sk-...\")\n</code></pre> <p>Sidecar note: If your run needs channels, resumable waits, or shared services, start the sidecar server before using runtime registration.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#3-chat-embed-from-a-graph-function","title":"3. Chat &amp; Embed from a Graph Function","text":""},{"location":"tutorials/t5-add-intelligence-llm-rag/#chat-provideragnostic","title":"Chat (provider\u2011agnostic)","text":"<pre><code>@graph_fn(name=\"ask_llm\")\nasync def ask_llm(question: str, *, context):\n    llm = context.llm(profile=\"my_openai\")  # or omit profile for default\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are concise and helpful.\"},\n        {\"role\": \"user\",   \"content\": question},\n    ]\n    reply, usage = await llm.chat(messages)\n    return {\"answer\": reply, \"usage\": usage}\n</code></pre>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#embeddings","title":"Embeddings","text":"<pre><code>vectors = await context.llm(profile=\"my_openai\").embed([\n    \"First text chunk\", \"Second text chunk\"\n])\n</code></pre> <p>RAG needs an embed model configured on the chosen profile.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#optional-reasoning-knobs","title":"Optional reasoning knobs","text":"<p>Some models (e.g., GPT\u20115) accept reasoning parameters such as <code>reasoning_effort=\"low|medium|high\"</code> via <code>llm.chat(..., reasoning_effort=...)</code>.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#4-raw-api-escape-hatch","title":"4. Raw API escape hatch","text":"<p>For power users who need endpoints not yet covered by the high\u2011level client (such as low-level inputs, VLM models, custom models):</p> <pre><code>openai = context.llm(profile=\"my_openai\")\npayload = {\n    \"model\": \"gpt-4o-mini\",\n    \"input\": [\n        {\"role\": \"system\", \"content\": \"You are concise.\"},\n        {\"role\": \"user\",   \"content\": \"Explain attention in one sentence.\"}\n    ],\n    \"max_output_tokens\": 128,\n    \"temperature\": 0.3,\n}\nraw = await openai.raw(path=\"/responses\", json=payload)\n</code></pre> <ul> <li><code>raw(path=..., json=...)</code> sends a verbatim request to the provider base URL.</li> <li>You are responsible for parsing the returned JSON shape.</li> </ul> <p>Use this when experimenting with new provider features before first\u2011class support lands in the client.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#5-rag-from-docs-memory-to-grounded-answers","title":"5. RAG: From Docs &amp; Memory to Grounded Answers","text":"<p>Flow: <code>Files/Events \u2192 chunk + embed \u2192 index \u2192 retrieve top\u2011k \u2192 LLM answers with context</code>.</p> <ul> <li>Corpora live behind <code>context.rag()</code>.</li> <li>Ingest files (by path) and inline text, and/or promote Memory events into a corpus.</li> </ul>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#a-backend-storage","title":"A) Backend &amp; storage","text":"<p>Default vector index: SQLite (local, zero\u2011dep) \u2014 great for laptops and small corpora.</p> <p>Switch to FAISS: faster ANN search for larger corpora.</p> <p>Set up RAG backend: </p> <ul> <li>Env:</li> </ul> <pre><code># RAG Settings\nAETHERGRAPH_RAG__BACKEND=faiss        # or sqlite\nAETHERGRAPH_RAG__DIM=1536             # embedding dimension (e.g., OpenAI text-embedding-3-small)\n</code></pre> <ul> <li>Runtime:</li> </ul> <pre><code>from aethergraph.services.rag import set_rag_index_backend\n\nset_rag_index_backend(backend=\"faiss\", dim=1536)\n# If FAISS is not installed, it logs a warning and falls back to SQLite automatically.\n</code></pre> <ul> <li>On\u2011disk layout: each corpus stores <code>corpus.json</code>, <code>docs.jsonl</code>, <code>chunks.jsonl</code>; source files are saved as Artifacts for provenance.</li> </ul>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#b-build-update-a-corpus-from-files-text","title":"B) Build / update a corpus from files &amp; text","text":"<pre><code>await context.rag().upsert_docs(\n    corpus_id=\"my_docs\",\n    docs=[\n        {\"path\": \"data/report.pdf\", \"labels\": {\"type\": \"report\"}},\n        {\"text\": \"Experiment hit 91.2% accuracy on CIFAR-10.\", \"title\": \"exp-log\"},\n    ],\n)\n</code></pre> <ul> <li> <p>Use file docs when you already have a local file: <code>{\"path\": \"/abs/or/relative.ext\", \"labels\": {...}}</code>. Supported \u201csmart-parsed\u201d types are <code>.pdf</code>, <code>.md/markdown</code>, and <code>.txt</code> (others are treated as plain text). The original file is saved as an Artifact for provenance; if your PDF is a scan, run OCR first (we only extract selectable text). </p> </li> <li> <p>Use inline docs when you have content in memory: <code>{\"text\": \"...\", \"title\": \"nice-short-title\", \"labels\": {...}}</code>. Keep titles short and meaningful; add 1\u20133 optional labels you\u2019ll actually filter by (e.g., <code>{\"source\":\"lab\", \"week\":2}</code>).</p> </li> </ul> <p>Behind the scenes: documents are stored as Artifacts, parsed, chunked, embedded, and added to the vector index.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#c-promote-memory-events-into-rag","title":"C) Promote Memory events into RAG","text":"<pre><code>corpus = await context.memory().rag_bind()\nawait context.memory().rag_promote_events(\n    corpus_id=corpus,\n    where={\"kinds\": [\"tool_result\"], \"limit\": 200},\n)\n</code></pre> <p>You can promote any custom <code>kind</code> you recorded for later vector-based search and answer in a same <code>corpus_id</code>.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#d-search-retrieve-answer-with-citations","title":"D) Search, retrieve, answer (with citations)","text":"<pre><code>hits = await context.rag().search(\"my_docs\", \"key findings\", k=8, mode=\"hybrid\")\nans  = await context.rag().answer(\n    corpus_id=\"my_docs\",\n    question=\"Summarize the main findings and list key metrics.\",\n    style=\"concise\",\n    with_citations=True,\n    k=6,\n)\n# ans \u2192 { \"answer\": str, \"citations\": [...], \"resolved_citations\": [...], \"usage\": {...} }\n</code></pre> <p>Use <code>resolved_citations</code> to map snippets back to Artifact URIs for auditability.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#e-choosing-the-llm-for-rag","title":"E) Choosing the LLM for RAG","text":"<p>RAG uses a dedicated RAG LLM client that must have both <code>model</code> and <code>embed_model</code> set.</p> <p>Runtime:</p> <pre><code>from aethergraph.llm import set_rag_llm_client\nset_rag_llm_client(provider=\"openai\", model=\"gpt-4o-mini\", embed_model=\"text-embedding-3-small\", api_key=\"sk-\u2026\")\n</code></pre> <p>If you don\u2019t set one, it falls back to the default LLM profile (ensure that profile also has an <code>embed_model</code>).</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#f-corpus-management-ops","title":"F) Corpus management (ops)","text":"<p>For maintenance and ops you can:</p> <ul> <li>List corpora / docs to inspect what\u2019s indexed.</li> <li>Delete docs to remove vectors and records.</li> <li>Re\u2011embed to refresh vectors after changing embed model or chunking.</li> <li>Stats to view counts of docs/chunks and corpus metadata.</li> </ul> <p>These live on the same facade: <code>rag.list_corpora()</code>, <code>rag.list_docs(...)</code>, <code>rag.delete_docs(...)</code>, <code>rag.reembed(...)</code>, <code>rag.stats(...)</code>.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#6-practical-recipes","title":"6. Practical recipes","text":"<ul> <li>Switch providers by changing <code>profile=</code> in <code>context.llm(...)</code> without touching your code elsewhere.</li> <li>Save docs as Artifacts (e.g., <code>save_text</code>, <code>save(path=...)</code>) and ingest by <code>{\"path\": local_path}</code> so RAG can cite their URIs.</li> <li>Log LLM outputs with <code>context.memory().record(...)</code> or <code>write_result(...)</code> to enable recency views, distillation, and RAG promotion later.</li> </ul>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#7-troubleshooting","title":"7. Troubleshooting","text":"<ul> <li>Auth/Endpoints: Check keys; for Azure, confirm deployment + endpoint. For LM Studio, the base URL must include <code>/v1</code>.</li> <li>No citations or odd snippets: Verify parsing (PDFs can be tricky). Consider storing originals as Artifacts alongside parsed text.</li> <li>Answers miss context: Increase <code>k</code>, adjust chunk sizes, or broaden your <code>where</code> filter when promoting events.</li> <li>Latency/Cost: Keep chunks compact, and filter ingestion to what you\u2019ll actually ask about.</li> </ul>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#summary","title":"Summary","text":"<ul> <li>Configure LLM profiles via <code>.env</code> or runtime registration, then use <code>llm.chat()</code> / <code>llm.embed()</code>.</li> <li>Build RAG corpora from files and Memory events, then call <code>rag.answer(..., with_citations=True)</code> for grounded responses.</li> <li>Use Artifacts + Memory for provenance so you can trace what the model answered and why.</li> </ul> <p>See also: <code>context.llm()</code> \u00b7 <code>context.rag()</code> \u00b7 <code>context.memory().rag_*</code> \u00b7 <code>register_llm_client</code> \u00b7 <code>set_rag_llm_client</code> \u00b7 <code>llm.raw</code></p>"},{"location":"tutorials/t6-use-external-tools-mcp/","title":"Tutorial 6: Use External Tools \u2014 The MCP Example","text":"<p>AetherGraph supports external tool integration via the Model Context Protocol (MCP) \u2014 a simple JSON\u2011RPC 2.0 interface for listing and calling tools, reading resources, and managing structured outputs from remote services. In short, MCP lets your graph talk to anything that can expose a compliant interface: local CLI utilities, web services, or even another AI system.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#1-what-is-mcp","title":"1. What Is MCP?","text":"<p>The Model Context Protocol defines a standard way for an AI or orchestration framework to:</p> <ul> <li>List tools that an external process or service provides.</li> <li>Call those tools with structured arguments.</li> <li>List or read resources, such as files, datasets, or model outputs.</li> </ul> <p>AetherGraph\u2019s <code>MCPService</code> provides a unified layer for managing multiple MCP clients \u2014 e.g. a local subprocess (<code>StdioMCPClient</code>), a WebSocket endpoint (<code>WsMCPClient</code>), or an HTTP service (<code>HttpMCPClient</code>).</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#each-mcp-client-conforms-to-a-minimal-contract","title":"Each MCP client conforms to a minimal contract:","text":"<pre><code>class MCPClientProtocol:\n    async def list_tools(self) -&gt; List[MCPTool]: ...\n    async def call(self, tool: str, params: Dict[str, Any]) -&gt; Dict[str, Any]: ...\n    async def list_resources(self) -&gt; List[MCPResource]: ...\n    async def read_resource(self, uri: str) -&gt; Dict[str, Any]: ...\n</code></pre> <p>You can register many clients under names (e.g. <code>default</code>, <code>local</code>, <code>remote</code>), and access them via <code>context.mcp(name)</code>.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#2-minimal-example-query-a-local-mcp-tool","title":"2. Minimal Example \u2014 Query a Local MCP Tool","text":"<p>Suppose you have a local script or service exposing MCP over stdio. You can wrap it with <code>StdioMCPClient</code>:</p> <pre><code>from aethergraph.services import StdioMCPClient, MCPService\n\n# Initialize the service manually (usually handled by container)\nclient = StdioMCPClient([\"python\", \"my_mcp_server.py\"])\nmcp = MCPService({\"default\": client})\n\n# Example call to a tool named \"summarize_text\"\nasync def main():\n    await mcp.open(\"default\")\n    tools = await mcp.list_tools(\"default\")\n    print(\"Available tools:\", [t.name for t in tools])\n\n    result = await mcp.call(\"default\", \"summarize_text\", {\"text\": \"Hello MCP!\"})\n    print(\"Result:\", result)\n\nasyncio.run(main())\n</code></pre> <p>This works the same if you use a WebSocket or HTTP\u2011based MCP server \u2014 just replace the client class.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#3-inside-aethergraph-access-via-nodecontext","title":"3. Inside AetherGraph \u2014 Access via NodeContext","text":"<p>In real usage, you need to register the MCP service after starting the server:</p> <pre><code>from aethergraph import start_server\nfrom aethergraph.services import StdioMCPClient, MCPService\nfrom aethergraph.runtime import register_mcp_client \n\nstart_server()\nclient = StdioMCPClient([\"python\", \"my_mcp_server.py\"])\n\nregister_mcp_client(\"default\", client=client) # accessed by context.mcp(\"default)\n</code></pre> <p>The <code>NodeContext</code> injects it automatically, so any <code>graph_fn</code> or <code>@tool</code> can access it:</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"mcp_query\")\nasync def mcp_query(*, context: NodeContext):\n    # List tools available on the default MCP server\n    tools = await context.mcp(\"default\").list_tools()\n    context.logger().info(f\"Available MCP tools: {[t.name for t in tools]}\")\n\n    # Call a specific tool\n    result = await context.mcp(\"default\").call(\"summarize_text\", {\"text\": \"Explain MCP in one line.\"})\n    await context.channel().send_text(f\"Summary: {result['summary']}\")\n\n    return {\"result\": result}\n</code></pre> <p>Run this function via <code>run(graph, inputs)</code> or within a larger workflow \u2014 it will connect automatically to the configured MCP client.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#4-when-to-use-mcp","title":"4. When to Use MCP","text":"<p>MCP is useful when you want to:</p> <ul> <li>Bridge external AI systems (like a local LM Studio model or an in\u2011house LLM server) into AetherGraph.</li> <li>Integrate existing Python tools or APIs without writing new wrappers.</li> <li>Query live data services (e.g., weather, finance, or database APIs) through a JSON\u2011RPC layer.</li> </ul> <p>Because MCP uses async JSON\u2011RPC, it can easily be multiplexed across multiple nodes or graphs \u2014 even concurrently.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#5-available-clients","title":"5. Available Clients","text":"Client Type Class Transport Use Case Stdio <code>StdioMCPClient</code> stdin/stdout subprocess Run local CLI tools WebSocket <code>WsMCPClient</code> persistent WS channel Long\u2011lived AI services HTTP <code>HttpMCPClient</code> RESTful endpoint Web APIs with JSON\u2011RPC routes <p>You can register any number of them:</p> <pre><code>from aethergraph.services.mcp.ws_client import WsMCPClient\nmcp = MCPService({\n    \"default\": WsMCPClient(\"wss://example.com/mcp\"),\n    \"local\": StdioMCPClient([\"python\", \"my_local_tool.py\"])\n})\n</code></pre>"},{"location":"tutorials/t6-use-external-tools-mcp/#6-combined-example-http-tool-call-in-graph","title":"6. Combined Example \u2014 HTTP Tool Call in Graph","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"fetch_weather\")\nasync def fetch_weather(city: str, *, context: NodeContext):\n    # Connect to HTTP MCP backend\n    result = await context.mcp(\"default\").call(\"get_weather\", {\"city\": city})\n\n    report = result.get(\"report\", \"No data returned.\")\n    await context.channel().send_text(f\"Weather in {city}: {report}\")\n    return {\"city\": city, \"report\": report}\n</code></pre> <p>This looks like any other AetherGraph node \u2014 but the heavy lifting happens externally. MCP makes any compliant server a first\u2011class citizen in your graphs.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#7-notes-tips","title":"7. Notes &amp; Tips","text":"<ul> <li>Auto\u2011Reconnect: MCP clients auto\u2011reopen when disconnected, so you can safely call <code>context.mcp(\"local\")</code> multiple times.</li> <li>Multiple Servers: You can connect to multiple MCPs simultaneously for different tool domains.</li> </ul> <ul> <li>Chained Tools: Results from MCP calls are just Python dicts \u2014 they can be piped to other <code>@tool</code>s or stored as artifacts for later retrieval.</li> </ul>"},{"location":"tutorials/t6-use-external-tools-mcp/#summary","title":"Summary","text":"<p>MCP integration turns AetherGraph into a universal agent\u2011to\u2011agent protocol bridge. You can:</p> <ol> <li>Connect external AI or data tools via stdio, WebSocket, or HTTP.</li> <li>Access them with one unified API: <code>context.mcp(name)</code>.</li> <li>Call, list, and read resources without writing custom adapters.</li> </ol> <p>In the next section, we\u2019ll explore Extending Services \u2014 showing how to register your own MCP\u2011like service or log LLM prompts for inspection.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/","title":"Tutorial 7: Plug In Your World \u2014 Custom Services","text":"<p>Sometimes your agents need to talk to the rest of your world\u2014clusters, databases, storage systems, internal APIs. Instead of wiring that logic into every <code>graph_fn</code>, AetherGraph lets you attach custom services to the <code>context</code> object:</p> <pre><code># Later inside a graph or tool\nawait context.trainer().submit(spec)\nawait context.storage().put(\"/tmp/report.pdf\")\nstatus = await context.tracker().job_status(job_id)\n</code></pre> <p>This tutorial shows how to:</p> <ol> <li>Define a small service class (just Python).</li> <li>Register it so it appears as <code>context.&lt;name&gt;()</code>.</li> <li>Use it from <code>graph_fn</code> / <code>@tool</code> code.</li> <li>Apply practical patterns (HPC jobs, storage, external APIs).</li> </ol> <p>Goal: keep agent logic clean and move integration glue into reusable, testable services.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#1-what-is-a-custom-service-really","title":"1. What is a Custom Service, Really?","text":"<p>A custom service is a long\u2011lived Python object the runtime injects into every <code>NodeContext</code> under a chosen name.</p> <p>Once registered, it works anywhere:</p> <pre><code>@graph_fn(name=\"demo_trainer\")\nasync def demo_trainer(*, context):\n    job_id = await context.trainer().submit({\"epochs\": 10})\n    return {\"job_id\": job_id}\n</code></pre> <p>Key properties</p> <ul> <li>Named entrypoint \u2014 you choose the accessor (e.g., <code>trainer</code>, <code>storage</code>, <code>models</code>).</li> <li>Shared instance \u2014 one instance reused across nodes/runs (unless you design otherwise).</li> <li>Context\u2011aware \u2014 methods can access the current <code>NodeContext</code> (<code>run_id</code>, <code>graph_id</code>, <code>node_id</code>).</li> <li>Async\u2011first \u2014 works naturally with <code>await</code> and the event loop.</li> </ul> <p>Use a service when you have state or connectivity to share: clients, pools, caches, queues, background workers. For pure functions, a regular module is fine.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#2-minimal-service-from-zero-to-contexttrainer","title":"2. Minimal Service: from Zero to <code>context.trainer()</code>","text":""},{"location":"tutorials/t7-extend-runtime-custom-services/#step-1-define-a-service-class","title":"Step 1: Define a service class","text":"<p>Most custom services inherit from <code>Service</code> (aka <code>BaseContextService</code>) to get handy utilities: access to the current context, a service\u2011wide mutex, and helpers to run blocking code.</p> <pre><code>from aethergraph.services.runtime.base import Service\n\nclass Trainer(Service):\n    async def submit(self, spec: dict) -&gt; str:\n        \"\"\"Submit a training job to your cluster/scheduler.\"\"\"\n        ctx = self.ctx()  # NodeContext bound at call time\n        ctx.logger().info(\"trainer.submit\", extra={\"spec\": spec})\n\n        job_id = await self._submit_to_cluster(spec)  # implement backend call\n\n        # Optional: record to Memory for traceability\n        await ctx.memory().write_result(\n            topic=\"trainer.submit\",\n            outputs=[{\"name\": \"job_id\", \"kind\": \"text\", \"value\": job_id}],\n        )\n        return job_id\n\n    async def inspect_job(self, job_id: str) -&gt; dict:\n        status = await self._query_cluster(job_id)\n        return {\"job_id\": job_id, \"status\": status}\n\n    async def _submit_to_cluster(self, spec: dict) -&gt; str: ...\n    async def _query_cluster(self, job_id: str) -&gt; str: ...\n</code></pre> <p>Notes</p> <ul> <li><code>self.ctx()</code> gives you the current <code>NodeContext</code> at call time\u2014so logs, memory, and artifacts are run\u2011scoped automatically.</li> <li>The service can hold internal state (connection pools, caches) across calls.</li> </ul>"},{"location":"tutorials/t7-extend-runtime-custom-services/#step-2-register-the-service","title":"Step 2: Register the service","text":"<p>Register an instance at startup (e.g., when your sidecar/server boots):</p> <pre><code>from aethergraph import start_server\nfrom aethergraph.services.runtime.registry import register_context_service\n\nstart_server()  # start sidecar so services can be wired\n\ntrainer_service = Trainer()\nregister_context_service(\"trainer\", trainer_service)\n</code></pre> <p>From now on, inside any node:</p> <pre><code>job_id = await context.trainer().submit(spec)\n</code></pre> <p>Pattern: register once \u2192 call anywhere.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#3-using-services-inside-graph_fn-and-tool","title":"3. Using Services Inside <code>graph_fn</code> and <code>@tool</code>","text":"<p>Services behave like built\u2011ins on <code>context</code>.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#example-a-submit-and-track-a-job","title":"Example A \u2014 Submit and track a job","text":"<pre><code>from aethergraph import graph_fn, tool\n\n@graph_fn(name=\"train_and_wait\", outputs=[\"job_id\", \"done\"])\nasync def train_and_wait(spec: dict, *, context):\n    job_id = await context.trainer().submit(spec)\n    ready = await wait_for_training(job_id=job_id, context=context)\n    return {\"job_id\": job_id, \"done\": ready[\"ready\"]}\n\n@tool(name=\"wait_for_training\", outputs=[\"ready\"])\nasync def wait_for_training(job_id: str, *, context) -&gt; dict:\n    info = await context.trainer().inspect_job(job_id)\n    return {\"ready\": info[\"status\"] == \"COMPLETED\"}\n</code></pre> <p>Why this is nice:</p> <ul> <li>Cluster logic in one place (<code>Trainer</code>), not scattered across graphs.</li> <li>Tests can swap in a fake <code>Trainer</code> that returns canned statuses.</li> </ul>"},{"location":"tutorials/t7-extend-runtime-custom-services/#example-b-custom-storage-wrapper","title":"Example B \u2014 Custom storage wrapper","text":"<pre><code>class Storage(Service):\n    async def put(self, local_path: str, key: str) -&gt; str:\n        uri = await self._upload(local_path, key)  # implement upload\n        self.ctx().logger().info(\"storage.put\", extra={\"uri\": uri})\n        return uri\n\n    async def get(self, uri: str, dest: str) -&gt; None:\n        await self._download(uri, dest)\n\n@graph_fn(name=\"upload_report\", outputs=[\"uri\"])\nasync def upload_report(*, context):\n    uri = await context.storage().put(\"/tmp/report.pdf\", key=\"reports/2025-01-01.pdf\")\n    return {\"uri\": uri}\n</code></pre> <p>You can mix <code>context.storage()</code> with core features like <code>artifacts()</code> and <code>memory()</code>\u2014for example, storing the CAS URI next to an external bucket URI.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#4-concurrency-shared-state","title":"4. Concurrency &amp; Shared State","text":"<p>Because a service instance is shared, multiple nodes (or graphs) may hit it concurrently. If you expect concurrent accesses to a service, protect shared state inside the service, not at every call site.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#a-servicewide-mutex-recommended-pattern","title":"A) Service\u2011wide mutex (recommended pattern)","text":"<p>Use the built\u2011in <code>critical()</code> helper to guard a method. The pattern below binds the mutex to an instance method immediately after <code>__init__</code>, ensuring <code>self</code> exists:</p> <pre><code>import asyncio\nfrom aethergraph.services.runtime.base import Service\n\nclass CounterService(Service):\n    def __init__(self):\n        super().__init__()\n        self._value = 0\n        # Decorate incr with the bound service-wide mutex\n        # The entire method runs under a critical section\n        self.incr = self.critical()(self.incr)  # type: ignore\n\n    async def incr(self, n: int = 1) -&gt; int:\n        self._value += n\n        await asyncio.sleep(0)  # yield to event loop\n        return self._value\n</code></pre> <p>If you need finer\u2011grained control (e.g., per\u2011key locks, rate windows), design your own locking scheme inside the service. The point is to centralize concurrency policy in one place.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#b-offload-blocking-work","title":"B) Offload blocking work","text":"<pre><code>class Heavy(Service):\n    async def compute(self, x: int) -&gt; int:\n        return await self.run_blocking(self._slow_cpu_fn, x)  # threadpool offload\n\n    def _slow_cpu_fn(self, x: int) -&gt; int:\n        ...  # pure CPU work\n</code></pre> <p>This keeps agents responsive even when a service must do something synchronous or CPU\u2011heavy (e.g. heavy local simulation, training etc.).</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#5-service-lifecycle-startclose","title":"5. Service Lifecycle (start/close)","text":"<p>Some integrations need setup/teardown\u2014opening DB pools, authenticating SDKs, or warming models. Implement optional hooks on your service:</p> <pre><code>class Tracker(Service):\n    async def start(self):\n        self._client = ...  # open DB/HTTP client\n\n    async def close(self):\n        if getattr(self, \"_client\", None):\n            await self._client.aclose()\n</code></pre> <p>Call these from your process bootstrap/shutdown (sidecar, web server, CLI). The runtime doesn\u2019t force a pattern\u2014choose how you host services.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#6-testing-swapping-implementations","title":"6. Testing &amp; Swapping Implementations","text":"<p>Because services are registered by name, they\u2019re easy to replace in tests:</p> <pre><code>class FakeTrainer(Service):\n    async def submit(self, spec: dict) -&gt; str:\n        return \"job-test-123\"\n\n    async def inspect_job(self, job_id: str) -&gt; dict:\n        return {\"job_id\": job_id, \"status\": \"COMPLETED\"}\n\n# Test setup\nregister_context_service(\"trainer\", FakeTrainer())\n# All code using context.trainer() now talks to the fake.\n</code></pre>"},{"location":"tutorials/t7-extend-runtime-custom-services/#7-design-tips-common-patterns","title":"7. Design Tips &amp; Common Patterns","text":"<p>A few patterns that work well in real projects:</p> <ul> <li> <p>One concept \u2192 one service <code>context.trainer()</code> for orchestration, <code>context.storage()</code> for object stores, <code>context.materials()</code> for domain registries, etc.</p> </li> <li> <p>Keep names explicit   Prefer <code>context.k8s_jobs()</code> or <code>context.minio()</code> over vague <code>context.utils()</code>.</p> </li> <li> <p>Use services for anything stateful   HTTP clients, ORM sessions, caches, in\u2011memory registries, queues, schedulers.</p> </li> <li> <p>Don\u2019t replace built\u2011ins   Leave <code>context.memory()</code>, <code>context.artifacts()</code>, <code>context.channel()</code> alone. If you mirror to another system, create a separate service that consumes those.</p> </li> </ul> <p>More handy service ideas</p> Scenario Accessor What it wraps / does HPC / Training cluster <code>context.trainer()</code> Slurm/K8s jobs, Ray, internal queue External object storage <code>context.storage()</code> S3/GCS/MinIO, signed URLs, lifecycle/pinning Job/run tracking <code>context.tracker()</code> DB for job metadata, status dashboards Feature or embedding store <code>context.vectorstore()</code> Vector DB client, batch upserts, hybrid search Materials/parts registry <code>context.materials()</code> Domain DB + caching (e.g., refractive indices) Metrics/telemetry export <code>context.metrics()</code> Push to Prometheus/OTel/Grafana Lineage/BI export <code>context.lineage()</code> Push run/graph/node metadata to warehouse PDF/Doc processing <code>context.docs()</code> OCR, parsing, chunking utilities Secure secrets broker <code>context.secrets()</code> Rotation, envelope decryption Payment/billing <code>context.billing()</code> Client to your billing/ledger microservice License/Entitlements <code>context.license()</code> Gate features per user/org Remote execution (HPC/VM functions) <code>context.runner()</code> Dispatch Python/CLI jobs to remote workers Caching layer for expensive API calls <code>context.cache()</code> Memoization + TTL + invalidation Model hosting / inference gateway <code>context.predict()</code> Internal inference service with model registry"},{"location":"tutorials/t7-extend-runtime-custom-services/#8-optional-callable-services","title":"8. Optional: Callable Services","text":"<p>If you like compact call sites, implement <code>__call__</code>:</p> <pre><code>class Predictor(Service):\n    async def __call__(self, prompt: str) -&gt; str:\n        return await self.generate(prompt)\n\n    async def generate(self, prompt: str) -&gt; str:\n        ...\n\n# After registration as \"predictor\":\ntext = await context.predictor(\"hello\")               # calls __call__\ntext = await context.predictor().generate(\"hello\")    # explicit method\n</code></pre> <p>Sugar only; explicit method names (<code>submit</code>, <code>inspect_job</code>, <code>upload</code>, <code>generate</code>) are often clearer for teams.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#9-how-this-fits-with-mcp-and-other-integrations","title":"9. How This Fits with MCP and Other Integrations","text":"<p>In the previous section, MCP treated external processes (HTTP/WebSocket/stdio) as tools your agent can call. Custom services are the other half:</p> <ul> <li>MCP: great when the external system already speaks MCP and you want tools/resources auto\u2011described.</li> <li>Custom services: great when you want a plain Python wrapper around internal systems\u2014no extra server, no protocol.</li> </ul> <p>Projects often mix both:</p> <ul> <li>Use an MCP server for generic capabilities (filesystem, SQL, web search).</li> <li>Use services like <code>context.trainer()</code> and <code>context.storage()</code> for tightly\u2011coupled, org\u2011specific infrastructure.</li> </ul> <p>With this pattern in place, you can keep adding capabilities by teaching the runtime new services, while keeping agent code small, readable, and testable.</p>"},{"location":"tutorials/t8-design-your-own-waits/","title":"Design Your Own Waits &amp; Adapters (Coming Soon)","text":"<p>This chapter is a roadmap for a feature that\u2019s not public yet: defining your own dual\u2011stage waits and adapters so external systems can safely pause and resume graphs.</p> <p>You can treat this as a design note: it explains what is coming and how to think about it, without locking you into any final API.</p>"},{"location":"tutorials/t8-design-your-own-waits/#1-why-custom-waits","title":"1. Why Custom Waits?","text":"<p>Today you\u2019ve seen two wait styles:</p> <ul> <li> <p>Cooperative waits via <code>context.channel().ask_*</code> inside <code>@graph_fn</code>.</p> </li> <li> <p>Great for interactive agents while the process is alive.</p> </li> <li>Not resumable after the Python process dies.</li> <li> <p>Dual\u2011stage waits via built\u2011in tools used in <code>@graphify</code> static graphs.</p> </li> <li> <p>The graph can pause indefinitely (e.g., waiting for Slack/Web reply).</p> </li> <li>You can cold\u2011resume with the same <code>run_id</code> much later.</li> </ul> <p>Custom waits are about taking that second pattern and making it available for your own backends:</p> <ul> <li>external approval systems</li> <li>internal job schedulers</li> <li>lab/experiment queues</li> <li>human\u2011in\u2011the\u2011loop tools that live outside channels like Slack/Telegram</li> </ul> <p>The goal: let you say \u201cpause here until X happens in system Y, then resume this node safely\u201d.</p>"},{"location":"tutorials/t8-design-your-own-waits/#2-what-exists-today","title":"2. What Exists Today","text":"<p>You already have:</p> <ul> <li>Cooperative waits on <code>NodeContext.channel()</code> (<code>ask_text</code>, <code>ask_approval</code>, <code>ask_files</code>, etc.).</li> <li> <p>Built\u2011in dual\u2011stage wait tools (used in the channel tutorial) that:</p> </li> <li> <p>create a continuation token</p> </li> <li>emit an outgoing event (e.g. to Slack/Web)</li> <li>persist a snapshot</li> <li>resume when an inbound event matches that token</li> </ul> <p>These built\u2011ins are wired to existing adapters (console, Slack, etc.) and work out of the box for common interaction flows.</p> <p>What\u2019s missing right now is a public, stable API for defining your own dual\u2011stage tools and adapters.</p> <p>That\u2019s what this \u201ccoming soon\u201d chapter is preparing you for.</p>"},{"location":"tutorials/t8-design-your-own-waits/#3-design-shape-of-a-dualstage-tool-conceptual","title":"3. Design Shape of a Dual\u2011Stage Tool (Conceptual)","text":"<p>At a high level, a dual\u2011stage tool will look like:</p> <ol> <li> <p>Stage A \u2013 schedule / emit</p> </li> <li> <p>Construct a request payload.</p> </li> <li>Register a continuation token with the runtime.</li> <li>Send an event to some external system (HTTP, MQ, email, etc.).</li> <li> <p>Return a WAITING status instead of a final result.</p> </li> <li> <p>Stage B \u2013 resume / handle reply</p> </li> <li> <p>An inbound event (webhook, poller, bridge) calls back with the same token.</p> </li> <li>Runtime restores the graph + node and hands you the reply payload.</li> <li>Your tool continues execution and returns a normal result.</li> </ol>"},{"location":"tutorials/t8-design-your-own-waits/#possible-future-shape-pseudocode-not-final","title":"Possible future shape (pseudo\u2011code, not final)","text":"<pre><code>class ApproveJob(DualStageTool):  # name TBD\n    async def build_request(self, spec: dict, *, context):\n        # Stage A: emit request + return a continuation descriptor\n        token = await context.create_continuation(kind=\"job_approval\", payload={\"spec\": spec})\n        await self.emit_request(spec=spec, token=token)\n        return self.wait(token)   # tells runtime: this node is now WAITING\n\n    async def on_resume(self, reply: dict, *, context):\n        # Stage B: this runs when the continuation is resumed\n        approved = bool(reply.get(\"approved\", False))\n        return {\"approved\": approved}\n</code></pre> <p>Again: this is illustrative only \u2014 the real base class and method names will be documented once the API is ready.</p> <p>The important idea is the split between \u201cset up a wait + emit a request\u201d and \u201chandle the resume payload\u201d, wired together by a continuation token.</p>"},{"location":"tutorials/t8-design-your-own-waits/#4-adapters-bridging-external-systems","title":"4. Adapters: Bridging External Systems","text":"<p>Custom waits are only half the story \u2014 you also need a way to bridge an external system to AetherGraph\u2019s continuation store.</p> <p>Conceptually, an adapter will:</p> <ol> <li>Listen for inbound events from your system (webhook, queue consumer, polling loop).</li> <li>Parse them into a payload <code>{token, data}</code>.</li> <li>Call the runtime to resolve the corresponding continuation and resume the graph.</li> </ol> <p>In practice this might look like (pseudo\u2011code):</p> <pre><code># somewhere in your web app / worker\n\n@app.post(\"/callbacks/job-approved\")\nasync def job_approved(req):\n    token = req.json()[\"token\"]\n    payload = {\"approved\": req.json()[\"approved\"]}\n    await runtime.resolve_continuation(token=token, payload=payload)\n    return {\"ok\": True}\n</code></pre> <p>Behind the scenes, the runtime will:</p> <ul> <li>load the snapshot for the relevant <code>run_id</code> / graph</li> <li>mark the waiting node as ready</li> <li>resume execution from that node using the payload</li> </ul> <p>The adapter API that makes this nicer to write will be documented with the dual\u2011stage tool support.</p>"},{"location":"tutorials/t8-design-your-own-waits/#5-how-this-relates-to-graph_fn-and-graphify","title":"5. How This Relates to <code>graph_fn</code> and <code>graphify</code>","text":"<p>It\u2019s useful to remember the three modes:</p> Mode Wait type available today Cold\u2011resume after process exit? <code>graph_fn</code> + <code>context.channel()</code> Cooperative waits (<code>ask_*</code>) \u274c No <code>graphify</code> + built\u2011in dual waits Dual\u2011stage tools \u2705 Yes <code>graphify</code> + custom dual waits Coming soon \u2705 Yes (for your own waits) <p>Once custom dual\u2011stage tools are available, you\u2019ll:</p> <ul> <li>Use <code>graph_fn</code> when you want live, in\u2011process agents. You can certainly combine <code>graph_fn</code> and <code>DualStageTool</code>, it's just <code>graph_fn</code> does not preserve states and resumption is not protected if the process gets interrupted. </li> <li>Use <code>graphify</code> + dual\u2011stage waits when you want hard guarantees about resuming long\u2011running flows.</li> <li>Wrap external systems (HPC, approval workflows, human review portals) as first\u2011class wait nodes.</li> </ul>"},{"location":"tutorials/t8-design-your-own-waits/#6-what-you-can-do-today","title":"6. What You Can Do Today","text":"<p>While the custom API is still baking, you can already:</p> <ul> <li>Use built\u2011in dual\u2011stage waits with channels (Slack/Web) in static graphs to pause and resume runs.</li> <li>Use cooperative waits (<code>context.channel().ask_*</code>) in <code>graph_fn</code> agents when you don\u2019t need cold\u2011resume.</li> <li>Front external systems with simple scripts or services that call existing channel tools (for example, sending a Slack approval that resumes a static graph).</li> </ul> <p>When the public dual\u2011stage API lands, you\u2019ll be able to replace those scripts with:</p> <ul> <li>explicit <code>@tool</code>\u2011style wait nodes, and</li> <li>small, typed adapters that speak your domain\u2019s language.</li> </ul>"},{"location":"tutorials/t8-design-your-own-waits/#7-looking-ahead","title":"7. Looking Ahead","text":"<p>This chapter is intentionally high\u2011level and labeled Coming Soon. The final documentation will include:</p> <ul> <li>A concrete base class or decorator for defining dual\u2011stage wait tools.</li> <li>A small adapter kit for wiring external callbacks into the continuation store.</li> <li>End\u2011to\u2011end examples: long\u2011running jobs, external approval systems, and custom interactive apps.</li> </ul> <p>Until then, you can design your flows with this mental model in mind:</p> <p>\u201cAnything that can emit a token and later send it back can be turned into a resumable node.\u201d</p> <p>Once the API is ready, you\u2019ll drop in the real primitives and your graphs will gain robust, resumable waits across all your systems.</p>"}]}