{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AetherGraph","text":"<p>AetherGraph is a Python\u2011first agentic framework designed to supercharge your R&amp;D workflows. Effortlessly build, orchestrate, and trace research pipelines with a powerful combination of graph-based function composition, context-aware services, and safe tool integrations.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Graph Functions: Use Python functions as first-class graph nodes with the intuitive <code>@graph_fn</code> decorator.</li> <li>Integrated Context Services: Out-of-the-box support for LLMs, memory, channels, artifacts, and key-value stores\u2014no bolt\u2011ons required, but fully extensible.</li> <li>Traceable Artifacts &amp; Memory: Capture every step and result for reproducible, auditable research.</li> <li>Safe Tooling: Invoke external capabilities securely and reliably.</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to dive in? Follow our Quickstart guide and build your first agentic workflow in minutes.</p> <p>AetherGraph empowers researchers, engineers, and teams to create robust, transparent, and scalable R&amp;D solutions\u2014all in pure Python.</p>"},{"location":"concept/","title":"AetherGraph \u2014 Architecture Overview (1\u2011page)","text":"<p>Goal: Give newcomers a single \"big picture\" of how AetherGraph fits together, then provide a tiny legend so they know what to look up next.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          AetherGraph Runtime                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                    \u2502\n\u2502  Python Code (your repo)                                           \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500          \u2502\n\u2502  @graph_fn nodes           Tools (@tool)            Services       \u2502\n\u2502  (code-native agents)      (reusable ops,           (external ctx) \u2502\n\u2502                            checkpointable)                          \u2502\n\u2502       \u2502                           \u2502                    \u2502            \u2502\n\u2502       \u25bc                           \u25bc                    \u25bc            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  Node Exec   \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  Tool Exec  \u2502      \u2502  Service API \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502        \u2502                                                         \u2502    \u2502\n\u2502        \u25bc                                                         \u2502    \u2502\n\u2502                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502                       \u2502        NodeContext       \u2502  (per node call)  \u2502\n\u2502                       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                   \u2502\n\u2502                       \u2502 channel()   \u2192  chat/CLI/GUI (send/ask)       \u2502\n\u2502                       \u2502 memory()    \u2192  record/recent/query           \u2502\n\u2502                       \u2502 artifacts() \u2192  write/read refs (provenance)  \u2502\n\u2502                       \u2502 kv()        \u2192  small fast key\u2013value          \u2502\n\u2502                       \u2502 logger()    \u2192  structured logs               \u2502\n\u2502                       \u2502 services()  \u2192  external ctx (domain APIs)    \u2502\n\u2502                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502                                \u2502                                     \u2502\n\u2502                                \u25bc                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502               Sidecar / Adapters (inline server)               \u2502 \u2502\n\u2502  \u2502  - Console/CLI channel                                         \u2502 \u2502\n\u2502  \u2502  - Slack / PyQt / HTTP webhooks                                \u2502 \u2502\n\u2502  \u2502  - File/artifact endpoints (optional, later hosted)            \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concept/#legend-skim-first","title":"Legend (skim-first)","text":"<ul> <li><code>@graph_fn</code> (code\u2011native agents): Turn a plain async Python function into a node with a <code>NodeContext</code> injected.</li> <li>Tools (<code>@tool</code>): Small, explicit, reusable operations. Great for checkpoints, retries, and sharing across graphs.</li> <li>NodeContext: Where your node talks to the world: <code>channel()</code>, <code>memory()</code>, <code>artifacts()</code>, <code>kv()</code>, <code>logger()</code>, <code>services()</code>.</li> <li>Channel: Unifies human I/O (console/Slack/PyQt). Use <code>send_text</code>, <code>ask_text</code>, and progress APIs.</li> <li>Memory &amp; Artifacts: Event\u2011first memory with provenance; artifacts store files/results with stable refs.</li> <li>External Context (Services): Register domain services (e.g., job runner, materials DB) so nodes call them like built\u2011ins.</li> <li>Sidecar: Inline server that powers channels/adapters locally; later you can host these endpoints.</li> </ul> <p>Next: See Memory Internals below, then the Submit \u2192 Poll \u2192 Notify tutorial.</p>"},{"location":"concept/#memory-internals-diagram","title":"Memory Internals (diagram)","text":"<p>Goal: Show how event logging, persistence, indices, and optional RAG hang together.</p> <pre><code>              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502          memory().record(...)           \u2502\n              \u2502   kind \u2022 data \u2022 tags \u2022 entities \u2022 ...   \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                   (in\u2011process event stream / bus)\n                                  \u2502\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u25bc                                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  JSONL Persistence Log    \u2502  append\u2011only     \u2502   KV / Indices            \u2502\n\u2502  (provenance timeline)    \u2502  (durable)       \u2502   (fast lookup/filter)    \u2502\n\u2502  e.g., runs/YYYY/MM/*.jsonl\u2502                 \u2502   tags, kinds, entity ids \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                                 \u2502\n          \u2502                                                 \u2502\n          \u25bc                                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Derived Views / Cursors   \u2502  recent(...)     \u2502   Optional RAG Binding    \u2502\n\u2502 e.g., last_by_name,       \u2502  query(...)      \u2502   (vector index)          \u2502\n\u2502 latest_refs_by_kind       \u2502                  \u2502   embed(data/artifacts)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                                 \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba  NodeContext.memory().query(...) \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Record: Write small structured events with <code>kind</code>, <code>data</code>, <code>tags</code>, <code>entities</code>, <code>metrics</code>.</li> <li>Persist: Append to a JSONL log for durability &amp; replay; perfect for provenance.</li> <li>Index: Maintain fast KV/indices for quick filters (by kind/tags/entity/time).</li> <li>RAG (optional): Bind a vector index to selectively embed text/artifact content for semantic search.</li> <li>Query: Use <code>recent</code>, <code>query</code>, and helpers like <code>latest_refs_by_kind</code> to drive summaries &amp; reports.</li> </ul> <p>When to use RAG? When you want semantic retrieval over larger text blocks or artifact\u2011derived content; otherwise rely on indices and tags for speed/clarity.</p>"},{"location":"concept/#tutorial-submit-poll-notify-single-file-runnable","title":"Tutorial \u2014 Submit \u2192 Poll \u2192 Notify (single file, runnable)","text":"<p>Goal: Minimal end\u2011to\u2011end job orchestration with human notification and provenance. Keep it console\u2011only, no external infra.</p> <pre><code># examples/tutorial_submit_poll_notify.py\nfrom __future__ import annotations\nimport asyncio, random, time\nfrom typing import Dict, Optional\n\n# AetherGraph imports (adjust paths/names to your package layout)\nfrom aethergraph import graph_fn, NodeContext\nfrom aethergraph.server import start\nfrom aethergraph.v3.core.runtime.runtime_services import register_context_service\n\n# --- 1) Start the inline sidecar so channel/memory/artifacts work locally ---\nstart()  # prints a local URL; not required to save here\n\n# --- 2) A tiny external service: fake job runner (auto\u2011bound to NodeContext) ---\nclass FakeJobRunner:\n    \"\"\"Pretend to submit a remote job and poll until it finishes.\n    In a real impl, call your cloud API here.\n    \"\"\"\n    def __init__(self):\n        self._jobs: Dict[str, Dict[str, Optional[str]]] = {}\n\n    async def submit(self, spec: Dict) -&gt; str:\n        job_id = f\"job_{int(time.time()*1000)}_{random.randint(100,999)}\"\n        # status can be: queued \u2192 running \u2192 (succeeded | failed)\n        self._jobs[job_id] = {\"status\": \"queued\", \"result\": None}\n        # Background simulation\n        asyncio.create_task(self._simulate(job_id, spec))\n        return job_id\n\n    async def poll(self, job_id: str) -&gt; Dict[str, Optional[str]]:\n        return self._jobs[job_id]\n\n    async def _simulate(self, job_id: str, spec: Dict):\n        # Fake lifecycle with sleeps\n        await asyncio.sleep(0.5)\n        self._jobs[job_id][\"status\"] = \"running\"\n        await asyncio.sleep(1.2)\n        if random.random() &lt; 0.85:\n            self._jobs[job_id][\"status\"] = \"succeeded\"\n            self._jobs[job_id][\"result\"] = f\"Result for {spec.get('name','demo')}\"\n        else:\n            self._jobs[job_id][\"status\"] = \"failed\"\n            self._jobs[job_id][\"result\"] = None\n\n# Register the service under the name \"jobs\" and auto\u2011bind it to context as context.jobs()\nregister_context_service(\"jobs\", FakeJobRunner())\n\n# --- 3) The graph node: submit \u2192 poll \u2192 notify, with artifacts &amp; memory ---\n@graph_fn(name=\"submit_poll_notify\")\nasync def submit_poll_notify(spec: Dict, *, context: NodeContext) -&gt; Dict:\n    ch = context.channel()\n    mem = context.memory()\n    arts = context.artifacts()\n    jobs = context.jobs()  # auto\u2011bound external service\n\n    await ch.send_text(\"Submitting your job\u2026\")\n    job_id = await jobs.submit(spec)\n    await mem.record(kind=\"job_submitted\", data={\"job_id\": job_id, \"spec\": spec}, tags=[\"demo\"])\n\n    # Persist the spec as an artifact\n    spec_ref = await arts.write_text(f\"spec_{job_id}.json\", content=str(spec))\n\n    # Poll until terminal\n    while True:\n        info = await jobs.poll(job_id)\n        status = info.get(\"status\")\n        await ch.send_text(f\"Status: {status}\")\n        if status in {\"succeeded\", \"failed\"}:\n            break\n        await asyncio.sleep(0.6)\n\n    if status == \"succeeded\":\n        result_text = info.get(\"result\") or \"&lt;no result&gt;\"\n        res_ref = await arts.write_text(f\"result_{job_id}.txt\", content=result_text)\n        await mem.record(kind=\"job_succeeded\", data={\"job_id\": job_id, \"result_ref\": res_ref})\n        await ch.send_text(f\"\u2705 Job {job_id} finished. Saved result \u2192 {res_ref}\")\n        return {\"job_id\": job_id, \"status\": status, \"spec_ref\": spec_ref, \"result_ref\": res_ref}\n    else:\n        await mem.record(kind=\"job_failed\", data={\"job_id\": job_id})\n        ans = await ch.ask_text(f\"\u274c Job {job_id} failed. Retry? (yes/no)\")\n        if str(ans).strip().lower().startswith(\"y\"):\n            return await submit_poll_notify(spec=spec, context=context)\n        await ch.send_text(\"Not retrying; stopping here.\")\n        return {\"job_id\": job_id, \"status\": status, \"spec_ref\": spec_ref}\n\n# --- 4) Tiny runner for local testing ---\nif __name__ == \"__main__\":\n    async def main():\n        out = await submit_poll_notify(spec={\"name\": \"toy-sim\", \"steps\": 3})\n        print(\"FINAL OUTPUT:\\n\", out)\n    asyncio.run(main())\n</code></pre>"},{"location":"concept/#what-this-tutorial-demonstrates","title":"What this tutorial demonstrates","text":"<ul> <li>Channel I/O: human\u2011visible status + retry prompt.</li> <li>External Service: a domain API (<code>jobs</code>) registered once, used like a built\u2011in via <code>context.jobs()</code>.</li> <li>Memory: durable events (<code>job_submitted</code>, <code>job_succeeded</code>, <code>job_failed</code>).</li> <li>Artifacts &amp; provenance: spec/result written with stable refs; returned in the node output.</li> <li>Low friction: single file; console channel only; no extra infra.</li> </ul> <p>Next:</p> <ul> <li>Swap <code>FakeJobRunner</code> for your cloud client.</li> <li>Replace <code>ask_text</code> with an approval UI (Slack/PyQt) once you enable those adapters.</li> <li>Emit metrics in <code>mem.record(..., metrics={...})</code> and add a summary node to close the loop.</li> </ul>"},{"location":"external-context-services/","title":"External Context Services (Revised)","text":"<p>Make reusable, lifecycle\u2011aware helpers available as <code>context.&lt;name&gt;</code> inside any <code>@graph_fn</code>.</p> <p>This page explains what an external context service is, why you might use one, how it looks at a high level, and the APIs you\u2019ll use to define and register services. It also clarifies lifecycle behavior today vs. after you add a server/sidecar, and shows how services can access the active <code>NodeContext</code>.</p>"},{"location":"external-context-services/#1-what-is-an-external-context-service","title":"1) What is an external context service?","text":"<p>An external context service is a Python object managed by AetherGraph\u2019s runtime and exposed to your graph functions through the <code>NodeContext</code>. Once registered, you can access it as <code>context.svc(\"name\")</code> or simply <code>context.&lt;name&gt;</code>.</p> <p>Key ideas:</p> <ul> <li>Dependency injection: Centralize clients, caches, and policies in one place and inject them wherever needed.</li> <li>Lifecycle\u2011ready: Services can implement <code>start()</code> and <code>close()</code> for setup/teardown (e.g., open a pool, kick off a background task). Today these hooks are optional and not auto\u2011invoked unless you wire them (see \u00a74.1).</li> <li>Concurrency controls: Built\u2011in mutex and read/write helpers to safely share state across concurrent nodes.</li> <li>Per\u2011run binding: Each call is bound to a <code>NodeContext</code> so the service can access run_id, logger, artifacts, memory, etc.</li> <li>Uniform surface: The same service works in local scripts today and can be proxied or hosted later without changing call sites.</li> </ul> <p>Use services when logic benefits from a long\u2011lived instance, shared state, or orchestration\u2014not for tiny, pure functions (plain imports are fine there).</p>"},{"location":"external-context-services/#2-highlevel-usage-sketch","title":"2) High\u2011level usage sketch","text":"<p>Below is a conceptual outline (intentionally abstract) of how you would define and call a service.</p>"},{"location":"external-context-services/#define-highlevel","title":"Define (high\u2011level)","text":"<pre><code>class MyService(Service):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self._cache = {}\n\n    async def start(self):\n        # optional: warm up connections, threads, or caches\n        ...\n\n    async def close(self):\n        # optional: flush or close resources\n        ...\n\n    async def do_something(self, key: str) -&gt; str:\n        # example: consult cache, maybe call out to an API, return a value\n        ...\n</code></pre>"},{"location":"external-context-services/#register-at-app-startup","title":"Register (at app startup)","text":"<pre><code>register_context_service(\"myservice\", MyService(config={\"mode\": \"dev\"}))\n</code></pre>"},{"location":"external-context-services/#use-in-a-graph-function","title":"Use in a graph function","text":"<pre><code>@graph_fn(name=\"demo\")\nasync def demo(*, context: NodeContext):\n    value = await context.myservice.do_something(\"foo\")\n    return {\"value\": value}\n</code></pre> <p>That\u2019s it: once registered, your service is reachable from any node via <code>context</code>.</p>"},{"location":"external-context-services/#3-why-use-external-context-benefits-use-cases","title":"3) Why use external context? (Benefits + use cases)","text":""},{"location":"external-context-services/#benefits","title":"Benefits","text":"<ul> <li>Replaceable implementations: Swap local vs. remote, mock vs. real, dev vs. prod\u2014without editing call sites.</li> <li>Centralized auth &amp; config: Put tokens, endpoints, retry/timeout policy, telemetry in one place.</li> <li>Lifecycle &amp; performance: Reuse clients, connection pools, thread pools; warm caches once.</li> <li>Concurrency safety: Use the provided <code>critical()</code> mutex or <code>AsyncRWLock</code> to protect shared state.</li> <li>Per\u2011run awareness: Access <code>self.ctx()</code> to reach logger, artifacts, memory, continuations, etc.</li> <li>Future\u2011proof: The same surface can later be proxied (sidecar/hosted) while keeping your graph code unchanged.</li> </ul>"},{"location":"external-context-services/#itemized-scenarios-no-code","title":"Itemized scenarios (no code)","text":"<ul> <li>Model/Tool Clients: Wrap an LLM, embedding service, vector DB, or a simulation engine with retry, rate limit, and consistent API.</li> <li>Job Orchestration: Submit long\u2011running jobs to a queue/cluster and expose <code>submit/status/wait</code> for nodes.</li> <li>Caching/Indexing: Provide a shared in\u2011memory or on\u2011disk cache with strict read (R/W lock) semantics.</li> <li>Policy Enforcement: Centralize tenant limits, quotas, audit logging, and redaction.</li> <li>Data Access Facades: Read domain data (materials table, experiment registry) with local cache + background refresh.</li> <li>Adapters: Present a unified interface over heterogeneous backends (e.g., multiple vendor APIs behind one broker).</li> </ul>"},{"location":"external-context-services/#4-apis-defining-registering-and-binding-services","title":"4) APIs: defining, registering, and binding services","text":"<p>AetherGraph provides small primitives for service registration and a base class with helpful utilities.</p>"},{"location":"external-context-services/#41-lifecycle-today-vs-serversidecar","title":"4.1 Lifecycle (today vs. server/sidecar)","text":"<ul> <li>Today (no server yet): <code>start()</code>/<code>close()</code> exist but are not auto\u2011invoked. You can omit them or leave them as no\u2011ops.</li> <li>When you add a server/sidecar: wire lifecycle once at boot/shutdown (pseudo\u2011code):</li> </ul> <pre><code># After install_services(...) and registrations\nawait start_all_services()\n# ... run your app/sidecar ...\nawait close_all_services()\n</code></pre> <p>Until those hooks are added, services work fine without lifecycle calls.</p>"},{"location":"external-context-services/#42-registry-functions-runtimelevel","title":"4.2 Registry functions (runtime\u2011level)","text":"<ul> <li><code>install_services(container)</code> \u2013 Set the process\u2011wide service container at startup.</li> <li><code>ensure_services_installed(factory)</code> \u2013 Lazily create/install the container if missing.</li> <li><code>register_context_service(name, instance)</code> \u2013 Add a concrete service instance under <code>name</code>.</li> <li><code>get_context_service(name)</code> \u2013 Retrieve a registered instance.</li> <li><code>list_context_services()</code> \u2013 List the names currently registered.</li> </ul>"},{"location":"external-context-services/#43-base-class-service-aka-basecontextservice","title":"4.3 Base class: <code>Service</code> (aka <code>BaseContextService</code>)","text":"<p>The base class gives you batteries\u2011included ergonomics:</p> <ul> <li> <p>Lifecycle</p> </li> <li> <p><code>async def start(self) -&gt; None</code> \u2013 Optional setup hook.</p> </li> <li> <p><code>async def close(self) -&gt; None</code> \u2013 Optional teardown hook.</p> </li> <li> <p>Binding</p> </li> <li> <p><code>def bind(self, *, context: NodeContext) -&gt; Service</code> \u2013 Called by the runtime so <code>self.ctx()</code> works.</p> </li> <li> <p><code>def ctx(self) -&gt; NodeContext</code> \u2013 Access the current node context (logger, memory, artifacts, etc.).</p> </li> <li> <p>Concurrency</p> </li> <li> <p><code>self._lock</code> \u2013 An async mutex available for your own critical sections.</p> </li> <li><code>def critical()(fn)</code> \u2013 Decorator that serializes an async method (easy mutual exclusion).</li> <li> <p><code>class AsyncRWLock</code> \u2013 Many\u2011readers/one\u2011writer lock for shared tables and caches.</p> </li> <li> <p>Offloading</p> </li> <li> <p><code>async def run_blocking(self, fn, *a, **kw)</code> \u2013 Run CPU or blocking I/O on a worker thread (keeps the event loop responsive).</p> </li> </ul>"},{"location":"external-context-services/#44-accessing-services-from-nodes","title":"4.4 Accessing services from nodes","text":"<ul> <li>Dynamic attribute: <code>context.&lt;name&gt;</code> resolves to the registered service (e.g., <code>context.myservice</code>).</li> <li>Explicit lookup: <code>context.svc(\"name\")</code> (equivalent to the dynamic attribute).</li> </ul>"},{"location":"external-context-services/#45-accessing-nodecontext-from-inside-a-service-essential","title":"4.5 Accessing <code>NodeContext</code> from inside a service (essential)","text":"<p>Services frequently need run\u2011scoped utilities (logger, memory, artifacts, kv, llm, rag, etc.). Enable per\u2011call binding so <code>self.ctx()</code> returns the right <code>NodeContext</code>.</p> <p>Use <code>self.ctx()</code> in the service:</p> <pre><code>class MyService(Service):\n    async def do_work(self, x: int) -&gt; int:\n        ctx = self.ctx()  # NodeContext bound for this call\n        ctx.logger().info(\"working\", extra={\"x\": x})\n        await ctx.memory().record(kind=\"note\", data={\"x\": x})\n        uri = ctx.artifacts().put_text(\"result.txt\", f\"value={x}\")\n        return x + 1\n</code></pre>"},{"location":"external-context-services/#46-event-loop-locking-model","title":"4.6 Event loop &amp; locking model","text":"<ul> <li>External services run on the main event loop used by the executing node.</li> <li>Locks (<code>_lock</code>, <code>AsyncRWLock</code>) coordinate on that loop; use <code>run_blocking()</code> for CPU/IO work.</li> </ul>"},{"location":"external-context-services/#5-summary","title":"5) Summary","text":"<p>External context services provide a clean way to share long\u2011lived capabilities across nodes while keeping graph code small and portable:</p> <ul> <li>Inject reusable helpers via <code>context.&lt;name&gt;</code> (or <code>context.svc(name)</code>).</li> <li>Manage concurrency and performance in one place; offload blocking work with <code>run_blocking()</code>.</li> <li>Abstract environments (mock/local/dev/prod) without touching business logic.</li> <li>Bind to <code>NodeContext</code> automatically so services can use logger, memory, artifacts, kv, llm/rag, etc.</li> <li>Lifecycle now vs later: Today you can skip <code>start()</code>/<code>close()</code>; add startup/shutdown hooks when you introduce a server/sidecar.</li> </ul> <p>Use services for shared state, orchestration, specialized clients, or cross\u2011cutting policies. Use plain imports for tiny, stateless helpers.</p>"},{"location":"graph_fn/","title":"Graph Function <code>graph_fn</code> Quickstart &amp; Reference","text":"<p>Make any Python async function a runnable, inspectable Graph Function with a single decorator. You keep normal Python control\u2011flow; AetherGraph wires in runtime services via <code>context</code> and exposes your outputs as graph boundaries.</p>"},{"location":"graph_fn/#tldr","title":"TL;DR","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}! \ud83d\udc4b\")\n    return {\"greeting\": f\"Hello, {name}\"}\n\n# Run (async)\nres = await hello(name=\"Aether\")          # \u2192 {\"greeting\": \"Hello, Aether\"}\n\n# Or run (sync) for quick scripts\nout = hello.sync(name=\"Aether\")            # same result\n</code></pre>"},{"location":"graph_fn/#what-is-a-graph-function","title":"What is a Graph Function?","text":"<p>A Graph Function is a small wrapper around your Python function that:</p> <ul> <li> <p>builds a fresh internal TaskGraph,</p> </li> <li> <p>injects a <code>NodeContext</code> if your function declares <code>*, context</code>,</p> </li> <li> <p>executes your function (awaiting if needed),</p> </li> <li> <p>normalizes the return value into named outputs, and</p> </li> <li> <p>records graph boundary outputs for downstream composition/inspection.</p> </li> </ul> <p>You do not need to learn a new DSL. Write Python; use <code>context.&lt;service&gt;()</code> when you need IO/state.</p>"},{"location":"graph_fn/#decorator-signature","title":"Decorator signature","text":"<pre><code>@graph_fn(\n    name: str,\n    inputs: list[str] | None = None,\n    outputs: list[str] | None = None,\n    version: str = \"0.1.0\",\n    agent: str | None = None,  # optional: also register as an agent name\n)\n</code></pre> <p>Required</p> <ul> <li>name (str) \u2013 Unique identifier for this graph function.</li> </ul> <p>Optional</p> <ul> <li> <p>inputs (list[str]) \u2013 Declares input names for docs/registry (not enforced at call time).</p> </li> <li> <p>outputs (list[str]) \u2013 Declares output names/order; enables single\u2011literal returns.</p> </li> <li> <p>version (str) \u2013 Semantic version for registry/discovery.</p> </li> <li> <p>agent (str) \u2013 Also register in the <code>agent</code> namespace (advanced).</p> </li> </ul>"},{"location":"graph_fn/#function-shape","title":"Function shape","text":"<p><pre><code>@graph_fn(name=\"example\", inputs=[\"x\"], outputs=[\"y\"])\nasync def example(x: int, *, context):\n    # use services via context: channel/memory/artifacts/kv/llm/rag/mcp/logger\n    await context.channel().send_text(f\"x={x}\")\n    return {\"y\": x + 1}\n</code></pre> - Positional/keyword parameters are your API.</p> <ul> <li>Include <code>*, context</code> to receive the <code>NodeContext</code>. If you don\u2019t declare it, nothing is injected.</li> </ul>"},{"location":"graph_fn/#returning-values-normalization-rules","title":"Returning values (normalization rules)","text":"<p>Your return can be:</p> <p>1) Dict of outputs (recommended) <pre><code>return {\"result\": 42, \"note\": \"ok\"}\n</code></pre></p> <p>2) Single literal \u2014 only if you declared exactly one output <pre><code>@graph_fn(name=\"one\", outputs=[\"y\"])\nasync def one(*, context):\n    return 123  # normalized to {\"y\": 123}\n</code></pre></p> <p>3) NodeHandle / Refs (advanced) If you return node handles or refs created by graph utilities, they\u2019re exposed as boundary outputs automatically. For most users, plain dicts/literals are enough.</p> <p>Validation - If <code>outputs</code> are declared, missing keys raise: <code>ValueError(\"Missing declared outputs: ...\")</code>. - Returning a single literal without exactly one declared output raises an error.</p>"},{"location":"graph_fn/#running","title":"Running","text":"<p><pre><code># Async (preferred in apps/servers)\nres = await my_fn(a=1, b=2)\n\n# Sync helper (scripts/CLI/tests)\nout = my_fn.sync(a=1, b=2)\n</code></pre> Internally this builds a fresh runtime environment, constructs a TaskGraph, executes your function in an interpreter, and returns the normalized outputs.</p>"},{"location":"graph_fn/#accessing-context","title":"Accessing Context","text":"<p>Declare <code>*, context</code> to use built\u2011ins: <pre><code>@graph_fn(name=\"report\", outputs=[\"uri\"])\nasync def report(data: dict, *, context):\n    # Log breadcrumbs\n    log = context.logger(); log.info(\"building report\")\n\n    # Save an artifact\n    art = await context.artifacts().save(path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\":\"A\"})\n\n    # Record a typed result in memory\n    await context.memory().write_result(topic=\"report\", outputs=[{\"name\":\"uri\",\"kind\":\"uri\",\"value\": art.uri}])\n\n    # Notify user\n    await context.channel().send_text(f\"Report ready: {art.uri}\")\n    return {\"uri\": art.uri}\n</code></pre></p>"},{"location":"graph_fn/#concurrency-retry-advanced","title":"Concurrency &amp; retry (advanced)","text":"<p><code>GraphFunction.run()</code> accepts knobs used by the interpreter/runtime: <pre><code>await my_fn.run(\n    env=None,                            # supply a prebuilt RuntimeEnv, or let the runner build one\n    retry=RetryPolicy(),                 # backoff/retries for node execution\n    max_concurrency: int | None = None,  # cap parallelism inside the interpreter\n    **inputs,\n)\n</code></pre> For most users, calling <code>await my_fn(...)</code> / <code>.sync(...)</code> is sufficient; the runner chooses sensible defaults.</p>"},{"location":"graph_fn/#minimal-patterns","title":"Minimal patterns","text":"<p>Hello + context <pre><code>@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}\")\n    return {\"greeting\": f\"Hello, {name}\"}\n</code></pre></p> <p>One output (literal) <pre><code>@graph_fn(name=\"square\", outputs=[\"y\"])\nasync def square(x: int, *, context):\n    return x * x\n</code></pre></p> <p>Multi\u2011output dict <pre><code>@graph_fn(name=\"stats\", outputs=[\"mean\",\"std\"])\nasync def stats(xs: list[float], *, context):\n    import statistics as st\n    return {\"mean\": st.mean(xs), \"std\": st.pstdev(xs)}\n</code></pre></p>"},{"location":"graph_fn/#tips-gotchas","title":"Tips &amp; gotchas","text":"<ul> <li>Always include <code>*, context</code> when you need services (channel/memory/llm/etc.).</li> <li>Declare <code>outputs=[...]</code> if you want to return a single literal; otherwise return a dict.</li> <li>Output validation is strict when <code>outputs</code> are declared\u2014return all of them.</li> <li><code>inputs=[...]</code> is for documentation/registry; your Python signature is the source of truth at call time.</li> <li>You can also register the function as an agent by passing <code>agent=\"name\"</code> (covered later).</li> </ul>"},{"location":"graph_fn/#next-steps","title":"Next steps","text":"<ul> <li><code>graphify</code>: combine multiple functions into a larger graph with explicit edges.</li> <li><code>@tool</code>: publish functions as reusable nodes (IO typed), then orchestrate with <code>graphify</code>.</li> <li>Context services: <code>channel</code>, <code>artifacts</code>, <code>memory</code>, <code>kv</code>, <code>llm</code>, <code>rag</code>, `m</li> </ul>"},{"location":"graphify/","title":"AetherGraph \u2014 <code>@graphify</code> (Builder Decorator)","text":"<p><code>@graphify</code> lets you write a plain Python function whose body builds a <code>TaskGraph</code> using tool calls. Instead of executing immediately, the function becomes a graph factory: call <code>.build()</code> to get a concrete graph, <code>.spec()</code> to inspect, and <code>.io()</code> to see its input/output signature.</p>"},{"location":"graphify/#why-graphify-vs-graph_fn","title":"Why <code>graphify</code> vs <code>graph_fn</code>?","text":"Aspect <code>graph_fn</code> <code>graphify</code> Primary purpose Execute now as a single graph node Build a graph (explicit fan\u2011in/fan\u2011out wiring) Return at call Dict of outputs (or awaitable) A builder you later <code>.build()</code> into a graph Control\u2011flow Pythonic, implicit graph behind the scenes Explicit nodes &amp; edges via tool calls (<code>NodeHandle</code>) Best for Orchestration + <code>context.*</code> services Pipelines, DAGs, reusable subgraphs <p>Use <code>graphify</code> when you want:</p> <ul> <li>Multiple tool calls as separate nodes</li> <li>Explicit dependencies (<code>_after</code>) and fan\u2011in/fan\u2011out</li> <li>To inspect/serialize the graph spec for registry/UI</li> <li>To reuse the same pipeline with different inputs</li> </ul> <p>Use <code>graph_fn</code> when you want:</p> <ul> <li>A simple function that runs immediately and returns values</li> <li>Access to <code>context.channel()/memory()/artifacts()/llm()</code> services</li> <li>Minimal ceremony (one decorator and go)</li> </ul>"},{"location":"graphify/#decorator-signature","title":"Decorator Signature","text":"<pre><code>from aethergraph import graphify\n\n@graphify(*, name=\"default_graph\", inputs=(), outputs=None, version=\"0.1.0\", agent: str | None = None)\ndef build_fn(...):\n    ...  # tool calls returning NodeHandles\n    return {\"y\": handle.y}\n</code></pre> <p>Parameters</p> <ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (Iterable[str] or dict) \u2014 Declare required/optional inputs.  </li> <li>If <code>list/tuple</code>: treated as required input names.  </li> <li>If <code>dict</code>: <code>{required_name: ..., ...}</code> for optional mapping; builder will declare required/optional accordingly.</li> <li>outputs (list[str] | None) \u2014 Names to expose. If you return a single literal, you must declare exactly one.</li> <li>version (str) \u2014 Semantic version for registry/spec metadata.</li> <li>agent (str | None) \u2014 Optionally register the built graph under <code>agent</code> namespace.</li> </ul> <p>Return value</p> <p>The decorated symbol becomes a builder function with helpers:</p> <ul> <li><code>.build() -&gt; TaskGraph</code></li> <li><code>.spec() -&gt; GraphSpec</code></li> <li><code>.io() -&gt; IOSignature</code></li> <li>Attributes: <code>.graph_name</code>, <code>.version</code></li> </ul>"},{"location":"graphify/#writing-a-graphify-body","title":"Writing a <code>@graphify</code> Body","text":"<p>Inside the function:</p> <ol> <li>Use <code>arg(\"name\")</code> to reference declared inputs.</li> <li>Call <code>@tool</code> functions (or <code>call_tool(\"pkg.mod:fn\", ...)</code>) \u2014 each returns a <code>NodeHandle</code> in build mode.</li> <li>Return outputs as:</li> <li>A dict mapping names \u2192 <code>NodeHandle</code> outputs or refs/literals, or</li> <li>A single <code>NodeHandle</code> (its outputs will be exposed), or</li> <li>A single literal only if <code>outputs</code> has length 1.</li> </ol> <pre><code>from aethergraph import graphify, tool\nfrom aethergraph.graph import arg\n\n@tool(outputs=[\"embedded\"])\ndef embed(text: str): ...\n\n@tool(outputs=[\"score\"])\ndef score(vec, query_vec): ...\n\n@graphify(name=\"ranker\", inputs=[\"texts\",\"query\"], outputs=[\"scores\"])\ndef ranker(texts, query):\n    q = embed(text=query)\n    # fan\u2011out: call `embed` for each text\n    vecs = [embed(text=t) for t in texts]  # list[NodeHandle]\n    # fan\u2011in: score each against query vec\n    scs = [score(vec=v.embedded, query_vec=q.embedded) for v in vecs]\n    return {\"scores\": [s.score for s in scs]}\n\nG = ranker.build()\n</code></pre>"},{"location":"graphify/#control-dependencies-without-data-edges","title":"Control Dependencies without Data Edges","text":"<p>Use <code>_after</code> when you must enforce order but don\u2019t pass outputs: <pre><code>@tool(outputs=[\"ok\"])\ndef fetch(): return {\"ok\": True}\n\n@tool(outputs=[\"done\"])\ndef train(): return {\"done\": True}\n\n@graphify(name=\"seq\", inputs=[], outputs=[\"done\"])\ndef seq():\n    a = fetch()\n    b = train(_after=a)   # run b after a\n    return {\"done\": b.done}\n</code></pre></p>"},{"location":"graphify/#registration","title":"Registration","text":"<p>If a registry is active, <code>@graphify</code> registers the built graph under <code>nspace=\"graph\"</code> with <code>name</code>/<code>version</code> so it can be listed or launched elsewhere. You can also register it as an <code>agent</code> via the <code>agent=</code> parameter.</p>"},{"location":"graphify/#example-endtoend-pipeline","title":"Example: End\u2011to\u2011End Pipeline","text":"<pre><code>from aethergraph import tool, graphify\nfrom aethergraph.graph import arg\n\n@tool(outputs=[\"rows\"])\ndef load_csv(path: str): ...\n\n@tool(outputs=[\"clean\"])\ndef clean(rows): ...\n\n@tool(outputs=[\"model\"])\ndef train(data): ...\n\n@tool(outputs=[\"uri\"])\ndef report(model): ...\n\n@graphify(name=\"etl_train_report\", inputs=[\"csv_path\"], outputs=[\"uri\"])\ndef etl_train_report(csv_path):\n    raw  = load_csv(path=arg(\"csv_path\"))\n    tidy = clean(rows=raw.rows)\n    mod  = train(data=tidy.clean)\n    rep  = report(model=mod.model)\n    return {\"uri\": rep.uri}\n\nG = etl_train_report.build()\n</code></pre>"},{"location":"graphify/#using-tool-inside-graph_fn-brief","title":"Using <code>@tool</code> Inside <code>@graph_fn</code> (Brief)","text":"<p>While <code>@graph_fn</code> is for immediate execution, you can drop explicit tool nodes inside a <code>graph_fn</code> when you want finer\u2011grained tracing or parallelism:</p> <pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int): return {\"y\": x*x}\n\n@graph_fn(name=\"mix\")\nasync def mix(x: int, *, context):\n    h = square(x=x)                 # schedules a tool node in the implicit graph\n    await context.channel().send_text(\"running square\u2026\")\n    return {\"y\": h.y}               # exposes tool output as graph_fn output\n</code></pre> <p>Prefer <code>@graphify</code> for full pipeline construction; use <code>@graph_fn</code> when you want to orchestrate services (<code>context.*</code>) and run quickly.</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>A 5\u2011minute on\u2011ramp to AetherGraph: install, start the sidecar server, and run your first <code>@graph_fn</code>.</p>"},{"location":"quickstart/#1-install","title":"1. Install","text":"<pre><code>pip install aethergraph\n# or, from source\n# pip install -e .\n</code></pre> <p>Python: 3.10+</p>"},{"location":"quickstart/#2-start-the-sidecar-server-oneliner","title":"2. Start the sidecar server (one\u2011liner)","text":"<p>AetherGraph ships a lightweight sidecar that wires up core services (logger, artifacts, memory, KV, channels, etc.)</p> <pre><code># quickstart_server.py\nfrom aethergraph import start_server\n\nurl = start_server()\n</code></pre> <p>Run it:</p> <pre><code>python quickstart_server.py\n</code></pre> <p>You should see a default HTTP URL <code>http://127.0.0.1:8745</code> printed with UI (if available) and API urls. </p>"},{"location":"quickstart/#3-your-first-graph-function","title":"3. Your first graph function","text":"<p><code>@graph_fn</code> turns an ordinary async Python function into a runnable graph entrypoint. If you include a <code>context</code> parameter, you get access to built\u2011in services like <code>context.channel()</code> and <code>context.memory()</code>.</p> <pre><code># quickstart_graph_fn.py\nfrom aethergraph import graph_fn, NodeContext\nfrom aethergraph import start_server\n\n# 1) Start the sidecar so services are available\nstart_server()\n\n# 2) Define a small graph function\n@graph_fn(name=\"hello.world\", inputs=[\"name\"], outputs=[\"greeting\"])\nasync def hello_world(name: str, *, context: NodeContext):\n    # Use the channel to send a message (console by default)\n    await context.channel().send_text(f\"\ud83d\udc4b Hello, {name}! Running graph\u2026\")\n\n    # Do any Python you want here \u2014 call tools, query memory, etc.\n    greeting = f\"Hello, {name}. Nice to meet you from AetherGraph.\"\n\n    # Return outputs as a dict (keys must match `outputs=[...]`)\n    return {\"greeting\": greeting}\n\n# 3) Run it (async wrapper provided)\nif __name__ == \"__main__\":\n    import asyncio\n    async def main():\n        res = await hello_world(name=\"Researcher\")\n        print(\"Result:\", res)\n    asyncio.run(main())\n</code></pre> <p>Run it:</p> <pre><code>python quickstart_graph_fn.py\n</code></pre> <p>You should see a console message from the channel and printed output like:</p> <pre><code>Result: {\"greeting\": \"Hello, Researcher. Nice to meet you from AetherGraph.\"}\n</code></pre>"},{"location":"quickstart/#4-what-just-happened","title":"4. What just happened?","text":"<ul> <li>Sidecar server booted in the background and installed default services (channels, artifacts, memory, KV, logger).</li> <li><code>@graph_fn</code> built a tiny task graph from your function and executed it.</li> <li><code>context.channel()</code> used the default channel (console) to emit a message.</li> </ul> <p>Tip: You can override the channel at call\u2011site with <code>context.channel(channel_key=...)</code>, once you\u2019ve configured adapters like Slack, Telegram or Aethergraph UI.</p>"},{"location":"quickstart/#5-next-steps","title":"5. Next steps","text":"<ul> <li>Add tools with <code>@tool</code> to wrap reusable steps and surface inputs/outputs.</li> <li>Use <code>@graphify</code> for fan\u2011in / fan\u2011out graph construction when the body is mostly tool calls.</li> <li>Explore artifacts (<code>context.artifacts()</code>), memory (<code>context.memory()</code>), and RAG (<code>context.rag()</code>)</li> <li>Config <code>.env</code> file to integrate external channel and llm features</li> </ul>"},{"location":"server/","title":"AetherGraph \u2014 Server (Sidecar) Overview","text":"<p>The AetherGraph server is a lightweight sidecar that wires up all runtime services (channels, memory, artifacts, KV, LLM, RAG, MCP, logging, etc.) and exposes a small HTTP/WebSocket surface for adapters and tools. You can run AetherGraph without the server, but the sidecar makes it easy to:</p> <ul> <li>Use GUI/chat adapters (Slack/Telegram/Console UI) that push events back to your runs</li> <li>Host continuation callbacks for <code>ask_text()</code> / <code>ask_approval()</code></li> <li>Centralize service wiring (secrets, paths, corpora, registries)</li> <li>Inspect/trace runs, artifacts, and health in one place</li> </ul> <p>Think of it as your local control plane so your graph functions can stay plain Python.</p>"},{"location":"server/#quick-start","title":"Quick Start","text":"<pre><code>from aethergraph.server import start, stop\n\n# 1) Start the sidecar (in a background thread) and get its base URL\nurl = start(host=\"127.0.0.1\", port=0)   # port=0 \u2192 auto-pick a free port\nprint(\"AetherGraph sidecar:\", url)\n\n# 2) Run your graph functions as usual\nfrom aethergraph import graph_fn\n\n@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}\")\n    return {\"greeting\": f\"Hello, {name}\"}\n\n# ... elsewhere ...\n# res = await hello(name=\"ZC\")\n\n# 3) (Optional) Stop when done (tests/CLI)\nstop()\n</code></pre>"},{"location":"server/#api-start-start_async-stop","title":"API \u2014 <code>start()</code> / <code>start_async()</code> / <code>stop()</code>","text":""},{"location":"server/#start","title":"start","text":"<p><pre><code>start(*, workspace: str = \"./aeg_workspace\", session_id: str | None = None,\n      host: str = \"127.0.0.1\", port: int = 0, log_level: str = \"warning\") -&gt; str\n</code></pre> Start the sidecar in a background thread. Safe to call at the top of scripts or notebook cells.</p> <p>Parameters</p> <ul> <li> <p>workspace (str) \u2013 Root directory for runtime state (artifacts, logs, corpora, temp files). Auto\u2011created.</p> </li> <li> <p>session_id (str, optional) \u2013 Override the logical session. If <code>None</code>, the runtime will create one.</p> </li> <li> <p>host (str) \u2013 Bind address (defaults to loopback).</p> </li> <li> <p>port (int) \u2013 <code>0</code> picks a free port automatically; otherwise bind an explicit port.</p> </li> <li> <p>log_level (str) \u2013 Uvicorn log level (e.g., <code>\"info\"</code>, <code>\"warning\"</code>).</p> </li> </ul> <p>Returns str \u2013 Base URL, e.g., <code>\"http://127.0.0.1:54321\"</code>.</p>"},{"location":"server/#start_async","title":"start_async","text":"<p><pre><code>start_async(**kwargs) -&gt; str\n</code></pre> Async\u2011friendly wrapper that still runs the server in a thread to avoid clashing with your event loop.</p>"},{"location":"server/#stop","title":"stop","text":"<p><pre><code>stop() -&gt; None\n</code></pre> Signal the background server to shut down and join its thread (useful in tests/CI or ephemeral scripts).</p>"},{"location":"server/#why-a-sidecar","title":"Why a sidecar?","text":"<ul> <li>Continuations: <code>context.channel().ask_*</code> creates a continuation token and waits for a resume callback; the server receives user replies (Slack/Telegram/HTTP) and wakes your run.</li> <li>Adapters: chat/file/progress adapters connect over HTTP/WS to publish events (<code>agent.message</code>, <code>agent.progress.*</code>, uploads) into your run.</li> <li>Central config: one place to load settings, secrets, workspace paths, and register services (LLM, RAG, MCP, artifact store, memory backends).</li> <li>Inspection: optional health and tracing endpoints (depending on your app factory) to debug runs locally.</li> </ul>"},{"location":"server/#what-start-actually-does","title":"What <code>start()</code> actually does","text":"<ol> <li>Loads app settings (<code>load_settings()</code>), installs them as current (<code>set_current_settings(...)</code>).</li> <li>Builds a FastAPI app via <code>create_app(workspace=..., cfg=...)</code> \u2014 this registers services and routes.</li> <li>Picks a free port if <code>port=0</code> and launches Uvicorn in a background thread (non\u2011blocking).</li> <li>Returns the base URL so other components (e.g., WS/HTTP MCP clients) can connect.</li> </ol>"},{"location":"server/#typical-usage-patterns","title":"Typical usage patterns","text":""},{"location":"server/#notebooks-quick-scripts","title":"Notebooks &amp; quick scripts","text":"<pre><code>url = start(port=0)\n# \u2026 run several cells that use context.channel()/continuations\n# restart kernel or call stop() when done\n</code></pre>"},{"location":"server/#longrunning-dev-server","title":"Long\u2011running dev server","text":"<ul> <li>Call <code>start(host=\"0.0.0.0\", port=8787, log_level=\"info\")</code> once at process start.</li> <li>Point Slack/Telegram adapters or local tools at <code>http://localhost:8787</code>.</li> </ul>"},{"location":"server/#testsci","title":"Tests/CI","text":"<pre><code>url = start(port=0)\ntry:\n    # run test suite that uses continuations/artifacts\n    ...\nfinally:\n    stop()\n</code></pre>"},{"location":"server/#interop-with-context-services","title":"Interop with context services","text":"<p>Once the sidecar is up, graph functions can rely on bound services:</p> <ul> <li> <p><code>context.channel()</code> \u2013 routes via the server to your chat adapters</p> </li> <li> <p><code>context.artifacts()</code> \u2013 saves to the workspace CAS under the sidecar</p> </li> <li> <p><code>context.memory()</code> \u2013 hotlog/persistence live alongside the server\u2019s config</p> </li> <li> <p><code>context.rag()</code> \u2013 corpora root under workspace; embedders/indices wired here</p> </li> <li> <p><code>context.mcp(...)</code> \u2013 WS/HTTP MCP clients often target sidecar endpoints</p> </li> </ul>"},{"location":"server/#security-notes","title":"Security notes","text":"<ul> <li>Default bind is <code>127.0.0.1</code> (local only). Use <code>0.0.0.0</code> only in trusted networks.</li> <li>Protect WS/HTTP endpoints behind auth headers/tokens if exposing beyond localhost.</li> <li>Never log plaintext API keys; prefer a Secrets store.</li> </ul>"},{"location":"server/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Port already in use: pass <code>port=0</code> or another free port.</li> <li>Nothing happens after ask_text(): ensure the chat adapter posts replies to the sidecar (correct base URL / token).</li> <li>No LLM/kv/rag configured: your <code>create_app()</code> must wire these services (or the accessors will raise \"\u2026 not available\").</li> <li>Jupyter hangs on restart: call <code>stop()</code> before restarting the kernel, or rely on kernel shutdown to terminate the thread.</li> </ul>"},{"location":"server/#minimal-adapter-sketch-optional","title":"Minimal adapter sketch (optional)","text":"<pre><code># Example: WebSocket adapter connecting to sidecar URL\na_sync_ws_client.connect(f\"{url.replace('http','ws')}/events\", headers={\"Authorization\": \"Bearer demo\"})\n# publish OutEvent / listen for Continuation notifications\n</code></pre>"},{"location":"server/#summary","title":"Summary","text":"<p>Run the sidecar server to centralize runtime services, handle continuations/adapters, and keep your graph functions clean. Use <code>start()</code> to launch in\u2011process, <code>start_async()</code> in async apps, and <code>stop()</code> for tests/CI. Configure paths and services once; build everything else in plain Python.</p>"},{"location":"tools/","title":"AetherGraph \u2014 <code>@tool</code> Decorator (Reference &amp; How\u2011to)","text":"<p><code>@tool</code> turns a plain Python function into a tool node that can be executed immediately or added to a graph during build time. You write ordinary Python, declare outputs, and AetherGraph handles result normalization and graph node creation.</p>"},{"location":"tools/#what-is-a-tool","title":"What is a Tool?","text":"<p>A tool is a reusable, IO\u2011typed operation that can be executed on its own or orchestrated inside a graph. Tools are perfect for things like \u201cload CSV\u201d, \u201ctrain model\u201d, \u201cplot chart\u201d, \u201csend_slack\u201d, etc.</p> <ul> <li>Immediate mode (no graph builder active): calling the tool runs the Python function right away and returns a dict of outputs.</li> <li>Graph mode (inside a <code>with graph(...):</code> block or a <code>@graphify</code> body): calling the tool adds a node to the graph and returns a <code>NodeHandle</code> you can wire to other nodes (fan\u2011in/fan\u2011out).</li> <li>Tools automatically register in the runtime registry (<code>nspace=\"tool\"</code>) when a registry is active.</li> </ul> <p>This page covers the simple function form. (The advanced waitable class form is documented separately.)</p>"},{"location":"tools/#decorator-signature","title":"Decorator Signature","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs: list[str], *, inputs: list[str] | None = None,\n      name: str | None = None, version: str = \"0.1.0\")\ndef your_function(...): ...\n</code></pre> <p>Parameters</p> <ul> <li>outputs (list[str], required) \u2014 Declares the output keys your tool will produce.</li> <li>inputs (list[str], optional) \u2014 Explicit input names. Omit to infer from function signature (excluding <code>*args</code>/<code>**kwargs</code>).  </li> <li>name (str, optional) \u2014 Registry/display name. Defaults to the function\u2019s <code>__name__</code>.  </li> <li>version (str, optional) \u2014 Semantic version recorded in the registry (default: <code>\"0.1.0\"</code>).</li> </ul> <p>Return value (call\u2011site dependent)</p> <ul> <li>Immediate mode: returns a <code>dict</code> of outputs.  </li> <li>Graph mode: returns a <code>NodeHandle</code> (or an awaitable handle under an interpreter) to be wired/exposed by the builder.</li> </ul>"},{"location":"tools/#return-normalization","title":"Return Normalization","text":"<p>The wrapped function can return different shapes; the decorator normalizes into a dict that must include every declared output:</p> <ul> <li><code>None</code> \u2192 <code>{}</code></li> <li><code>dict</code> \u2192 used as\u2011is</li> <li><code>tuple</code> \u2192 <code>{\"out0\": v0, \"out1\": v1, ...}</code></li> <li>single value \u2192 <code>{\"result\": value}</code></li> </ul> <p>If any declared <code>outputs</code> are missing from the normalized dict, a <code>ValueError</code> is raised.</p>"},{"location":"tools/#control-keywords-graph-mode","title":"Control Keywords (graph mode)","text":"<p>When calling a tool while building a graph (e.g., inside a <code>with graph(...):</code> or <code>@graphify</code> body), you may pass these special kwargs to influence scheduling/metadata:</p> <ul> <li><code>_after</code> (NodeHandle | list[NodeHandle | node_id]): explicit dependency edges (fan\u2011in).  </li> <li><code>_name</code> (str): display name for UI/spec.  </li> <li><code>_id</code> (str): hard override of the node ID (must be unique in the graph).  </li> <li><code>_alias</code> (str): optional alias for reverse lookups.  </li> <li><code>_labels</code> (Iterable[str]): lightweight tags for search/grouping.</li> </ul> <p>Example:</p> <pre><code>res = my_tool(a=arg_a, b=arg_b, _after=[prev1, prev2], _name=\"preprocess\", _labels=[\"data\",\"prep\"])\n</code></pre> <p>These control keys are stripped before calling your function and only affect graph construction.</p>"},{"location":"tools/#simple-examples","title":"Simple Examples","text":""},{"location":"tools/#1-immediate-execution-no-graph-builder-active","title":"1) Immediate execution (no graph builder active)","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"sum\", \"mean\"])  # outputs you promise to return\ndef stats(xs: list[float]):\n    s = sum(xs)\n    return {\"sum\": s, \"mean\": s / len(xs)}\n\nout = stats([1,2,3,4])   # \u2192 {\"sum\": 10, \"mean\": 2.5}\n</code></pre>"},{"location":"tools/#2-graph-construction-inside-a-builder","title":"2) Graph construction (inside a builder)","text":"<pre><code>from aethergraph import tool\nfrom aethergraph import graphify\nfrom aethergraph.graph import arg  # or from aethergraph.graph.graph_refs import arg\n\n@tool(outputs=[\"y\"])\ndef add(x: int, z: int): return {\"y\": x + z}\n\n@tool(outputs=[\"z\"])\ndef mul(x: int, k: int): return {\"z\": x * k}\n\n@graphify(name=\"pipeline\", inputs=[\"x\"], outputs=[\"y\"])\ndef pipeline(x):\n    a = mul(x=arg(\"x\"), k=2)          # NodeHandle(\"mul_...\")\n    b = add(x=arg(\"x\"), z=a.z)        # depends on `a` automatically via data edge\n    return {\"y\": b.y}\n\nG = pipeline.build()                    # TaskGraph\nspec = pipeline.spec()                  # graph spec for inspection/registry\nio = pipeline.io()                      # IO signature\n</code></pre>"},{"location":"tools/#3-forcing-an-order-with-_after-no-data-edge","title":"3) Forcing an order with <code>_after</code> (no data edge)","text":"<pre><code>@tool(outputs=[\"ok\"])\ndef init(): return {\"ok\": True}\n\n@tool(outputs=[\"ready\"])\ndef warmup(): return {\"ready\": True}\n\n@graphify(name=\"order_demo\", inputs=[], outputs=[\"ready\"])\ndef order_demo():\n    n1 = init()\n    n2 = warmup(_after=n1)   # enforce sequencing without passing data\n    return {\"ready\": n2.ready}\n</code></pre>"},{"location":"tools/#registration-optional","title":"Registration (Optional)","text":"<p>If a runtime registry is active (via <code>current_registry()</code>), the decorator auto\u2011registers your tool under the <code>tool</code> namespace with its <code>name</code> and <code>version</code> so it can be listed and referenced later.</p> <p>You can also call tools by dotted path via <code>call_tool(\"pkg.module:function\", arg1=..., ...)</code> to avoid importing at build sites, but the recommended ergonomic flow is to <code>import</code> the tool and call it directly.</p>"},{"location":"tools/#best-practices","title":"Best Practices","text":"<ul> <li>Keep tools focused and side\u2011effect aware (e.g., write artifacts via <code>context.artifacts()</code> inside <code>@graph_fn</code> wrappers).</li> <li>Always declare <code>outputs</code> and make your function return those keys.</li> <li>Use <code>_after</code> for control dependencies when no data edge exists.</li> <li>Prefer composing tools via <code>@graphify</code> for explicit fan\u2011in/fan\u2011out graphs.</li> <li>Inside <code>@graph_fn</code>, you can call tools to create explicit nodes, but <code>@graph_fn</code> is for immediate orchestration.</li> </ul>"},{"location":"build-graphs/","title":"Build Graphs in AetherGraph","text":"<p>Welcome! This section is the fastest way to grok how to build and run graphs with Python-first ergonomics.</p> <p>We introduce things in the order you will actually use them:</p> <ol> <li><code>@graph_fn</code> \u2014 the on-ramp. Wrap a regular Python function so it runs as a single graph node, with full <code>context.*</code> access. Great for demos, services, notebooks.</li> <li><code>@tool</code> \u2014 make any function a graph node. Use it inside <code>graph_fn</code> for per-step visibility, metrics, artifacts, and reuse.</li> <li><code>@graphify</code> \u2014 build an explicit DAG for fan-out/fan-in, ordering via <code>_after</code>, subgraphs, and reuse.</li> </ol> <p>Tip: Start with <code>@graph_fn</code> (plus a couple of <code>@tool</code> calls). Move to <code>@graphify</code> when you want explicit topology, parallel map/reduce, barriers, or long-lived pipelines.</p>"},{"location":"build-graphs/#what-is-a-graph-here","title":"What is a \"graph\" here?","text":"<ul> <li>AetherGraph executes TaskGraphs \u2014 directed acyclic graphs of nodes.</li> <li>A node can be:</li> <li>a graph function (<code>@graph_fn</code>) \u2014 runs immediately and can call context services.</li> <li>a tool node (<code>@tool</code>) \u2014 a typed, reusable operation with visible inputs/outputs.</li> <li>The Context (<code>context.*</code>) gives every node uniform access to runtime services:   <code>channel()</code>, <code>artifacts()</code>, <code>memory()</code>, <code>kv()</code>, <code>llm()</code>, <code>rag()</code>, <code>mcp()</code>, <code>logger()</code>.</li> </ul>"},{"location":"build-graphs/#quickstart-30-lines","title":"Quickstart (30 lines)","text":"<pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int):\n    return {\"y\": x * x}\n\n@graph_fn(name=\"demo\", outputs=[\"y\"])\nasync def demo(x: int, *, context):\n    await context.channel().send_text(f\"Computing square of {x}\u2026\")\n    h = square(x=x)              # creates a node you can inspect later\n    await context.channel().send_text(\"Done.\")\n    return {\"y\": h.y}            # expose tool output\n</code></pre> <p>Why this design? - You get instant run semantics (like a normal async function), but steps you mark with <code>@tool</code> become visible graph nodes with metrics/artifacts. - When your flow grows and needs explicit fan-out/fan-in or ordering, switch to <code>@graphify</code>.</p>"},{"location":"build-graphs/#next-steps","title":"Next steps","text":"<ul> <li><code>graph_fn</code> (on-ramp) -&gt; graph_fn.md</li> <li><code>@tool</code> reference -&gt; tool.md</li> <li><code>@graphify</code> (explicit DAG + fan-in/out) -&gt; graphify.md</li> <li>Choosing the right approach -&gt; choosing.md</li> </ul>"},{"location":"build-graphs/choosing/","title":"Choosing: <code>graph_fn</code> vs <code>@graphify</code> vs <code>@tool</code>","text":"<p>Use this one-screen guide to pick the right entry point.</p>"},{"location":"build-graphs/choosing/#start-simple","title":"Start simple","text":"<ul> <li><code>@graph_fn</code> \u2014 quickest way to ship a working function with <code>context.*</code>. Add a couple of <code>@tool</code> calls inside if you want visible/inspectable steps.</li> </ul>"},{"location":"build-graphs/choosing/#scale-up-when-needed","title":"Scale up when needed","text":"<ul> <li><code>@graphify</code> \u2014 when you need explicit DAG control:</li> <li>fan-out / fan-in / map-reduce</li> <li><code>_after</code> (barriers) and <code>_alias</code>/<code>_labels</code> for orchestration and UI</li> <li>subgraph reuse and IO/spec inspection</li> </ul>"},{"location":"build-graphs/choosing/#tool-is-a-building-block","title":"<code>@tool</code> is a building block","text":"<ul> <li>Wrap any function to make it a typed node.</li> <li>Works in both: inside <code>@graph_fn</code> (immediate run, visible steps) and in <code>@graphify</code> (adds nodes to DAG).</li> <li>Control kwargs (<code>_after</code>, <code>_alias</code>, <code>_labels</code>, <code>_id</code>) apply only in graph build contexts.</li> </ul>"},{"location":"build-graphs/choosing/#quick-comparison","title":"Quick comparison","text":"Capability <code>@graph_fn</code> <code>@graphify</code> <code>@tool</code> Immediate \"just run\" Yes Build first Yes (outside graph) Full <code>context.*</code> access Yes (via <code>context</code>) via tools/subgraphs when called under <code>graph_fn</code> Visible per-step nodes via <code>@tool</code> calls native yes Fan-out / fan-in (map/reduce) limited (Python loops) Yes (concise) building block Control edges (<code>_after</code>/barrier) No Yes Yes in graph build Graph spec/IO inspection implicit Yes (<code>.spec()/.io()</code>) n/a Best for demos, services pipelines, orchestration atomic operations <p>Rule of thumb: Start with <code>@graph_fn</code>. When you feel the need for explicit topology or orchestration, switch the same steps into <code>@graphify</code> using the exact same <code>@tool</code>s.</p>"},{"location":"build-graphs/graph_fn/","title":"<code>@graph_fn</code> \u2014 Python-first on-ramp","text":"<p>Wrap a normal (async) Python function so it runs as a single graph node with full access to <code>context.*</code> services. Return values are exposed as graph outputs.</p>"},{"location":"build-graphs/graph_fn/#signature","title":"Signature","text":"<pre><code>@graph_fn(name: str, *, inputs: list[str] | None = None, outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\ndef|async def fn(..., *, context: NodeContext) -&gt; dict | value | NodeHandle\n</code></pre> <ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (list[str], optional) \u2014 Declared input keys (used for IO spec; optional for quickstart).</li> <li>outputs (list[str], optional) \u2014 Declared output keys (enables single-value return).</li> <li>version (str) \u2014 SemVer for registry/lineage.</li> <li>agent (str, optional) \u2014 If set, register this graph function as an agent (advanced).</li> </ul>"},{"location":"build-graphs/graph_fn/#return-normalization","title":"Return normalization","text":"<ul> <li>dict -&gt; keys become outputs; NodeHandles/Refs are exposed.</li> <li>single value -&gt; allowed only if exactly one <code>outputs</code> key is declared (collapsed to that name).</li> <li>NodeHandle -&gt; its outputs are exposed (single output collapses).</li> </ul>"},{"location":"build-graphs/graph_fn/#using-tool-inside-graph_fn","title":"Using <code>@tool</code> inside <code>graph_fn</code>","text":"<p>You can call <code>@tool</code> functions to create visible/inspectable nodes while keeping immediate Python control flow:</p> <pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int): return {\"y\": x*x}\n\n@graph_fn(name=\"demo\", outputs=[\"y\"])\nasync def demo(x: int, *, context):\n    h = square(x=x)          # creates a node\n    await context.channel().send_text(\"computed\")\n    return {\"y\": h.y}\n</code></pre> <p>Important: Control kwargs like <code>_after</code>, <code>_alias</code>, <code>_labels</code> are only honored in graph build contexts (e.g., <code>@graphify</code>). Inside <code>graph_fn</code>, execution order follows normal Python semantics. If you need control edges without passing data, use <code>@graphify</code>.</p>"},{"location":"build-graphs/graph_fn/#when-to-use-graph_fn","title":"When to use <code>@graph_fn</code>","text":"<ul> <li>Quick demos, notebooks, service-style tasks.</li> <li>One to a few steps, mostly sequential.</li> <li>You want full <code>context.*</code> access and instant execution, with optional visibility via <code>@tool</code> calls.</li> </ul> <p>See also: tool.md, graphify.md.</p>"},{"location":"build-graphs/graphify/","title":"<code>@graphify</code> \u2014 Build an explicit DAG (fan-out, fan-in, ordering)","text":"<p>Use <code>@graphify</code> when you need clear topology: map/fan-out, reduce/fan-in, barriers via <code>_after</code>, subgraphs, or reusable pipelines.</p>"},{"location":"build-graphs/graphify/#signature","title":"Signature","text":"<p><pre><code>@graphify(*, name=\"default_graph\", inputs=(), outputs=None, version=\"0.1.0\", agent: str | None = None)\ndef body(...):\n    # Use tool calls to add nodes and return NodeHandles/Refs\n    return {...}\n</code></pre> - The decorated function returns a builder: call <code>.build()</code> to get a <code>TaskGraph</code> instance; <code>.spec()</code> for a serializable spec; <code>.io()</code> for IO signature.</p>"},{"location":"build-graphs/graphify/#control-edges-and-labels-graph-build-only","title":"Control edges and labels (graph build only)","text":"<p><code>@tool</code> control kwargs are honored here: - <code>_after</code>, <code>_alias</code>, <code>_labels</code>, <code>_id</code>, <code>_name</code></p>"},{"location":"build-graphs/graphify/#patterns","title":"Patterns","text":""},{"location":"build-graphs/graphify/#fan-out-map-over-inputs","title":"Fan-out (map over inputs)","text":"<pre><code>from aethergraph import tool, graphify\n\n@tool(outputs=[\"vec\"])\ndef embed(text: str): ...\n\n@graphify(name=\"fanout_demo\", inputs=[\"texts\"], outputs=[\"vecs\"])\ndef fanout_demo(texts):\n    handles = [embed(text=t) for t in texts]          # fan-out\n    return {\"vecs\": [h.vec for h in handles]}         # expose list of outputs\n</code></pre>"},{"location":"build-graphs/graphify/#fan-in-reduce","title":"Fan-in (reduce)","text":"<pre><code>@tool(outputs=[\"score\"])\ndef dot(a, b): ...\n\n@graphify(name=\"fanin_demo\", inputs=[\"query\", \"vecs\"], outputs=[\"scores\"])\ndef fanin_demo(query, vecs):\n    q = embed(text=query)\n    scores = [dot(a=v, b=q.vec) for v in vecs]        # fan-in through q\n    return {\"scores\": [s.score for s in scores]}\n</code></pre>"},{"location":"build-graphs/graphify/#control-edge-without-data","title":"Control edge without data","text":"<pre><code>@tool(outputs=[\"ok\"])   def init(): ...\n@tool(outputs=[\"done\"]) def train(): ...\n\n@graphify(name=\"order\", outputs=[\"done\"])\ndef order():\n    a = init()\n    b = train(_after=a)            # sequence a -&gt; b\n    return {\"done\": b.done}\n</code></pre>"},{"location":"build-graphs/graphify/#subgraph-reuse-optional","title":"Subgraph reuse (optional)","text":"<p>You can register graphs and call them as nodes (advanced). For most cases, compose <code>@tool</code>s directly inside <code>@graphify</code>.</p>"},{"location":"build-graphs/graphify/#when-to-use-graphify","title":"When to use <code>@graphify</code>","text":"<ul> <li>You need parallelism (map) or aggregation (reduce).</li> <li>You need ordering without data flow (<code>_after</code>/barriers).</li> <li>You want a reusable / inspectable DAG (e.g., schedule in a UI).</li> </ul> <p>See also: graph_fn.md, tool.md, choosing.md.</p>"},{"location":"build-graphs/tool/","title":"<code>@tool</code> \u2014 Turn any function into a graph node","text":"<p>Make a plain function a typed, reusable node with explicit inputs/outputs. Works in both <code>@graph_fn</code> (immediate run with visible steps) and <code>@graphify</code> (graph build).</p>"},{"location":"build-graphs/tool/#decorator","title":"Decorator","text":"<pre><code>@tool(outputs: list[str], inputs: list[str] | None = None, *, name: str | None = None, version: str = \"0.1.0\")\ndef fn(...): ...\n</code></pre> <ul> <li>outputs (list[str]) \u2014 Output field names this tool produces.</li> <li>inputs (list[str], optional) \u2014 Input names; inferred from signature if omitted.</li> <li>name (str, optional) \u2014 Registry name (defaults to function name).</li> <li>version (str) \u2014 SemVer for registry/lineage.</li> </ul>"},{"location":"build-graphs/tool/#return-normalization","title":"Return normalization","text":"<ul> <li><code>None</code> -&gt; <code>{}</code></li> <li><code>dict</code> -&gt; as-is</li> <li><code>tuple</code> -&gt; <code>{ \"out0\": v0, \"out1\": v1, ... }</code></li> <li>single value -&gt; <code>{ \"result\": value }</code></li> </ul> <p>Contract check: Declared <code>outputs</code> must be present in the normalized return, otherwise a <code>ValueError</code> is raised.</p>"},{"location":"build-graphs/tool/#two-modes-same-decorator","title":"Two modes (same decorator)","text":"Where called from Behavior Outside any graph Runs immediately and returns a dict. Inside <code>@graph_fn</code> Creates a node handle you can expose. Inside <code>@graphify</code> Adds a node to the DAG (honors control kw). <p>Control kwargs (graph build only): - <code>_after</code> (NodeHandle | list) \u2014 add control-edge dependency. - <code>_alias</code> / <code>_id</code> \u2014 override node id / alias. - <code>_labels</code> (list[str]) \u2014 annotate node for UI/search. - <code>_name</code> \u2014 display name hint.</p>"},{"location":"build-graphs/tool/#minimal-example","title":"Minimal example","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"y\"])\ndef square(x:int) -&gt; dict:\n    return {\"y\": x*x}\n</code></pre> <p>Use in <code>graph_fn</code> or <code>@graphify</code> as shown in their pages.</p>"},{"location":"channel-setup/channel-backend-intro/","title":"Channels &amp; External Interaction","text":"<p>AetherGraph\u2019s channel system is how graphs talk to the outside world:</p> <ul> <li>Send messages, buttons, progress, and files out to users / tools.</li> <li>Receive replies and uploads in and resume waiting nodes.</li> <li>Integrate with Slack, Telegram, web UIs, CLIs, and custom transports.</li> </ul> <p>At a high level, there are five pieces:</p> <ol> <li>ChannelBus \u2013 orchestrates outbound events and binds correlators.</li> <li>ChannelAdapter \u2013 per-platform sender (Slack, Telegram, WS/HTTP, etc.).</li> <li>Continuation Store \u2013 remembers which continuation is waiting on which channel/thread.</li> <li>ChannelIngress \u2013 canonical inbound entry point (external \u2192 AG).</li> <li>HTTP/WS endpoints + ChannelClient \u2013 optional transport for generic web UIs / scripts.</li> </ol> <p>The design is:</p> <p>Adapters handle outbound; Ingress handles inbound; the continuation store ties them together.</p>"},{"location":"channel-setup/channel-backend-intro/#1-outbound-channelbus-channeladapter","title":"1. Outbound: ChannelBus &amp; ChannelAdapter","text":"<p>When a graph calls <code>context.channel().send_text()</code> or <code>context.channel().ask_text()</code>, the flow is:</p> <ol> <li>The ChannelSession builds an <code>OutEvent</code> and hands it to ChannelBus.</li> <li>ChannelBus picks an adapter based on the channel key (e.g. <code>\"slack:team/T:chan/C\"</code>, <code>\"tg:chat/123\"</code>, <code>\"ext:chan/user-123\"</code>).</li> <li>ChannelBus applies capability-aware fallbacks (e.g. buttons \u2192 text if the adapter has no <code>\"buttons\"</code> capability).</li> <li>The adapter sends the shaped event to the external platform.</li> <li>If the adapter returns a Correlator, ChannelBus binds it to the continuation token via the continuation store.</li> </ol>"},{"location":"channel-setup/channel-backend-intro/#11-channelbus-basics","title":"1.1 ChannelBus basics","text":"<pre><code>class ChannelBus:\n    def __init__(\n        self,\n        adapters: dict[str, ChannelAdapter],\n        *,\n        default_channel: str = \"console:stdin\",\n        channel_aliases: dict[str, str] | None = None,\n        logger=None,\n        resume_router=None,\n        store=None,\n    ):\n        ...\n\n    async def publish(self, event: OutEvent) -&gt; dict | None:\n        \"\"\"Send any OutEvent; smart fallbacks; bind correlator if any.\"\"\"\n\n    async def notify(self, continuation) -&gt; dict | None:\n        \"\"\"Ask for input/approval/files from a Continuation.\"\"\"\n\n    async def peek_correlator(self, channel_key: str) -&gt; Correlator | None:\n        ...\n</code></pre> <p>Key ideas:</p> <ul> <li>Channel prefix \u2192 adapter: <code>\"slack:...\"</code> goes to the Slack adapter, <code>\"tg:...\"</code> to Telegram, <code>\"ext:...\"</code> to the generic WS/HTTP adapter, etc.</li> <li>Capability-aware fallbacks: if an adapter doesn\u2019t support buttons or file upload, ChannelBus degrades gracefully to text (numbered options, file links, etc.).</li> <li>Correlator binding: when an adapter returns a <code>Correlator</code>, ChannelBus records a mapping <code>token \u2192 (scheme, channel, thread)</code> in the continuation store.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#12-channeladapter-protocol","title":"1.2 ChannelAdapter protocol","text":"<p>Adapters implement a simple protocol:</p> <pre><code>class ChannelAdapter(Protocol):\n    capabilities: set[str]  # e.g. {\"text\", \"buttons\", \"image\", \"file\", \"stream\"}\n\n    async def send(self, event: OutEvent) -&gt; dict | None:\n        ...\n</code></pre> <ul> <li> <p><code>OutEvent</code> carries everything the adapter might need:</p> </li> <li> <p><code>type</code>: e.g. <code>\"agent.message\"</code>, <code>\"session.need_input\"</code>, <code>\"session.need_approval\"</code>, <code>\"agent.stream.delta\"</code>, <code>\"agent.progress.update\"</code>, <code>\"file.upload\"</code>, etc.</p> </li> <li><code>channel</code>: channel key string (e.g. <code>\"slack:team/T:chan/C:thread/TS\"</code>).</li> <li><code>text</code>, <code>buttons</code>, <code>file</code>, <code>rich</code>, <code>upsert_key</code>, <code>meta</code> (including <code>run_id</code>, <code>node_id</code>, <code>token</code>, <code>resume_key</code>, etc.).</li> <li>The adapter is responsible for mapping <code>OutEvent</code> to the platform-specific API.</li> </ul> <p>You can register custom adapters at container wiring time:</p> <pre><code>queue_adapter = QueueChannelAdapter(container, scheme=\"ext\")\ncontainer.channel_bus.register_adapter(\"ext\", queue_adapter)\ncontainer.channel_bus.set_default_channel_key(\"ext:chan/default\")\n</code></pre> <p>You are not limited to built-in ones; any prefix (e.g. <code>\"mychat\"</code>) can be mapped to your own adapter.</p>"},{"location":"channel-setup/channel-backend-intro/#2-continuations-correlators-and-resumption","title":"2. Continuations, correlators, and resumption","text":"<p>Whenever a node calls <code>ask_text</code>, <code>ask_approval</code>, or similar:</p> <ol> <li> <p>A Continuation object is created and stored:</p> </li> <li> <p><code>run_id</code>, <code>node_id</code>, <code>token</code>, <code>kind</code> (e.g. <code>\"user_input\"</code>, <code>\"approval\"</code>, <code>\"user_files\"</code>).</p> </li> <li><code>channel</code> (where replies must come back).</li> <li><code>prompt</code> and optional payload.</li> <li><code>ChannelBus.notify(continuation)</code> sends a prompt event.</li> <li> <p>If the adapter returns a Correlator, ChannelBus binds:</p> </li> <li> <p>continuation token \u2192 correlator <code>(scheme, channel, thread)</code> in the continuation store.</p> </li> <li>When an inbound message arrives, AG uses this mapping to find and resume the right continuation.</li> </ol> <p>A <code>Correlator</code> typically looks like:</p> <pre><code>Correlator(\n    scheme=\"slack\",\n    channel=\"slack:team/T:chan/C:thread/TS\",\n    thread=\"TS\",          # Slack thread_ts\n    message=\"1700000000.1\" # (optional) message ts\n)\n</code></pre>"},{"location":"channel-setup/channel-backend-intro/#21-matching-inbound-messages-to-a-continuation","title":"2.1 Matching inbound messages to a continuation","text":"<p>On the inbound side, we want to answer:</p> <p>\u201cGiven this (scheme, channel, thread), which continuation is waiting?\u201d</p> <p>The pattern is:</p> <ol> <li> <p>Reconstruct a <code>Correlator</code> key from the inbound event:</p> </li> <li> <p>Slack: <code>scheme=\"slack\"</code>, <code>channel=\"slack:team/T:chan/C[:thread/TS]\"</code>, <code>thread=thread_ts or \"\"</code>.</p> </li> <li>Telegram: <code>scheme=\"tg\"</code>, <code>channel=\"tg:chat/&lt;chat_id&gt;[:topic/&lt;topic_id&gt;]\"</code>, <code>thread=str(topic_id or \"\")</code>.</li> <li> <p>Generic: <code>scheme=\"ext\"</code>, <code>channel=\"ext:chan/&lt;channel_id&gt;\"</code>, <code>thread=provided thread_id or \"\"</code>.</p> </li> <li> <p>Ask the continuation store:</p> </li> </ol> <pre><code>cont = await cont_store.find_by_correlator(corr)\n</code></pre> <ol> <li>If found, call:</li> </ol> <pre><code>await resume_router.resume(run_id=cont.run_id, node_id=cont.node_id, token=cont.token, payload=...)\n</code></pre> <p>For advanced use cases, there is also a manual resume endpoint that bypasses channels entirely and resumes directly with <code>run_id/node_id/token</code>.</p>"},{"location":"channel-setup/channel-backend-intro/#3-slack-telegram-full-featured-adapters","title":"3. Slack &amp; Telegram: full-featured adapters","text":"<p>Slack and Telegram are the most feature-complete channel adapters today. They serve as reference implementations for how to:</p> <ul> <li>Send text, buttons, streamed updates, progress, and file uploads.</li> <li>Verify inbound webhooks.</li> <li>Match inbound messages back to continuations.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#31-slack-http-and-socket-mode","title":"3.1 Slack: HTTP and Socket Mode","text":"<p>Slack integration consists of:</p> <ul> <li> <p>A <code>SlackChannelAdapter</code> implementing <code>ChannelAdapter</code>:</p> </li> <li> <p><code>capabilities = {\"text\", \"buttons\", \"image\", \"file\", \"edit\", \"stream\"}</code>.</p> </li> <li>Sends messages via <code>chat.postMessage</code>, <code>chat.update</code>, <code>files.upload_v2</code>, blocks for buttons, etc.</li> <li>Returns a <code>Correlator</code> so the continuation store can bind channel/thread \u2194 token.</li> <li> <p>HTTP routes:</p> </li> <li> <p><code>POST /slack/events</code> (Events API webhooks).</p> </li> <li><code>POST /slack/interact</code> (interactive buttons).</li> <li>Both verify signatures via Slack\u2019s signing secret.</li> <li> <p>Optional Socket Mode runner:</p> </li> <li> <p>Uses <code>SocketModeClient</code> to receive events and interactive payloads over WebSocket instead of HTTP.</p> </li> <li>Calls the same shared handlers as HTTP.</li> </ul> <p>Slack uses a rich channel key format, e.g.:</p> <pre><code>slack:team/T123:chan/C456[:thread/1700000000.12345]\n</code></pre> <p>The Slack utilities:</p> <ul> <li>Normalize this into a <code>channel_key</code>.</li> <li>Download and save files as artifacts.</li> <li>Append file_refs to a per-channel inbox in <code>kv_hot</code>.</li> <li>Resume continuations with <code>{text, files}</code> payloads when appropriate.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#32-telegram","title":"3.2 Telegram","text":"<p>Telegram has a similar structure:</p> <ul> <li>Webhook route: <code>POST /telegram/webhook</code> with custom header verification.</li> <li> <p>Helpers to:</p> </li> <li> <p>Build channel keys: <code>tg:chat/&lt;chat_id&gt;[:topic/&lt;topic_id&gt;]</code>.</p> </li> <li>Download photos/documents and store them as artifacts.</li> <li>Append file_refs to a per-channel inbox.</li> <li> <p>Matching strategy:</p> </li> <li> <p>Uses correlators with <code>scheme=\"tg\"</code>, <code>channel=\"tg:chat/...\"</code>, <code>thread=str(topic_id or \"\")</code>.</p> </li> <li>For inline buttons (callback queries), it can also use a <code>resume_key</code> and alias \u2192 token mapping for more precise routing.</li> </ul> <p>Slack and Telegram show the full power of the channel system: provider-specific verification, downloads, capability mapping, and rich correlator usage.</p>"},{"location":"channel-setup/channel-backend-intro/#4-generic-channels-channelingress-http-and-ws","title":"4. Generic channels: ChannelIngress, HTTP, and WS","text":"<p>Most users will eventually want a custom UI (React app, internal tool, notebook script) rather than Slack/Telegram. For that, AG provides a generic, provider-agnostic path:</p> <ul> <li><code>ChannelIngress</code> \u2013 handles inbound messages (external \u2192 AG) in a uniform way.</li> <li><code>QueueChannelAdapter</code> \u2013 writes outbound events to <code>kv_hot</code> outboxes.</li> <li>HTTP route <code>POST /channel/incoming</code> \u2013 generic inbound endpoint.</li> <li>WS route <code>/ws/channel</code> \u2013 generic outbound event stream.</li> <li><code>ChannelClient</code> \u2013 a small Python client for talking to the server.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#41-channelingress-the-inbound-core","title":"4.1 ChannelIngress: the inbound core","text":"<p><code>ChannelIngress</code> is the canonical entry point for inbound messages:</p> <pre><code>@dataclass\nclass IncomingMessage:\n    scheme: str             # e.g. \"ext\", \"mychat\", \"tg\", \"slack-http\"\n    channel_id: str         # logical channel / user id\n    thread_id: str | None = None\n\n    text: str | None = None\n    files: list[IncomingFile] | None = None\n    choice: str | None = None\n    meta: dict[str, Any] | None = None\n\n\nclass ChannelIngress:\n    async def handle(self, msg: IncomingMessage) -&gt; bool:\n        \"\"\"Resume the matching continuation if any. Returns True if resumed.\"\"\"\n</code></pre> <p>The default implementation:</p> <ol> <li>Builds a canonical <code>channel_key</code> from <code>(scheme, channel_id)</code> (e.g. <code>\"ext:chan/user-123\"</code>).</li> <li> <p>Finds a continuation by correlator:</p> </li> <li> <p>Prefer <code>(scheme, channel_key, thread_id)</code>.</p> </li> <li>Fallback to <code>(scheme, channel_key, thread=\"\")</code>.</li> <li>Optionally downloads files or uses provided URIs and writes them to the artifact store.</li> <li> <p>If a continuation is found, builds a payload based on its <code>kind</code>:</p> </li> <li> <p><code>\"approval\"</code> \u2192 <code>{\"choice\", \"channel_key\", \"thread_id\", \"meta\"}</code>.</p> </li> <li><code>\"user_files\"</code> / <code>\"user_input_or_files\"</code> \u2192 <code>{\"text\", \"files\", ...}</code>.</li> <li>default \u2192 <code>{\"text\", ...}</code>.</li> <li>Calls <code>resume_router.resume(...)</code> and returns <code>True</code>.</li> </ol> <p>You can sub-class <code>ChannelIngress</code> to support existing formats (e.g. Telegram\u2019s <code>tg:chat/...</code> keys) by overriding <code>_channel_key</code>.</p>"},{"location":"channel-setup/channel-backend-intro/#42-queuechanneladapter-a-generic-outbox","title":"4.2 QueueChannelAdapter: a generic outbox","text":"<p>For custom UIs, AG ships a generic adapter that pushes events into an outbox in <code>kv_hot</code>:</p> <pre><code>class QueueChannelAdapter(ChannelAdapter):\n    capabilities: set[str] = {\"text\", \"buttons\", \"image\", \"file\", \"edit\", \"stream\"}\n\n    async def send(self, event: OutEvent) -&gt; dict | None:\n        ch_key = event.channel       # e.g. \"ext:chan/user-123\"\n        outbox_key = f\"outbox://{ch_key}\"\n\n        payload = {\n            \"type\": event.type,\n            \"channel\": event.channel,\n            \"text\": event.text,\n            \"meta\": event.meta,\n            \"rich\": event.rich,\n            \"upsert_key\": event.upsert_key,\n            \"file\": event.file,\n            \"buttons\": [...],   # flattened label/value/style/url\n            \"ts\": ...,\n        }\n\n        await container.kv_hot.list_append(outbox_key, [payload])\n        return {}\n</code></pre> <p>The purpose of <code>kv_hot</code> here is not correlation, but buffering:</p> <ul> <li>The runtime writes outbound events into outboxes.</li> <li>The WS server (and any debugging tools) read from these outboxes.</li> <li>You can cap/trim these lists or move them to a more scalable queue later.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#43-http-inbound-channelincoming-and-manual-channelresume","title":"4.3 HTTP inbound: <code>/channel/incoming</code> and manual <code>/channel/resume</code>","text":"<p>A generic HTTP route for inbound messages looks like:</p> <pre><code>@router.post(\"/channel/incoming\")\nasync def channel_incoming(body: ChannelIncomingBody, request: Request):\n    container = request.app.state.container\n    ingress: ChannelIngress = container.channel_ingress\n\n    files = [... convert to IncomingFile ...]\n\n    ok = await ingress.handle(\n        IncomingMessage(\n            scheme=body.scheme,\n            channel_id=body.channel_id,\n            thread_id=body.thread_id,\n            text=body.text,\n            files=files or None,\n            choice=body.choice,\n            meta=body.meta,\n        )\n    )\n    return {\"ok\": True, \"resumed\": ok}\n</code></pre> <p>For power users, a manual resume endpoint bypasses channels and correlators entirely:</p> <pre><code>@router.post(\"/channel/resume\")\nasync def channel_resume(body: ChannelManualResumeBody, request: Request):\n    container = request.app.state.container\n    await container.resume_router.resume(\n        run_id=body.run_id,\n        node_id=body.node_id,\n        token=body.token,\n        payload=body.payload or {},\n    )\n    return {\"ok\": True}\n</code></pre>"},{"location":"channel-setup/channel-backend-intro/#44-ws-outbound-wschannel","title":"4.4 WS outbound: <code>/ws/channel</code>","text":"<p>To stream outbound events to a UI over WebSocket, AG exposes a generic endpoint:</p> <pre><code>@router.websocket(\"/ws/channel\")\nasync def ws_channel(ws: WebSocket):\n    await ws.accept()\n\n    hello = await ws.receive_json()\n    scheme = hello.get(\"scheme\") or \"ext\"\n    channel_id = hello[\"channel_id\"]\n\n    container = ws.app.state.container\n    c = container\n\n    ch_key = f\"{scheme}:chan/{channel_id}\"\n    outbox_key = f\"outbox://{ch_key}\"\n\n    last_idx = 0\n    try:\n        while True:\n            await asyncio.sleep(0.25)\n            events = await c.kv_hot.list_get(outbox_key) or []\n            if last_idx &lt; len(events):\n                for ev in events[last_idx:]:\n                    await ws.send_json(ev)\n                last_idx = len(events)\n    except WebSocketDisconnect:\n        return\n</code></pre> <p>This gives you a generic AG channel over WS:</p> <ul> <li>UI connects to <code>/ws/channel</code> and sends a handshake: <code>{ \"scheme\": \"ext\", \"channel_id\": \"user-123\" }</code>.</li> <li>UI calls <code>POST /channel/incoming</code> to send messages back.</li> <li>All heavy lifting (continuation matching, resumption) is done by <code>ChannelIngress</code> + continuation store.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#45-channelclient-talking-to-ag-from-python","title":"4.5 ChannelClient: talking to AG from Python","text":"<p>For scripts, notebooks, or simple tools, AG provides a small <code>ChannelClient</code>:</p> <pre><code>client = ChannelClient(\n    base_url=\"http://localhost:8000\",  # your AG server\n    scheme=\"ext\",\n    channel_id=\"me\",\n)\n\n# send message into AG\nawait client.send_text(\"hello from my script\")\n\n# listen for events from AG\nasync for ev in client.iter_events():\n    print(\"AG event:\", ev[\"type\"], ev.get(\"text\"))\n</code></pre> <p>This is purely a convenience wrapper around:</p> <ul> <li><code>POST /channel/incoming</code> (for inbound \u2192 AG), and</li> <li><code>/ws/channel</code> (for outbound \u2192 client).</li> </ul> <p>You can subclass or wrap it to add headers, auth tokens, retry logic, etc.</p>"},{"location":"channel-setup/channel-backend-intro/#5-auth-security","title":"5. Auth &amp; security","text":"<p>AG does not enforce a global authentication scheme for channels. Responsibilities are split:</p> <ul> <li> <p>Provider-specific webhooks (Slack, Telegram, etc.):</p> </li> <li> <p>The integration modules include helpers like <code>_verify_sig</code> (Slack) and <code>_verify_secret</code> (Telegram).</p> </li> <li>These implement the provider\u2019s required verification (HMAC signatures, secret headers).</li> <li> <p>They are examples of transport-level security, not a global policy.</p> </li> <li> <p>Generic endpoints (<code>/channel/incoming</code>, <code>/ws/channel</code>, <code>/channel/resume</code>):</p> </li> <li> <p>AG treats these as application-level concerns.</p> </li> <li>The framework assumes that if a request reaches these routes, it has already passed whatever authentication / authorization your app requires.</li> <li> <p>You are expected to wrap these routes with your own auth, for example:</p> <pre><code>from fastapi import Depends\nfrom myapp.auth import require_user\n\n@router.post(\"/channel/incoming\")\nasync def channel_incoming(\n    body: ChannelIncomingBody,\n    request: Request,\n    user = Depends(require_user),  # your auth\n):\n    ...\n</code></pre> </li> <li> <p>ChannelClient:</p> </li> <li> <p>Out of the box, it performs no auth.</p> </li> <li> <p>To use it against a secured AG server, you should:</p> <ul> <li>wrap it to add headers (e.g. <code>Authorization: Bearer ...</code> or <code>X-AG-Token</code>), and</li> <li>configure your server-side routes to check those.</li> </ul> </li> </ul>"},{"location":"channel-setup/channel-backend-intro/#51-future-simple-shared-channel-token-idea","title":"5.1 Future: simple shared channel token (idea)","text":"<p>For solo researchers or simple setups, we may add an optional single shared channel token:</p> <ul> <li> <p>Env var: <code>AETHERGRAPH_CHANNEL_TOKEN=\"some-long-random-secret\"</code>.</p> </li> <li> <p>If set, the built-in <code>/channel/incoming</code> and <code>/ws/channel</code> routes would require a header like:</p> </li> </ul> <pre><code>X-AG-Channel-Token: some-long-random-secret\n</code></pre> <ul> <li><code>ChannelClient</code> would grow a <code>token=</code> argument that automatically adds this header.</li> </ul> <p>This would provide a very simple \u201cpersonal secure channel\u201d without forcing a full auth stack, while still leaving real authentication/authorization to the host application for multi-user deployments.</p>"},{"location":"channel-setup/channel-backend-intro/#6-recommended-usage-patterns","title":"6. Recommended usage patterns","text":""},{"location":"channel-setup/channel-backend-intro/#61-simple-chat-ui-one-conversation-per-channel","title":"6.1 Simple chat UI (one conversation per channel)","text":"<p>For a basic custom UI:</p> <ul> <li>Use <code>QueueChannelAdapter</code> with prefix <code>\"ext\"</code>.</li> <li>Use <code>/channel/incoming</code> + <code>/ws/channel</code> + <code>ChannelClient</code>.</li> <li>Treat <code>(scheme, channel_id)</code> as one conversation at a time.</li> <li>Let <code>ChannelIngress</code> handle matching and resumption.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#62-multiple-flows-per-user","title":"6.2 Multiple flows per user","text":"<p>If you need multiple concurrent prompts per user:</p> <ul> <li> <p>Distinguish flows via <code>thread_id</code> or synthetic <code>channel_id</code> values:</p> </li> <li> <p>e.g. <code>channel_id=\"user-123:flow-1\"</code>, <code>thread_id=\"run-abc\"</code>.</p> </li> <li>Ensure adapters return correlators with these details so the continuation store can distinguish them.</li> <li>Optionally expose <code>resume_key</code> / <code>run_id/node_id/token</code> to the UI and use <code>/channel/resume</code> directly.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#63-provider-specific-integrations","title":"6.3 Provider-specific integrations","text":"<p>For Slack/Telegram/other platforms:</p> <ul> <li>Implement a dedicated <code>ChannelAdapter</code> and small provider-specific HTTP routes.</li> <li>Use their verification mechanisms (signing secrets, webhook tokens).</li> <li>Normalize channel keys and threads into a <code>Correlator</code> pattern compatible with your continuation store.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#7-mental-model","title":"7. Mental model","text":"<p>You can summarize the channel system as:</p> <ul> <li>ChannelBus \u2013 decides what to send where, and binds correlators.</li> <li>ChannelAdapter \u2013 knows how to talk to a specific platform.</li> <li>Continuation store \u2013 remembers who is waiting on which channel/thread.</li> <li>ChannelIngress \u2013 turns inbound messages into resumes.</li> <li>HTTP/WS endpoints + ChannelClient \u2013 optional, generic transport for custom UIs.</li> </ul> <p>Everything else is a specialization of this pattern.</p> <p>This separation keeps AetherGraph\u2019s core runtime clean while still making it easy to:</p> <ul> <li>integrate deeply with rich platforms like Slack/Telegram, and</li> <li>support lightweight custom channels via simple HTTP/WS + a small Python client.</li> </ul>"},{"location":"channel-setup/console-setup/","title":"Console Channel Setup","text":"<p>The console channel is the simplest built\u2011in channel in AetherGraph. It prints messages to your terminal and, when possible, reads replies from standard input.</p> <p>\u2705 No setup required \u2014 enabled by default.</p> <p>\ud83d\udda5\ufe0f Default key: <code>console:stdin</code></p>"},{"location":"channel-setup/console-setup/#usage","title":"Usage","text":"<pre><code># Default: uses console if no other channel is configured\nawait context.channel().send_text(\"Hello from AetherGraph \ud83d\udc4b\")\n\n# Explicit reference\nchan = context.channel(\"console:stdin\")\nawait chan.send_text(\"This goes to the terminal\")\n</code></pre>"},{"location":"channel-setup/console-setup/#capabilities","title":"Capabilities","text":"<ul> <li>Text output (printed to terminal)</li> <li>Input via <code>ask_text</code></li> <li>Buttons/approvals via <code>ask_approval</code> (rendered as numbered options)</li> </ul> <p>Internally:</p> <pre><code>capabilities = {\"text\", \"input\", \"buttons\"}\n</code></pre>"},{"location":"channel-setup/console-setup/#send_text","title":"<code>send_text</code>","text":"<pre><code>await context.channel().send_text(\"Hello \ud83d\udc4b\")\n</code></pre> <p>Prints a line like:</p> <pre><code>[console] agent.message :: Hello \ud83d\udc4b\n</code></pre> <p>Returns immediately (no continuation).</p>"},{"location":"channel-setup/console-setup/#ask_","title":"<code>ask_*</code>","text":"<pre><code>name = await context.channel().ask_text(\"What is your name?\")\nawait context.channel().send_text(f\"Nice to meet you, {name}!\")\n</code></pre> <p>Prompts on the terminal, reads a line from stdin, and resumes inline (no external wait).</p> <p>Notes: In non\u2011interactive environments (CI, no stdin), input may not be available; the runtime persists a continuation for consistency, but does not allow resumption. For normal local terminals, <code>ask_*</code> works inline without extra config.</p>"},{"location":"channel-setup/file-setup/","title":"File Channel Setup","text":"<p>The file channel is a simple, one\u2011way output channel that appends messages from AetherGraph to files on disk.</p> <p>\u2705 No setup required \u2014 writes under your workspace <code>workspace/channel_files</code>.</p> <p>\ud83d\uddc2\ufe0f Key format: <code>file:&lt;relative/path/to/file&gt;</code></p>"},{"location":"channel-setup/file-setup/#when-to-use","title":"When to Use","text":"<ul> <li>Persistent, local run logs (steps, status, results) with custom format</li> <li>Transcripts for papers/debugging</li> <li>Plain\u2011text output you can open with any editor</li> </ul>"},{"location":"channel-setup/file-setup/#where-files-are-written","title":"Where Files Are Written","text":"<p>Files are created under:</p> <pre><code>&lt;workspace&gt;/channel_files\n</code></pre> <ul> <li><code>&lt;workspace&gt;</code> is your AetherGraph data root.</li> <li>The portion after <code>file:</code> becomes a relative path under <code>channel_files</code>.</li> </ul> <p>Example</p> <pre><code>chan = context.channel(\"file:runs/demo_run.log\")\nawait chan.send_text(\"Demo run started\")\n</code></pre> <p>Writes (appends) to:</p> <pre><code>&lt;workspace&gt;/channel_files/runs/demo_run.log\n</code></pre> <p>Parent directories are created automatically.</p>"},{"location":"channel-setup/file-setup/#usage","title":"Usage","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"file_channel_demo\")\nasync def file_channel_demo(*, context: NodeContext):\n    chan = context.channel(\"file:logs/experiment_01.txt\")\n    await chan.send_text(\"Run began\")\n    await chan.send_text(\"Metric: acc=0.93, loss=0.12\")\n    return {\"logged\": True}\n</code></pre>"},{"location":"channel-setup/file-setup/#notes","title":"Notes","text":"<ul> <li>One\u2011way: no <code>ask_*</code> prompts (write\u2011only).</li> <li>Append behavior: messages are appended; rotate/cleanup as needed.</li> <li>Organization tip: include date/run IDs in paths (e.g., <code>file:runs/2025-11-15/expA.txt</code>).</li> </ul>"},{"location":"channel-setup/introduction/","title":"Channels Overview","text":"<p>AetherGraph ships with multiple channels for delivering messages and (optionally) interacting with users or tools. This page gives you a compact map of what exists, when to use each, and what they can do.</p> <p>Scope (OSS build): Channels are designed for personal, local, and small\u2011team use. Treat them as convenient building blocks, not as a hardened, multi\u2011tenant messaging stack. For public or sensitive deployments, keep channels behind trusted networks and review security settings carefully.</p>"},{"location":"channel-setup/introduction/#1-channel-quick-picks-when-to-use-what","title":"1. Channel quick picks (when to use what)","text":"<ul> <li>Console (<code>console:</code>) \u2013 Default. Local dev, quick demos, CLI\u2011style prompts. No setup.</li> <li>Slack (<code>slack:</code>) \u2013 Team chat, rich approvals, durable <code>ask_*</code> resumes. Requires Slack app.</li> <li>Telegram (<code>tg:</code>) \u2013 Mobile\u2011friendly prompts and notifications. Polling (local) or webhook (advanced). Experimental for <code>ask_*</code>.</li> <li>File (<code>file:</code>) \u2013 Write\u2011only logs/transcripts to disk under your workspace. Zero setup.</li> <li>Webhook (<code>webhook:</code>) \u2013 Write\u2011only JSON POST to any incoming webhook (Slack Incoming, Discord, Zapier, etc.). Zero setup in AetherGraph.</li> <li>Aethergraph UI \u2013 Built-in channels with most supported features for runs and sessions with no additional config. </li> </ul>"},{"location":"channel-setup/introduction/#2-capabilities-at-a-glance","title":"2. Capabilities at a glance","text":"<p>Legend: \u2705 supported \u2022 \ud83d\udcdd forwarded/logged only \u2022 \u2716\ufe0f not supported</p> Channel Key prefix / example Default Text Input / <code>ask_*</code> Buttons / approval Image File Streaming/Edit Inbound resume Console <code>console:stdin</code> \u2705 \u2705 \u2705 (inline via stdin) \u2705 (numbered) \u2716\ufe0f \u2716\ufe0f \u2716\ufe0f \u2716\ufe0f Slack <code>slack:team/T:chan/C[:thread/TS]</code> \u2716\ufe0f \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Telegram <code>tg:chat/&lt;id&gt;[:topic/&lt;thread_id&gt;]</code> \u2716\ufe0f \u2705 \u2705 (experimental) \u2705 \u2705 \u2705 \u2705 \u2705 (experimental) File <code>file:logs/experiment_01.txt</code> \u2716\ufe0f \u2705 \u2716\ufe0f \ud83d\udcdd \ud83d\udcdd \ud83d\udcdd \ud83d\udcdd N/A Webhook <code>webhook:https://hooks.zapier.com/...</code> \u2716\ufe0f \u2705 \u2716\ufe0f \ud83d\udcdd \ud83d\udcdd \ud83d\udcdd \ud83d\udcdd N/A AG UI use <code>context.ui_session_channel()</code> or <code>context.ui_run_channel()</code> for fast access \u2716\ufe0f \u2705 \u2705 \u2705 \ud83d\udcdd \ud83d\udcdd \ud83d\udcdd \u2705 \u2705 <p>Notes</p> <ul> <li>Console is simple and local; great for development but not built for durable, cross\u2011process resumes.</li> <li>Slack/Telegram provide two\u2011way chat; Telegram\u2019s interaction support is still experimental.</li> <li>File/Webhook are inform\u2011only: push messages out, no replies.</li> </ul>"},{"location":"channel-setup/introduction/#3-ask_-vs-informonly","title":"3. <code>ask_*</code> vs inform\u2011only","text":"<p>Interactive (<code>ask_*</code>)</p> <ul> <li>Supported: Console, Slack, Telegram.</li> <li>In <code>@graphify</code>, Slack/Telegram create a Continuation and can resume even if your Python process restarts.</li> <li>Console reads inline from stdin; no durable, cross\u2011process resume.</li> </ul> <p>Inform\u2011only</p> <ul> <li>File and Webhook only push out.</li> <li>Use them for notifications, logging, and triggering automations; do not call <code>ask_*</code> on these.</li> </ul>"},{"location":"channel-setup/introduction/#4-concurrency-multichannel-patterns","title":"4. Concurrency &amp; multi\u2011channel patterns","text":"<ul> <li>You can send multiple messages concurrently on a channel.</li> <li>You can use multiple channels in the same graph (e.g., console debug + file log + Slack + webhook).</li> </ul> <p>Example:</p> <pre><code>async def run_with_notifications(context):\n    await context.channel(\"console:stdin\").send_text(\"Run started\")\n    await context.channel(\"file:runs/exp_01.log\").send_text(\"Run started\")\n    await context.channel(\"slack:team/T:chan/C\").send_text(\"Run started (Slack)\")\n    await context.channel(\"webhook:https://hooks.zapier.com/hooks/catch/.../\").send_text(\"Run started (Webhook)\")\n</code></pre>"},{"location":"channel-setup/introduction/#5-security-tips-webhook-chat","title":"5. Security tips (webhook &amp; chat)","text":"<ul> <li>Treat webhook URLs as secrets; rotate if leaked. Prefer HTTPS.</li> <li>For custom receivers, add a shared secret header (e.g., <code>X-AetherGraph-Secret</code>) and verify server\u2011side.</li> <li>Keep public exposure minimal; prefer trusted networks or intermediaries (Zapier/Make, your backend) when adapting payload shapes.</li> <li>Review bot/app permissions on Slack/Telegram\u2014grant only what you need.</li> </ul>"},{"location":"channel-setup/introduction/#6-key-formats-cheat-sheet","title":"6. Key formats (cheat sheet)","text":"<ul> <li>Console: <code>console:stdin</code></li> <li>Slack: <code>slack:team/&lt;TEAM_ID&gt;:chan/&lt;CHANNEL_ID&gt;[:thread/&lt;TS&gt;]</code></li> <li>Telegram: <code>tg:chat/&lt;CHAT_ID&gt;[:topic/&lt;THREAD_ID&gt;]</code></li> <li>File: <code>file:&lt;relative/path/inside/channel_files&gt;</code></li> <li>Webhook: <code>webhook:&lt;FULL_WEBHOOK_URL&gt;</code></li> </ul> <p>Use these keys directly with <code>context.channel(&lt;key&gt;)</code>, or set defaults/aliases as needed.</p>"},{"location":"channel-setup/slack-setup/","title":"Slack Integration Setup (Socket Mode)","text":"<p>This guide shows you how to connect Slack to AetherGraph using Socket Mode \u2014 ideal for local or individual use.</p> <p>\u2705 No public URL or ngrok required.</p> <p>\u2705 Runs securely via WebSocket.</p>"},{"location":"channel-setup/slack-setup/#before-you-start","title":"Before You Start","text":"<ol> <li>Install AetherGraph with Slack extras:</li> </ol> <pre><code>pip install \"aethergraph[slack]\"\n</code></pre> <ol> <li>Make sure you have a <code>.env</code> file in your project root. AetherGraph will read Slack configuration from it.</li> </ol>"},{"location":"channel-setup/slack-setup/#1-create-a-slack-app-with-manifest-json","title":"1. Create a Slack App (with Manifest JSON)","text":"<ol> <li>Go to https://api.slack.com/apps \u2192 click \u201cCreate New App\u201d \u2192 \u201cFrom an app manifest.\u201d</li> <li>Choose your workspace.</li> <li>Paste the following JSON manifest (you can rename the app if you wish):</li> </ol> <pre><code> {\n     \"display_information\": {\n         \"name\": \"AetherGraph\"\n     },\n     \"features\": {\n         \"bot_user\": {\n             \"display_name\": \"AetherGraph\",\n             \"always_online\": true\n         }\n     },\n     \"oauth_config\": {\n         \"scopes\": {\n             \"bot\": [\n                 \"app_mentions:read\",\n                 \"channels:history\",\n                 \"chat:write\",\n                 \"channels:manage\",\n                 \"channels:read\",\n                 \"files:read\",\n                 \"files:write\",\n                 \"groups:read\",\n                 \"groups:history\"\n             ]\n         }\n     },\n     \"settings\": {\n         \"event_subscriptions\": {\n             \"bot_events\": [\n                 \"app_mention\",\n                 \"message.channels\",\n                 \"message.groups\"\n             ]\n         },\n         \"interactivity\": {\n             \"is_enabled\": true\n         },\n         \"org_deploy_enabled\": false,\n         \"socket_mode_enabled\": true,\n         \"token_rotation_enabled\": false\n     }\n }\n</code></pre> <p>Note: For Socket Mode, you do not need to configure an HTTP Request URL for events or interactivity.</p> <ol> <li>Click Create App.</li> <li>Go to OAuth &amp; Permissions \u2192 Install App to Workspace and complete installation.</li> </ol>"},{"location":"channel-setup/slack-setup/#2-enable-socket-mode-and-get-tokens","title":"2. Enable Socket Mode and Get Tokens","text":"<ol> <li>In your app\u2019s left sidebar, go to Socket Mode.</li> <li>Toggle Enable Socket Mode \u2192 ON.</li> <li> <p>Click \u201cGenerate App-Level Token\u201d:</p> </li> <li> <p>Name it something like <code>aethergraph-app-token</code>.</p> </li> <li>Grant it the <code>connections:write</code> scope.</li> <li>Copy the token (starts with <code>xapp-...</code>).</li> <li>Go to OAuth &amp; Permissions, install the app to Slack, and copy the Bot User OAuth Token (starts with <code>xoxb-...</code>).</li> </ol> <p>You now have:</p> <ul> <li>Bot token (<code>xoxb-\u2026</code>)</li> <li>App token (<code>xapp-\u2026</code>)</li> <li>(Optional) Signing secret \u2014 found under Basic Information \u2192 App Credentials \u2192 Signing Secret</li> </ul>"},{"location":"channel-setup/slack-setup/#3-configure-env-for-aethergraph","title":"3. Configure <code>.env</code> for AetherGraph","text":"<p>AetherGraph reads Slack settings from your environment variables.</p> <p>Add the following lines to your <code>.env</code>:</p> <pre><code># Slack (optional)\nAETHERGRAPH_SLACK__ENABLED=true             # must be true to enable\nAETHERGRAPH_SLACK__BOT_TOKEN=xoxb-your-bot-token-here\nAETHERGRAPH_SLACK__APP_TOKEN=xapp-your-app-token-here\nAETHERGRAPH_SLACK__SIGNING_SECRET=your-signing-secret-here\nAETHERGRAPH_SLACK__SOCKET_MODE_ENABLED=true  # usually true for local testing\nAETHERGRAPH_SLACK__WEBHOOK_ENABLED=false     # usually false for local testing\n</code></pre> <p>After saving <code>.env</code>, restart your AetherGraph sidecar so the new settings take effect.</p> <p>With this setup:</p> <ul> <li>AetherGraph connects to Slack via WebSocket (Socket Mode).</li> <li>You don\u2019t need ngrok or a public URL.</li> </ul>"},{"location":"channel-setup/slack-setup/#4-setting-up-slack-channels-and-aliases","title":"4. Setting Up Slack Channels and Aliases","text":"<p>Once Slack is enabled, you can define which channel AetherGraph should talk to.</p> <p>Here\u2019s a typical setup pattern:</p> <pre><code>import os\nfrom aethergraph.channels import set_default_channel, set_channel_alias\n\nSLACK_TEAM_ID = os.getenv(\"SLACK_TEAM_ID\", \"your-slack-team-id\")\nSLACK_CHANNEL_ID = os.getenv(\"SLACK_CHANNEL_ID\", \"your-slack-channel-id\")\nslack_channel_key = f\"slack:team/{SLACK_TEAM_ID}:chan/{SLACK_CHANNEL_ID}\"  # Slack channel key format\n\n# Set as the default channel\nset_default_channel(slack_channel_key)\n\n# Optional: define an alias\nset_channel_alias(\"my_slack\", slack_channel_key)\n</code></pre> <p>Usage examples:</p> <pre><code>chan = context.channel()  # uses the default Slack channel\nawait chan.send_text(\"Hello from AetherGraph \ud83d\udc4b\")\n\nchan2 = context.channel(\"my_slack\")  # use a named alias\nawait chan2.send_text(\"Message to my_slack alias\")\n\n# or directly specify the channel\nawait context.channel().send_text(\"Custom target\", channel=slack_channel_key)\n</code></pre> <p>If nothing is set up, AetherGraph automatically falls back to <code>console:stdin</code>.</p> <p>Finding your Team &amp; Channel IDs</p> <ul> <li>Channel ID: Open Slack in a browser \u2192 navigate to the channel \u2192 copy the <code>C\u2026</code> (public) or <code>G\u2026</code> (private) part from the URL.</li> <li>Team ID: In the same URL, copy the <code>T\u2026</code> segment (your workspace ID).</li> </ul>"},{"location":"channel-setup/slack-setup/#5-quick-test","title":"5. Quick Test","text":"<p>Once everything is configured, test your integration:</p> <p>Invite the bot to your channel</p> <ul> <li> <p>Private channels require the bot to be a member before it can post. Invite it via Add people or mention <code>@YourBot</code> and select Invite to channel.</p> </li> <li> <p>For DMs, post to the DM channel ID (<code>D\u2026</code>), not a user ID.</p> </li> </ul> <p>Run the graph \u2014 if your message appears in Slack, you\u2019re all set!</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"hello_slack\")\nasync def hello_slack(*, context: NodeContext):\n    chan = context.channel()\n    await chan.send_text(\"Hello from AetherGraph \ud83d\udc4b\")\n    return {\"ok\": True}\n</code></pre>"},{"location":"channel-setup/slack-setup/#notes","title":"Notes","text":"<ul> <li>This Socket Mode setup is for local / individual use only.</li> <li>Do not expose your sidecar server directly to the internet.</li> <li>Future versions will include webhook-based production integrations.</li> </ul>"},{"location":"channel-setup/telegram-setup/","title":"Telegram Integration Setup (Local, Experimental)","text":"<p>Connect a Telegram bot to AetherGraph for local / personal use via polling.</p> <p>\u2705 No public URL or webhook required (polling).</p> <p>\u2705 Great for demos and quick experiments.</p> <p>\u26a0\ufe0f Status: Experimental \u2014 if the first message is slow to appear or polling stalls, wait a few more seconds before sending or restart the sidecar and send a fresh message.</p>"},{"location":"channel-setup/telegram-setup/#before-you-start","title":"Before You Start","text":"<ol> <li> <p>You don't need to install additional dependencies for local Telegram setup</p> </li> <li> <p>Ensure you have a <code>.env</code> file in your project root. AetherGraph reads Telegram settings from it.</p> </li> </ol>"},{"location":"channel-setup/telegram-setup/#1-create-a-telegram-bot-botfather","title":"1. Create a Telegram Bot (BotFather)","text":"<ol> <li>In Telegram, start a chat with @BotFather.</li> <li> <p>Send <code>/newbot</code> and follow prompts:</p> </li> <li> <p>Pick a name (e.g., <code>AetherGraph Telegram Bot</code>).</p> </li> <li>Pick a username ending in <code>bot</code> (e.g., <code>aethergraph_dev_bot</code>).</li> <li>Copy the bot token BotFather returns (looks like <code>123456789:ABC...</code>).</li> </ol> <p>That\u2019s all you need for local polling mode.</p>"},{"location":"channel-setup/telegram-setup/#2-configure-env-for-aethergraph","title":"2. Configure <code>.env</code> for AetherGraph","text":"<p>Add the following variables (update the values you received):</p> <pre><code># Telegram (optional)\nAETHERGRAPH_TELEGRAM__ENABLED=true               # must be true to enable\nAETHERGRAPH_TELEGRAM__BOT_TOKEN=123456789:ABC... # from BotFather\n\n# Local/dev polling mode (keep this for local usage)\nAETHERGRAPH_TELEGRAM__POLLING_ENABLED=true\nAETHERGRAPH_TELEGRAM__WEBHOOK_ENABLED=false\n</code></pre> <p>After saving, restart your AetherGraph sidecar so the new settings take effect.</p> <p>If you previously used webhooks with this bot, disable them once so polling receives updates: <pre><code>curl \"https://api.telegram.org/bot&lt;YOUR_BOT_TOKEN&gt;/deleteWebhook\"\n</code></pre></p>"},{"location":"channel-setup/telegram-setup/#3-channel-keys-defaults-and-aliases","title":"3. Channel Keys, Defaults, and Aliases","text":"<p>AetherGraph uses a channel key to address targets. For Telegram, the canonical format is:</p> <pre><code>tg:chat/&lt;CHAT_ID&gt;\n</code></pre> <p>You can wire this up in startup code just like Slack:</p> <pre><code>import os\nfrom aethergraph.channels import set_default_channel, set_channel_alias\n\nTELEGRAM_CHAT_ID = os.getenv(\"TELEGRAM_CHAT_ID\", \"your-telegram-chat-id\")\ntelegram_channel_key = f\"tg:chat/{TELEGRAM_CHAT_ID}\"  # Telegram channel key format\n\n# Set as the default channel for context.channel()\nset_default_channel(telegram_channel_key)\n\n# Optional: create a friendly alias\nset_channel_alias(\"my_tg\", telegram_channel_key)\n</code></pre> <p>Usage patterns:</p> <pre><code>chan = context.channel()              # uses default Telegram chat (if set)\nawait chan.send_text(\"Hello from AetherGraph via Telegram \ud83d\udc4b\")\n\nchan2 = context.channel(\"my_tg\")     # use alias explicitly\nawait chan2.send_text(\"Message via alias\")\n\n# Or target explicitly at call time\nawait context.channel().send_text(\"Custom target\", channel=telegram_channel_key)\n</code></pre> <p>Fallback: If Telegram isn\u2019t configured, <code>context.channel()</code> falls back to <code>console:stdin</code>.</p>"},{"location":"channel-setup/telegram-setup/#4-finding-your-telegram-chat-id","title":"4. Finding Your Telegram Chat ID","text":"<ul> <li> <p>1:1 chats: Start a conversation with your bot (send <code>/start</code>). Then either:</p> <ul> <li>Check recent updates using the Bot API <code>getUpdates</code> (your chat ID appears in the payload), or</li> <li>Forward any message to a utility bot like <code>@userinfobot</code> to read the numeric ID it reports. (Recommended)</li> </ul> </li> <li> <p>Groups / supergroups: Add your bot to the group and send a message in the group. The chat ID is usually a negative number (often begins with <code>-100...</code>). Retrieve it via <code>getUpdates</code>.</p> </li> </ul> <p>See Appendix to learn how to use <code>@userinfobot</code> and <code>getUpdates</code></p>"},{"location":"channel-setup/telegram-setup/#5-add-the-bot-to-a-group-optional","title":"5. Add the Bot to a Group (Optional)","text":"<p>If you want the bot to post in a group:</p> <ol> <li>Add the bot to the group (use the group\u2019s add dialog or mention the bot and choose Add to Group).</li> <li>If your flow requires reading history or reacting to commands, ensure the bot has the needed group permissions.</li> <li>Use the group\u2019s chat ID (negative number) as your target.</li> </ol>"},{"location":"channel-setup/telegram-setup/#6-quick-test","title":"6. Quick Test","text":"<p>Create a tiny graph and send a message:</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"ping_telegram\")\nasync def ping_telegram(*, context: NodeContext):\n    chan = context.channel()  # uses default Telegram chat if configured\n    await chan.send_text(\"Ping from AetherGraph \ud83d\udef0\ufe0f\")\n    return {\"ok\": True}\n</code></pre> <p>Run the graph and confirm the message appears in your Telegram chat.</p>"},{"location":"channel-setup/telegram-setup/#notes-troubleshooting","title":"Notes &amp; Troubleshooting","text":"<ul> <li>First message pickup can be slow on some networks. Send a new message to the bot (e.g., <code>/start</code>) and re-run the test.</li> <li>If polling appears stuck, restart the sidecar.</li> <li>Treat Telegram as best-effort for now; for robust production flows, prefer Slack or your internal web UI until Telegram stabilizes.</li> </ul>"},{"location":"channel-setup/telegram-setup/#appendix-get-your-telegram-chat-id-easiest-userinfobot","title":"Appendix: Get Your Telegram Chat ID (easiest: @userinfobot)","text":"<ul> <li> <p>Private 1:1 (DM):</p> <ul> <li>Open @userinfobot and tap Start.</li> <li>It replies with Your ID:  \u2014 this is your chat ID. <li>Use it as: tg:chat/. <li> <p>Group / Supergroup:</p> <ul> <li>Add @userinfobot to the group.</li> <li>Send any message (e.g., /start or any text).</li> <li>The bot posts the Group ID (a negative number, often -100\u2026).</li> <li>Use it as: tg:chat/. <li> <p>Channel:</p> <ul> <li>Option A (forward): Post in the channel, then forward that post to @userinfobot \u2014 it replies with the Channel ID (negative number).</li> <li>Option B (temporary add): Add @userinfobot to the channel (temporarily, usually as admin), post once, and the bot will report the Channel ID.</li> </ul> </li> <p>You can remove @userinfobot after you\u2019ve captured the ID.</p> <p>Optional alternative: You can also retrieve the ID using Telegram\u2019s Bot API getUpdates and reading chat.id from the JSON. See Telegram official documents on how to use it.</p>"},{"location":"channel-setup/webhook-setup/","title":"Webhook Channel Setup &amp; Usage","text":"<p>The webhook channel lets AetherGraph send JSON payloads via HTTP POST to any service that accepts incoming webhooks (Slack Incoming Webhooks, Discord, Zapier, etc.).</p> <p>\u2705 No installation or configuration in AetherGraph \u2014 just use a webhook URL.</p> <p>\ud83d\udd14 One\u2011way only: webhooks push notifications out; they cannot receive replies or run <code>ask_*</code> prompts.</p>"},{"location":"channel-setup/webhook-setup/#when-to-use","title":"When to Use","text":"<p>Use webhooks for:</p> <ul> <li>Notifications (\"run finished\", progress updates).</li> <li>Logging / audit into external systems.</li> <li>Triggering automations (Zapier, Make) without writing adapters.</li> </ul>"},{"location":"channel-setup/webhook-setup/#key-format","title":"Key Format","text":"<pre><code>webhook:&lt;WEBHOOK_URL&gt;\n</code></pre> <p>Where <code>&lt;WEBHOOK_URL&gt;</code> is the full URL provided by your target service (Slack, Discord, Zapier, etc.).</p>"},{"location":"channel-setup/webhook-setup/#minimal-payload-what-we-send","title":"Minimal Payload (what we send)","text":"<pre><code>{\n  \"type\": \"agent.message\",\n  \"channel\": \"webhook:&lt;WEBHOOK_URL&gt;\",\n  \"text\": \"Run finished \u2705\",\n  \"content\": \"Run finished \u2705\",\n  \"meta\": {},\n  \"timestamp\": \"...\"\n}\n</code></pre> <p>Services that require a custom shape can be adapted via Zapier/Make or by transforming on the receiving side.</p>"},{"location":"channel-setup/webhook-setup/#tested-targets-examples","title":"Tested Targets (Examples)","text":""},{"location":"channel-setup/webhook-setup/#slack-incoming-webhook","title":"Slack \u2013 Incoming Webhook","text":"<ol> <li>Add Incoming Webhooks in Slack \u2192 create a webhook for a channel.</li> <li>Copy the URL like <code>https://hooks.slack.com/services/XXX/YYY/ZZZ</code>.</li> <li>Use it directly as <code>webhook:&lt;URL&gt;</code>.</li> </ol> <pre><code>SLACK_URL = \"https://hooks.slack.com/services/XXX/YYY/ZZZ\"\nchan = context.channel(f\"webhook:{SLACK_URL}\")\nawait chan.send_text(\"AetherGraph run completed \u2705\")\n</code></pre>"},{"location":"channel-setup/webhook-setup/#discord-channel-webhook","title":"Discord \u2013 Channel Webhook","text":"<ol> <li>Server Settings \u2192 Integrations \u2192 Webhooks \u2192 New Webhook \u2192 choose channel.</li> <li>Copy URL like <code>https://discord.com/api/webhooks/123/ABC...</code>.</li> <li>Use as <code>webhook:&lt;URL&gt;</code>.</li> </ol> <pre><code>DISCORD_URL = \"https://discord.com/api/webhooks/123/ABC...\"\nawait context.channel(f\"webhook:{DISCORD_URL}\").send_text(\"Experiment done \ud83c\udf89\")\n</code></pre>"},{"location":"channel-setup/webhook-setup/#zapier-catch-hook","title":"Zapier \u2013 Catch Hook","text":"<ol> <li>Create a Zap \u2192 Trigger: Webhooks by Zapier \u2192 Catch Hook.</li> <li>Copy the Catch Hook URL.</li> <li>Use as <code>webhook:&lt;URL&gt;</code> and map <code>text</code>/<code>content</code> in Zapier.</li> </ol> <pre><code>ZAP_URL = \"https://hooks.zapier.com/hooks/catch/123456/abcdef/\"\nawait context.channel(f\"webhook:{ZAP_URL}\").send_text(\"Model training completed \ud83e\uddea\")\n</code></pre> <p>The same pattern usually works with Microsoft Teams, Google Chat, Mattermost, Rocket.Chat, Zulip, or any endpoint that accepts JSON POSTs.</p>"},{"location":"channel-setup/webhook-setup/#usage-pattern-general","title":"Usage Pattern (General)","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\nWEBHOOK_URL = \"https://example.com/incoming\"\n\n@graph_fn(name=\"webhook_demo\")\nasync def webhook_demo(*, context: NodeContext):\n    chan = context.channel(f\"webhook:{WEBHOOK_URL}\")\n    await chan.send_text(\"Run finished \u2705\")\n    await chan.send_text(\"Metrics: acc=0.93, loss=0.12\")\n    return {\"notified\": True}\n</code></pre>"},{"location":"channel-setup/webhook-setup/#notes-best-practices","title":"Notes &amp; Best Practices","text":"<ul> <li>One\u2011way only: no replies/continuations; combine with other channels for interactions.</li> <li>Resilience: if a webhook returns 4xx/5xx or times out, we log the error; your graph continues by default.</li> <li>Security: treat URLs as secrets; rotate if leaked. Consider using Zapier/Make as a buffer when adapting payload shapes.</li> <li>Multiple endpoints: you can create multiple webhook channels within the same graph run.</li> </ul>"},{"location":"design/server/","title":"AetherGraph Deployment Modes &amp; Data Flow","text":"<p>This doc outlines three primary deployment modes for AetherGraph (AG), how the sidecar/server behaves in each, and how data flows between clients (Slack, browser UI, other services), AG, and storage.</p> <p>We\u2019ll describe:</p> <ul> <li>Mode 1 \u2014 Local Sidecar (Developer / Researcher)</li> <li>Mode 2 \u2014 Single-Tenant App Server (Enterprise Self-Hosted)</li> <li>Mode 3 \u2014 SaaS Control Plane + Worker Pool (Hosted by AIperture)</li> </ul> <p>For each mode, we\u2019ll specify:</p> <ul> <li>Responsibilities of the server</li> <li>Data flow for channel events (e.g., Slack) and UI</li> <li>Where storage lives and how the primitives (BlobStore, DocStore, EventLog, VectorIndex) fit</li> </ul> <p>At the end, we\u2019ll summarize how to evolve from Mode 1 \u2192 Mode 2 \u2192 Mode 3 without changing graph/tool APIs.</p>"},{"location":"design/server/#shared-concepts-across-all-modes","title":"Shared Concepts Across All Modes","text":"<p>Before diving into modes, here are the shared pieces that exist in every deployment:</p> <ul> <li> <p>AG Runtime</p> </li> <li> <p>Executes graphs and tools.</p> </li> <li> <p>Uses <code>ExecutionContext</code> and services (artifacts, memory, state, continuations, LLM, custom services, etc.).</p> </li> <li> <p>Storage primitives</p> </li> <li> <p>BlobStore \u2013 raw bytes (artifacts, bundles, large snapshots).</p> </li> <li>DocStore \u2013 keyed JSON docs (summaries, config, snapshot metadata, continuation payloads, etc.).</li> <li>EventLog \u2013 append-only events (chat turns, tool_results, state events, memory events).</li> <li> <p>VectorIndex \u2013 embeddings search for RAG and semantic retrieval.</p> </li> <li> <p>Domain facades</p> </li> <li> <p><code>ArtifactFacade</code> (ArtifactStore + ArtifactIndex)</p> </li> <li><code>MemoryFacade</code> (hot log + durable log + doc summaries + optional vector)</li> <li><code>GraphStateStore</code> (snapshots + state events)</li> <li> <p><code>ContinuationStore</code> (continuations + token/correlator indices)</p> </li> <li> <p>Channel / Interaction layer</p> </li> <li> <p>Normalises external messages into AG events (e.g. Slack, HTTP chat, WebSocket).</p> </li> <li> <p>Uses continuations + resume router for DualStage tools.</p> </li> <li> <p>HTTP/WS API surface (varies by mode)</p> </li> <li> <p><code>/api/graph/{graph_id}/run</code> \u2013 invoke a graph.</p> </li> <li><code>/api/events</code> \u2013 query events for observability.</li> <li><code>/api/events/stream</code> \u2013 live events via WS or SSE.</li> <li><code>/api/artifacts</code> \u2013 upload/download artifact bytes.</li> <li><code>/api/channel/*</code> \u2013 channel-specific endpoints (Slack, HTTP chat, etc.).</li> </ul>"},{"location":"design/server/#mode-1-local-sidecar-developer-researcher","title":"Mode 1 \u2014 Local Sidecar (Developer / Researcher)","text":"<p>Mental model:</p> <p>\u201cAG runs on my laptop as a sidecar process. It executes graphs and exposes local HTTP/WS APIs for UI &amp; tools. Storage is local.\u201d</p>"},{"location":"design/server/#responsibilities","title":"Responsibilities","text":"<ul> <li>Run the AG runtime and scheduler.</li> <li>Own all storage primitives (BlobStore, DocStore, EventLog, VectorIndex) locally.</li> <li> <p>Expose HTTP/WS endpoints on <code>localhost</code> for:</p> </li> <li> <p>Running graphs (<code>/api/graph/{id}/run</code>).</p> </li> <li>Observability (<code>/api/events</code>, <code>/api/runs</code>, <code>/api/artifacts</code>).</li> <li>Optional local UI (web frontend) and channel adapters.</li> </ul>"},{"location":"design/server/#typical-topology","title":"Typical Topology","text":"<pre><code>+-------------------------------+\n|  User Laptop                  |\n|                               |\n|  +------------------------+   |\n|  | AG Sidecar Process     |   |\n|  |  - Runtime             |   |\n|  |  - Storage primitives  |   |\n|  |  - Channel endpoints   |   |\n|  +-----------+------------+   |\n|              | HTTP/WS        |\n|         +----v-------------+  |\n|         | Local UI / CLI   |  |\n|         |  (React app,     |  |\n|         |   console, etc.) |  |\n|         +------------------+  |\n+-------------------------------+\n</code></pre> <ul> <li>All connections are local (<code>127.0.0.1</code>).</li> <li>Slack (if used) might be wired directly to the laptop via a tunnel (ngrok) or WS hack, but that\u2019s a power-user setup.</li> </ul>"},{"location":"design/server/#data-flow-examples","title":"Data Flow Examples","text":"<ol> <li>Run a graph from the terminal</li> </ol> <pre><code>User \u2192 `python my_graph.py`\n  \u2192 AG Runtime executes graph\n    \u2192 Facades write artifacts/memory/state to local FS/DB\n  \u2192 Terminal prints outputs\n</code></pre> <ol> <li>Inspect runs via local UI</li> </ol> <pre><code>Browser (localhost:3000) \u2192 /api/events?run_id=... (HTTP)\n                           /api/events/stream (WS/SSE)\n  \u2192 Sidecar reads from EventLog / DocStore\n  \u2192 UI renders run timeline &amp; artifacts\n</code></pre>"},{"location":"design/server/#when-to-use","title":"When to Use","text":"<ul> <li>Individual R&amp;D, notebooks, experiments.</li> <li>Local \u201cagent companions\u201d or sidecar tools.</li> <li>No need for remote access by default; safe by binding to <code>localhost</code> only.</li> </ul>"},{"location":"design/server/#mode-2-single-tenant-app-server-enterprise-self-hosted","title":"Mode 2 \u2014 Single-Tenant App Server (Enterprise Self-Hosted)","text":"<p>Mental model:</p> <p>\u201cThe sidecar is our main production AG server. It runs in the company\u2019s cloud or on-prem cluster and exposes AG APIs internally.\u201d</p>"},{"location":"design/server/#responsibilities_1","title":"Responsibilities","text":"<ul> <li> <p>Same as Mode 1, but now:</p> </li> <li> <p>Runs on a server/cluster inside the enterprise network.</p> </li> <li>Might have multiple replicas behind a load balancer.</li> <li> <p>Uses shared storage backends (S3, Postgres, Redis, etc.).</p> </li> <li> <p>Expose stable HTTP/WS APIs for:</p> </li> <li> <p>Internal services (e.g., <code>foo-service</code> calling <code>/api/graph/optimize_lens/run</code>).</p> </li> <li>Internal UIs (AG dashboard, custom apps).</li> <li>Channel integrations (Slack, Teams, internal chat).</li> </ul>"},{"location":"design/server/#topology","title":"Topology","text":"<pre><code>                 Enterprise Network\n+-------------------------------------------------+\n|                                                 |\n|  +----------------------+     +--------------+  |\n|  | Load Balancer        |     | Storage      |  |\n|  | (HTTPS)              |     | (S3, DB, KV) |  |\n|  +----------+-----------+     +------+-------+  |\n|             |                         ^          |\n|       +-----v----------------+        |          |\n|       |  AG App Server       |        |          |\n|       |  (one or many pods)  |        |          |\n|       |  - Runtime           |        |          |\n|       |  - Storage adapters  +--------+          |\n|       |  - Channel endpoints |                   |\n|       +----------+-----------+                   |\n|                  | HTTP/WS                       |\n|   +--------------v-------------+                 |\n|   | Internal UIs / Services    |                 |\n|   | (Dashboards, APIs, etc.)   |                 |\n|   +----------------------------+                 |\n+-------------------------------------------------+\n</code></pre>"},{"location":"design/server/#data-flow-slack-example","title":"Data Flow (Slack example)","text":"<pre><code>Slack \u2192 HTTPS \u2192 AG App Server /api/channel/slack/events\n      \u2192 AG normalizes to ChannelEvent\n      \u2192 AG runtime routes to appropriate graph / continuation\n      \u2192 Graph runs, writes events/artifacts to storage\n      \u2192 AG App Server responds to Slack via Slack Web API\n</code></pre>"},{"location":"design/server/#data-flow-internal-http-client","title":"Data Flow (internal HTTP client)","text":"<pre><code>Internal service \u2192 POST /api/graph/{id}/run {input JSON}\nAG App Server   \u2192 Executes graph\n                \u2192 Writes artifacts / events / memory\n                \u2192 Returns outputs (or run_id for async)\n</code></pre>"},{"location":"design/server/#when-to-use_1","title":"When to Use","text":"<ul> <li>Enterprise wants full control and self-hosts AG.</li> <li>AG integrates with internal systems, SSO, internal Slack.</li> <li>Scaling = add more AG App Server replicas using shared storage.</li> </ul>"},{"location":"design/server/#mode-3-saas-control-plane-worker-pool-hosted-by-aiperture","title":"Mode 3 \u2014 SaaS Control Plane + Worker Pool (Hosted by AIperture)","text":"<p>Mental model:</p> <p>\u201cAG is a cloud platform. A control-plane handles APIs, channels, and UI, while a pool of workers runs graphs. Storage is shared. Slack and UIs talk only to the control-plane.\u201d</p>"},{"location":"design/server/#responsibilities_2","title":"Responsibilities","text":"<p>Control Plane</p> <ul> <li> <p>Owns:</p> </li> <li> <p>Public APIs (<code>/api/graph/run</code>, <code>/api/channel/*</code>, <code>/api/events</code>, <code>/api/artifacts</code>).</p> </li> <li>Auth, multi-tenant routing (which workspace/tenant is this?).</li> <li>Slack/Teams/other channel integrations.</li> <li>Web UI and dashboards.</li> <li>Job dispatch to workers (via queue or internal RPC).</li> </ul> <p>Worker Plane</p> <ul> <li> <p>Multiple AG worker processes/containers:</p> </li> <li> <p>Each runs the AG runtime.</p> </li> <li>Pulls jobs from a queue (or receives them via WS/gRPC).</li> <li>Writes artifacts/memory/state to shared storage.</li> <li>Optionally opens a WS back to control-plane for live events.</li> </ul>"},{"location":"design/server/#topology-diagram","title":"Topology Diagram","text":"<pre><code>                   Internet\n                      |\n          +-----------+------------+\n          |  Control Plane (CP)    |\n          |  - Public APIs         |\n          |  - Channel endpoints   |\n          |  - UI                  |\n          |  - Auth &amp; routing      |\n          +-----+-------------+----+\n                |             |\n      Slack /   |             | HTTP/WS\n      Webhooks  |             v\n                |      +-------------+\n                |      |  Job Queue  |\n                |      +------+------+ \n                |             ^\n                |             |\n                |      (jobs: run graph X)\n                |             |\n+-----------------------------+---------------------------+\n|                   Worker Plane                           |\n|                                                         |\n|   +--------------------+     +--------------------+     |\n|   | AG Worker 1        | ... | AG Worker N        |     |\n|   | - Runtime          |     | - Runtime          |     |\n|   | - Services         |     | - Services         |     |\n|   +---------+----------+     +----------+---------+     |\n|             |                           |               |\n|             +------------+--------------+               |\n|                          |                              |\n|                     Shared Storage                      |\n|               (BlobStore, DocStore, EventLog,          |\n|                VectorIndex: S3/DB/etc.)                |\n+---------------------------------------------------------+\n</code></pre>"},{"location":"design/server/#data-flow-slack-cp-worker-slack","title":"Data Flow (Slack \u2192 CP \u2192 Worker \u2192 Slack)","text":"<ol> <li>Incoming event</li> </ol> <pre><code>Slack \u2192 CP /api/channel/slack/events\n  \u2192 CP authenticates &amp; normalizes event\n  \u2192 CP enqueues a job: {workspace, graph_id, run_id?, input}\n</code></pre> <ol> <li>Execution</li> </ol> <pre><code>Worker \u2192 pulls job from queue\n       \u2192 runs graph via AG runtime\n       \u2192 writes events to EventLog, artifacts to BlobStore, etc.\n       \u2192 optionally sends live events back to CP via WS\n</code></pre> <ol> <li>Outgoing message</li> </ol> <pre><code>CP \u2192 reads events (or receives streamed ones)\n   \u2192 detects outgoing messages for Slack\n   \u2192 calls Slack Web API using stored tokens\nSlack channel shows assistant response\n</code></pre>"},{"location":"design/server/#data-flow-browser-ui","title":"Data Flow (Browser UI)","text":"<pre><code>Browser \u2192 CP /api/graph/run (start run)\nCP      \u2192 enqueues job\nWorker  \u2192 executes, writes events/artifacts\nBrowser \u2192 CP /api/events?run_id=... (poll) or /api/events/stream (WS)\n</code></pre>"},{"location":"design/server/#local-worker-hybrid-optional-future-pattern","title":"Local Worker Hybrid (optional future pattern)","text":"<ul> <li>A user can run an AG Worker locally that connects outbound to the control-plane over WS:</li> </ul> <pre><code>Local Worker \u2192 opens WS to CP: \"I am worker for workspace X\"\nCP           \u2192 sends jobs over WS instead of cloud queue\nWorker       \u2192 runs graphs locally, writes to either:\n              - local storage, and forwards key metadata, or\n              - remote storage via HTTP APIs\n</code></pre> <p>This allows \u201cno exposed local IP\u201d while still leveraging local compute.</p>"},{"location":"design/server/#capability-summary-per-mode","title":"Capability Summary Per Mode","text":"Capability Mode 1: Local Sidecar Mode 2: Single-Tenant Server Mode 3: SaaS CP + Workers Where AG runs Laptop / single machine Single service / several replicas Control plane + many workers Storage backends Local FS / SQLite S3, Postgres, Redis (enterprise infra) Managed S3/DB, multi-tenant Channel entrypoint Localhost / optional tunnel Internal URL (Slack, internal apps) Public URL at your domain Who owns Slack app User (local dev) Enterprise (self-hosted app) You (AIperture-managed Slack app) Graph invocation CLI, localhost HTTP/WS Internal HTTP/WS Public API \u2192 queued \u2192 workers Observability Local UI/CLI via /api/events Internal dashboards via /api/events SaaS UI reading from shared EventLog + DocStore Scaling Single process Scale out app servers Scale control plane + worker pool independently"},{"location":"design/server/#how-to-think-about-evolution","title":"How to Think About Evolution","text":""},{"location":"design/server/#from-mode-1-mode-2","title":"From Mode 1 \u2192 Mode 2","text":"<ul> <li> <p>Take the existing sidecar and:</p> </li> <li> <p>Run it on a server instead of a laptop.</p> </li> <li>Swap storage adapters for cloud ones (S3/DB instead of local FS/SQLite).</li> <li> <p>Add auth, TLS, and a proper domain.</p> </li> <li> <p>Graphs, tools, and services do not need to change \u2014 they still talk to <code>ArtifactFacade</code>, <code>MemoryFacade</code>, etc.</p> </li> </ul>"},{"location":"design/server/#from-mode-2-mode-3","title":"From Mode 2 \u2192 Mode 3","text":"<ul> <li> <p>Split responsibilities into:</p> </li> <li> <p>Control Plane: keep existing HTTP/WS APIs, add multi-tenancy, job queue, auth, and UI.</p> </li> <li> <p>Workers: run a headless version of the AG runtime that:</p> <ul> <li>Listens for jobs (via queue/WS/gRPC).</li> <li>Uses the same storage adapters.</li> </ul> </li> <li> <p>Again, graph/tool APIs stay the same; only the deployment topology changes.</p> </li> </ul>"},{"location":"design/server/#takeaways","title":"Takeaways","text":"<ul> <li> <p>The sidecar you\u2019re building now is already the core of:</p> </li> <li> <p>A local dev tool (Mode 1),</p> </li> <li>A self-hosted app server (Mode 2), and</li> <li> <p>The worker &amp; API pieces of a SaaS platform (Mode 3).</p> </li> <li> <p>By keeping:</p> </li> <li> <p>storage unified via <code>BlobStore / DocStore / EventLog / VectorIndex</code>, and</p> </li> <li>interaction unified via channel + continuation APIs,</li> </ul> <p>you can change where and how many processes run AG without changing how users write graphs and tools.</p>"},{"location":"examples/1-chat-with-memory/","title":"Example: First Steps with Memory, Channel, LLM &amp; Artifacts","text":"<p>Who is this for? Folks who have never used AetherGraph (AG). We\u2019ll build a tiny chat agent that remembers what was said, responds using an LLM, and saves a session summary \u2014 all step\u2011by\u2011step.</p>"},{"location":"examples/1-chat-with-memory/#what-youll-build","title":"What you\u2019ll build","text":"<p>A stateful chat agent that:</p> <ol> <li>Stores each message as a <code>chat_turn</code> event in AG Memory.</li> <li>Reads prior events on startup to preload context.</li> <li>Chats with you via <code>context.channel()</code> (console/Slack/Web supported).</li> <li>Uses <code>context.llm()</code> to reply and later summarize the session.</li> <li>Saves the transcript + summary as an artifact (a file you can inspect).</li> </ol> <p>This mirrors real apps: assistants, experiment logs, ops runbooks, or optimization loops that need persistent context.</p>"},{"location":"examples/1-chat-with-memory/#prerequisites-2-minutes","title":"Prerequisites (2 minutes)","text":"<ul> <li>Python 3.10+</li> <li>Install AG (adjust to your package source):</li> </ul> <p><pre><code>pip install aethergraph\n</code></pre> * LLM key (e.g., OpenAI) in your environment or <code>.env</code>:</p> <p><pre><code>export OPENAI_API_KEY=sk-...\n</code></pre> * (Optional) Slack/Web UI: Not required for this tutorial; we use the console channel. You can switch to Slack/Web later without changing the agent code.</p>"},{"location":"examples/1-chat-with-memory/#glossary-ag-in-60-seconds","title":"Glossary (AG in 60 seconds)","text":"<ul> <li><code>@graph_fn</code>: A Python function that AG can schedule/run (think: a task node with helpful runtime wiring).</li> <li> <p><code>NodeContext</code>: Passed into your <code>@graph_fn</code>; gives you services:</p> </li> <li> <p><code>context.memory()</code> \u2013 record &amp; query events (structured log).</p> </li> <li><code>context.channel()</code> \u2013 input/output with the user (console/Slack/Web).</li> <li><code>context.llm()</code> \u2013 talk to an LLM provider using a unified API.</li> <li><code>context.artifacts()</code> \u2013 save/load files, JSON, blobs.</li> <li>Events (Memory): Append\u2011only records with fields like <code>kind</code>, <code>text</code>, <code>metrics</code>, <code>tags</code>, <code>stage</code>, <code>severity</code>. You choose the schema; AG stores and fetches them for you.</li> </ul> <p>Key idea: keep your logic in plain Python; treat services (memory, channel, llm, artifacts) as pluggable I/O.</p>"},{"location":"examples/1-chat-with-memory/#step-0-minimal-run-harness","title":"Step 0 \u2014 Minimal run harness","text":"<p>We\u2019ll run two functions in one process: one to seed memory, one to chat.</p> <pre><code># run_harness.py\nif __name__ == \"__main__\":\n    import asyncio\n    from aethergraph.runner import run_async\n    from aethergraph import start_server\n\n    # Start the sidecar: enables interactive I/O and resumable waits\n    url = start_server(port=8000, log_level=\"warning\")\n    print(\"Sidecar:\", url)\n\n    async def main():\n        # Same run_id so they share the same memory namespace\n        await run_async(seed_chat_memory_demo, inputs={}, run_id=\"demo_chat_with_memory\")\n        result = await run_async(chat_agent_with_memory, inputs={}, run_id=\"demo_chat_with_memory\")\n        print(\"Result:\", result)\n\n    asyncio.run(main())\n</code></pre> <p>Why a sidecar? For console/Slack/Web prompts (<code>ask_*</code>) AG uses event\u2011driven waits that the sidecar hosts. Pure compute graphs can run without it.</p>"},{"location":"examples/1-chat-with-memory/#step-1-seed-memory-why-how","title":"Step 1 \u2014 Seed memory (why &amp; how)","text":"<p>Why: Many real agents need past context on first run (previous chats, last experiment settings, etc.). We\u2019ll preload two <code>chat_turn</code> events so the agent \u201cremembers\u201d something before you type.</p> <pre><code># seed.py\nfrom aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"seed_chat_memory_demo\")\nasync def seed_chat_memory_demo(*, context: NodeContext):\n    mem = context.memory()\n    logger = context.logger()\n\n    # Each record() call writes an Event. `data` is JSON\u2011encoded into Event.text.\n    await mem.record(\n        kind=\"chat_turn\",\n        data={\"role\": \"user\", \"text\": \"We talked about integrating AetherGraph into my project.\"},\n        tags=[\"chat\", \"user\", \"seed\"], severity=2, stage=\"observe\",\n    )\n    await mem.record(\n        kind=\"chat_turn\",\n        data={\"role\": \"assistant\", \"text\": \"Start with a simple graph_fn and add services later.\"},\n        tags=[\"chat\", \"assistant\", \"seed\"], severity=2, stage=\"act\",\n    )\n\n    logger.info(\"Seeded two chat turns.\")\n    return {\"seeded\": True}\n</code></pre> <p>Design tip: Use <code>kind</code> consistently (<code>\"chat_turn\"</code>) so you can query exactly what you need later. <code>tags</code>, <code>stage</code>, and <code>severity</code> help with reporting and filtering.</p>"},{"location":"examples/1-chat-with-memory/#step-2-load-prior-events-make-memory-useful","title":"Step 2 \u2014 Load prior events (make memory useful)","text":"<p>Goal: On startup, fetch recent <code>chat_turn</code> events, decode them, and prime the chat history.</p> <pre><code># load_history.py (excerpt inside the agent)\nprevious_turns = await mem.recent_data(kinds=[\"chat_turn\"], limit=50)\nconversation = []\nfor d in previous_turns:\n    if isinstance(d, dict) and d.get(\"role\") in (\"user\", \"assistant\") and d.get(\"text\"):\n        conversation.append({\"role\": d[\"role\"], \"text\": d[\"text\"]})\n\nif conversation:\n    await chan.send_text(f\"\ud83e\udde0 I loaded {len(conversation)} prior turns. I\u2019ll use them as context.\")\nelse:\n    await chan.send_text(\"\ud83d\udc4b New session. I\u2019ll remember as we go.\")\n</code></pre> <p>Why not read raw events? <code>recent_data()</code> returns the decoded JSON payloads you originally wrote via <code>data=...</code>. It\u2019s the fastest way to get back to your domain objects.</p>"},{"location":"examples/1-chat-with-memory/#step-3-talk-to-the-user-channel-101","title":"Step 3 \u2014 Talk to the user (Channel 101)","text":"<p>Goal: Use <code>channel.ask_text()</code> to get input and <code>channel.send_text()</code> to reply. This works the same in console, Slack, or a web adapter.</p> <pre><code># channel_loop.py (excerpt inside the agent)\nwhile True:\n    user = await chan.ask_text(\"You:\")\n    if not user:\n        continue\n    if user.strip().lower() in (\"quit\", \"exit\"):\n        await chan.send_text(\"\ud83d\udc4b Ending. Let me summarize...\")\n        break\n\n    # Store the user turn in memory *and* in our local transcript\n    conversation.append({\"role\": \"user\", \"text\": user})\n    await mem.record(kind=\"chat_turn\", data={\"role\": \"user\", \"text\": user},\n                     tags=[\"chat\", \"user\"], severity=2, stage=\"observe\")\n</code></pre> <p>Why Channel? It abstracts the transport. Your agent code stays the same whether you test locally or ship to Slack/Web.</p>"},{"location":"examples/1-chat-with-memory/#step-4-call-the-llm-compact-history","title":"Step 4 \u2014 Call the LLM (compact history)","text":"<p>Goal: Build a small window from the transcript (e.g., last 10 turns) and call <code>llm.chat()</code>.</p> <pre><code># llm_reply.py (excerpt inside the agent)\nhistory_tail = conversation[-10:]\nmessages = ([{\"role\": \"system\", \"content\": \"You are a helpful, concise assistant.\"}] +\n            [{\"role\": t[\"role\"], \"content\": t[\"text\"]} for t in history_tail])\n\nreply, _usage = await llm.chat(messages=messages)\nconversation.append({\"role\": \"assistant\", \"text\": reply})\nawait mem.record(kind=\"chat_turn\", data={\"role\": \"assistant\", \"text\": reply},\n                 tags=[\"chat\", \"assistant\"], severity=2, stage=\"act\")\nawait chan.send_text(reply)\n</code></pre> <p>Why only last 10 turns? Keep prompts cheap. Memory retains all history; your prompt includes a smart slice.</p>"},{"location":"examples/1-chat-with-memory/#step-5-summarize-save-artifacts","title":"Step 5 \u2014 Summarize &amp; Save (Artifacts)","text":"<p>Goal: Generate a session summary, then persist both transcript and summary for later inspection.</p> <pre><code># summarize_and_save.py (excerpt inside the agent)\nhist_text = \"\n\".join(f\"{t['role']}: {t['text']}\" for t in conversation[-20:])\nsummary_text, _ = await llm.chat(messages=[\n    {\"role\": \"system\", \"content\": \"You write clear, concise summaries.\"},\n    {\"role\": \"user\", \"content\": \"Summarize the conversation, focusing on main topics and TODOs.\n\n\" + hist_text},\n])\nawait chan.send_text(\"\ud83d\udccc Session summary:\n\" + summary_text)\n\npayload = {\"conversation\": conversation, \"summary\": summary_text}\ntry:\n    saved = await artifacts.save_json(payload, suggested_uri=\"./chat_session_with_memory.json\")\nexcept Exception:\n    saved = None\n</code></pre> <p>Artifacts act like a project filesystem managed by AG. Save JSON, images, binaries \u2014 and load them from other runs.</p>"},{"location":"examples/1-chat-with-memory/#step-6-put-it-together-the-agent","title":"Step 6 \u2014 Put it together: the agent","text":"<p>Below is the full <code>@graph_fn</code> combining Steps 2\u20135. (Utility imports omitted for brevity.)</p> <pre><code>from typing import Any, Dict, List\nfrom aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"chat_agent_with_memory\")\nasync def chat_agent_with_memory(*, context: NodeContext):\n    logger = context.logger()\n    chan = context.channel()\n    mem = context.memory()\n    artifacts = context.artifacts()\n    llm = context.llm()\n\n    conversation: List[Dict[str, Any]] = []\n\n    # Load prior history\n    try:\n        previous_turns = await mem.recent_data(kinds=[\"chat_turn\"], limit=50)\n    except Exception:\n        previous_turns = []\n    for d in previous_turns:\n        if isinstance(d, dict) and d.get(\"role\") in (\"user\", \"assistant\") and d.get(\"text\"):\n            conversation.append({\"role\": d[\"role\"], \"text\": d[\"text\"]})\n\n    await chan.send_text(\n        f\"\ud83e\udde0 I loaded {len(conversation)} prior turns. Type 'quit' to end.\"\n        if conversation else \"\ud83d\udc4b New session. Type 'quit' to end.\"\n    )\n\n    # Chat loop\n    while True:\n        user = await chan.ask_text(\"You:\")\n        if not user:\n            continue\n        if user.strip().lower() in (\"quit\", \"exit\"):\n            await chan.send_text(\"\ud83d\udc4b Ending. Let me summarize...\")\n            break\n\n        conversation.append({\"role\": \"user\", \"text\": user})\n        await mem.record(kind=\"chat_turn\", data={\"role\": \"user\", \"text\": user},\n                         tags=[\"chat\", \"user\"], severity=2, stage=\"observe\")\n\n        history_tail = conversation[-10:]\n        messages = ([{\"role\": \"system\", \"content\": \"You are a helpful, concise assistant.\"}] +\n                    [{\"role\": t[\"role\"], \"content\": t[\"text\"]} for t in history_tail])\n        reply, _ = await llm.chat(messages=messages)\n\n        conversation.append({\"role\": \"assistant\", \"text\": reply})\n        await mem.record(kind=\"chat_turn\", data={\"role\": \"assistant\", \"text\": reply},\n                         tags=[\"chat\", \"assistant\"], severity=2, stage=\"act\")\n        await chan.send_text(reply)\n\n    # Summarize &amp; save\n    hist_text = \"\n\".join(f\"{t['role']}: {t['text']}\" for t in conversation[-20:])\n    summary_text, _ = await llm.chat(messages=[\n        {\"role\": \"system\", \"content\": \"You write clear, concise summaries.\"},\n        {\"role\": \"user\", \"content\": \"Summarize the conversation with decisions/TODOs.\n\n\" + hist_text},\n    ])\n    await chan.send_text(\"\ud83d\udccc Session summary:\n\" + summary_text)\n\n    try:\n        await artifacts.save_json({\"conversation\": conversation, \"summary\": summary_text},\n                                  suggested_uri=\"./chat_session_with_memory.json\")\n    except Exception:\n        pass\n\n    return {\"turns\": len(conversation), \"summary\": summary_text}\n</code></pre>"},{"location":"examples/1-chat-with-memory/#step-7-run-it","title":"Step 7 \u2014 Run it","text":"<p>From your shell:</p> <pre><code>python run_harness.py\n</code></pre> <p>You\u2019ll see the sidecar URL and then a prompt:</p> <pre><code>You: hello\n... assistant replies ...\nYou: quit\n</code></pre> <p>A <code>chat_session_with_memory.json</code> artifact will be saved. Rerun \u2014 the agent will preload what was said last time.</p>"},{"location":"examples/1-chat-with-memory/#variations-next-steps","title":"Variations &amp; next steps","text":"<ul> <li>Filter by time/kind/tags: <code>recent_data(kinds=[...], limit=..., since=...)</code>.</li> <li>Track metrics: add <code>metrics={...}</code> to <code>record()</code> and chart them later.</li> <li>Multiple channels: same agent works with Slack/Web by switching adapters.</li> <li>Long\u2011term summaries: store occasional <code>kind=\"session_summary\"</code> events.</li> <li>Privacy/retention: implement deletion or redaction policies per <code>tags</code> or <code>stage</code>.</li> </ul>"},{"location":"examples/1-chat-with-memory/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>No replies? Check your LLM key and provider quota.</li> <li>Ask/answer stuck? Ensure the sidecar started (see console log for URL).</li> <li>Artifact not saved? Verify write permissions in the working directory.</li> </ul>"},{"location":"examples/1-chat-with-memory/#summary-full-example-link","title":"Summary &amp; Full Example Link","text":"<p>You built a stateful chat agent without any AG prior knowledge: you logged events, recalled them, chatted via Channel, used LLM to reply/summarize, and persisted results with Artifacts.</p> <p>Full example in repo: TBD (link placeholder)</p>"},{"location":"examples/channel/","title":"Channels \u2013 Practical Usage Cheatsheet","text":"<p>This page is a lightweight guide, not a full API reference. It shows how to use channels in everyday graphs and what the most common operations look like.</p>"},{"location":"examples/channel/#1-getting-a-channel","title":"1. Getting a channel","text":"<p>In any graph, you start from the <code>NodeContext</code>:</p> <pre><code>chan = context.channel()                # default channel (often console or a default chat)\nchan_slack = context.channel(\"slack:team/T:chan/C\")\nchan_file  = context.channel(\"file:runs/demo.log\")\n</code></pre> <p>Use cases:</p> <ul> <li>No argument \u2192 send to whatever default channel you configured.</li> <li>Explicit key \u2192 target a specific channel (Slack, Telegram, file, webhook, etc.).</li> </ul> <p>You then call convenient methods on <code>chan</code>.</p>"},{"location":"examples/channel/#2-sending-messages-send_text","title":"2. Sending messages (<code>send_text</code>)","text":"<pre><code>await chan.send_text(\"Run started \ud83d\ude80\")\n</code></pre> <p>When to use:</p> <ul> <li> <p>Any time you want to tell the user something:</p> </li> <li> <p>status updates (\"loading data\u2026\"),</p> </li> <li>final results (\"accuracy = 0.93\"),</li> <li>errors, tips, or links.</li> </ul> <p>Where it goes depends on the channel:</p> <ul> <li>console \u2192 prints to the terminal.</li> <li>Slack / Telegram \u2192 sends a chat message.</li> <li>file \u2192 appends a line to a log file.</li> <li>webhook \u2192 POSTs JSON to the external URL.</li> </ul> <p>You usually don\u2019t need to worry about the return value; it\u2019s handled internally.</p>"},{"location":"examples/channel/#3-asking-for-input-ask_text","title":"3. Asking for input (<code>ask_text</code>)","text":"<pre><code>name = await context.channel().ask_text(\"What is your name?\")\nawait context.channel().send_text(f\"Nice to meet you, {name}!\")\n</code></pre> <p>When to use:</p> <ul> <li> <p>You need free-form input from a human:</p> </li> <li> <p>names, descriptions, small pieces of text,</p> </li> <li>short commands (\"yes/no\", \"option A\", etc.).</li> </ul> <p>Supported channels:</p> <ul> <li>console \u2192 reads from stdin inline.</li> <li>Slack / Telegram \u2192 sends a prompt and waits for a reply.</li> </ul> <p>Not supported / not meaningful on:</p> <ul> <li>file and webhook (those are inform-only channels).</li> </ul> <p>Behind the scenes, Slack/Telegram use continuations, so the run can be resumed when a reply arrives.</p>"},{"location":"examples/channel/#4-approvals-choices-ask_approval","title":"4. Approvals &amp; choices (<code>ask_approval</code>)","text":"<pre><code>res = await context.channel().ask_approval(\n    \"Deploy the model to production?\",\n    options=[\"Approve\", \"Reject\"],\n)\n\nif res[\"approved\"]:\n    await context.channel().send_text(\"Deployment approved \u2705\")\nelse:\n    await context.channel().send_text(\"Deployment cancelled \u274c\")\n</code></pre> <p>When to use:</p> <ul> <li> <p>You want a simple decision from the user:</p> </li> <li> <p>approve/reject,</p> </li> <li>pick from a short list of options.</li> </ul> <p>Channel behavior:</p> <ul> <li>console \u2192 shows numbered options and waits for a number/label.</li> <li>Slack / Telegram \u2192 renders real buttons; user clicks, continuation resumes.</li> </ul> <p>Again, this is not meant for file/webhook channels.</p>"},{"location":"examples/channel/#5-files-uploads-high-level","title":"5. Files &amp; uploads (high level)","text":"<p>For interactive channels (Slack, Telegram, console) and some tools, you\u2019ll often work with files directly, not just links.</p> <p>Typical high-level patterns:</p>"},{"location":"examples/channel/#51-sending-a-file-send_file","title":"5.1. Sending a file (<code>send_file</code>)","text":"<pre><code># e.g. you just generated a local report\nreport_path = \"./outputs/report.pdf\"\n\nchan = context.channel()  # default chat (Slack/Telegram/console)\nawait chan.send_file(report_path, caption=\"Here is your report \ud83d\udcce\")\n</code></pre> <p>When to use:</p> <ul> <li>You want the user to receive the actual file in their chat or UI.</li> <li>Slack/Telegram will show the file as an attachment; console/file/webhook channels may log or reference it instead, depending on implementation.</li> </ul>"},{"location":"examples/channel/#52-asking-the-user-to-upload-a-file-ask_file","title":"5.2. Asking the user to upload a file (<code>ask_file</code>)","text":"<pre><code>files = await context.channel().ask_file(\"Please upload a CSV file with your data.\")\n\n# `files` is typically a list of file references with\n# fields like name, uri, mimetype, etc.\nfor f in files:\n    await context.channel().send_text(f\"Got file: {f['name']}\")\n</code></pre> <p>When to use:</p> <ul> <li>You need the user to provide input as a file (datasets, configs, documents).</li> <li>Works best on Slack/Telegram or a web UI that supports uploads.</li> </ul>"},{"location":"examples/channel/#53-sending-buttons-for-links","title":"5.3. Sending buttons for links","text":"<p>Buttons are a convenient way to surface links (e.g. to artifacts, dashboards) instead of pasting raw URLs.</p> <p>Conceptually, you can build a message with one or more buttons that open URLs:</p> <pre><code>from aethergraph.contracts.services.channel import Button\n\nreport_url = \"https://example.com/artifacts/runs/123/report.pdf\"\n\nbuttons = {\n    \"open_report\": Button(label=\"Open report\", url=report_url),\n}\n\nchan = context.channel()\nawait chan.send_buttons(\"Your report is ready:\", buttons=buttons)\n</code></pre> <p>On rich channels (Slack/Telegram/web UI) this can render as clickable buttons; on simpler channels, it may fall back to plain text.</p> <p>Use file and button helpers when you want the user to act on artifacts directly (download, open, inspect) rather than just reading text.</p>"},{"location":"examples/channel/#6-streaming-progress-high-level","title":"6. Streaming &amp; progress (high level)","text":"<p>Some adapters (Slack, Telegram, web UI) support streaming and progress updates, so the user sees things evolve in place instead of a single final message.</p> <p>A common pattern is to use a progress helper that periodically updates a status message while your work runs:</p> <pre><code>@graph_fn(name=\"train_with_progress\")\nasync def train_with_progress(*, context: NodeContext):\n    await context.channel().send_text(\"Training started \u23f3\")\n\n    total_steps = 5\n    for step in range(1, total_steps + 1):\n        # Do some work here...\n        await context.channel().send_text(f\"Progress: {step}/{total_steps}\")\n\n    await context.channel().send_text(\"Training finished \u2705\")\n</code></pre> <p>On rich channels (Slack/Telegram/web UI), the framework can render more advanced streaming or progress UIs using specialized helpers; the core idea is the same: send updates frequently so the user sees the task moving forward.</p>"},{"location":"examples/channel/#7-putting-it-together-a-small-example","title":"7. Putting it together \u2013 a small example","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"simple_run\")\nasync def simple_run(*, context: NodeContext):\n    # 1) Notify in the default channel\n    await context.channel().send_text(\"Run started \ud83d\ude80\")\n\n    # 2) Ask the user a question (console/Slack/Telegram)\n    name = await context.channel().ask_text(\"What should we name this experiment?\")\n\n    # 3) Log the name to a file channel\n    await context.channel(\"file:runs/experiment_names.log\").send_text(name)\n\n    # 4) Optionally notify an external system via webhook\n    # await context.channel(\"webhook:https://hooks.zapier.com/hooks/catch/.../\").send_text(\n    #     f\"New experiment: {name}\"\n    # )\n\n    await context.channel().send_text(f\"All set, {name} \u2705\")\n    return {\"experiment_name\": name}\n</code></pre> <p>Use this cheatsheet as a mental model for channels:</p> <ul> <li><code>channel()</code> \u2192 pick where messages go.</li> <li><code>send_text</code> \u2192 tell the user or an external system something.</li> <li><code>ask_*</code> \u2192 only on interactive channels (console/Slack/Telegram), when you need input.</li> <li>Let higher-level tooling take care of files and streaming; the channel adapters handle the transport details.</li> </ul>"},{"location":"examples/example-list/","title":"Examples of Aethergraph Usage","text":"<p>This page is your guided tour to AetherGraph (AG) through runnable, copy\u2011pasteable examples. Start with the Hero Demos to experience a complete flow in minutes (console + LLM only). Then explore two complementary pattern pools:</p> <ul> <li>Method Showcase \u2014 small, focused snippets for individual <code>context.*</code> capabilities and services.</li> <li>Concrete Example Patterns \u2014 larger recipes that combine multiple features for real\u2011world tasks.</li> </ul> <p>This catalog evolves over time. We periodically add new demos, refine existing ones, and may adjust paths as the repo grows. The tables below always reflect the current recommended entry points.</p> <p>All the examples are in a separate example repo. Please clone the repo and set up <code>.env</code> to run the examples.</p>"},{"location":"examples/example-list/#hero-demos","title":"Hero Demos","text":"<p>Short, polished demos that tell a clear story. Each fits in a single file and returns observable outputs (messages, artifacts, or both).</p> # Demo What you\u2019ll see (story) Path 1 Chat Agent with Memory Seeds or loads chat memory \u2192 you chat \u2192 it summarizes the whole session and saves transcript + summary as artifacts. <code>demo_examples/1_chat_with_memory.py</code> 2 Channel Wizard A one\u2011function wizard that collects config using <code>ask_*</code>, shows a recap, lets you Confirm/Restart/Cancel, then saves <code>run_config.json</code>. <code>demo_examples/2_channel_wizard_interactive_workflow.py</code> 3 Optimization Loop A tiny gradient\u2011descent run that logs step metrics to Memory, writes periodic checkpoints as artifacts, and returns final params. <code>demo_examples/3_optimization_loop_with_artifacts.py</code> 4 Extend Prompt Services Agents use <code>context.prompt_store()</code> to fetch prompts and <code>context.llm_observer()</code> to log calls\u2014clean agent code, centralized control. <code>servdemo_examplesices/4_external_service_prompt_store.py</code> 5 Simple Copilot (Tool Router) A console copilot routes each query to a calculator, a summarizer, or a direct answer using a tiny LLM classifier; replies inline. <code>demo_examples/5_simple_copilot_tool_using_router.py</code> 6 Resume a Static Graph (graphify) A static graph with a slow, checkpointed node crashes mid\u2011run; re\u2011running with the same <code>run_id</code> resumes from the checkpoint and completes. <code>demo_examples/6_crash_resume_static_graph.py</code>"},{"location":"examples/example-list/#method-showcase-patterns-by-feature","title":"Method Showcase \u2014 Patterns by Feature","text":"<p>Concise examples organized by capability. Use these as copy\u2011paste starting points.</p>"},{"location":"examples/example-list/#a-interaction-channel","title":"A) Interaction (Channel)","text":"<ul> <li>A.1 <code>send_text</code> \u2014 Minimal \"hello\" send.   Path: <code>method_showcase/1_channels/1_channel_send_text.py</code></li> <li>A.2 <code>ask_text</code> \u2014 Prompt \u2192 await user reply \u2192 echo.   Path: <code>method_showcase/1_channels/2_channel_ask_text.py</code></li> <li>A.3 <code>ask_approval</code> \u2014 Present options, branch on choice.   Path: <code>method_showcase/1_channels/3_channel_ask_approval.py</code></li> <li>A.4 Channel setup \u2014 Configure Slack / Telegram / Webhook adapters and key conventions.   Path: <code>method_showcase/1_channels/4_channel_setup.py</code></li> <li>A.5 Method walk\u2011through \u2014 Linear tour of all channel methods using a small \u201cportfolio\u201d demo.   Path: <code>method_showcase/1_channels/5_channel_method_walkthrough.py</code></li> <li>A.6 File channel \u2014 Ask for files, read them, return attachments/links.   Path: <code>method_showcase/1_channels/6_file_channel_example.py</code></li> </ul>"},{"location":"examples/example-list/#b-memory-artifacts","title":"B) Memory &amp; Artifacts","text":"<ul> <li>B.1 Memory \u2014 record &amp; query \u2014 Append events and fetch recent history.   Path: <code>method_showcase/2_artifacts_memory/1_memory_record.py</code></li> <li>B.2 Memory \u2014 typed results \u2014 Store structured tool results for fast retrieval.   Path: <code>method_showcase/2_artifacts_memory/2_memory_write_result.py</code></li> <li>B.3 Artifacts \u2014 save text/JSON \u2014 Persist text/JSON and auto\u2011index.   Path: <code>method_showcase/2_artifacts_memory/3_artifacts_save_txt_json.py</code></li> <li>B.4 Artifacts \u2014 save &amp; search files \u2014 Save arbitrary files and rank/search across agents.   Path: <code>method_showcase/2_artifacts_memory/4_artifacts_save_search_files.py</code></li> <li>B.5 Memory \u2192 RAG \u2014 Promote memory events into a vector index and answer with citations.   Path: <code>method_showcase/2_artifacts_memory/5_memory_rag.py</code></li> </ul>"},{"location":"examples/example-list/#c-logger-kv","title":"C) Logger &amp; KV","text":"<ul> <li>C.1 Logger \u2014 Structured logs, levels, and tracing.   Path: <code>method_showcase/3_logger_kv/1_logger_usage.py</code></li> <li>C.2 KV \u2014 Store and fetch run\u2011level globals.   Path: <code>method_showcase/3_logger_kv/2_kv_usage.py</code></li> </ul>"},{"location":"examples/example-list/#d-llm","title":"D) LLM","text":"<ul> <li>D.1 Chat \u2014 One\u2011shot and multi\u2011turn <code>context.llm().chat(...)</code>.   Path: <code>method_showcase/4_llm/1_llm_chat.py</code></li> <li>D.2 Multiple profiles \u2014 Configure and use multiple LLM clients.   Path: <code>method_showcase/4_llm/2_setup_multiple_llm_profiles.py</code></li> <li>D.3 Inline profile \u2014 Set keys/models at runtime (no preregistration).   Path: <code>method_showcase/4_llm/3_inline_llm_setup.py</code></li> <li>D.4 Raw API \u2014 Use <code>.raw()</code> to pass advanced payloads directly.   Path: <code>method_showcase/4_llm/4_passing_raw_api.py</code></li> </ul>"},{"location":"examples/example-list/#e-rag","title":"E) RAG","text":"<ul> <li>E.1 Ingest files \u2014 Upsert docs into a vector index.   Path: <code>method_showcase/5_rag/ingest_files.py</code></li> <li>E.2 Inspect corpora \u2014 List/inspect existing corpora.   Path: <code>method_showcase/5_rag/list_inspect_corpora.py</code></li> <li>E.3 Search &amp; answer \u2014 Retrieve \u2192 synthesize with citations.   Path: <code>method_showcase/5_rag/search_retrieve_answer.py</code></li> </ul>"},{"location":"examples/example-list/#f-extending-services","title":"F) Extending Services","text":"<ul> <li>F.1 Materials DB \u2014 Register a materials property service for quick lookups.   Path: <code>method_showcase/6_extending_services/1_material_db.py</code></li> <li>F.2 HuggingFace model \u2014 Expose an external model as a service.   Path: <code>method_showcase/6_extending_services/2_huggingface_model.py</code></li> <li>F.3 Rate limiting \u2014 Token\u2011bucket wrapper with retries/backoff.   Path: <code>method_showcase/6_extending_services/3_rate_limit.py</code></li> <li>F.4 Access NodeContext in a service \u2014 Patterns for using <code>context</code> safely inside services.   Path: <code>method_showcase/6_extending_services/4_access_ctx_in_service.py</code></li> <li>F.5 Critical sections / mutex \u2014 Design a thread\u2011safe service API.   Path: <code>method_showcase/6_extending_services/5_critical_mutex_usage.py</code></li> </ul>"},{"location":"examples/example-list/#g-concurrency","title":"G) Concurrency","text":"<ul> <li>G.1 <code>graphify</code> map\u2011reduce \u2014 Fan\u2011out + fan\u2011in with static graphs.   Path: <code>method_showcase/7_concurrency/graphify_map_reduce.py</code></li> <li>G.2 <code>graph_fn</code> concurrency \u2014 Launch concurrent tasks with a concurrency cap.   Path: <code>method_showcase/7_concurrency/graph_fn_concurrency.py</code></li> </ul>"},{"location":"examples/example-list/#concrete-example-patterns-advanced-recipes","title":"Concrete Example Patterns \u2014 Advanced Recipes","text":"<p>Bigger compositions that mirror real\u2011world tasks.</p>"},{"location":"examples/example-list/#a-state-resumption","title":"A) State &amp; Resumption","text":"<ul> <li>A.1 Crash &amp; Resume (static graph) \u2014 Design a static graph so a long node can checkpoint and resume indefinitely using the same <code>run_id</code>.   Path: <code>pattern_examples/1_state_resumption/1_resume_external_waits.py</code> Run: <code>python 1_resume_external_waits.py run_id</code> </li> <li>A.2 Long Job Monitor \u2014 Submit to <code>job_manager</code>, poll with backoff, surface failures via channel, let the user Retry/Abort.   Path: <code>pattern_examples/1_state_resumption/2_long_job_monitor.py</code></li> </ul>"},{"location":"examples/example-list/#b-agent-patterns","title":"B) Agent Patterns","text":"<ul> <li>B.1 Chain\u2011of\u2011Thought Agent \u2014 Two\u2011stage flow: CoT reasoning trace \u2192 final concise answer (optionally store traces).   Path: <code>pattern_examples/2_agent_patterns/1_chain_of_thought.py</code></li> <li>B.2 ReAct Agent \u2014 Thought \u2192 Action (tool) \u2192 Observation loop until \u201cFinish\u201d, with a compact history state.   Path: <code>pattern_examples/agents/2_simple_react.py</code></li> <li>B.3 RL Policy as <code>graph_fn</code> \u2014 Treat a graph as a policy: observation in, action out; log trajectories via Memory/Artifacts.   Path: <code>pattern_examples/agents/3_reinforcement_learnining_policy.py</code></li> </ul>"},{"location":"examples/example-list/#c-applied-endtoend","title":"C) Applied End\u2011to\u2011End","text":"<ul> <li>C.1 CSV Analyzer (interactive) \u2014 Ask for a CSV, summarize shape (rows/cols), headers, and simple stats; return findings and artifacts.   Path: <code>pattern_examples/3_e2e_patterns/1_csv_analyzer.py</code></li> <li>C.2 Paper \u2192 Implementation Sketch (interactive) \u2014 Ask for a text/PDF, sketch a Python implementation, run sandboxed, return logs/files as artifacts.   Path: <code>pattern_examples/3_e2e_patterns/2_paper_implementation_sketch.py</code></li> <li>C.3 Deep Research Agent \u2014 Use <code>graphify</code> concurrency to parallelize retrieval/summarization and synthesize findings.   Path: <code>pattern_examples/3_e2e_patterns/3_deep_research_agent.py</code></li> </ul>"},{"location":"examples/llm/","title":"<code>context.llm()</code> Mini Tutorial","text":"<p>This is a quick guide to the <code>NodeContext.llm()</code> helper in Aethergraph: how it\u2019s wired, how to use profiles, how to configure providers in <code>.env</code>, and how embeddings + RAG fit in.</p>"},{"location":"examples/llm/#1-what-contextllm-is-and-supported-providers","title":"1. What <code>context.llm()</code> is (and supported providers)","text":"<p><code>context.llm()</code> is a convenience accessor that gives you a ready-to-use LLM client for the current run.</p> <p>Under the hood it returns a <code>GenericLLMClient</code>, which implements:</p> <ul> <li><code>await client.chat(messages, **kw)</code>  \u2013 text (and multimodal) chat</li> <li><code>await client.embed(texts, **kw)</code>    \u2013 embeddings</li> </ul> <p>The client is created from config (<code>LLMSettings</code>) and currently supports these providers:</p> <ul> <li><code>openai</code>    \u2013 OpenAI-compatible API (GPT\u20114o, GPT\u20115, etc.)</li> <li><code>anthropic</code> \u2013 Claude 3.x models via <code>/v1/messages</code></li> <li><code>google</code>    \u2013 Gemini models via <code>generateContent</code> / <code>embedContent</code></li> <li><code>lmstudio</code>  \u2013 LM Studio\u2019s OpenAI-compatible local server</li> <li><code>ollama</code>    \u2013 Ollama\u2019s OpenAI-compatible local server</li> </ul> <p>(Plus <code>azure</code> and <code>openrouter</code> if you configure them.)</p> <p>Important: This layer is meant as a lightweight helper. It is not exhaustively tested across every model and provider variant. If you hit an edge case or a model with a quirky API, you can:</p> <ul> <li>Call the HTTP APIs yourself with plain Python, or</li> <li>Register your own extended service and bypass <code>context.llm()</code> for that use case.</li> </ul>"},{"location":"examples/llm/#2-what-is-a-profile","title":"2. What is a \u201cprofile\u201d?","text":"<p>A profile is a named LLM configuration: provider + model + optional base URL, timeout, and secrets.</p> <p>Structurally (from config):</p> <pre><code>class LLMProfile(BaseModel):\n    provider: Provider = \"openai\"      # e.g. \"openai\", \"anthropic\", \"google\", \"lmstudio\", \"ollama\"\n    model: str = \"gpt-4o-mini\"         # chat / reasoning model\n    embed_model: str | None = None      # optional embedding model\n    base_url: str | None = None\n    timeout: float = 60.0\n    azure_deployment: str | None = None\n    api_key: SecretStr | None = None\n    api_key_ref: str | None = None\n\nclass LLMSettings(BaseModel):\n    enabled: bool = True\n    default: LLMProfile = LLMProfile()\n    profiles: Dict[str, LLMProfile] = Field(default_factory=dict)\n</code></pre> <p>At runtime, these become <code>GenericLLMClient</code> instances keyed by profile name:</p> <ul> <li><code>\"default\"</code> \u2013 always present</li> <li><code>\"gemini\"</code>, <code>\"anthropic\"</code>, <code>\"local\"</code>, etc. \u2013 optional extra profiles</li> </ul>"},{"location":"examples/llm/#3-using-contextllm-inside-a-node","title":"3. Using <code>context.llm()</code> inside a node","text":""},{"location":"examples/llm/#31-basic-usage-default-profile","title":"3.1 Basic usage (default profile)","text":"<pre><code>async def hello_world(*, context: NodeContext, input_text: str):\n    llm = context.llm()  # same as context.llm(\"default\")\n\n    text, usage = await llm.chat([\n        {\"role\": \"system\", \"content\": \"Be brief.\"},\n        {\"role\": \"user\", \"content\": f\"Say hi back to: {input_text}\"},\n    ])\n\n    return {\"reply\": text, \"usage\": usage}\n</code></pre>"},{"location":"examples/llm/#32-using-a-named-profile","title":"3.2 Using a named profile","text":"<pre><code>async def multi_vendor_demo(*, context: NodeContext, query: str):\n    openai_client   = context.llm(\"default\")   # e.g. OpenAI\n    gemini_client   = context.llm(\"gemini\")    # Google Gemini profile\n    anthropic_client = context.llm(\"anthropic\") # Claude profile\n\n    o_text, _ = await openai_client.chat([\n        {\"role\": \"user\", \"content\": f\"OpenAI: {query}\"},\n    ])\n\n    g_text, _ = await gemini_client.chat([\n        {\"role\": \"user\", \"content\": f\"Gemini: {query}\"},\n    ])\n\n    a_text, _ = await anthropic_client.chat([\n        {\"role\": \"user\", \"content\": f\"Claude: {query}\"},\n    ])\n\n    return {\"openai\": o_text, \"gemini\": g_text, \"anthropic\": a_text}\n</code></pre>"},{"location":"examples/llm/#33-embeddings-via-embed","title":"3.3 Embeddings via <code>embed()</code>","text":"<pre><code>async def embed_example(*, context: NodeContext, texts: list[str]):\n    # Use default profile\u2019s embedding model (see .env config section below)\n    client = context.llm()\n\n    vectors = await client.embed(texts)\n    # vectors: List[List[float]]\n\n    return {\"embeddings\": vectors}\n</code></pre>"},{"location":"examples/llm/#34-reasoning-effort-for-gpt5-openai","title":"3.4 Reasoning effort for GPT\u20115 (OpenAI)","text":"<p>For OpenAI GPT\u20115-family models (e.g. <code>gpt-5-nano</code>, <code>gpt-5-mini</code>, etc.), you can pass an optional <code>reasoning_effort</code> kwarg:</p> <pre><code>async def gpt5_reasoning_example(*, context: NodeContext):\n    client = context.llm(\"default\")  # configured with a gpt-5-* model\n\n    text, usage = await client.chat(\n        [\n            {\"role\": \"system\", \"content\": \"Think step-by-step.\"},\n            {\"role\": \"user\", \"content\": \"Explain why 2 + 2 = 4.\"},\n        ],\n        reasoning_effort=\"high\",  # \"low\" | \"medium\" | \"high\" (OpenAI GPT\u20115 only)\n    )\n\n    return {\"answer\": text, \"usage\": usage}\n</code></pre> <p>For non\u2011GPT\u20115 models or non\u2011OpenAI providers, <code>reasoning_effort</code> is ignored.</p>"},{"location":"examples/llm/#4-configuring-profiles-via-env","title":"4. Configuring profiles via <code>.env</code>","text":""},{"location":"examples/llm/#41-default-profile","title":"4.1 Default profile","text":"<p><code>AppSettings</code> uses:</p> <ul> <li><code>env_prefix=\"AETHERGRAPH_\"</code></li> <li><code>env_nested_delimiter=\"__\"</code></li> </ul> <p>So the default LLM profile is configured by env vars like:</p> <pre><code># Turn LLM on globally\nAETHERGRAPH_LLM__ENABLED=true\n\n# Default profile (\"default\")\nAETHERGRAPH_LLM__DEFAULT__PROVIDER=openai\nAETHERGRAPH_LLM__DEFAULT__MODEL=gpt-4o-mini\nAETHERGRAPH_LLM__DEFAULT__EMBED_MODEL=text-embedding-3-small\nAETHERGRAPH_LLM__DEFAULT__BASE_URL=https://api.openai.com/v1\nAETHERGRAPH_LLM__DEFAULT__TIMEOUT=60\nAETHERGRAPH_LLM__DEFAULT__API_KEY=sk-proj-...\n</code></pre>"},{"location":"examples/llm/#42-additional-named-profiles","title":"4.2 Additional named profiles","text":"<p>Profiles live under <code>llm.profiles[\"NAME\"]</code>, which maps to env like:</p> <pre><code># Gemini profile\nAETHERGRAPH_LLM__PROFILES__GEMINI__PROVIDER=google\nAETHERGRAPH_LLM__PROFILES__GEMINI__MODEL=gemini-1.5-pro-latest\nAETHERGRAPH_LLM__PROFILES__GEMINI__EMBED_MODEL=text-embedding-004\nAETHERGRAPH_LLM__PROFILES__GEMINI__TIMEOUT=60\nAETHERGRAPH_LLM__PROFILES__GEMINI__API_KEY=AIzaSy...\n\n# Anthropic profile\nAETHERGRAPH_LLM__PROFILES__ANTHROPIC__PROVIDER=anthropic\nAETHERGRAPH_LLM__PROFILES__ANTHROPIC__MODEL=claude-3-7-sonnet-20250219\nAETHERGRAPH_LLM__PROFILES__ANTHROPIC__TIMEOUT=60\nAETHERGRAPH_LLM__PROFILES__ANTHROPIC__API_KEY=ant-...\n\n# LM Studio local profile\nAETHERGRAPH_LLM__PROFILES__LOCAL__PROVIDER=lmstudio\nAETHERGRAPH_LLM__PROFILES__LOCAL__MODEL=your-lmstudio-model-id\nAETHERGRAPH_LLM__PROFILES__LOCAL__BASE_URL=http://localhost:1234/v1\nAETHERGRAPH_LLM__PROFILES__LOCAL__TIMEOUT=60\n</code></pre> <p>Then you can access them via:</p> <pre><code>context.llm()               # default\ncontext.llm(\"gemini\")      # Gemini\ncontext.llm(\"anthropic\")   # Claude\ncontext.llm(\"local\")       # LM Studio\n</code></pre> <p>Note: <code>embed_model</code> is optional. If omitted, <code>embed()</code> will fall back to <code>EMBED_MODEL</code> env or a sensible default (e.g. <code>text-embedding-3-small</code>).</p>"},{"location":"examples/llm/#5-embeddings-rag-default-behavior","title":"5. Embeddings &amp; RAG default behavior","text":"<p>For RAG and other embedding-heavy workflows, Aethergraph\u2019s helpers (e.g. index / vector store integration) will typically use the default profile\u2019s embedding configuration, i.e.:</p> <ul> <li><code>AETHERGRAPH_LLM__DEFAULT__EMBED_MODEL</code> if set,</li> <li>otherwise <code>EMBED_MODEL</code> env var,</li> <li>otherwise a built\u2011in default like <code>text-embedding-3-small</code>.</li> </ul> <p>So if you want to control which embedding model is used for global RAG, set:</p> <pre><code>AETHERGRAPH_LLM__DEFAULT__EMBED_MODEL=text-embedding-3-small\n</code></pre> <p>or override per call:</p> <pre><code>vectors = await context.llm().embed(texts, model=\"text-embedding-3-large\")\n</code></pre>"},{"location":"examples/llm/#6-adding-overriding-profiles-at-runtime","title":"6. Adding / overriding profiles at runtime","text":"<p>You don\u2019t have to declare everything in <code>.env</code>. You can also create or update profiles in code at runtime.</p>"},{"location":"examples/llm/#61-quick-runtime-profile-with-llm_set_key","title":"6.1 Quick runtime profile with <code>llm_set_key</code>","text":"<p><code>NodeContext.llm_set_key()</code> is a convenience for creating or updating a profile in memory:</p> <pre><code>async def runtime_profile_demo(*, context: NodeContext):\n    # Create/update profile \"runtime-openai\" on the fly\n    context.llm_set_key(\n        provider=\"openai\",\n        model=\"gpt-4o-mini\",               # NEW: model included for convenience\n        api_key=\"sk-proj-...\",            # in-memory only\n        profile=\"runtime-openai\",\n    )\n\n    client = context.llm(\"runtime-openai\")\n\n    text, _ = await client.chat([\n        {\"role\": \"user\", \"content\": \"Hello from runtime profile!\"},\n    ])\n\n    return {\"reply\": text}\n</code></pre> <p>This does not persist anything to disk or secrets store; it\u2019s only for the current process.</p>"},{"location":"examples/llm/#62-fully-configuring-a-profile-via-llm","title":"6.2 Fully configuring a profile via <code>llm()</code>","text":"<p>You can also configure a profile in one shot using <code>context.llm()</code> with overrides:</p> <pre><code>async def llm_inline_config_demo(*, context: NodeContext):\n    client = context.llm(\n        profile=\"lab\",\n        provider=\"google\",\n        model=\"gemini-1.5-pro-latest\",\n        api_key=\"AIzaSy...\",\n        base_url=\"https://generativelanguage.googleapis.com\",\n        timeout=60.0,\n    )\n\n    text, _ = await client.chat([\n        {\"role\": \"user\", \"content\": \"Hi from the lab profile!\"},\n    ])\n\n    return {\"reply\": text}\n</code></pre> <p>If the profile doesn\u2019t exist, it will be created. If it exists, it will be updated in place.</p>"},{"location":"examples/llm/#7-provider-specific-config-notes","title":"7. Provider-specific config notes","text":""},{"location":"examples/llm/#71-openai-provideropenai","title":"7.1 OpenAI (<code>provider=\"openai\"</code>)","text":"<p>Required:</p> <ul> <li><code>AETHERGRAPH_LLM__...__API_KEY</code> \u2013 or <code>OPENAI_API_KEY</code> if using env-based fallback.</li> </ul> <p>Optional / defaults:</p> <ul> <li><code>base_url</code> \u2013 defaults to <code>https://api.openai.com/v1</code>.</li> <li><code>timeout</code> \u2013 defaults to <code>60</code> seconds.</li> <li><code>model</code> \u2013 any chat/vision/reasoning model (e.g. <code>gpt-4o-mini</code>, <code>gpt-4o</code>, <code>gpt-5-nano</code>).</li> <li><code>embed_model</code> \u2013 e.g. <code>text-embedding-3-small</code>.</li> </ul>"},{"location":"examples/llm/#72-anthropic-provideranthropic","title":"7.2 Anthropic (<code>provider=\"anthropic\"</code>)","text":"<p>Required:</p> <ul> <li><code>api_key</code> \u2013 <code>ANTHROPIC_API_KEY</code> or <code>AETHERGRAPH_LLM__...__API_KEY</code>.</li> <li><code>model</code> \u2013 e.g. <code>claude-3-7-sonnet-20250219</code>.</li> </ul> <p>Optional / defaults:</p> <ul> <li><code>base_url</code> \u2013 defaults to <code>https://api.anthropic.com</code>.</li> <li><code>timeout</code> \u2013 defaults to <code>60</code>.</li> </ul> <p>Anthropic does not support embeddings via this client. <code>embed()</code> will raise <code>NotImplementedError</code> for <code>provider=\"anthropic\"</code>.</p>"},{"location":"examples/llm/#73-google-gemini-providergoogle","title":"7.3 Google / Gemini (<code>provider=\"google\"</code>)","text":"<p>Required:</p> <ul> <li><code>api_key</code> \u2013 <code>AETHERGRAPH_LLM__...__API_KEY</code>.</li> <li><code>model</code> \u2013 e.g. <code>gemini-1.5-pro-latest</code> for chat.</li> </ul> <p>Optional / defaults:</p> <ul> <li><code>base_url</code> \u2013 defaults to <code>https://generativelanguage.googleapis.com</code>.</li> <li><code>embed_model</code> \u2013 e.g. <code>text-embedding-004</code>.</li> </ul> <p>Endpoints used:</p> <ul> <li>Chat: <code>POST /v1/models/{model}:generateContent</code>.</li> <li>Embeddings: <code>POST /v1/models/{embed_model}:embedContent</code>.</li> </ul>"},{"location":"examples/llm/#74-lm-studio-providerlmstudio","title":"7.4 LM Studio (<code>provider=\"lmstudio\"</code>)","text":"<p>LM Studio exposes an OpenAI-compatible server.</p> <p>Required:</p> <ul> <li><code>base_url</code> \u2013 usually <code>http://localhost:1234/v1</code> (or whatever the LM Studio UI shows).</li> <li><code>model</code> \u2013 the LM Studio model ID (shown in the UI).</li> </ul> <p>Optional:</p> <ul> <li>No API key is required by default.</li> </ul> <p>Endpoints used:</p> <ul> <li>Chat: <code>POST {base_url}/chat/completions</code>.</li> <li>Embeddings: <code>POST {base_url}/embeddings</code>.</li> </ul>"},{"location":"examples/llm/#75-ollama-providerollama","title":"7.5 Ollama (<code>provider=\"ollama\"</code>)","text":"<p>Ollama also provides an OpenAI-compatible mode.</p> <p>Required/Defaults:</p> <ul> <li><code>base_url</code> \u2013 defaults to <code>http://localhost:11434/v1</code> if not set.</li> <li><code>model</code> \u2013 an Ollama model name (e.g. <code>llama3</code>, <code>mistral</code>, etc. configured in Ollama).</li> </ul> <p>Optional:</p> <ul> <li>Usually no API key.</li> </ul> <p>Endpoints used:</p> <ul> <li>Chat: <code>POST {base_url}/chat/completions</code>.</li> <li>Embeddings: <code>POST {base_url}/embeddings</code>.</li> </ul>"},{"location":"examples/llm/#8-when-not-to-use-contextllm","title":"8. When not to use <code>context.llm()</code>","text":"<p><code>context.llm()</code> is intentionally thin and opinionated. You might want to bypass it when:</p> <ul> <li>You need cutting-edge / experimental API features that aren\u2019t wired yet.</li> <li>You want a very custom request/response shape.</li> <li>You\u2019re targeting a provider that isn\u2019t in the built-in list.</li> </ul> <p>In those cases you can:</p> <ul> <li>Use <code>httpx</code> (or the vendor\u2019s official SDK) directly inside your node, or</li> <li>Wrap your own client as a separate service and inject it into <code>NodeServices</code>.</li> </ul> <p>The built-in <code>llm()</code> helper gives you a fast \u201chappy path\u201d for common providers and models, without preventing you from going lower-level when you need to.</p>"},{"location":"examples/memory-artifact/","title":"Memory &amp; Artifact Mini Examples","text":"<p>These snippets assume you\u2019re inside an async function where you already have:</p> <pre><code>mem: MemoryFacade = context.memory()\narts: ArtifactFacade = context.artifacts()\n</code></pre> <p>They\u2019re meant as supplementary examples, not main docs.</p>"},{"location":"examples/memory-artifact/#1-events-101-what-is-saved-and-how-to-read-it","title":"1. Events 101 \u2013 what is saved and how to read it","text":""},{"location":"examples/memory-artifact/#11-recording-a-simple-event","title":"1.1 Recording a simple event","text":"<pre><code># Record a simple user message as an event.\nevt = await mem.record(\n    kind=\"user_msg\",\n    data={\"role\": \"user\", \"text\": \"How do I use AetherGraph?\"},\n    tags=[\"chat\", \"demo\"],\n    severity=2,\n    stage=\"observe\",\n)\n\nprint(\"Event ID:\", evt.event_id)\nprint(\"Kind:\", evt.kind)\nprint(\"Text payload (JSON string):\", evt.text)\n</code></pre> <p>What happens:</p> <ul> <li><code>record(...)</code> JSON-serializes <code>data</code> and stores it in <code>evt.text</code>.</li> <li>Adds scope fields like <code>session_id</code>, <code>run_id</code>, <code>graph_id</code>, <code>node_id</code>, <code>agent_id</code>.</li> <li>Appends the <code>Event</code> to HotLog (recent buffer) and Persistence (JSONL).</li> </ul> <p>Conceptually, an Event is:</p> <p>\u201cSomething happened (tool call, chat turn, metric, etc.), scoped to this session/run.\u201d</p>"},{"location":"examples/memory-artifact/#12-reading-raw-events-vs-decoded-data","title":"1.2 Reading raw events vs decoded data","text":"<pre><code># Raw events (Event objects)\nfrom aethergraph.contracts.services.memory import Event\n\nevents: list[Event] = await mem.recent(kinds=[\"user_msg\"], limit=10)\nfor e in events:\n    print(\"Raw event kind:\", e.kind, \"text:\", e.text)\n\n# Decoded data (whatever you passed as data=...)\ndata_items = await mem.recent_data(kinds=[\"user_msg\"], limit=10)\nfor d in data_items:\n    print(\"Decoded data:\", d)   # dict: {\"role\": \"...\", \"text\": \"...\"}\n</code></pre> <ul> <li><code>recent(...)</code> \u2192 <code>list[Event]</code> (full event objects).</li> <li><code>recent_data(...)</code> \u2192 <code>list[Any]</code> using the JSON-in-<code>text</code> convention of <code>record()</code>.</li> </ul> <p>Users who just want \u201cthe thing I logged\u201d should use <code>recent_data</code>.</p>"},{"location":"examples/memory-artifact/#2-write_result-indices-structured-toolagent-outputs","title":"2. <code>write_result</code> &amp; indices \u2013 structured tool/agent outputs","text":"<p><code>write_result</code> is a convenience for logging structured outputs from a tool/agent and updating indices so you can ask things like:</p> <ul> <li>\u201cWhat was the last value named <code>result</code>?\u201d</li> <li>\u201cWhat are the latest outputs for <code>tool.calculator</code>?\u201d</li> </ul>"},{"location":"examples/memory-artifact/#21-recording-a-tool-result","title":"2.1 Recording a tool result","text":"<pre><code># Imagine a tiny calculator tool\ninputs = [\n    {\"name\": \"expression\", \"kind\": \"text\", \"value\": \"1 + 2 * 3\"},\n]\noutputs = [\n    {\"name\": \"result\", \"kind\": \"number\", \"value\": 7},\n]\n\nevt = await mem.write_result(\n    topic=\"tool.calculator\",      # identifier for this tool/agent\n    inputs=inputs,\n    outputs=outputs,\n    tags=[\"tool\", \"calculator\"],\n    metrics={\"latency_ms\": 12.3},\n    message=\"Evaluated 1 + 2 * 3\",\n)\n\nprint(\"tool_result event_id:\", evt.event_id)\nprint(\"Kind:\", evt.kind)   # \"tool_result\"\nprint(\"Tool:\", evt.tool)   # \"tool.calculator\"\n</code></pre>"},{"location":"examples/memory-artifact/#22-reading-via-indices","title":"2.2 Reading via indices","text":"<pre><code># 1) Last output value by name (fast)\nlast_result = await mem.get_last_value(\"result\")\nprint(\"get_last_value('result') -&gt;\", last_result)\n# e.g. {\"name\":\"result\",\"kind\":\"number\",\"value\":7}\n\n# 2) Latest reference outputs by kind (e.g. \"number\", \"json\", \"uri\")\nnumber_refs = await mem.get_latest_values_by_kind(\"number\", limit=5)\nprint(\"get_latest_values_by_kind('number'):\", number_refs)\n# e.g. [{\"name\":\"result\",\"kind\":\"number\",\"value\":7}, ...]\n\n# 3) Last outputs for a given topic (tool/agent)\ncalc_outputs = await mem.get_last_outputs_for_topic(\"tool.calculator\")\nprint(\"get_last_outputs_for_topic('tool.calculator'):\", calc_outputs)\n# e.g. {\"result\": 7, \"latency_ms\": 12.3, ...} (depends on index impl)\n</code></pre> <p>Purpose of <code>write_result</code>:</p> <ul> <li>Normalizes tool/agent outputs into a <code>tool_result</code> event.</li> <li>Keeps HotLog + Persistence in sync.</li> <li>Updates indices so other code can quickly answer questions about latest outputs.</li> </ul>"},{"location":"examples/memory-artifact/#3-artifacts-101-save-list-search-best","title":"3. Artifacts 101 \u2013 save, list, search, best","text":"<p>An Artifact is an immutable asset:</p> <ul> <li>Models, reports, checkpoints, metrics files, directories, etc.</li> <li>Stored via an <code>AsyncArtifactStore</code> and indexed via <code>AsyncArtifactIndex</code>.</li> </ul> <p>In agents, you normally access them through ArtifactFacade via <code>context.artifacts()</code>.</p>"},{"location":"examples/memory-artifact/#31-save-small-text-json-artifacts","title":"3.1 Save small text &amp; JSON artifacts","text":"<pre><code># Save a plain-text log\nlog_art = await arts.save_text(\n    \"This is a tiny experiment log.\",\n    suggested_uri=\"./logs/experiment_001.txt\",\n)\nprint(\"Log artifact URI:\", log_art.uri)\n\n# Save structured metrics as JSON\nmetrics_art = await arts.save_json(\n    {\"epoch\": 3, \"train_loss\": 0.42, \"val_loss\": 0.55},\n    suggested_uri=\"./metrics/exp001_epoch3.json\",\n)\nprint(\"Metrics artifact URI:\", metrics_art.uri)\n</code></pre>"},{"location":"examples/memory-artifact/#32-save-a-file-with-kindlabelsmetrics","title":"3.2 Save a file with kind/labels/metrics","text":"<pre><code>checkpoint_path = \"./checkpoints/exp001_step100.pt\"\n\nckpt_art = await arts.save(\n    checkpoint_path,\n    kind=\"model_checkpoint\",\n    labels={\"experiment\": \"exp001\", \"step\": \"100\"},\n    metrics={\"val_loss\": 0.55},\n    suggested_uri=\"./checkpoints/exp001_step100.pt\",\n    pin=True,   # mark as important/keep\n)\n\nprint(\"Checkpoint id:\", ckpt_art.id)\nprint(\"Checkpoint kind:\", ckpt_art.kind)\nprint(\"Checkpoint labels:\", ckpt_art.labels)\nprint(\"Checkpoint metrics:\", ckpt_art.metrics)\n</code></pre>"},{"location":"examples/memory-artifact/#4-listing-searching-artifacts-scope-labels-metrics","title":"4. Listing &amp; searching artifacts (scope, labels, metrics)","text":""},{"location":"examples/memory-artifact/#41-list-all-artifacts-for-this-run","title":"4.1 List all artifacts for this run","text":"<pre><code>arts_in_run = await arts.list(scope=\"run\")\nprint(\"Artifacts in this run:\", [a.uri for a in arts_in_run])\n</code></pre>"},{"location":"examples/memory-artifact/#42-search-by-kind-label","title":"4.2 Search by kind + label","text":"<pre><code>exp_ckpts = await arts.search(\n    kind=\"model_checkpoint\",\n    labels={\"experiment\": \"exp001\"},\n    scope=\"run\",\n)\nprint(\"Exp001 checkpoints:\", [a.uri for a in exp_ckpts])\n</code></pre> <ul> <li><code>kind</code> narrows by artifact type.</li> <li><code>labels</code> filters by label key/value.</li> <li> <p><code>scope</code> controls implicit filters:</p> </li> <li> <p><code>\"run\"</code> = current run only (default).</p> </li> <li><code>\"graph\"</code> / <code>\"node\"</code> = more specific.</li> <li><code>\"project\"</code> / <code>\"all\"</code> = wider.</li> </ul>"},{"location":"examples/memory-artifact/#43-selecting-the-best-artifact-by-metric","title":"4.3 Selecting the \"best\" artifact by metric","text":"<pre><code>best_ckpt = await arts.best(\n    kind=\"model_checkpoint\",\n    metric=\"val_loss\",\n    mode=\"min\",                # minimize validation loss\n    scope=\"run\",\n    filters={\"experiment\": \"exp001\"},\n)\n\nif best_ckpt:\n    print(\"Best checkpoint (by val_loss):\", best_ckpt.uri, best_ckpt.metrics)\nelse:\n    print(\"No checkpoint found.\")\n</code></pre> <p>Here, <code>best(...)</code> asks the index to:</p> <ul> <li>Filter by <code>kind</code> + <code>filters</code> (labels).</li> <li>Select the artifact with min or max on the given <code>metric</code>.</li> </ul>"},{"location":"examples/memory-artifact/#5-loading-artifacts-and-turning-uris-into-paths","title":"5. Loading artifacts and turning URIs into paths","text":""},{"location":"examples/memory-artifact/#51-load-payload-back-from-the-store","title":"5.1 Load payload back from the store","text":"<pre><code># If the artifact payload is JSON\nmetrics = await arts.load_artifact(metrics_art.uri)\nprint(\"Loaded metrics json:\", metrics)\n\n# If it's bytes (e.g., a binary checkpoint)\nckpt_bytes = await arts.load_artifact_bytes(ckpt_art.uri)\nprint(\"Loaded checkpoint size:\", len(ckpt_bytes))\n</code></pre>"},{"location":"examples/memory-artifact/#52-convert-artifact-uri-to-local-filesystem-path","title":"5.2 Convert artifact URI to local filesystem path","text":"<pre><code># Turn an artifact URI into a local file path (for external libs)\nlocal_ckpt_path = arts.to_local_file(ckpt_art)\nprint(\"Local checkpoint path:\", local_ckpt_path)\n\n# Same idea for directories:\n# local_dir = arts.to_local_dir(dir_artifact)\n</code></pre> <p>These helpers are handy when your artifacts are tracked as <code>file://...</code> URIs but some library expects a plain <code>str</code> path.</p>"},{"location":"examples/memory-artifact/#6-combining-memory-artifacts","title":"6. Combining memory + artifacts","text":"<p>Typical pattern:</p> <ol> <li>Agent runs a job.</li> <li>Saves results as artifacts.</li> <li>Logs a <code>tool_result</code> event with artifact URIs in outputs.</li> <li>Later, uses memory indices + artifact search/load to inspect results.</li> </ol> <pre><code># 1) Save metrics as an artifact\nmetrics_art = await arts.save_json(\n    {\"epoch\": 10, \"train_loss\": 0.21, \"val_loss\": 0.24},\n    suggested_uri=\"./metrics/exp002_epoch10.json\",\n)\n\n# 2) Log a structured result referencing the artifact\nawait mem.write_result(\n    topic=\"trainer.exp002\",\n    outputs=[\n        {\"name\": \"final_val_loss\", \"kind\": \"number\", \"value\": 0.24},\n        {\"name\": \"metrics_uri\", \"kind\": \"uri\", \"value\": metrics_art.uri},\n    ],\n    tags=[\"training\", \"exp002\"],\n    message=\"Training finished for exp002\",\n    metrics={\"epoch\": 10},\n)\n\n# 3) Later: quickly get last trainer outputs via indices\ntrainer_outs = await mem.get_last_outputs_for_topic(\"trainer.exp002\")\nprint(\"Trainer last outputs:\", trainer_outs)\n# e.g. {\"final_val_loss\": 0.24, \"metrics_uri\": \"file://.../metrics/exp002_epoch10.json\"}\n\n# 4) Load the metrics artifact via the recorded URI\nloaded_metrics = await arts.load_artifact(trainer_outs[\"metrics_uri\"])\nprint(\"Loaded metrics from artifact:\", loaded_metrics)\n</code></pre> <p>This example shows how memory (events + indices) and artifacts work together:</p> <ul> <li>Memory tells you what happened last and which artifact URIs matter.</li> <li>ArtifactFacade lets you search, rank, and then actually load those files.</li> </ul>"},{"location":"examples/memory-artifact/#7-rag-memory-turning-events-into-searchable-knowledge","title":"7. RAG + Memory \u2013 turning events into searchable knowledge","text":"<p>RAG (Retrieval-Augmented Generation) here is wired through MemoryFacade to let you:</p> <ol> <li>Create or bind to a corpus (a logical collection of documents/chunks).</li> <li>Promote events (e.g., tool results, chat summaries) into that corpus.</li> <li>Search / answer questions over it using an LLM.</li> <li>Optionally snapshot or compact the corpus over time.</li> </ol> <p>These examples assume <code>mem: MemoryFacade</code> is configured with a <code>RAGFacade</code>.</p>"},{"location":"examples/memory-artifact/#71-binding-to-a-corpus-projectsessionrun","title":"7.1 Binding to a corpus (project/session/run)","text":"<pre><code># Common pattern: bind to a project-level corpus.\ncorpus_id = await mem.rag_bind(scope=\"project\")\nprint(\"Using corpus:\", corpus_id)\n\n# Or a session-specific corpus\nsession_corpus = await mem.rag_bind(scope=\"session\")\nprint(\"Session corpus:\", session_corpus)\n\n# Or an explicitly named key (stable across runs if you reuse it)\nteam_corpus = await mem.rag_bind(scope=\"project\", key=\"team-alpha-notes\")\nprint(\"Team corpus:\", team_corpus)\n</code></pre> <ul> <li> <p><code>scope</code> controls how the corpus is keyed:</p> </li> <li> <p><code>\"project\"</code> \u2192 tied to workspace/project.</p> </li> <li><code>\"session\"</code> \u2192 tied to session_id.</li> <li><code>\"run\"</code> \u2192 tied to particular run (more ephemeral).</li> <li>You can override with <code>corpus_id=</code> directly if you already know the ID.</li> </ul>"},{"location":"examples/memory-artifact/#72-promoting-events-into-rag-event-doc","title":"7.2 Promoting events into RAG (event \u2192 doc)","text":"<p>You can convert existing memory events into RAG documents with <code>rag_promote_events()</code>.</p> <pre><code>corpus_id = await mem.rag_bind(scope=\"project\")\n\n# Promote recent high-signal tool_result events into the corpus\nstats = await mem.rag_promote_events(\n    corpus_id=corpus_id,\n    where={\n        \"kinds\": [\"tool_result\"],   # filter by Event.kind\n        \"limit\": 200,\n    },\n    policy={\n        \"min_signal\": 0.3,           # ignore low-signal noise\n        \"chunk\": {\"size\": 800, \"overlap\": 120},  # (if your RAG index supports chunking)\n    },\n)\n\nprint(\"Promoted events stats:\", stats)\n# e.g. {\"added\": 12, \"chunks\": 48, \"index\": \"SomeIndexImpl\"}\n</code></pre> <p>What happens:</p> <ul> <li><code>rag_promote_events</code> pulls events (via <code>recent</code> or your provided <code>events</code> list).</li> <li> <p>For each event, it builds a document:</p> </li> <li> <p><code>text</code> from <code>Event.text</code> (or a JSON of inputs/outputs/metrics).</p> </li> <li><code>title</code> + <code>labels</code> derived from kind/tool/stage/tags.</li> <li>Upserts docs into the RAG index.</li> <li>Logs a <code>tool_result</code> under topic <code>rag.promote.&lt;corpus_id&gt;</code> for traceability.</li> </ul> <p>You can also pass <code>events=</code> yourself if you already filtered them manually.</p>"},{"location":"examples/memory-artifact/#73-direct-upsert-of-custom-docs-bypassing-events","title":"7.3 Direct upsert of custom docs (bypassing events)","text":"<p>If you just have ad-hoc docs (e.g., notes, specs), you can call <code>rag_upsert</code>:</p> <pre><code>corpus_id = await mem.rag_bind(scope=\"project\")\n\ndocs = [\n    {\n        \"text\": \"AetherGraph is a framework for building agentic graphs.\",\n        \"title\": \"AG overview\",\n        \"labels\": {\"topic\": \"overview\", \"source\": \"docs\"},\n    },\n    {\n        \"text\": \"MemoryFacade coordinates HotLog, Persistence, Indices, and optional RAG.\",\n        \"title\": \"MemoryFacade design\",\n        \"labels\": {\"topic\": \"memory\", \"source\": \"notes\"},\n    },\n]\n\nupsert_stats = await mem.rag_upsert(corpus_id=corpus_id, docs=docs)\nprint(\"RAG upsert stats:\", upsert_stats)\n</code></pre> <p>This bypasses events entirely and goes straight to documents.</p>"},{"location":"examples/memory-artifact/#74-searching-the-corpus","title":"7.4 Searching the corpus","text":"<p>Once docs are in the corpus, you can run semantic/hybrid search:</p> <pre><code>corpus_id = await mem.rag_bind(scope=\"project\")\n\nhits = await mem.rag_search(\n    corpus_id=corpus_id,\n    query=\"How does the memory system work?\",\n    k=5,\n    filters={\"topic\": \"memory\"},  # optional label filter\n    mode=\"hybrid\",                 # or \"dense\"\n)\n\nfor h in hits:\n    print(\"Score:\", h[\"score\"])\n    print(\"Title:\", h[\"meta\"].get(\"title\"))\n    print(\"Text snippet:\", h[\"text\"][:120], \"...\")\n    print(\"Labels:\", h[\"meta\"].get(\"labels\"))\n    print(\"---\")\n</code></pre> <p><code>rag_search</code> returns a list of serializable dicts:</p> <ul> <li><code>text</code>  \u2013 chunk text.</li> <li><code>meta</code>  \u2013 metadata (labels, title, etc.).</li> <li><code>score</code> \u2013 similarity/relevance score.</li> </ul>"},{"location":"examples/memory-artifact/#75-rag-answer-retrieval-llm-citations","title":"7.5 RAG answer \u2013 retrieval + LLM + citations","text":"<p>For \u201cask a question over everything in the corpus\u201d you use <code>rag_answer</code>:</p> <pre><code>corpus_id = await mem.rag_bind(scope=\"project\")\n\nanswer = await mem.rag_answer(\n    corpus_id=corpus_id,\n    question=\"Summarize how MemoryFacade and ArtifactFacade work together.\",\n    style=\"concise\",          # or \"detailed\"\n    with_citations=True,\n    k=6,\n)\n\nprint(\"Answer:\\n\", answer.get(\"answer\"))\nprint(\"Citations:\")\nfor c in answer.get(\"resolved_citations\", []):\n    print(\"- From doc:\", c.get(\"doc_id\"), \"score=\", c.get(\"score\"))\n</code></pre> <p><code>rag_answer</code> will:</p> <ol> <li>Run retrieval over the corpus.</li> <li>Call the LLM with retrieved chunks.</li> <li>Return an <code>answer</code> plus <code>resolved_citations</code>.</li> <li>Log a <code>tool_result</code> under topic <code>rag.answer.&lt;corpus_id&gt;</code> with outputs    and usage metrics (via <code>write_result</code>).</li> </ol> <p>These helpers are optional, but they show how the RAG integration fits the same pattern as memory + artifacts:</p> <ul> <li>Memory: events + indices for \u201cwhat happened and when?\u201d.</li> <li>Artifacts: big immutable assets (files, bundles) with labels/metrics.</li> <li>RAG: a semantic index over the content of your events/docs, used by   your agents via standard tools (<code>rag_search</code>, <code>rag_answer</code>, etc.).</li> </ul>"},{"location":"key-concepts/agent-via-graph-fn/","title":"Agents via <code>@graph_fn</code>","text":"<p>This chapter introduces agents in AetherGraph through the <code>@graph_fn</code> decorator. You\u2019ll learn how <code>@tool</code> functions become nodes on the fly, when and why to use async functions, and how to chain or nest them to form structured yet reactive agentic workflows.</p>"},{"location":"key-concepts/agent-via-graph-fn/#1-what-is-a-graph_fn","title":"1. What is a <code>graph_fn</code>?","text":"<p>A <code>graph_fn</code> turns a plain Python function into an agent with access to rich context services\u2014channel, memory, artifacts, logger, and more. It runs in the normal Python runtime by default; no DAG is captured automatically when you invoke it. For most interactive or agentic workflows, this lightweight mode is ideal: you get an ergonomic async function with context utilities for I/O, persistence, and orchestration without committing to graph capture.</p>"},{"location":"key-concepts/agent-via-graph-fn/#function-shape","title":"Function shape","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"example\")\nasync def example(x: int, *, context: NodeContext):\n    # Access runtime services from the context\n    await context.channel().send_text(f\"x={x}\")\n    return {\"y\": x + 1}\n</code></pre> <ul> <li>Define your own API through standard parameters.</li> <li>Include <code>*, context</code> to access the <code>NodeContext</code>; if omitted, nothing is injected.</li> </ul> <p>Minimal example:</p> <pre><code>@graph_fn(name=\"hello_agent\")\nasync def hello_agent(name: str = \"world\", *, context: NodeContext):\n    await context.channel().send_text(f\"\ud83d\udc4b Hello, {name}!\")\n    context.memory().record(kind=\"usr_data\", data={\"name\": name})\n    context.logger().info(\"Greeted user\", extra={\"name\": name})\n    return {\"message\": f\"Hello, {name}\"}\n</code></pre> <p>Key idea: <code>@graph_fn</code> provides a reactive agent interface\u2014async execution with contextual power\u2014while keeping runtime overhead minimal. Nodes are only added when you explicitly use <code>@tool</code> or call other graphs.</p>"},{"location":"key-concepts/agent-via-graph-fn/#2-tools-nodes-on-the-fly","title":"2. Tools: nodes on the fly","text":"<p>The <code>@tool</code> decorator marks a Python function as a tool node. When called inside a <code>graph_fn</code>, the runtime creates a node on the fly and records its inputs/outputs for provenance, inspection, or future resumptions.</p> <p>Rule of thumb: for exploratory, reactive development, call regular Python functions freely. Reach for <code>@tool</code> when you need traceable state, durability, or resume checkpoints.</p>"},{"location":"key-concepts/agent-via-graph-fn/#example-a-simple-sum-tool","title":"Example: a simple sum tool","text":"<pre><code>from typing import List\nfrom aethergraph import tool\n\n@tool(outputs=[\"total\"])\ndef sum_vec(xs: List[float]) -&gt; dict:\n    return {\"total\": float(sum(xs))}\n</code></pre> <p>Use inside a <code>graph_fn</code>:</p> <pre><code>@graph_fn(name=\"tool_demo\")\nasync def tool_demo(values: list[float], *, context: NodeContext):\n    stats = {\"n\": len(values)}             # executed inline\n    out = sum_vec(values)                  # \u2190 captured as a node\n    await context.channel().send_text(f\"n={stats['n']}, sum={out['total']}\")\n    return {\"total\": out[\"total\"]}\n</code></pre> <p>You can mix normal Python code and <code>@tool</code> calls seamlessly. Only <code>@tool</code> calls create nodes.</p> <p>To inspect the implicit graph created during execution, call <code>graph_fn.last_graph</code> \u2014 it returns the captured <code>TaskGraph</code> for visualization or reuse.</p>"},{"location":"key-concepts/agent-via-graph-fn/#3-async-first-chaining-nesting-and-concurrency","title":"3. Async-first: chaining, nesting, and concurrency","text":"<p>AetherGraph adopts async-first design because agents often:</p> <ul> <li>Wait for user input (<code>ask_text</code>, <code>ask_approval</code>)</li> <li>Perform I/O (HTTP, file writes, DB queries)</li> <li>Launch parallel sub-tasks</li> </ul>"},{"location":"key-concepts/agent-via-graph-fn/#chaining-and-nesting-graph_fns","title":"Chaining and nesting <code>graph_fn</code>s","text":"<p>You can call one <code>graph_fn</code> from another. Each call creates a child subgraph node:</p> <pre><code>@graph_fn(name=\"step1\")\nasync def step1(x: int, *, context: NodeContext) -&gt; dict:\n    return {\"y\": x + 1}\n\n@graph_fn(name=\"step2\")\nasync def step2(y: int, *, context: NodeContext) -&gt; dict:\n    return {\"z\": y * 2}\n\n@graph_fn(name=\"pipeline\")\nasync def pipeline(x: int, *, context: NodeContext) -&gt; dict:\n    a = await step1(x)       # \u2192 child node\n    b = await step2(a[\"y\"]) # \u2192 child node\n    return {\"z\": b[\"z\"]}\n</code></pre>"},{"location":"key-concepts/agent-via-graph-fn/#fan-out-concurrency","title":"Fan-out concurrency","text":"<p>Launch multiple subgraphs concurrently with <code>asyncio.gather</code>:</p> <pre><code>import asyncio\n\n@graph_fn(name=\"concurrent_steps\")\nasync def concurrent_steps(a: int, b: int, *, context: NodeContext) -&gt; dict:\n    r1, r2 = await asyncio.gather(step1(a), step2(b))\n    return {\"r1\": r1[\"y\"], \"r2\": r2[\"z\"]}\n</code></pre> <p>This pattern enables natural fan-out/fan-in parallelism within a single reactive agent.</p>"},{"location":"key-concepts/agent-via-graph-fn/#4-running-a-graph_fn","title":"4. Running a <code>graph_fn</code>","text":"<p>You can execute a <code>graph_fn</code> directly from async code or through the provided runners.</p>"},{"location":"key-concepts/agent-via-graph-fn/#option-a-direct-await","title":"Option A \u2013 Direct await","text":"<pre><code># In an async function\nresult = await pipeline(3)\n</code></pre>"},{"location":"key-concepts/agent-via-graph-fn/#option-b-synchronous-helper","title":"Option B \u2013 Synchronous helper","text":"<p><pre><code>from aethergraph.runner import run\nfinal = run(pipeline, inputs={\"x\": 3})\n</code></pre> This is preferred in Jupyter Notebook. </p>"},{"location":"key-concepts/agent-via-graph-fn/#option-c-explicit-async-runner","title":"Option C \u2013 Explicit async runner","text":"<pre><code>from aethergraph.runner import run_async\n# In an async function\nresult = await run_async(pipeline, inputs={\"x\": 3})\n</code></pre> <p>The <code>run_*</code> helpers drive the event loop and normalize execution for both reactive and static graphs.</p>"},{"location":"key-concepts/agent-via-graph-fn/#5-summary","title":"5. Summary","text":"<ul> <li><code>@graph_fn</code> wraps a Python function into an async agent with an injected <code>NodeContext</code> exposing rich runtime services.</li> <li>Execution stays in normal Python until you invoke <code>@tool</code> or another <code>graph_fn</code>\u2014only those create nodes.</li> <li><code>@tool</code> functions let you capture intermediate steps for provenance and durability.</li> <li>Agents are composable: call one <code>graph_fn</code> from another or fan out with <code>asyncio.gather</code>.</li> <li>Use <code>run()</code> or <code>run_async()</code> for simple orchestration; prefer plain calls + context for lightweight workflows.</li> </ul> <p>AetherGraph\u2019s agent model combines Pythonic simplicity with event-driven introspection\u2014reactive first, deterministic when needed.</p>"},{"location":"key-concepts/artifacts-memory/","title":"Artifacts and Memory","text":"<p>This chapter covers two foundational pillars of AetherGraph\u2019s runtime: Artifacts and Memory. Together, they form the provenance backbone \u2014 making every result, file, and intermediate step traceable, reproducible, and retrievable long after execution.</p> <p>Mental model: Artifacts capture what was produced; Memory captures what happened (events, results, metrics) and why (context, summaries, links).</p>"},{"location":"key-concepts/artifacts-memory/#1-why-artifacts-memory-exist","title":"1. Why Artifacts &amp; Memory Exist","text":"<p>Most Python workflows scatter outputs across temp folders and logs with no consistent linkage. AetherGraph fixes this by binding everything to the active run/graph/node and exposing consistent, high\u2011level APIs for saving and recalling state.</p> Concern Manual management With AetherGraph Provenance Files &amp; logs scattered; hard to link Every record stamped with <code>{run_id, graph_id, node_id}</code> + tool metadata Reproducibility Filenames drift; env unknown Content\u2011addressed + typed records \u2192 deterministic recall Discoverability Grep and guess Query by <code>kind</code>, <code>labels</code>, <code>metrics</code>, scope; ask \u201cbest by metric\u201d Durability Ad\u2011hoc paths; stale temp dirs CAS store + index; pins; export/replay Collaboration Tribal conventions Shared schema (URIs/records) + searchable index <p>Takeaway: Use artifacts for durable assets; use memory for structured, queryable history. Both are scoped to your execution so you can reconstruct the story of a run.</p>"},{"location":"key-concepts/artifacts-memory/#2-artifacts-persistent-assets","title":"2. Artifacts \u2014 Persistent Assets","text":"<p>Artifacts are immutable, content\u2011addressed assets (CAS) produced or consumed by agents/tools: files, directories, JSON payloads, or serialized objects.</p>"},{"location":"key-concepts/artifacts-memory/#why-artifacts-vs-manual-files","title":"Why Artifacts (vs. manual files)?","text":"<ul> <li>Content\u2011addressed: the URI reflects the content (CAS) \u2014 no silent overwrites, no need for manual naming.</li> <li>Typed + labeled: add <code>kind</code>, <code>labels</code>, and <code>metrics</code> to organize results.</li> <li>Indexed: query by scope/labels or rank by metric. </li> <li>Provenance\u2011stamped: <code>{run_id, graph_id, node_id, tool_name, tool_version}</code> baked in.</li> <li>Portable: <code>to_local_path(uri)</code> resolves for local or remote stores.</li> </ul>"},{"location":"key-concepts/artifacts-memory/#architecture","title":"Architecture","text":""},{"location":"key-concepts/artifacts-memory/#core-api","title":"Core API","text":"Method Purpose <code>save_file()</code> Save an existing path and index it. Returns an artifact with <code>uri</code>. <code>save_text()</code> Store small text payloads. <code>save_json()</code> Store a JSON payload. <code>writer()</code> Context manager to stream\u2011write binary content; atomically indexes on close. <code>list()</code> / <code>search()</code> / <code>best()</code> Query and rank artifacts by descriptors or metrics. <code>pin()</code> Mark as retained (skip cleanup policies). <p><code>context.artifacts()</code> API link</p>"},{"location":"key-concepts/artifacts-memory/#examples","title":"Examples","text":"<p>Save a file</p> <pre><code>@graph_fn(name=\"produce_artifact\", outputs=[\"report_uri\"])\nasync def produce_artifact(*, context):\n    art = await context.artifacts().save_file(\n        path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\": \"A\"}\n    )\n    return {\"report_uri\": art.uri}\n</code></pre> <p>Search a past artfiact</p> <pre><code>@graph_fn(name=\"search_reports\", outputs=[\"top_uri\"])\nasync def search_reports(*, context):\n    results = await context.artifacts().search(\n        kind=\"report\", labels={\"exp\": \"A\"}\n    )\n    return {\"top_uri\": results[0].uri if results else None}\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#3-memory-structured-event-result-log","title":"3. Memory \u2014 Structured Event &amp; Result Log","text":"<p>Memory is a unified fa\u00e7ade for recording, persisting, and querying events during an agent\u2019s lifetime: raw logs, typed results, metrics, and their relationships with artifacts. </p>"},{"location":"key-concepts/artifacts-memory/#why-memory-design-intent","title":"Why Memory (design intent)","text":"<ul> <li>Contextual recall: agents can react based on recent or historical state.</li> <li>Typed outputs: <code>write_result</code> records semantic outputs with names/kinds/values.</li> <li>RAG\u2011ready: promote events to a vector index for retrieval\u2011augmented answers.</li> <li>Analytics: retrieve last actions for logical connection, track trends, or export for traceability.</li> </ul>"},{"location":"key-concepts/artifacts-memory/#architecture_1","title":"Architecture","text":""},{"location":"key-concepts/artifacts-memory/#core-api_1","title":"Core API","text":"Method Purpose <code>record()</code> Convenience structured logging. <code>record_chat()</code> / <code>record_chat_user()</code> / <code>record_chat_assistant()</code> Log a chat event (wrapper of <code>record()</code> with kind as <code>chat</code>) <code>record_tool_result()</code> Log a tool event with typed inputs/outputs (wrapper of <code>record()</code> with kind as <code>tool_result</code>) <code>recent()</code> / <code>recent_data()</code> Fetch most recent events / event data <code>distill_*()</code> Distillation of memory with persistence <code>rag_remember_events()</code> / <code>rag_search_by_key()</code> / <code>rag_answer_by_key()</code> RAG lifecycle helpers (requires LLM, API may change in future versions). <p><code>context.memory()</code> API link</p>"},{"location":"key-concepts/artifacts-memory/#examples_1","title":"Examples","text":"<p>Record an event</p> <pre><code>@graph_fn(name=\"remember_output\", outputs=[\"y\"])\nasync def remember_output(x: int, *, context):\n    y = x + 1\n    await context.memory().record(kind=\"cal.result\", data={\"y\": y})\n    return {\"y\": y}\n</code></pre> <p>Recall + summarization</p> <pre><code>recent = await context.memory().recent(limit=10) # return list of events\n</code></pre> <p>Promote to RAG</p> <pre><code>await context.memory().rag_remember_events(\n    key=\"knowledge_base\",\n    where={\"kinds\": [\"tool_result\"], \"limit\": 200},\n    policy={\"min_signal\": 0.25},\n)\nans = await context.memory().rag_answer_by_key(\n    key=\"knowledge_base\",\n    question=\"What is the result of last tool?\",\n    style=\"detailed\",\n)\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#4-artifacts-memory-better-together","title":"4. Artifacts \u00d7 Memory \u2014 Better Together","text":"<p>Artifacts and Memory reference each other: results and metrics point to artifact URIs; artifact metadata references the node that produced them. This bi\u2011directional linking enables:</p> <ul> <li>Reconstructing the full story of a result (inputs \u2192 tools \u2192 outputs \u2192 files).</li> <li>Ranking/searching results across runs/experiments.</li> <li>Efficient clean\u2011up strategies (e.g., keep pinned/best; GC the rest).</li> </ul>"},{"location":"key-concepts/artifacts-memory/#5-extensibility-external-systems","title":"5. Extensibility &amp; External Systems","text":"<p>AetherGraph\u2019s built\u2011ins for Artifacts and Memory are part of the OSS core runtime and are not swappable in place. That is intentional: we rely on their stable semantics for provenance, lineage, and tooling. If you need custom memory or storage systems (local or cloud), see Extending Context Services for <code>Service</code> APIs.</p>"},{"location":"key-concepts/artifacts-memory/#summary","title":"Summary","text":"<ul> <li>Artifacts make outputs durable, searchable, and reproducible with CAS URIs and rich indexing.</li> <li>Memory records the event stream and typed results for contextual recall, analytics, and RAG.</li> <li>Together they provide end\u2011to\u2011end provenance and effortless \u201ctime travel\u201d across runs.</li> </ul> <p>See also: <code>context.artifacts()</code> \u00b7 <code>context.memory()</code> \u00b7 <code>context.rag()</code> \u00b7 External Context Services</p>"},{"location":"key-concepts/channels-interaction/","title":"Channels and Interaction","text":"<p>A channel is how an agent communicates with the outside world \u2014 Slack, Telegram, Console, Web, or any other adapter. The <code>context.channel()</code> method returns a ChannelSession, a lightweight helper that provides a consistent Python API for sending and receiving messages, buttons, files, streams, and progress updates \u2014 regardless of which adapter you use.</p> <p>Default behavior: If no adapters are configured, AetherGraph automatically uses the console (<code>\"console:stdin\"</code>) as the default channel for input/output. To target Slack, Telegram, or Web, see Channel Setup section; your agent code remains unchanged in all channels.</p> <p>In short: Switch communication targets freely. The agent logic stays identical.</p>"},{"location":"key-concepts/channels-interaction/#1-what-is-a-channel","title":"1. What Is a Channel?","text":"<p>A channel is a routing target for interaction. It allows you to interact with an Agent inside a Python function. </p> <p>You can specify a channel key or alias (e.g., <code>\"slack:team/T:chan/C[:thread/TS]\"</code>) or rely on the system default. See Channel Setup for non-console key setup. </p>"},{"location":"key-concepts/channels-interaction/#resolution-order","title":"Resolution Order","text":"<ol> <li>Per-call override: <code>await context.channel().send_text(\"hi\", channel=\"slack:team/T:chan/C[:thread/TS]\")</code></li> <li>Bound session key: <code>ch = context.channel(\"slack:team/T:chan/C[:thread/TS]\"); await ch.send_text(\"hi\")</code></li> <li>Bus default: taken from <code>services.channels.get_default_channel_key()</code></li> <li>Fallback: <code>console:stdin</code></li> </ol> <p>When using Aethergraph UI</p> <ul> <li>Use <code>context.ui_run_channel()</code> to get the run workspace channel adapter (equivalent to `context.channel(\"ui:run/run_id\"))</li> <li>Use <code>context.ui_session_channel()</code> to get the session workspace channel adapter (equivalent to `context.channel(\"ui:session/session_id))</li> </ul>"},{"location":"key-concepts/channels-interaction/#2-quick-start","title":"2. Quick Start","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"channel_demo\")\nasync def channel_demo(*, context):\n    ch = context.channel()\n    await ch.send_text(\"Starting experiment\u2026\")\n    resp = await ch.ask_approval(\"Proceed?\", options=[\"Yes\", \"No\"])\n    if resp[\"approved\"]:\n        await ch.send_text(\"\u2705 Launching run.\")\n</code></pre> <p>Channels handle both output and input asynchronously \u2014 messages, approvals, file uploads, and more \u2014 using cooperative waits under the hood.</p>"},{"location":"key-concepts/channels-interaction/#3-core-methods","title":"3. Core Methods","text":"<p>Availability depends on the adapter\u2019s capabilities (e.g., file uploads are not supported in the console channel).</p> Method Purpose <code>send_text()</code>  / <code>ask_text()</code> Send/ask a plain text message. <code>send_file()</code> / <code>ask_file()</code> Upload/ask a file. <code>ask_approval()</code> Request approval or a choice. <code>send_buttons()</code> Send buttons to UI with links <code>stream()</code> Open a streaming session for incremental updates. <code>get_last_uploads()</code> Fetch uploaded files from UI at anytime <p>All <code>ask_*</code> methods use event-driven continuations, ensuring replies are properly correlated to their originating node.</p> <p>For exact usage, refer to <code>context.channel()</code> API. </p>"},{"location":"key-concepts/channels-interaction/#4-concurrency-and-fan-out","title":"4. Concurrency and Fan-Out","text":"<p>You can launch multiple concurrent asks in the same bound channel session and correlate the results:</p> <pre><code>import asyncio\n\n@graph_fn(name=\"concurrent_asks\")\nasync def concurrent_asks(*, context):\n    ch = context.channel()\n\n    async def one(tag):\n        name = await ch.ask_text(f\"[{tag}] What\u2019s your name?\")\n        await ch.send_text(f\"[{tag}] Thanks, {name}!\")\n        return {tag: name}\n\n    a, b = await asyncio.gather(one(\"A\"), one(\"B\"))\n    return {\"names\": a | b}\n</code></pre>"},{"location":"key-concepts/channels-interaction/#5-extensibility","title":"5. Extensibility","text":"<p>The channel interface can be extended to support any platform with a compatible API (HTTP, WebSocket, SDK). In practice, the inbound method for resuming interactions depends heavily on the target platform\u2019s event model.</p> <ul> <li>For notification-only channels, the API is straightforward \u2014 send events, no continuations.</li> <li>For interactive channels (e.g., Slack, Telegram, Web), resumptions rely on correlation IDs and continuation stores.</li> </ul> <p>In the OSS edition, AetherGraph currently includes built-in support for Console, Slack, Telegram,  Webhooks, and native Aethergraph UI. We will release adapter protocal API for extension and support for additional adapters in future releases.</p>"},{"location":"key-concepts/channels-interaction/#summary","title":"Summary","text":"<ul> <li>Channels unify all interaction patterns (text, files, approvals, progress, and streaming) under one async API.</li> <li>Default channel is console; others (Slack, Telegram, Web) are pluggable.</li> <li>All <code>ask_*</code> methods suspend execution via event-driven continuations, resuming seamlessly upon reply.</li> <li>Channels are adapter-agnostic and fully extensible \u2014 swap backends, not code.</li> </ul> <p>Write once, interact anywhere \u2014 your agents stay Pythonic, event\u2011driven, and platform\u2011neutral.</p>"},{"location":"key-concepts/concurrency-orchestration/","title":"Concurrency, Fan\u2011In/Fan\u2011Out &amp; Graph\u2011Level Orchestration","text":"<p>AetherGraph provides Python\u2011first concurrency that works from reactive agents to scheduled DAGs. You can orchestrate parallelism naturally in Python, while the runtime enforces safe scheduling and per\u2011run concurrency caps.</p>"},{"location":"key-concepts/concurrency-orchestration/#1-graph_fn-pythonic-concurrency-for-reactive-agents","title":"1. <code>@graph_fn</code> \u2014 Pythonic Concurrency for Reactive Agents","text":"<p><code>@graph_fn</code> functions execute through normal Python async semantics. Plain Python awaits run directly on the event loop, while any <code>@tool</code> calls inside a <code>@graph_fn</code> become implicit nodes managed by the agent\u2019s internal scheduler.</p> <p>Example: bounded fan\u2011out using a semaphore</p> <pre><code>import asyncio\nfrom aethergraph import graph_fn\n\nsem = asyncio.Semaphore(4)  # cap concurrent jobs (user-managed)\n\nasync def run_capped(fn, **kw):\n    async with sem:\n        return await fn(**kw)\n\n@graph_fn(name=\"batch_agent\")\nasync def batch_agent(items: list[str], *, context):\n    async def one(x):\n        await context.channel().send_text(f\"processing {x}\")\n        return {\"y\": x.upper()}\n\n    # fan\u2011out with manual cap\n    tasks = [run_capped(one, x=v) for v in items]\n    results = await asyncio.gather(*tasks)\n\n    # fan\u2011in\n    return {\"ys\": [r[\"y\"] for r in results]}\n</code></pre> <p>Notes:</p> <ul> <li>Plain Python steps execute immediately \u2014 not capped by the scheduler.</li> <li><code>@tool</code> calls are scheduled and counted toward the agent\u2019s concurrency cap through <code>max_concurrency</code> (default = 4).</li> <li>You can override per\u2011run limits by passing <code>max_concurrency=&lt;int&gt;</code> to <code>run()</code> or <code>run_async()</code> or use <code>graph_fn(.., max_concurrency=&lt;int&gt;)</code>.</li> <li>For nested or composed agents, effective concurrency multiplies; use semaphores or pools to control load.</li> <li>Ideal for reactive, exploratory agents or mixed I/O + compute logic.</li> </ul>"},{"location":"key-concepts/concurrency-orchestration/#2-graphify-schedulercontrolled-static-dags","title":"2. <code>@graphify</code> \u2014 Scheduler\u2011Controlled Static DAGs","text":"<p>In static DAGs built with <code>@graphify</code>, every <code>@tool</code> call becomes a node in a TaskGraph. Concurrency is automatically managed by the runtime scheduler, respecting per\u2011run limits.</p> <p>Minimal fan\u2011in/fan\u2011out example:</p> <pre><code>from aethergraph import graphify, tool\n\n@tool(outputs=[\"result\"])\nasync def pick(items: list[int], index: int):\n    return {\"result\": items[index]}\n\n@tool(outputs=[\"out\"])\nasync def work(x: int):\n    print(f\"Working on {x}...\")\n    return {\"out\": x * 2}\n\n@tool(outputs=[\"sum\"])\nasync def reduce_sum(xs: list[int]):\n    return {\"sum\": sum(xs)}\n\n@graphify(name=\"map_reduce\", inputs=[\"vals\"], outputs=[\"sum\"])\ndef map_reduce(vals):\n    results = [pick(items=vals, index=i) for i in range(len(vals))]  # We need use a tool to extract values as vals is a ref not a list \n    outs = [work(x=v.result) for v in results]       # fan\u2011out\n    total = reduce_sum(xs=[o.out for o in outs])     # fan\u2011in\n    return {\"sum\": total.sum}\n</code></pre> <p>Key points:</p> <ul> <li>The scheduler enforces <code>max_concurrency</code> automatically (default = 4).</li> <li>You can override per\u2011run limits by passing <code>max_concurrency=&lt;int&gt;</code> to <code>run()</code>, or <code>run_async()</code>.</li> <li>Static DAG concurrency is global and consistent across all tool nodes.</li> <li>Each node runs once dependencies resolve; no explicit <code>await</code> is required.</li> </ul>"},{"location":"key-concepts/concurrency-orchestration/#3-graphlevel-orchestration-patterns","title":"3. Graph\u2011Level Orchestration Patterns","text":"<p>All orchestration in AetherGraph is just Python. You can run sequentially or concurrently using standard async primitives.</p>"},{"location":"key-concepts/concurrency-orchestration/#a-sequential-orchestration-plain-python","title":"A) Sequential orchestration (plain Python)","text":"<pre><code>res1 = await graph_fn1(a=1, max_concurrency=N) # graph-level concurrency\nres2 = await graph_fn2(b=2, max_concurrency=N)\n</code></pre>"},{"location":"key-concepts/concurrency-orchestration/#b-concurrent-graph_fn-runs-asyncfriendly","title":"B) Concurrent <code>graph_fn</code> runs (async\u2011friendly)","text":"<pre><code>res1, res2 = await asyncio.gather(\n    graph_fn1(a=1, max_concurrency=N),\n    graph_fn2(b=2, max_concurrency=N),\n)\n</code></pre>"},{"location":"key-concepts/concurrency-orchestration/#c-concurrent-graph-runner-works-for-both-graph_fn-and-graphify","title":"C) Concurrent graph runner (works for both <code>graph_fn</code> and <code>graphify</code>)","text":"<pre><code>from aethergraph.runner import run_async\n\nres1, res2 = await asyncio.gather(\n    run_async(graph1, inputs={\"a\": 1}, max_concurrency=8),\n    run_async(graph2, inputs={\"b\": 2}, max_concurrency=2),\n)\n</code></pre> <p>Default concurrency for each graph is 4, but you can override it per call with <code>max_concurrency</code> in either <code>run()</code> or <code>run_async()</code>. Becareful of global concurrency limit. Use semaphores or pools to control load.  Do not use <code>runner.run()</code> for concurrent graph runs.</p>"},{"location":"key-concepts/concurrency-orchestration/#4-concurrency-comparison","title":"4. Concurrency Comparison","text":"Aspect <code>@graph_fn</code> (Reactive) <code>@graphify</code> (Static) Concurrency Control Automatic via scheduler (<code>max_concurrency</code>) Automatic via scheduler (<code>max_concurrency</code>) Default Limit Default 4 per run, multiply with nested calls Default 4 per run Plain Python Awaitables Run immediately, outside scheduler Not applicable (only tool nodes) Nested Calls Supported Not yet supported Failure Behavior Caught at runtime; user decides Scheduler stops on first error (configurable) Use Case Agents, exploration, hybrid control Pipelines, batch workflows, reproducible DAGs"},{"location":"key-concepts/concurrency-orchestration/#takeaways","title":"Takeaways","text":"<ul> <li>Reactive vs Deterministic: <code>graph_fn</code> for interactive exploration; <code>graphify</code> for reproducible pipelines.</li> <li>Fan\u2011In/Fan\u2011Out: Async patterns in <code>graph_fn</code>; data edges in <code>graphify</code>.</li> <li>Concurrency Control: Default cap = 4; override per run with <code>max_concurrency</code>.</li> <li>Scalability: Local schedulers per agent; a global scheduler orchestrates multiple runs.</li> <li>Everything is Python: The runtime extends standard async execution into persistent, inspectable DAG scheduling.</li> </ul>"},{"location":"key-concepts/context-services/","title":"Context Services Overview","text":"<p>Context is the lightweight runtime handle that every agent and tool receives during execution. It represents the active run, graph, and node scope and exposes AetherGraph\u2019s built-in runtime services\u2014channels, memory, artifacts, logs, and more\u2014through a clean, Pythonic interface.</p> <p>In short: Context is what makes an AetherGraph program \u201calive.\u201d It bridges your pure Python logic with interactive I/O, persistence, orchestration, and AI-powered capabilities\u2014without introducing a new DSL or framework-specific syntax.</p>"},{"location":"key-concepts/context-services/#1-why-context-matters","title":"1. Why Context Matters","text":"<p>AetherGraph\u2019s guiding principle is Python-first orchestration. The context system makes that possible by providing a unified way to connect logic and infrastructure.</p> <p>Core benefits:</p> <ul> <li>Decoupled logic: Agents and tools can call <code>context.&lt;service&gt;()</code> without worrying about back-end details or deployment environment.</li> <li>Automatic provenance: Each call carries its <code>run_id</code>, <code>graph_id</code>, and <code>node_id</code>, ensuring full traceability.</li> <li>Zero-friction orchestration: Handles message passing, persistence, and coordination transparently.</li> <li>Optional intelligence: Attach LLMs, RAG corpora, or MCP servers only when needed\u2014no dependencies until configured.</li> </ul> <p>In practice, <code>NodeContext</code> turns plain async functions into interactive, stateful agents that can communicate, remember, reason, and orchestrate\u2014all from Python.</p>"},{"location":"key-concepts/context-services/#2-quick-start","title":"2. Quick Start","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"hello_context\")\nasync def hello_context(*, context):\n    await context.channel().send_text(\"Hello from AetherGraph!\")\n    await context.memory().record(\n        kind=\"chat_data\",\n        data={\"message\": \"Hello from AetherGraph!\"},  \n    ) # remember key events\n    context.logger().info(\"finished\", extra={\"stage\": \"done\"})\n    return {\"ok\": True}\n</code></pre> <p>Each call operates within a specific node scope. The runtime automatically provides <code>run_id</code>, <code>graph_id</code>, and <code>node_id</code> to maintain context and provenance.</p>"},{"location":"key-concepts/context-services/#3-context-structure","title":"3. Context Structure","text":"<p>Each <code>NodeContext</code> carries stable identifiers and bound service references.</p> <pre><code>@dataclass\nclass NodeContext:\n    run_id: str\n    graph_id: str\n    node_id: str\n    services: NodeServices  # all bound runtime services\n</code></pre> <p>Identifiers</p> <ul> <li>run_id \u2014 unique per execution run.</li> <li>graph_id \u2014 identifies which graph the node belongs to.</li> <li>node_id \u2014 unique ID for the node invocation.</li> </ul>"},{"location":"key-concepts/context-services/#4-context-services","title":"4. Context Services","text":"<p>AetherGraph organizes its context services into core, optional, and utility layers.</p>"},{"location":"key-concepts/context-services/#core-services","title":"Core Services","text":"Method Purpose API reference <code>context.channel()</code> Message and interaction bus \u2014 send/receive text/approval/files, show progress, or streaming events. Link <code>context.memory()</code> Memory fa\u00e7ade \u2014 record events, write typed results, query history, or manage RAG-ready logs. Link <code>context.artifacts()</code> Artifact store fa\u00e7ade \u2014 save/retrieve files, track outputs, and query files by labels/metrics artifacts. Link <code>context.kv()</code> Lightweight key\u2013value store for ephemeral coordination and small caches. Link <code>context.logger()</code> Structured logger with <code>{run_id, graph_id, node_id}</code> metadata automatically included. Link"},{"location":"key-concepts/context-services/#optional-services-config-dependent","title":"Optional Services (config-dependent)","text":"<p>Optional services require API keys or runtime configuration. They are injected dynamically when available.</p> Method Purpose API reference <code>context.llm()</code> Access an LLM client for chat, embeddings, or raw APIs (OpenAI, Anthropic, Google, or local backends, etc.). Link <code>context.rag()</code> Retrieval-augmented generation fa\u00e7ade \u2014 build corpora, upsert documents, search, and answer queries. Link <code>context.mcp()</code> Connect to external MCP tool servers via stdio, WebSocket, or HTTP. Link <code>context.viz()</code> Display real-time plots in Aethergraph UI (need UI setup). Link"},{"location":"key-concepts/context-services/#run-management","title":"Run Management","text":"<p>Create and manage nested runs inside a graph. </p> Method Purpose API reference <code>context.spawn_run()</code> Spawn a non-blocking run in the background in a fire-and-forget manner Link <code>context.wait_run()</code> Wait for the spawned runs Link <code>context.run_and_wait()</code> Create runs and wait for them in a blocking manner Link <code>context.cancel_run()</code> Cancel a run with best effort Link"},{"location":"key-concepts/context-services/#utility-helpers","title":"Utility Helpers","text":"Method Purpose <code>context.clock()</code> Clock utilities for timestamps, delays, and scheduling. <code>context.continuations()</code> Access continuation store; used internally for dual-stage waits (<code>ask_text</code>, <code>ask_approval</code>). <p>If a service is unavailable, its accessor raises a clear runtime error (e.g., <code>LLMService not available</code>). Configure them globally or per-environment to enable.</p>"},{"location":"key-concepts/context-services/#5-typical-patterns","title":"5. Typical Patterns","text":""},{"location":"key-concepts/context-services/#1-ask-wait-resume","title":"1 Ask \u2192 Wait \u2192 Resume","text":"<pre><code>text = await context.channel().ask_text(\"Provide a dataset path\") \n# wait for external input and resume when done\nawait context.channel().send(f\"you provided the dataset path {text}\")\n</code></pre>"},{"location":"key-concepts/context-services/#2-artifacts-memory","title":"2 Artifacts + Memory","text":"<pre><code># save and return an artifact \nart = await context.artifacts().save_file(path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\": \"A\"}) \n# save and return an event  \nevt = await context.memory().record(kind=\"checkpoint\", data={\"info\": \"experiment A saved\"}) \n\n# In later stage or other agent:\n# list all previous saved art with kind == \"report\"\narts = await context.artifacts().search(kind=\"report\")      \n# list past 100 memory with kinds include \"checkpoint\" \nevts = await context.memory().recent(kinds=[\"checkpoint\"], limit=100)    \n</code></pre>"},{"location":"key-concepts/context-services/#4-rag-llm-answers","title":"4 RAG + LLM Answers","text":"<pre><code># ingest data into vector DB\ncorpus_id = \"notes\"\n_ = await context.rag().upsert_docs(corpus_id, docs)  # your documentations in a list \n\n# later search/answer\nhits = await context.rag().search(corpus_id, query=\"Tool used in experiment #A2?\", k=5)\nans = await context.rag().answer(corpus_id, question=\"What is the best iteration and what is the loss?\", style=\"concise\")\nawait context.channel().send_text(ans[\"answer\"])\n</code></pre>"},{"location":"key-concepts/context-services/#5-external-tools-via-mcp","title":"5 External Tools via MCP","text":"<pre><code>res = await context.mcp(\"ws\").call(\"search\", {\"q\": \"tolerance analysis\", \"k\": 5})\n</code></pre>"},{"location":"key-concepts/context-services/#6-custom-context-services","title":"6. Custom Context Services","text":"<p>The context system is fully extensible. You can define your own service and expose it via <code>context.&lt;name&gt;()</code> using <code>register_context_service()</code>.</p> <p>Use cases:</p> <ul> <li>Add domain-specific APIs (e.g., simulation, materials DB, experiment tracking).</li> <li>Provide custom persistence or distributed coordination layers.</li> <li>Implement bridges between external systems (e.g., job schedulers, cloud storage, or lab devices).</li> </ul> <p>See External Context Services for API details and examples.</p>"},{"location":"key-concepts/context-services/#7-design-philosophy","title":"7. Design Philosophy","text":"<ul> <li>Python-first: use direct calls, not DSL syntax.</li> <li>Minimal surface: each service follows a small, composable API.</li> <li>Composable orchestration: mix local and remote services freely.</li> <li>Swappable backends: replace LLM, KV, or artifact backends without touching agent logic.</li> </ul>"},{"location":"key-concepts/context-services/#see-also","title":"See Also","text":"<ul> <li><code>context.channel()</code> \u2014 cooperative waits, streaming, progress updates</li> <li><code>context.memory()</code> \u2014 event log, typed results, summaries, and RAG helpers</li> <li><code>context.artifacts()</code> \u2014 content-addressable storage and retrieval</li> </ul>"},{"location":"key-concepts/event-driven-waits/","title":"Event\u2011Driven Waits: Cooperative vs Dual\u2011Stage","text":"<p>AetherGraph agents are event\u2011driven: they can pause mid\u2011flow and safely resume when a reply, upload, or callback arrives. There are two complementary wait modes, and you can use them flexibly in both <code>@graph_fn</code> and <code>@graphify</code>\u2011built graphs.</p> <ul> <li>Cooperative waits \u2014 via <code>context.channel().ask_*</code>. Simplest way to prompt + wait in reactive agents.</li> <li>Dual\u2011stage waits \u2014 via <code>@tool</code> nodes that split into Stage A (prompt/setup) and Stage B (resume/produce). Best for static graphs and reliable orchestration.</li> </ul> <p>Flexibility: <code>context.*</code> methods are available inside <code>@tool</code> nodes (therefore inside <code>@graphify</code>). Dual\u2011stage tools can also be <code>await</code>\u2011ed directly inside <code>@graph_fn</code>. In either case, they form a node and persist a continuation.</p>"},{"location":"key-concepts/event-driven-waits/#1-cooperative-waits-channelfirst","title":"1 Cooperative Waits (Channel\u2011first)","text":"<p>What: <code>context.channel().ask_text / ask_approval / ask_files</code> send a prompt and yield until a reply or timeout. The runtime persists a continuation token so the run can resume after restarts.</p> <p>Where: Primarily inside <code>@graph_fn</code>. Can also be called from within a <code>@tool</code> if you want cooperative logic inside a node.</p> <p>Example</p> <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"cooperative_demo\", outputs=[\"msg\"]) \nasync def cooperative_demo(*, context):\n    name = await context.channel().ask_text(\"Your name?\")\n    await context.channel().send_text(f\"Hi, {name}!\")\n    return {\"msg\": f\"greeted:{name}\"}\n</code></pre> <p>Properties</p> <ul> <li>Minimal code, great for exploratory, chat\u2011style agents.</li> <li>Thread/channel\u2011aware correlation.</li> <li>Durable continuations; survives restarts.</li> </ul>"},{"location":"key-concepts/event-driven-waits/#2-dualstage-waits-toolfirst","title":"2 Dual\u2011Stage Waits (Tool\u2011first)","text":"<p>What: A node splits into two stages: A emits the prompt/sets up state, B resumes once the event arrives and produces outputs. Maps cleanly to static DAGs and lets the global scheduler manage resumptions and retries.</p> <p>Use in both places:</p> <ul> <li>In <code>@graphify</code> as standard tool nodes.</li> <li>In <code>@graph_fn</code> with <code>await</code> for immediate use \u2014 they still become nodes under the hood.</li> </ul> <p>Built\u2011in channel tools</p> <pre><code># Use these in either style:\nfrom aethergraph.tools import ask_text, ask_approval, ask_files\n\n# A) Inside a static graph\nfrom aethergraph import graphify\n\n@graphify(name=\"collect_input\", inputs=[], outputs=[\"greeting\"]) \ndef collect_input():\n    name = ask_text(prompt=\"Your name?\")      # node yields \u2192 resumes on reply\n    return {\"greeting\": name.text}\n\n# B) Await directly in a graph_fn\nfrom aethergraph import graph_fn\n\n@graph_fn(name=\"dualstage_in_fn\", outputs=[\"choice\"]) \nasync def dualstage_in_fn(*, context):\n    res = await ask_approval(prompt=\"Proceed?\", options=(\"Yes\",\"No\"))\n    return {\"choice\": res[\"choice\"]}\n</code></pre> <p>Properties</p> <ul> <li>Node\u2011level persistence, retries, and metrics.</li> <li>Works seamlessly with global scheduling (centralized control, resumptions at scale).</li> <li>Great for UI + pipeline hybrids (prompt in Stage A, compute in Stage B).</li> </ul>"},{"location":"key-concepts/event-driven-waits/#3-using-context-inside-graphify","title":"3 Using <code>context.*</code> inside <code>@graphify</code>","text":"<p><code>context</code> methods (channels, memory, artifacts, kv, logger, etc.) are available inside <code>@tool</code> nodes. This means your static graphs can still interact, log, and persist during node execution while retaining DAG inspectability.</p> <pre><code>from aethergraph import tool\n\n@tool(outputs=[\"ok\"]) \nasync def notify_and_tag(*, context):\n    await context.channel().send_text(\"Started node\u2026\")\n    await context.memory().record(kind=\"status\", data={\"stage\":\"start\"})\n    return {\"ok\": True}\n</code></pre>"},{"location":"key-concepts/event-driven-waits/#4-comparison-cooperative-vs-dualstage-vs-manual-checkpoints","title":"4 Comparison: Cooperative vs Dual\u2011Stage vs Manual Checkpoints","text":"Aspect Cooperative (<code>context.channel().ask_*</code>) Dual-Stage (<code>@tool</code> ask_*) Manual checkpoints Authoring style Inline, minimal Explicit node with A/B stages N/A in AG (not built-in) Resumability Hard \u2014 stateless unless save the state to memory manually Native continuations per node (resumeable after restart) Possible but manual/fragile Retry / Idempotency Coarse (re-invoke the whole function) Fine (node-level retry, idempotent resumes) Manual Scale Great for interactive sessions, small graphs Excellent for large runs / thousands of waits Limited by implementation CPU load (waiting) Keeps process / event loop alive; lightweight but not zero Zero CPU \u2014 node is dormant until resumed Depends on checkpointing backend Memory footprint Held in local task heap (light) Released after serialization; only metadata retained Depends on snapshot granularity Disk usage Optional if memory writes used Tiny (~1\u201310 KB per node) \u2014 correlator + inputs serialized Potentially heavy (full state dump) Latency to resume Instant within current process Slightly higher (resume event \u2192 lookup \u2192 dispatch) Potentially high (manual restore) <p>Why Dual\u2011Stage scales</p> <ul> <li>Node\u2011granular control: retries, backoff, and resumption are local to the waiting node.</li> <li>Central orchestration: the global scheduler can queue, shard, or migrate blocked nodes.</li> <li>Observability: each wait is a first\u2011class node with metrics and logs.</li> <li>Determinism: Stage boundaries clarify side\u2011effects and make runs reproducible.</li> </ul> <p>Manual checkpoints (framework\u2011agnostic snapshots) aren\u2019t part of AetherGraph. Dual\u2011stage nodes cover the same reliability space with less boilerplate and better provenance.</p>"},{"location":"key-concepts/event-driven-waits/#5-extending-dualstage-tools","title":"5 Extending Dual\u2011Stage Tools","text":"<p>You can author custom dual\u2011stage nodes with <code>DualStageTool</code> to model your own A/B waits (e.g., submit job \u2192 wait \u2192 collect). Some examples of the usage include </p> <ul> <li>custom channel waits</li> <li>submit/run long simualtion on cloud</li> <li>data/model training pipeline on external systems</li> <li>external API Polling that reports a compleltion asynchronously</li> </ul> <p>A compact public API for this is planned; detailed docs will ship soon.</p>"},{"location":"key-concepts/event-driven-waits/#6-takeaways","title":"6 Takeaways","text":"<ul> <li>All <code>context.channel().ask_*</code> calls are cooperative waits by default.</li> <li>Dual\u2011stage tools work in both <code>@graphify</code> and <code>@graph_fn</code> (awaitable) and always materialize as nodes.</li> <li>For large, reliable systems: prefer dual\u2011stage for node\u2011level retries, metrics, and scheduler control.</li> <li><code>context.*</code> is available inside <code>@tool</code> nodes, so static graphs can still interact, log, and persist cleanly.</li> <li>Manual checkpointing isn\u2019t needed; dual\u2011stage nodes give better reliability with less boilerplate.</li> </ul>"},{"location":"key-concepts/extending-context/","title":"Extending Context Services","text":"<p>AetherGraph lets you extend the runtime by adding your own <code>context.&lt;name&gt;</code> methods. These external context services live alongside built\u2011ins like <code>channel</code>, <code>memory</code>, and <code>artifacts</code>, and provide reusable, lifecycle\u2011aware helpers for clients, caches, orchestration, or domain APIs \u2014 without changing your agent code.</p> <p>Key idea: keep agent logic pure\u2011Python; move integration glue and shared state into services that the runtime injects per node.</p>"},{"location":"key-concepts/extending-context/#1-what-is-an-external-context-service","title":"1. What is an External Context Service?","text":"<p>A context service is a registered Python object bound into every <code>NodeContext</code>. After registration, you can use it anywhere inside a graph or tool:</p> <pre><code>@graph_fn(name=\"demo\")\nasync def demo(*, context):\n    info = await context.trainer().inspect_job(job_id=\"abc123\")\n    return {\"status\": info[\"status\"]}\n</code></pre>"},{"location":"key-concepts/extending-context/#whywhen-to-use","title":"Why/When to Use","text":"<ul> <li>Reusable helpers \u2014 share clients (e.g., HPC, S3, DB, solver), connection pools, or token buckets.</li> <li>Shared state \u2014 memoize expensive lookups; coordinate across nodes within a run.</li> <li>Centralized config \u2014 keep API keys, timeouts, routing, or policies in one place.</li> <li>Per\u2011node awareness \u2014 access to built-in services through <code>self.ctx()</code> for provenance or multi\u2011tenancy.</li> </ul> <p>Use a service for long\u2011lived instances or cross\u2011node coordination. For tiny stateless helpers, plain imports are fine.</p>"},{"location":"key-concepts/extending-context/#2-naming-boundaries-important","title":"2. Naming &amp; Boundaries (Important)","text":"<p>Built\u2011ins (<code>context.artifacts()</code>, <code>context.memory()</code>, etc.) are not swappable in OSS. To extend the system, register new services with new names (e.g., <code>context.trainer()</code>, <code>context.datasets()</code>, <code>context.lineage_store()</code>).</p> <ul> <li>Keep agent code explicit about which storage or API it\u2019s using.</li> <li>If mirroring/exporting, record links (artifact URIs, memory event IDs) inside your external system for provenance.</li> </ul>"},{"location":"key-concepts/extending-context/#3-minimal-service-instancebased","title":"3. Minimal Service (Instance\u2011based)","text":""},{"location":"key-concepts/extending-context/#define-a-service-and-use-selfctx","title":"Define a service and use <code>self.ctx()</code>","text":"<pre><code>from aethergraph import Service\n\nclass Trainer(Service):\n    async def submit(self, spec: dict) -&gt; str:\n        # Submit a training job to your HPC/cluster\n        ... \n        return job_id\n\n    async def inspect_job(self, job_id: str) -&gt; dict:\n        # Inspect the job status\n        ...\n        return {\"job_id\": job_id, \"status\": status}\n</code></pre>"},{"location":"key-concepts/extending-context/#register-at-startup-pass-an-instance","title":"Register at startup (pass an instance)","text":"<pre><code>from aethergraph.runtime import register_context_service\nfrom aethergraph import start_server()\n\n# register after server is started \nstart_server() \nregister_context_service(\"trainer\",  Trainer())\n</code></pre> <p>After this, <code>context.trainer()</code> is available everywhere in the runtime.</p>"},{"location":"key-concepts/extending-context/#4-usage-patterns","title":"4. Usage Patterns","text":""},{"location":"key-concepts/extending-context/#a-submit-training-link-artifacts","title":"A) Submit training &amp; link artifacts","text":"<pre><code>@graph_fn(name=\"train_model\", outputs=[\"job_id\", \"ckpt_uri\"]) \nasync def train_model(spec: dict, *, context):\n    #  Submit to your cluster via the custom service\n    job_id = await context.trainer().submit(spec)\n    return {\"job_id\": job_id, \"ckpt_uri\": ckpt.uri}\n</code></pre>"},{"location":"key-concepts/extending-context/#b-inspect-status-in-another-nodetool","title":"B) Inspect status in another node/tool","text":"<pre><code>@tool(name=\"wait_for_training\", outputs=[\"ready\"]) \nasync def wait_for_training(job_id: str, *, context) -&gt; dict:\n    # Inspect you job through your service\n    info = await context.trainer().inspect_job(job_id)\n    return {\"ready\": info[\"status\"] == \"COMPLETED\"}\n</code></pre>"},{"location":"key-concepts/extending-context/#5-concurrency-lifecycle","title":"5. Concurrency &amp; Lifecycle","text":"<p>If you expect your services are accessed by multiple agents concurrently, consider the designs: </p> <ul> <li>Lifecycle hooks: <code>start()</code> / <code>close()</code> are optional; call them from your app/server bootstrap.</li> <li>Shared access: use <code>self.critical()</code> to protect mutable shared state. Design your own mutex when scaling up. </li> <li>Per\u2011node context: call <code>self.ctx()</code> whenever you need <code>{run_id, graph_id, node_id}</code>.</li> <li>Async native: expose async APIs; if integrating queues, consider <code>asyncio.Queue</code>.</li> </ul>"},{"location":"key-concepts/extending-context/#6-common-service-patterns-examples","title":"6. Common Service Patterns (Examples)","text":"Scenario Suggested accessor What it abstracts Typical operations HPC / Training orchestration <code>context.trainer()</code> Submit/track jobs on Slurm/K8s/Ray <code>submit(spec)</code>, <code>inspect_job(id)</code>, <code>cancel(id)</code> External object storage <code>context.storage()</code> S3/GCS/MinIO buckets &amp; signed URLs <code>put(path)</code>, <code>get(uri)</code>, <code>sign(uri)</code>, <code>list(prefix)</code> Vendor API client <code>context.apiclient()</code> Rate\u2011limited, retried HTTP SDK <code>get/put/post</code>, <code>batch()</code>, <code>retry/backoff</code> In\u2011house AI models <code>context.models()</code> Local inference endpoints <code>embed(texts)</code>, <code>generate(prompt)</code> Materials DB / domain registry <code>context.materials()</code> Domain lookups &amp; cached tables <code>get_index(name)</code>, <code>search(filters)</code> Lineage export <code>context.lineage_store()</code> Mirror core provenance to BI/warehouse <code>export_run(run_id)</code>, <code>push(events)</code> <p>Pick names that are explicit in your org (e.g., <code>context.k8s_jobs()</code>, <code>context.minio()</code>). Avoid names that shadow built\u2011ins.</p>"},{"location":"key-concepts/extending-context/#summary","title":"Summary","text":"<ul> <li>External services add named capabilities to <code>context</code> without changing agent code.</li> <li>Built\u2011ins remain stable; extend via new names (no in\u2011place swaps).</li> <li>Register instances, not factories; services run on the main event loop.</li> <li>Use <code>self.ctx()</code> to fetch per\u2011node provenance on demand; protect shared state with <code>critical()</code> or your own lock design.</li> </ul>"},{"location":"key-concepts/introduction/","title":"AetherGraph \u2014 Key Concepts Overview","text":"<p>AetherGraph rethinks how agentic systems are built: graphs are agents, context is the runtime fabric, and execution is event\u2011driven. This document lays out the philosophy and the essential architecture so you can quickly reason about how it differs from typical frameworks\u2014and how to use it effectively.</p>"},{"location":"key-concepts/introduction/#1-introduction-graphs-context","title":"1. Introduction: Graphs + Context","text":"<p>AetherGraph departs from most agent frameworks in two fundamental ways:</p> <ol> <li>Every agent is a graph. You model behavior as a directed acyclic graph (DAG) with nodes that can expand dynamically (reactive) or be planned statically.</li> <li>Every node runs inside a <code>NodeContext</code>. The context is your per\u2011node control plane that exposes rich, injectable services.</li> </ol> <p>NodeContext services include (built\u2011ins plus anything you add):</p> <ul> <li>Channels (Slack, Console/Web, native UI \u2026) for I/O and interaction</li> <li>Artifacts (blob store) for large files and generated assets</li> <li>Memory (history + summaries + optional RAG)</li> <li>KV &amp; Logger for quick state and observability</li> <li>LLM / MCP / RAG bridges for model calls and tool use</li> <li>Visualization (only with Aethergraph UI) for automatic presentation of run plots</li> <li>Run Management for nested graph calling with and wihtout blocking</li> <li>Your custom services registered at runtime</li> </ul> <p>The system is pythonic, reactive, and extensible: author graphs directly in Python, then let context services handle communication, persistence, and orchestration details.</p>"},{"location":"key-concepts/introduction/#2-graphs-as-agents","title":"2. Graphs as Agents","text":"<p>AetherGraph unifies two modes under one mental model:</p>"},{"location":"key-concepts/introduction/#reactive-agent-graph_fn","title":"Reactive agent \u2014 <code>@graph_fn</code>","text":"<ul> <li>Executes immediately when called (behaves like an async Python function).</li> <li>Expands into an implicit DAG as tools run.</li> <li>Perfect for interactive, service\u2011rich, or conversational workflows.</li> </ul>"},{"location":"key-concepts/introduction/#static-agent-graphify","title":"Static agent \u2014 <code>@graphify</code>","text":"<ul> <li>First builds a concrete <code>TaskGraph</code>, then executes it via the scheduler.</li> <li>Ideal for stable, deterministic, or large\u2011scale workflows.</li> </ul> <p>One model, two tempos. Iterate fast with <code>@graph_fn</code>; snap to repeatable DAGs with <code>@graphify</code>.</p>"},{"location":"key-concepts/introduction/#3-context-as-the-runtime-fabric","title":"3. Context as the Runtime Fabric","text":"<p>Every node (and each <code>graph_fn</code> invocation) receives a NodeContext. Think of it as a scoped service locator with lifecycle hooks.</p> <ul> <li>Per\u2011node scoping \u2192 independent logging, persistence, messaging.</li> <li>Injectable services \u2192 bind built\u2011ins or your own with simple registration.</li> <li>Sidecar\u2011friendly \u2192 channel adapters and stores can run out\u2011of\u2011process.</li> </ul> <p>Common patterns:</p> <ul> <li><code>context.channel().ask_text()</code> to collect input from a user/UI.</li> <li><code>context.artifacts().save(...)</code> to persist files and structured outputs.</li> <li><code>context.memory().record(...)</code> to capture provenance and summaries.</li> <li><code>context.llm().chat(...)</code> to call your configured model provider.</li> </ul>"},{"location":"key-concepts/introduction/#4-eventdriven-execution-waits","title":"4. Event\u2011Driven Execution &amp; Waits","text":"<p>AetherGraph is event\u2011driven (not polling). Nodes suspend when waiting for input or an external event; the runtime persists and resumes continuations on demand so suspended nodes consume little to no CPU/RAM. Two primary wait styles are exposed:</p> <ul> <li>Cooperative waits \u2014 inline <code>await</code> for short, interactive flows where the coroutine remains in memory for low\u2011latency response.</li> <li>Dual\u2011stage waits \u2014 split a node into setup and resume phases; the continuation is durably stored so the process can free resources until an event triggers resume.</li> </ul> Mode Description CPU/RAM Ideal for Cooperative waits Inline <code>await</code> (e.g., <code>await context.channel().ask_text()</code>); coroutine stays in memory Minimal Small reactive loops, interactive sessions Dual\u2011stage waits Split node into setup/resume; continuation stored in a durable store Zero when paused Large DAGs, long\u2011lived pipelines Manual checkpoints User\u2011managed state snapshot/restore User\u2011managed Legacy integration, custom control <p>Dual\u2011stage waits + event routing enable massive concurrency with tiny footprint.</p>"},{"location":"key-concepts/introduction/#5-execution-scheduling","title":"5. Execution &amp; Scheduling","text":"<p>Each graph runs under a scheduler that selects ready nodes and respects your concurrency caps. Within a graph you get async DAG orchestration by default.</p>"},{"location":"key-concepts/introduction/#crossgraph-orchestration","title":"Cross\u2011graph orchestration","text":"<p>AetherGraph intentionally avoids a heavyweight orchestrator in the OSS core. For concurrent runs across multiple agents/graphs, use idiomatic Python:</p> <pre><code># Concurrent reactive runs\nawait asyncio.gather(\n    graph_fn_agent_a(...),   # a @graph_fn with async __call__\n    graph_fn_agent_b(...),\n)\n\n# Sequential orchestration\nawait graph_fn_agent_a(...)\nawait graph_fn_agent_b(...)\n</code></pre>"},{"location":"key-concepts/introduction/#6-extensibility-everywhere","title":"6. Extensibility Everywhere","text":"<p>Pure Python\u2014no DSLs, no codegen.</p> <ul> <li>Tools: <code>@tool</code> to define reusable, typed primitives.</li> <li>Graphs: <code>@graphify</code> to materialize a DAG for repeatable runs.</li> <li>Services: <code>register_context_service()</code> to inject your own capabilities.</li> <li>Adapters: extend ChannelBus to new transports (Slack, Web, PyQt, \u2026).</li> </ul> <p>Example (sketch)</p> <p>Define a <code>@tool</code>: <pre><code>@tool(name=\"analyze\", outputs=[\"val\"])\ndef analyze(x: int) -&gt; int:\n    return {\"val\": x * 2}\n</code></pre></p> <p>Define a <code>@graph_fn</code> (<code>@tool</code> is optional) <pre><code>@graph_fn(name=\"demo\")\nasync def demo(*, context):\n    y = await analyze(21)\n    await context.channel().send_text(f\"Answer: {y}\")\n</code></pre></p>"},{"location":"key-concepts/introduction/#7-how-aethergraph-differs","title":"7. How AetherGraph Differs","text":"Area AetherGraph Typical frameworks Runtime Unified Python, event\u2011driven Split control planes, mixed models Waits Zero\u2011CPU dual\u2011stage waits Blocking awaits, threads, polling Context Rich per\u2011node runtime fabric Globals, hidden singletons Composition Dynamic (<code>graph_fn</code>) + Static (<code>@graphify</code>) Usually one or the other Scheduling Async DAG scheduling per graph Step\u2011based loops, less reactive Extensibility Decorators + Python APIs Fixed plugin systems Provenance Built\u2011in memory/artifacts External or ad\u2011hoc"},{"location":"key-concepts/introduction/#8-takeaways","title":"8. Takeaways","text":"<p>AetherGraph bridges interactive research and deterministic automation:</p> <ul> <li>React first, then structure for production.</li> <li>Keep state, context, and scheduling where they belong\u2014with the nodes.</li> <li>Use event\u2011driven waits to scale without idle compute.</li> </ul>"},{"location":"key-concepts/other-services-overview/","title":"Other Services Overview","text":"<p>AetherGraph\u2019s context exposes a set of lightweight, composable runtime services that complement Channels, Artifacts, and Memory. Use them when your agent needs coordination, observability, or intelligent reasoning \u2014 while keeping your core logic pure Python.</p> <p>Philosophy: keep code idiomatic; reach for <code>context.&lt;service&gt;()</code> only when you need I/O, coordination, or intelligence.</p>"},{"location":"key-concepts/other-services-overview/#1-kv-ephemeral-coordination","title":"1. KV \u2014 Ephemeral Coordination","text":"<p>A minimal key\u2013value store for transient state, synchronization, and quick signals between nodes/agents.</p> <pre><code>@graph_fn(name=\"kv_demo\", outputs=[\"ok\"])\nasync def kv_demo(*, context):\n    kv = context.kv()\n    await kv.set(\"stage\", \"preflight\", ttl_s=300)\n    stage = await kv.get(\"stage\")  # \u2192 \"preflight\"\n    return {\"ok\": stage == \"preflight\"}\n</code></pre> <p>Why/When: feature flags, locks/counters, short-lived coordination.</p> <p>Default backend: in-memory KV </p>"},{"location":"key-concepts/other-services-overview/#2-logger-structured-logs-with-provenance","title":"2. Logger \u2014 Structured Logs with Provenance","text":"<p>Structured logging with <code>{run_id, graph_id, node_id}</code> automatically injected.</p> <pre><code>@graph_fn(name=\"log_demo\", outputs=[\"done\"])\nasync def log_demo(*, context):\n    log = context.logger()\n    log.info(\"starting\", extra={\"component\": \"ingest\"})\n    try:\n        ...\n        log.info(\"finished\", extra={\"component\": \"ingest\"})\n        return {\"done\": True}\n    except Exception:\n        log.exception(\"ingest failed\")\n        return {\"done\": False}\n</code></pre> <p>Why/When: lifecycle traces, metrics, error reporting.</p> <p>Default backend: Python <code>logging</code>.</p>"},{"location":"key-concepts/other-services-overview/#3-llm-unified-chat-embeddings-optional","title":"3. LLM \u2014 Unified Chat &amp; Embeddings (optional)","text":"<p>Provider-agnostic interface for chat/completions and embeddings. Requires configuration (API keys, profile).</p> <pre><code>@graph_fn(name=\"llm_demo\", outputs=[\"reply\"])\nasync def llm_demo(prompt: str, *, context):\n    llm = context.llm()\n    msg = await llm.chat([{\"role\": \"user\", \"content\": prompt}])\n    return {\"reply\": msg[\"content\"]}\n</code></pre> <p>Why/When: summarization, drafting, tool-use planning, embeddings.</p> <p>Backends: OpenAI, Anthropic, local, etc.</p>"},{"location":"key-concepts/other-services-overview/#4-rag-longterm-semantic-recall-optional","title":"4. RAG \u2014 Long\u2011Term Semantic Recall (optional)","text":"<p>Aethergraph provides basic RAG for fast prototype. Build searchable corpora from events/docs; retrieve or answer with citations. Requires an LLM for answering.</p> <pre><code>@graph_fn(name=\"rag_demo\", outputs=[\"answer\"])\nasync def rag_demo(q: str, *, context):\n    corpus_id: str = \"demo-corpus-inline\",\n    rag = context.rag()\n    docs = [\n        ... # you list of docs (text, markdown, or pdf path)\n    ]\n    result = await rag.upsert_docs(corpus_id, docs) # this will create and ingest into the corpus\n    ans = await rag.answer(corpus_id=corpus, question=q)\n    return {\"answer\": ans[\"answer\"]}\n</code></pre> <p>Why/When: semantic search, project recall, retrieval\u2011augmented QA.</p> <p>Default backend: Sqlite Vector DB, switchable to FAISS</p>"},{"location":"key-concepts/other-services-overview/#5-mcp-external-tool-bridges-optional","title":"5. MCP \u2014 External Tool Bridges (optional)","text":"<p>Connect to external tool servers over stdio/WebSocket/HTTP via Model Context Protocol (MCP).</p> <pre><code>@graph_fn(name=\"mcp_demo\", outputs=[\"hits\"])\nasync def mcp_demo(*, context):\n    ws = context.mcp(\"ws\")  # adapter name\n    res = await ws.call(\"search\", {\"q\": \"tolerance analysis\", \"k\": 3})\n    return {\"hits\": res.get(\"items\", [])}\n</code></pre> <p>Why/When: integrate non-Python tools or remote services with structured contracts.</p> <p>Default backend: HttpServer, WsServer, StdioServer</p>"},{"location":"key-concepts/other-services-overview/#takeaways","title":"Takeaways","text":"<ul> <li>Access everything through <code>context.&lt;service&gt;()</code> \u2014 no globals or custom wiring.</li> <li>KV and Logger work out of the box; LLM/RAG/MCP are optional and enabled by config.</li> <li>Backends are pluggable; you can move from local to managed services without changing agent code.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/","title":"Server (Sidecar) Overview","text":"<p>The AetherGraph sidecar is a lightweight process that boots your runtime services and exposes a tiny HTTP/WebSocket surface for adapters (Slack, Web, Telegram, \u2026) and continuations (event\u2011driven waits). With the server, you can: </p> <ul> <li>Real interactions: <code>ask_text/approval/files</code> from Slack/Web/Telegram and resume the run</li> <li>Centralized service wiring: artifacts, memory, kv, llm, rag, mcp, logger</li> <li>A shared control plane: health, upload hooks, progress streams, basic inspect</li> </ul> <p>In short: keep your agents plain Python; start the sidecar for I/O, resumability, and shared services. Always start the server before registering services and running agents</p>"},{"location":"key-concepts/server-start-sidecar/#quick-start","title":"Quick Start","text":"<pre><code>from aethergraph import start_server, stop_server\n\nurl = start_server()  # FastAPI + Uvicorn in a background thread\nprint(\"sidecar:\", url)  # Default to 127.0.0.1:8745\n\n# ... run @graph_fn / @graphify normally ...\n\nstop_server()  # optional (handy in tests/CI)\n</code></pre> <p>Tips</p> <ul> <li>If using Aethergraph UI, keep the same port everytime to retrieve the history and persist data. </li> <li>Start it once per process; reuse the base URL across adapters/UI.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/#what-start_server-does","title":"What <code>start_server()</code> Does","text":"<ol> <li>Load config &amp; workspace \u2014 resolve paths, secrets, and profiles; make the workspace if needed.</li> <li>Build &amp; register services \u2014 channels, artifacts (store/index), memory (hotlog/persistence/indices), kv, llm, rag, mcp, logger.</li> <li> <p>Expose endpoints \u2014</p> <ul> <li>Continuations: resume callbacks for <code>ask_text / ask_approval / ask_files</code></li> <li>Adapters: chat/events, uploads, progress streams</li> </ul> </li> <li> <p>Launch Uvicorn \u2014 run the app in a background thread and return the base URL.</p> </li> <li>Launch UI -- start the UI when possible for inspection of graph and agent runs. </li> </ol> <p>It is safe to use <code>start_server()</code> in Jupyter notebook When using UI, use CLI to serve the project with <code>--reload</code> for best DX.</p>"},{"location":"key-concepts/server-start-sidecar/#minimal-api","title":"Minimal API","text":"<p><code>start_server() -&gt; str</code></p> <p>Starts the sidecar in\u2011process and returns the base URL.</p> <p><code>stop_server() -&gt; None</code></p> <p>Stops the background server. Useful for teardown in tests/CI.</p> <p>Full API for server management is listed here</p>"},{"location":"key-concepts/static-graph-agent/","title":"Static Graphs with <code>@graphify</code>","text":"<p><code>@graphify</code> transforms a plain Python function into a graph builder. Instead of running immediately (like <code>@graph_fn</code>), it constructs a TaskGraph from <code>@tool</code> calls \u2014 a reusable, explicit DAG that you can run later.</p> <p>In short:</p> <ul> <li><code>@graph_fn</code> \u2192 executes now (reactive, dynamic)</li> <li><code>@graphify</code> \u2192 builds first, runs later (deterministic DAG)</li> </ul>"},{"location":"key-concepts/static-graph-agent/#1-what-is-a-static-graph","title":"1. What Is a Static Graph?","text":"<p>A static graph is a declarative DAG of tool nodes and dependencies. Each node is a <code>@tool</code> call; edges represent data flow or forced ordering. You build it once, then you can inspect, persist, visualize, and run it repeatedly.</p> <p>Why static? Repeatability, inspectability, resumability, and clear fan\u2011in/fan\u2011out. Static graphs shine for pipelines and reproducible experiments where determinism and analysis matter.</p>"},{"location":"key-concepts/static-graph-agent/#2-graphify-vs-graph_fn","title":"2. <code>@graphify</code> vs <code>@graph_fn</code>","text":"Aspect <code>@graph_fn</code> (Reactive) <code>@graphify</code> (Static) Execution Runs immediately when called Builds a DAG first; run later Composition Mix plain Python + <code>@tool</code> (implicit nodes) Only <code>@tool</code> nodes are valid steps Context usage Rich <code>context.*</code> available inline Need to wrap <code>context.*</code> in a tool to access it Inspectability Inspect implicit graph via <code>graph_fn.last_graph()</code> Full spec via <code>.io()</code>, <code>.spec()</code>, <code>TaskGraph.pretty()</code> Resumability Graph cannot be resumed (use memory to resume sementically) Graph execution can resume to last node without running from the start Best for Interactive agents, quick iteration Pipelines, reproducible runs, analytics <p>Note: Nested static\u2011graph calls are not supported at the moment (no calling one <code>@graphify</code> from another as a node). Compose via tools or run graphs separately.</p>"},{"location":"key-concepts/static-graph-agent/#3-define-and-build-a-graph","title":"3. Define and Build a Graph","text":"<pre><code>from aethergraph import graphify, tool\n\n@tool(outputs=[\"rows\"])       \ndef load_csv(path: str): ...\n\n@tool(outputs=[\"clean\"])      \ndef clean(rows): ...\n\n@tool(outputs=[\"model\"])      \ndef train(data): ...\n\n@tool(outputs=[\"uri\"])        \ndef report(model): ...\n\n@graphify(name=\"etl_train_report\", inputs=[\"csv_path\"], outputs=[\"uri\"]) \ndef etl_train_report(csv_path):\n    raw  = load_csv(path=csv_path)\n    tidy = clean(rows=raw.rows)\n    mod  = train(data=tidy.clean)\n    rep  = report(model=mod.model)\n    return {\"uri\": rep.uri}\n\nG = etl_train_report.build()     # \u2192 TaskGraph\n</code></pre> <p>All the input and output in the definition of a graph builder are <code>graph_ref</code> and <code>NodeHandler</code>, respectively. Accessing them in the graph builder will not display the actual value (e.g. you cannot access <code>raw[0]</code> or <code>raw.rows[0]</code> and pass it to the next tool). Always use a <code>@tool</code> to pack/uppack variables or integrate multiple small steps in a <code>@tool</code>. </p>"},{"location":"key-concepts/static-graph-agent/#control-ordering-without-data-edges","title":"Control ordering without data edges","text":"<p>Use <code>_after</code> to enforce sequence when there\u2019s no data dependency:</p> <pre><code>@tool(outputs=[\"ok\"])    \ndef fetch(): ...\n\n@tool(outputs=[\"done\"])  \ndef train(): ...\n\n@graphify(name=\"seq\", inputs=[], outputs=[\"done\"]) \ndef seq():\n    a = fetch()\n    b = train(_after=a)        # run `train` after `fetch`\n    return {\"done\": b.done}\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#referencing-tool-outputs-dot-vs-key","title":"Referencing Tool Outputs (dot vs. key)","text":"<p>Each <code>@tool</code> must declare its outputs. AetherGraph wraps the call in a handle whose fields mirror those names, so you can access them either as attributes or dict keys \u2014 both are equivalent.</p> <pre><code>@tool(outputs=[\"rows\"])\ndef load_csv(path: str):\n# must return a dict matching declared outputs\nreturn {\"rows\": parse_csv(path)}\n\n\n@tool(outputs=[\"clean\"])\ndef clean(rows):\nreturn {\"clean\": tidy(rows)}\n\n\n@graphify(name=\"etl\", inputs=[\"csv_path\"], outputs=[\"clean\"])\ndef etl(csv_path):\nraw = load_csv(path=csv_path)\n# Access either way; these are equivalent\ntidy1 = clean(rows=raw.rows) # dot access\ntidy2 = clean(rows=raw[\"rows\"]) # key access\nreturn {\"clean\": tidy1.clean}\n</code></pre> <p>Consistency matters: declared output names (e.g.,<code>outputs=[\"rows\"]</code>) must match the keys you return from the tool (e.g., <code>{\"rows\": ...}</code>). Mismatches raise clear build/runtime errors.</p> <p>Multiple outputs <pre><code>@tool(outputs=[\"mean\", \"std\"])\ndef stats(xs: list[float]):\nreturn {\"mean\": avg(xs), \"std\": stdev(xs)}\n\n\n@graphify(name=\"use_stats\", inputs=[\"xs\"], outputs=[\"m\"])\ndef use_stats(xs):\ns = stats(xs=xs)\nreturn {\"m\": s.mean} # or s[\"mean\"]\n</code></pre></p> <p>Think of tool calls as typed nodes whose declared outputs become fields on the node handle.</p>"},{"location":"key-concepts/static-graph-agent/#4-fanin-fanout-patterns","title":"4. Fan\u2011in / Fan\u2011out Patterns","text":"<pre><code>@tool(outputs=[\"v\"]) \ndef step(x: int): ...\n\n@tool(outputs=[\"z\"]) \ndef join(a, b): ...\n\n@graphify(name=\"fan\", inputs=[\"x1\", \"x2\"], outputs=[\"z\"]) \ndef fan(x1, x2):\n    a = step(x=\"x1\")  # fan\u2011out 1\n    b = step(x=\"x2\")  # fan\u2011out 2\n    j = join(a=a.v, b=b.v)  # fan\u2011in\n    return {\"z\": j.z}\n</code></pre> <p>Tips: you can use for loop to create fan-in and fan-out</p>"},{"location":"key-concepts/static-graph-agent/#5-run-a-built-graph","title":"5. Run a Built Graph","text":"<p>Run the materialized DAG with the runner (sync or async):</p> <pre><code>from aethergraph.runner import run, run_async\n\nresult = run(G, inputs={\"csv_path\": \"data/train.csv\"})\n# \u2192 {\"uri\": \"file://...\"}\n\n# Async form (e.g., inside another async function)\nfinal = await run_async(G, inputs={\"csv_path\": \"data/train.csv\"})\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#6-inspect-and-explore","title":"6. Inspect and Explore","text":"<p><code>@graphify</code> builders expose helpers for IO/signature and full spec:</p> <pre><code>sig  = etl_train_report.io()     # inputs/outputs signature\nspec = etl_train_report.spec()   # GraphSpec (nodes, edges, metadata)\n</code></pre> <p>Runtime helpers on <code>TaskGraph</code>:</p> <pre><code>print(G.pretty())                # human\u2011friendly table\nprint(G.ascii_overview())        # compact overview\n\n# Select / find nodes\nids     = G.list_nodes()                         # visible node_ids\nfirst_c = G.find_by_logic(\"clean\", first=True)  # by tool/logic name\nsome    = G.find_by_label(\"train\")              # by label\nsel     = G.select(\"@my_alias\")                 # mini\u2011DSL (@alias, #label, logic:, name:, id:, /regex/)\n\n# Topology &amp; subgraphs\norder   = G.topological_order()                  # raises if cycles\nup      = G.get_upstream_nodes(first_c)          # dependency closure\nsub     = G.get_subgraph_nodes(first_c)          # downstream closure\n</code></pre> <p>Export / visualize</p> <pre><code>dot = G.to_dot()                 # Graphviz DOT\n# G.visualize()                  # if enabled: render to file/viewer\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#key-points","title":"Key Points","text":"<ul> <li><code>@graphify</code> builds a DAG from <code>@tool</code> calls; you run it later.</li> <li>Use <code>_after</code> to force ordering without data edges.</li> <li>Fan\u2011out/fan\u2011in is natural with multiple <code>@tool</code> calls and a later join.</li> <li>Inspect via <code>.io()</code>, <code>.spec()</code>, <code>TaskGraph.pretty()</code>, <code>ascii_overview()</code>, <code>to_dot()</code>.</li> <li>No nested static graphs currently (don\u2019t call one <code>@graphify</code> from another as a node).</li> <li>For reactive agents, stick with <code>@graph_fn</code>; for pipelines, prefer <code>@graphify</code>.</li> </ul>"},{"location":"llm-setup/llm-setup/","title":"LLMs &amp; RAG \u2013 Central Setup Guide","text":"<p>This page is the one\u2011stop reference for configuring LLM profiles and RAG in AetherGraph. It covers environment\u2011based setup, runtime registration, and per\u2011call overrides.</p>"},{"location":"llm-setup/llm-setup/#1-quick-start-via-env","title":"1. Quick Start via <code>.env</code>","text":""},{"location":"llm-setup/llm-setup/#llm-settings-openai-example","title":"LLM Settings (OpenAI example)","text":"<pre><code># Enable LLM service\nAETHERGRAPH_LLM__ENABLED=true\n\n# Default profile \"default\"\nAETHERGRAPH_LLM__DEFAULT__PROVIDER=openai\nAETHERGRAPH_LLM__DEFAULT__MODEL=gpt-4o-mini\nAETHERGRAPH_LLM__DEFAULT__TIMEOUT=60\nAETHERGRAPH_LLM__DEFAULT__API_KEY=sk-...                # your OpenAI key\nAETHERGRAPH_LLM__DEFAULT__EMBED_MODEL=text-embedding-3-small\n</code></pre> <p>This makes <code>context.llm()</code> (or <code>context.llm(profile=\"default\")</code>) available everywhere.</p>"},{"location":"llm-setup/llm-setup/#extra-profiles-example-google-gemini","title":"Extra profiles (example: Google Gemini)","text":"<p><pre><code># Profile name: GEMINI\nAETHERGRAPH_LLM__PROFILES__GEMINI__PROVIDER=google\nAETHERGRAPH_LLM__PROFILES__GEMINI__MODEL=gemini-2.5-flash-lite\nAETHERGRAPH_LLM__PROFILES__GEMINI__TIMEOUT=60\nAETHERGRAPH_LLM__PROFILES__GEMINI__API_KEY=AIzaSy...     # your Google key\nAETHERGRAPH_LLM__PROFILES__GEMINI__EMBED_MODEL=text-embedding-004\n</code></pre> This makes <code>context.llm(profile=\"gemeni\")</code> available everywhere.</p> <p>You can add as many profiles as you like (Anthropic, LM Studio, Azure OpenAI, etc.).</p>"},{"location":"llm-setup/llm-setup/#2-runtime-registration-no-restart-needed","title":"2. Runtime Registration (no restart needed)","text":"<p>Use these helpers from <code>aethergraph.runtime</code> when you want to configure LLMs at runtime (e.g., notebooks, scripts):</p> <pre><code>from aethergraph.server import start as start_server\nfrom aethergraph.runtime import register_llm_client, set_rag_llm_client\n\n# 1) Start the sidecar so services are wired\nurl = start_server(port=0)\n\n# 2) Register multiple LLM profiles\nopenai_client = register_llm_client(\n    profile=\"my_openai\",\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    api_key=\"sk-...\",\n)\n\nregister_llm_client(\n    profile=\"my_lmstudio\",\n    provider=\"lmstudio\",\n    model=\"qwen/qwen2.5-vl-7b\",\n    base_url=\"http://localhost:1234/v1\",   # LM Studio uses /v1\n)\n\nregister_llm_client(\n    profile=\"my_anthropic\",\n    provider=\"anthropic\",\n    model=\"claude-3\",\n    api_key=\"sk-ant-...\",\n)\n\nregister_llm_client(\n    profile=\"my_gemini\",\n    provider=\"google\",\n    model=\"gemini-2.5-flash-lite\",\n    api_key=\"AIzaSy...\",\n)\n\n# 3) RAG LLM client (defaults to \"default\" profile if not set)\nset_rag_llm_client(client=openai_client)\n# or create by parameters\nset_rag_llm_client(\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    embed_model=\"text-embedding-3-small\",\n    api_key=\"sk-...\",\n)\n</code></pre>"},{"location":"llm-setup/llm-setup/#3-inline-overrides-in-code","title":"3. Inline Overrides in Code","text":"<p>When you need to switch providers/models on the fly, use the inline API on <code>NodeContext</code>:</p> <pre><code># Get existing profile (no overrides)\nllm_client = context.llm(profile=\"default\")\n\n# Or create/update a profile inline\nllm_client = context.llm(\n    profile=\"temp_profile\",\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    api_key=\"sk-...\",\n    timeout=60,\n)\n\n# Quick credential swap\ncontext.llm_set_key(provider=\"openai\", model=\"gpt-4o-mini\", api_key=\"sk-...\", profile=\"default\")\n</code></pre>"},{"location":"llm-setup/llm-setup/#4-rag-setup","title":"4. RAG Setup","text":""},{"location":"llm-setup/llm-setup/#defaults","title":"Defaults","text":"<ul> <li>The RAG service uses the default LLM profile unless you explicitly set a RAG client.</li> <li>Ensure the profile used by RAG has a valid <code>embed_model</code>.</li> </ul>"},{"location":"llm-setup/llm-setup/#env-rag-settings","title":"<code>.env</code> RAG Settings","text":"<pre><code># RAG Settings\nAETHERGRAPH_RAG__BACKEND=faiss   # e.g., faiss or sqlite (faiss may need extra deps)\nAETHERGRAPH_RAG__DIM=1536        # match your embedding dimensionality\n</code></pre> <p>If FAISS isn\u2019t available, the system will auto\u2011fallback to SQLite vector index.</p>"},{"location":"llm-setup/llm-setup/#runtime-rag-configuration","title":"Runtime RAG Configuration","text":"<p>Use these helpers from <code>aethergraph.runtime</code>:</p> <pre><code>from aethergraph.runtime import set_rag_llm_client, set_rag_index_backend\n\n# Choose the LLM for RAG (client or by params)\nrag_client = set_rag_llm_client(client=openai_client)\n# or\nrag_client = set_rag_llm_client(\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    embed_model=\"text-embedding-3-small\",\n    api_key=\"sk-...\",\n)\n\n# Configure the index backend (backend=\"sqlite\" | \"faiss\")\nset_rag_index_backend(backend=\"faiss\", index_path=None, dim=1536)\n</code></pre>"},{"location":"llm-setup/llm-setup/#5-minimal-usage-example","title":"5. Minimal Usage Example","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"llm.multiple.profile.demo\")\nasync def llm_multiple_profile_demo(profile: str, *, context: NodeContext):\n    llm_client = context.llm(profile=profile)\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a concise and helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"In one sentence, what is an attention layer in neural networks?\"},\n    ]\n    text, usage = await llm_client.chat(messages)\n\n    # GPT\u20115 models support optional reasoning controls, e.g.:\n    # text, usage = await llm_client.chat(messages, reasoning_effort=\"low\")\n\n    return {\"profile\": profile, \"answer\": text, \"usage\": usage}\n</code></pre>"},{"location":"llm-setup/llm-setup/#6-tips-notes","title":"6. Tips &amp; Notes","text":"<ul> <li>Security: Keep API keys in <code>.env</code> or secret managers; avoid hard\u2011coding in scripts.</li> <li>Profiles: Name them by purpose (e.g., <code>summarize</code>, <code>vision</code>, <code>rag</code>) to keep code expressive.</li> <li>Embedding dim: Ensure <code>AETHERGRAPH_RAG__DIM</code> matches your selected <code>embed_model</code>.</li> <li>LM Studio / self\u2011hosted: set <code>base_url</code> (and deployment name for Azure) as required by your provider.</li> </ul>"},{"location":"recipes/data-analysis-loop/","title":"Recipe: Iterative Data Analysis","text":"<ul> <li>User asks for analysis \u2192 generate code</li> <li>Run code; store figures &amp; tables in <code>artifacts()</code></li> <li>Record metrics in <code>memory()</code> and summarize at the end</li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"recipes/job-submit-poll/","title":"Recipe: Submit + Poll + Notify","text":"<ul> <li>Submit a long-running job</li> <li>Poll status and send progress via <code>channel()</code></li> <li>On failure, ask user to retry or stop; save logs to <code>artifacts()</code></li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"recipes/memory-rag-mini/","title":"Recipe: Mini Memory\u2011RAG","text":"<ul> <li>Ingest notes as <code>memory().record(kind=\"note\", data=...)</code></li> <li>Simple retrieval via <code>memory().recent()/query()</code> into <code>llm().chat()</code></li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"reference/config/","title":"Config Keys","text":"<ul> <li><code>AETH_PORT</code> \u2014 server port</li> <li><code>LOG_LEVEL</code> \u2014 logging level (e.g., INFO)</li> <li><code>MODEL_NAME</code> \u2014 default LLM</li> <li><code>SLACK_*</code> \u2014 Slack integration (bot token, signing secret)</li> </ul> <p>(Expand with your actual config schema.)</p>"},{"location":"reference/context-adhoc/","title":"Using <code>context</code> outside a graph (Ad\u2011hoc sessions)","text":"<p>Sometimes you want a fully\u2011featured <code>NodeContext</code> without constructing a graph\u2014e.g., quick scripts, notebooks, data prep, or admin tasks. AetherGraph exposes an async helper you can import from <code>aethergraph.runtime</code>:</p> <ul> <li><code>open_session(...)</code> \u2013 async context manager that yields a temporary, fully wired <code>NodeContext</code> and cleans up afterward.</li> </ul> <p>These sessions are \u201csingle\u2011node\u201d and aren\u2019t scheduled like a graph run, but they provide the same services: <code>context.llm()</code>, <code>context.artifacts()</code>, <code>context.memory()</code>, <code>context.kv()</code>, <code>context.logger()</code>, <code>context.rag</code>, <code>context.mcp()</code>, and any custom services you\u2019ve registered.</p>"},{"location":"reference/context-adhoc/#quick-start","title":"Quick Start","text":"<pre><code>from aethergraph.runtime import open_session\n\n# Recommended: context manager form (handles cleanup)\nasync with open_session(run_id=\"adhoc-demo\", graph_id=\"adhoc\", node_id=\"adhoc\") as context:\n    context.logger().info(\"hello from adhoc\")\n    txt, _ = await context.llm().chat([\n        {\"role\": \"user\", \"content\": \"Give me a haiku about photons.\"}\n    ])\n    print(txt)\n</code></pre>"},{"location":"reference/context-adhoc/#common-parameters","title":"Common parameters","text":"<ul> <li><code>run_id: str | None</code> \u2014 logical session/run identifier (default: auto\u2011generated)</li> <li><code>graph_id: str</code> \u2014 namespace label for this session (default: <code>\"adhoc\"</code>)</li> <li><code>node_id: str</code> \u2014 node label within the session (default: <code>\"adhoc\"</code>)</li> <li><code>**rt_overrides</code> \u2014 advanced runtime knobs you can pass through (e.g., <code>max_concurrency</code>, service injections)</li> </ul> <p>Tip: Start your sidecar server (<code>start_server(...)</code>) first if you plan to use external channels, resumable waits, or any service that relies on the sidecar\u2019s wiring.</p>"},{"location":"reference/context-adhoc/#api-reference","title":"API Reference","text":"open_session(run_id, graph_id, node_id, **rt_overrides) <p>Open an ad-hoc session context for advanced or scripting use.</p> <p>This asynchronous context manager yields a temporary context that mimics a <code>NodeContext</code> without requiring a real graph run. It is intended for advanced scenarios where a lightweight, ephemeral execution environment is needed, such as scripting, testing, or prototyping.</p> <p>Examples:</p> <p>Basic usage to open an ad-hoc session: <pre><code>async with open_session() as ctx:\n    # Use ctx as you would a NodeContext\n    ...\n</code></pre></p> <p>Overriding runtime parameters: <pre><code>async with open_session(graph_id=\"mygraph\", node_id=\"customnode\", foo=\"bar\") as ctx:\n    ...\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str | None</code> <p>Optional unique identifier for the run. If None, a random or default value is used.</p> <code>None</code> <code>graph_id</code> <code>str</code> <p>Identifier for the graph context. Defaults to \"adhoc\".</p> <code>'adhoc'</code> <code>node_id</code> <code>str</code> <p>Identifier for the node context. Defaults to \"adhoc\".</p> <code>'adhoc'</code> <code>**rt_overrides</code> <p>Arbitrary keyword arguments to override runtime context parameters.</p> <code>{}</code> <p>Yields:</p> Name Type Description <code>NodeContext</code> <p>An ad-hoc context object suitable for advanced scripting or testing.</p> Note <p>This context does not persist state or artifacts beyond its lifetime. Use only for non-production, ephemeral tasks.</p> build_adhoc_context(run_id, graph_id, node_id, **rt_overrides) <p>Build an ad-hoc execution context for running a single node outside of a scheduled graph. This function creates a minimal runtime environment suitable for quick, one-off executions, such as testing or interactive exploration. It generates a temporary run and graph context, instantiates an ad-hoc node, and returns a node-specific execution context. Examples:     Basic usage with default parameters:     <pre><code>node_ctx = await build_adhoc_context()\n</code></pre>     Customizing the run and session IDs:     <pre><code>node_ctx = await build_adhoc_context(run_id=\"test-run\", session_id=\"dev-session\")\n</code></pre>     Overriding runtime parameters:     <pre><code>node_ctx = await build_adhoc_context(max_concurrency=4)\n</code></pre> Args:     run_id: Optional string to uniquely identify this run. If not provided,         a random ID is generated.     session_id: Optional string to associate this context with a session.     graph_id: Identifier for the graph. Defaults to <code>\"adhoc\"</code>.     node_id: Identifier for the node. Defaults to <code>\"adhoc\"</code>.     **rt_overrides: Additional runtime overrides, such as <code>max_concurrency</code>. Returns:     NodeExecutionContext: The execution context for the ad-hoc node, ready for use.</p>"},{"location":"reference/context-adhoc/#notes-gotchas","title":"Notes &amp; gotchas","text":"<ul> <li><code>open_session()</code> gives you a real <code>NodeContext</code> bound to a throwaway run/graph/node. Treat it like a normal node.</li> <li>If you rely on external adapters (Slack/Telegram/Web UI) or resumable waits, ensure the sidecar server is running.</li> <li>For long\u2011lived scripting, prefer <code>open_session()</code>; reach for <code>build_adhoc_context()</code> only when you need total control.</li> <li>Services which are not configured (e.g., RAG, MCP) will raise a clear <code>RuntimeError</code>. Wire them via your runtime config or server startup.</li> </ul>"},{"location":"reference/context-artifacts/","title":"<code>context.artifacts()</code> \u2013 ArtifactFacade API Reference","text":"<p>The <code>ArtifactFacade</code> wraps an <code>AsyncArtifactStore</code> (persistence) and an <code>AsyncArtifactIndex</code> (search/metadata) and automatically indexes artifacts you create within a node/run.</p>"},{"location":"reference/context-artifacts/#1-save-api","title":"1. Save API","text":"save_file(path, *, kind, labels, ...) <p>Save an existing file and index it.</p> <p>This method saves a file to the artifact store, associates it with the current execution context, and records it in the artifact index. It supports adding metadata such as labels, metrics, and a suggested URI for logical organization.</p> <p>Examples:</p> <p>Basic usage with a file path: <pre><code>artifact = await context.artifacts().save_file(\n    path=\"/tmp/output.txt\",\n    kind=\"text\",\n    labels={\"category\": \"logs\"},\n)\n</code></pre></p> <p>Saving a file with a custom name and pinning it: <pre><code>artifact = await context.artifacts().save_file(\n    path=\"/tmp/data.csv\",\n    kind=\"dataset\",\n    name=\"data_backup.csv\",\n    pin=True,\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local file path to save.</p> required <code>kind</code> <code>str</code> <p>A string representing the artifact type (e.g., \"text\", \"dataset\").</p> required <code>labels</code> <code>dict | None</code> <p>A dictionary of metadata labels to associate with the artifact.</p> <code>None</code> <code>metrics</code> <code>dict | None</code> <p>A dictionary of numerical metrics to associate with the artifact.</p> <code>None</code> <code>suggested_uri</code> <code>str | None</code> <p>A logical URI for the artifact (e.g., \"s3://bucket/file\").</p> <code>None</code> <code>name</code> <code>str | None</code> <p>A custom name for the artifact, used as the <code>filename</code> label.</p> <code>None</code> <code>pin</code> <code>bool</code> <p>A boolean indicating whether to pin the artifact.</p> <code>False</code> <code>cleanup</code> <code>bool</code> <p>A boolean indicating whether to delete the local file after saving.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Artifact</code> <code>Artifact</code> <p>The saved <code>Artifact</code> object containing metadata and identifiers.</p> Notes <p>The <code>name</code> parameter is used to set the <code>filename</code> label for the artifact. If both <code>name</code> and <code>suggested_uri</code> are provided, <code>name</code> takes precedence for the filename.</p> save_text(payload, *, ...) <p>This method stages the text as a temporary <code>.txt</code> file, writes the payload, and persists it as an artifact with associated metadata. It is accessed via <code>context.artifacts().save_text(...)</code>.</p> <p>Examples:</p> <p>Basic usage to save a text artifact: <pre><code>await context.artifacts().save_text(\"Hello, world!\")\n</code></pre></p> <p>Saving with custom metadata and logical filename: <pre><code>await context.artifacts().save_text(\n    \"Experiment results\",\n    name=\"results.txt\",\n    labels={\"experiment\": \"A1\"},\n    metrics={\"accuracy\": 0.98},\n    pin=True\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>str</code> <p>The text content to be saved as an artifact.</p> required <code>suggested_uri</code> <code>str | None</code> <p>Optional logical URI for the artifact. If not provided,</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional logical filename for the artifact.</p> <code>None</code> <code>kind</code> <code>str</code> <p>The artifact kind, defaults to <code>\"text\"</code>.</p> <code>'text'</code> <code>labels</code> <code>dict | None</code> <p>Optional dictionary of string labels for categorization.</p> <code>None</code> <code>metrics</code> <code>dict | None</code> <p>Optional dictionary of numeric metrics for tracking.</p> <code>None</code> <code>pin</code> <code>bool</code> <p>If True, pins the artifact for retention.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Artifact</code> <code>Artifact</code> <p>The fully persisted <code>Artifact</code> object containing metadata and storage reference.</p> save_json(payload, *, ...) <p>Save a JSON payload as an artifact with full context metadata.</p> <p>This method stages the JSON data as a temporary <code>.json</code> file, writes the payload, and persists it as an artifact with associated metadata. It is accessed via <code>context.artifacts().save_json(...)</code>.</p> <p>Examples:</p> <p>Basic usage to save a JSON artifact: <pre><code>await context.artifacts().save_json({\"foo\": \"bar\", \"count\": 42})\n</code></pre></p> <p>Saving with custom metadata and logical filename: <pre><code>await context.artifacts().save_json(\n    {\"results\": [1, 2, 3]},\n    name=\"results.json\",\n    labels={\"experiment\": \"A1\"},\n    metrics={\"accuracy\": 0.98},\n    pin=True\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>dict</code> <p>The JSON-serializable dictionary to be saved as an artifact.</p> required <code>suggested_uri</code> <code>str | None</code> <p>Optional logical URI for the artifact. If not provided, the <code>name</code> will be used if available.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional logical filename for the artifact.</p> <code>None</code> <code>kind</code> <code>str</code> <p>The artifact kind, defaults to <code>\"json\"</code>.</p> <code>'json'</code> <code>labels</code> <code>dict | None</code> <p>Optional dictionary of string labels for categorization.</p> <code>None</code> <code>metrics</code> <code>dict | None</code> <p>Optional dictionary of numeric metrics for tracking.</p> <code>None</code> <code>pin</code> <code>bool</code> <p>If True, pins the artifact for retention.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Artifact</code> <code>Artifact</code> <p>The fully persisted <code>Artifact</code> object containing metadata and storage reference.</p> writer(*, kind, planned_ext, ...) <p>Async context manager for streaming artifact writes.</p> <p>This method yields a writer object that supports:</p> <ul> <li><code>writer.write(bytes)</code> for streaming data</li> <li><code>writer.add_labels(...)</code> to attach metadata</li> <li><code>writer.add_metrics(...)</code> to record metrics</li> </ul> <p>After the context exits, the writer's artifact is finalized and recorded in the index. Accessed via <code>context.artifacts().writer(...)</code>.</p> <p>Examples:</p> <p>Basic usage to stream a file artifact: <pre><code>async with context.artifacts().writer(kind=\"binary\") as w:\n    await w.write(b\"some data\")\n</code></pre></p> <p>Streaming with custom file extension and pinning: <pre><code>async with context.artifacts().writer(\n    kind=\"log\",\n    planned_ext=\".log\",\n    pin=True\n) as w:\n    await w.write(b'Log entry 1\\n')\n    w.add_labels({\"source\": 'app'})\n    w.add_metrics({\"lines\": 1})\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>The artifact type (e.g., \"binary\", \"log\", \"text\").</p> required <code>planned_ext</code> <code>str | None</code> <p>Optional file extension for the staged artifact (e.g., \".txt\").</p> <code>None</code> <code>pin</code> <code>bool</code> <p>If True, pins the artifact for retention.</p> <code>False</code> <p>Returns:</p> Type Description <code>AsyncIterator[Any]</code> <p>AsyncIterator[Any]: Yields a writer object for streaming data and metadata.</p>"},{"location":"reference/context-artifacts/#2-search-api","title":"2. Search API","text":"get_by_id(artifact_id) <p>Retrieve a single artifact by its unique identifier.</p> <p>This asynchronous method queries the configured artifact index for the specified <code>artifact_id</code>. If the index is not set up, a <code>RuntimeError</code> is raised. The method is typically accessed via <code>context.artifacts().get_by_id(...)</code>.</p> <p>Examples:</p> <p>Fetching an artifact by ID: <pre><code>artifact = await context.artifacts().get_by_id(\"artifact_123\")\nif artifact:\n    print(artifact.name)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>artifact_id</code> <code>str</code> <p>The unique string identifier of the artifact to retrieve.</p> required <p>Returns:</p> Type Description <code>Artifact | None</code> <p>Artifact | None: The matching <code>Artifact</code> object if found, otherwise <code>None</code>.</p> list(*, view) <p>List artifacts scoped to the current run, graph, or node.</p> <p>This method provides a quick way to enumerate artifacts associated with the current execution context. The <code>view</code> parameter controls the scope of the listing:</p> <ul> <li><code>\"node\"</code>: artifacts for the current run, graph, and node</li> <li><code>\"graph\"</code>: artifacts for the current run and graph</li> <li><code>\"run\"</code>: artifacts for the current run (default)</li> <li><code>\"all\"</code>: all artifacts (tenant-scoped if applicable)</li> </ul> <p>Examples:</p> <p>List all artifacts for the current run: <pre><code>artifacts = await context.artifacts().list()\nfor a in artifacts:\n    print(a.artifact_id, a.kind)\n</code></pre></p> <p>List artifacts for the current node: <pre><code>node_artifacts = await context.artifacts().list(view=\"node\")\n</code></pre></p> <p>List all tenant-visible artifacts: <pre><code>all_artifacts = await context.artifacts().list(view=\"all\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>view</code> <code>ArtifactView</code> <p>The scope for listing artifacts. Must be one of: <code>\"node\"</code>, <code>\"graph\"</code>, <code>\"run\"</code>, or <code>\"all\"</code>.</p> <code>'run'</code> <p>Returns:</p> Type Description <code>list[Artifact]</code> <p>list[Artifact]: A list of <code>Artifact</code> objects matching the specified scope.</p> search(*, kind, labels, metric, ...) <p>Search for artifacts with flexible scoping and filtering.</p> <p>This method allows you to query artifacts by type, labels, metrics, and other criteria. It automatically applies view-based scoping and merges any additional scope labels provided. The search is dispatched to the underlying index.</p> <p>Examples:</p> <p>Basic usage to find all artifacts of a given kind: <pre><code>results = await context.artifacts().search(kind=\"model\")\n</code></pre></p> <p>Searching with specific labels and metric optimization: <pre><code>results = await context.artifacts().search(\n    kind=\"dataset\",\n    labels={\"domain\": \"finance\"},\n    metric=\"accuracy\",\n    mode=\"max\",\n    limit=10,\n)\n</code></pre> Extending scope with extra labels: <pre><code>results = await context.artifacts().search(\n    extra_scope_labels={\"project\": \"alpha\"}\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str | None</code> <p>The type of artifact to search for (e.g., \"model\", \"dataset\").</p> <code>None</code> <code>labels</code> <code>dict[str, str] | None</code> <p>Dictionary of label key-value pairs to filter artifacts.</p> <code>None</code> <code>metric</code> <code>str | None</code> <p>Name of a metric to optimize (e.g., \"accuracy\").</p> <code>None</code> <code>mode</code> <code>Literal['max', 'min'] | None</code> <p>Optimization mode for the metric, either \"max\" or \"min\".</p> <code>None</code> <code>view</code> <code>ArtifactView</code> <p>The artifact view context, which determines default scoping.</p> <code>'run'</code> <code>extra_scope_labels</code> <code>dict[str, str] | None</code> <p>Additional labels to further scope the search.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Maximum number of results to return.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Artifact]</code> <p>list[Artifact]: A list of matching <code>Artifact</code> objects.</p> Notes <ul> <li>The <code>view</code> parameter controls the base scoping of the search. Additional labels provided     in <code>extra_scope_labels</code> are merged on top of the view-based labels.</li> <li>If both <code>labels</code> and <code>extra_scope_labels</code> are provided, they are combined for filtering.</li> </ul> best(*, kind, metric, ...) <p>Retrieve the best artifact by optimizing a specified metric.</p> <p>This method searches for artifacts of a given kind and returns the one that maximizes or minimizes the specified metric, scoped by the provided view and filters. It is accessed via <code>context.artifacts().best(...)</code>.</p> <p>Examples:</p> <p>Find the best model by accuracy for the current run: <pre><code>best_model = await context.artifacts().best(\n    kind=\"model\",\n    metric=\"accuracy\",\n    mode=\"max\"\n)\n</code></pre></p> <p>Find the lowest-loss dataset for the current graph: <pre><code>best_dataset = await context.artifacts().best(\n    kind=\"dataset\",\n    metric=\"loss\",\n    mode=\"min\",\n    view=\"graph\"\n)\n</code></pre></p> <p>Apply additional label filters: <pre><code>best_artifact = await context.artifacts().best(\n    kind=\"model\",\n    metric=\"f1_score\",\n    mode=\"max\",\n    filters={\"domain\": \"finance\"}\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>The type of artifact to search for (e.g., \"model\", \"dataset\").</p> required <code>metric</code> <code>str</code> <p>The metric name to optimize (e.g., \"accuracy\", \"loss\").</p> required <code>mode</code> <code>Literal['max', 'min']</code> <p>Optimization mode, either <code>\"max\"</code> for highest or <code>\"min\"</code> for lowest value.</p> required <code>view</code> <code>ArtifactView</code> <p>The artifact view context, which determines default scoping. Must be one of <code>\"node\"</code>, <code>\"graph\"</code>, <code>\"run\"</code>, or <code>\"all\"</code>.</p> <code>'run'</code> <code>filters</code> <code>dict[str, str] | None</code> <p>Additional label filters to further restrict the search.</p> <code>None</code> <p>Returns:</p> Type Description <code>Artifact | None</code> <p>Artifact | None: The best matching <code>Artifact</code> object, or <code>None</code> if no match is found.</p> pin(artifact_id, pinned) <p>Mark or unmark an artifact as pinned for retention.</p> <p>This asynchronous method updates the <code>pinned</code> status of the specified artifact in the artifact index. Pinning an artifact ensures it is retained and not subject to automatic cleanup. It is accessed via <code>context.artifacts().pin(...)</code>.</p> <p>Examples:</p> <p>Pin an artifact for retention: <pre><code>await context.artifacts().pin(\"artifact_123\", pinned=True)\n</code></pre></p> <p>Unpin an artifact to allow cleanup: <pre><code>await context.artifacts().pin(\"artifact_456\", pinned=False)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>artifact_id</code> <code>str</code> <p>The unique string identifier of the artifact to update.</p> required <code>pinned</code> <code>bool</code> <p>Boolean indicating whether to pin (<code>True</code>) or unpin (<code>False</code>) the artifact.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"reference/context-artifacts/#3-stageingest-api","title":"3. Stage/Ingest API","text":"stage_path(ext) <p>Plan a staging file path for artifact creation.</p> <p>This method requests a temporary file path from the underlying artifact store, suitable for staging a new artifact. The file extension can be specified to guide downstream handling (e.g., \".txt\", \".json\").</p> <p>Examples:</p> <p>Stage a temporary text file: <pre><code>staged_path = await context.artifacts().stage_path(\".txt\")\n</code></pre></p> <p>Stage a file with a custom extension: <pre><code>staged_path = await context.artifacts().stage_path(\".log\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>ext</code> <code>str</code> <p>Optional file extension for the staged file (e.g., \".txt\", \".json\").</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The planned staging file path as a string.</p> stage_dir(suffix) <p>Plan a staging directory for artifact creation.</p> <p>This method requests a temporary directory path from the underlying artifact store, suitable for staging a directory artifact. The suffix can be used to distinguish different staging contexts.</p> <p>Examples:</p> <p>Stage a temporary directory: <pre><code>staged_dir = await context.artifacts().stage_dir()\n</code></pre></p> <p>Stage a directory with a custom suffix: <pre><code>staged_dir = await context.artifacts().stage_dir(\"_images\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>suffix</code> <code>str</code> <p>Optional string to append to the directory name for uniqueness.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The planned staging directory path as a string.</p> ingest_file(staged_path, *, kind, ...) <p>Ingest a staged file as an artifact and record it in the index.</p> <p>This method takes a file that has been staged locally, persists it in the artifact store, and records its metadata in the artifact index. It supports adding labels, metrics, and logical URIs for organization.</p> <p>Examples:</p> <p>Ingest a staged model file: <pre><code>artifact = await context.artifacts().ingest_file(\n    staged_path=\"/tmp/model.bin\",\n    kind=\"model\",\n    labels={\"domain\": \"vision\"},\n    pin=True\n)\n</code></pre></p> <p>Ingest with a suggested URI: <pre><code>artifact = await context.artifacts().ingest_file(\n    staged_path=\"/tmp/data.csv\",\n    kind=\"dataset\",\n    suggested_uri=\"s3://bucket/data.csv\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>staged_path</code> <code>str</code> <p>The local path to the staged file.</p> required <code>kind</code> <code>str</code> <p>The artifact type (e.g., \"model\", \"dataset\").</p> required <code>labels</code> <code>dict | None</code> <p>Optional dictionary of metadata labels.</p> <code>None</code> <code>metrics</code> <code>dict | None</code> <p>Optional dictionary of numeric metrics.</p> <code>None</code> <code>suggested_uri</code> <code>str | None</code> <p>Optional logical URI for the artifact.</p> <code>None</code> <code>pin</code> <code>bool</code> <p>If True, pins the artifact for retention.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Artifact</code> <code>Artifact</code> <p>The fully persisted <code>Artifact</code> object with metadata and identifiers.</p> Notes <p>The <code>staged_path</code> must point to an existing file. The method will handle cleanup of the staged file if configured in the underlying store. If you already have a file at a specific URI (e.g. \"s3://bucket/file\" or local file path), consider using <code>save_file</code> instead.</p> ingest_dir(staged_path, **kwargs) <p>Ingest a staged directory as a directory artifact and record it in the index.</p> <p>This method takes a directory that has been staged locally, persists its contents in the artifact store (optionally creating a manifest or archive), and records its metadata in the artifact index. Additional keyword arguments are passed to the store's ingest logic.</p> <p>Examples:</p> <p>Ingest a staged directory with manifest: <pre><code>artifact = await context.artifacts().ingest_dir(\n    staged_dir=\"/tmp/output_dir\",\n    kind=\"directory\",\n    labels={\"type\": \"images\"}\n)\n</code></pre></p> <p>Ingest with custom metrics: <pre><code>artifact = await context.artifacts().ingest_dir(\n    staged_dir=\"/tmp/logs\",\n    kind=\"log_dir\",\n    metrics={\"file_count\": 12}\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>staged_dir</code> <code>str</code> <p>The local path to the staged directory.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for artifact metadata (e.g., kind, labels, metrics).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Artifact</code> <code>Artifact</code> <p>The fully persisted <code>Artifact</code> object with metadata and identifiers.</p>"},{"location":"reference/context-artifacts/#4-load-api","title":"4. Load API","text":"load_bytes_by_id(artifact_id) <p>Load raw bytes for a file-like artifact by its unique identifier.</p> <p>This asynchronous method retrieves the artifact metadata from the index using the provided <code>artifact_id</code>, then loads the underlying bytes from the artifact store. It is accessed via <code>context.artifacts().load_bytes_by_id(...)</code>.</p> <p>Examples:</p> <p>Basic usage to load bytes for an artifact: <pre><code>data = await context.artifacts().load_bytes_by_id(\"artifact_123\")\n</code></pre></p> <p>Handling missing artifacts: <pre><code>try:\n    data = await context.artifacts().load_bytes_by_id(\"artifact_456\")\nexcept FileNotFoundError:\n    print(\"Artifact not found.\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>artifact_id</code> <code>str</code> <p>The unique string identifier of the artifact to retrieve.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The raw byte content of the artifact.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the artifact is not found or missing a URI.</p> load_text_by_id(artifact_id, *, ...) <p>Load the text content of an artifact by its unique identifier.</p> <p>This asynchronous method retrieves the raw bytes for the specified <code>artifact_id</code> and decodes them into a string using the provided encoding. It is accessed via <code>context.artifacts().load_text_by_id(...)</code>.</p> <p>Examples:</p> <p>Basic usage to load text from an artifact: <pre><code>text = await context.artifacts().load_text_by_id(\"artifact_123\")\nprint(text)\n</code></pre></p> <p>Loading with custom encoding and error handling: <pre><code>text = await context.artifacts().load_text_by_id(\n    \"artifact_456\",\n    encoding=\"utf-16\",\n    errors=\"ignore\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>artifact_id</code> <code>str</code> <p>The unique string identifier of the artifact to retrieve.</p> required <code>encoding</code> <code>str</code> <p>The text encoding to use for decoding bytes (default: <code>\"utf-8\"</code>).</p> <code>'utf-8'</code> <code>errors</code> <code>str</code> <p>Error handling strategy for decoding (default: <code>\"strict\"</code>).</p> <code>'strict'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The decoded text content of the artifact.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the artifact is not found or missing a URI.</p> load_json_by_id(artifact_id, *, ...) <p>Load and parse a JSON artifact by its unique identifier.</p> <p>This asynchronous method retrieves the raw text content for the specified <code>artifact_id</code>, decodes it using the provided encoding, and parses it as JSON. It is accessed via <code>context.artifacts().load_json_by_id(...)</code>.</p> <p>Examples:</p> <p>Basic usage to load a JSON artifact: <pre><code>data = await context.artifacts().load_json_by_id(\"artifact_123\")\nprint(data[\"foo\"])\n</code></pre></p> <p>Loading with custom encoding and error handling: <pre><code>data = await context.artifacts().load_json_by_id(\n    \"artifact_456\",\n    encoding=\"utf-16\",\n    errors=\"ignore\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>artifact_id</code> <code>str</code> <p>The unique string identifier of the artifact to retrieve.</p> required <code>encoding</code> <code>str</code> <p>The text encoding to use for decoding bytes (default: <code>\"utf-8\"</code>).</p> <code>'utf-8'</code> <code>errors</code> <code>str</code> <p>Error handling strategy for decoding (default: <code>\"strict\"</code>).</p> <code>'strict'</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The parsed JSON object from the artifact.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the artifact is not found or missing a URI.</p> <code>JSONDecodeError</code> <p>If the artifact content is not valid JSON.</p> load_bytes(uri) <p>Load raw bytes from a file or URI in a backend-agnostic way.</p> <p>This method retrieves the byte content from the specified <code>uri</code>, supporting both local files and remote storage backends. It is accessed via <code>context.artifacts().load_bytes(...)</code>.</p> <p>Examples:</p> <p>Basic usage to load bytes from a local file: <pre><code>data = await context.artifacts().load_bytes(\"file:///tmp/model.bin\")\n</code></pre></p> <p>Loading bytes from an S3 URI: <pre><code>data = await context.artifacts().load_bytes(\"s3://bucket/data.bin\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The URI or path of the file to load. Supports local files and remote storage backends.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The raw byte content of the file or artifact.</p> load_text(uri) <p>Load the text content from a file or URI in a backend-agnostic way.</p> <p>This method retrieves the raw bytes from the specified <code>uri</code>, decodes them into a string using the provided encoding, and returns the text. It is accessed via <code>context.artifacts().load_text(...)</code>.</p> <p>Examples:</p> <p>Basic usage to load text from a local file: <pre><code>text = await context.artifacts().load_text(\"file:///tmp/output.txt\")\nprint(text)\n</code></pre></p> <p>Loading text from an S3 URI with custom encoding: <pre><code>text = await context.artifacts().load_text(\n    \"s3://bucket/data.txt\",\n    encoding=\"utf-16\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The URI or path of the file to load. Supports local files and remote storage backends.</p> required <code>encoding</code> <code>str</code> <p>The text encoding to use for decoding bytes (default: <code>\"utf-8\"</code>).</p> <code>'utf-8'</code> <code>errors</code> <code>str</code> <p>Error handling strategy for decoding (default: <code>\"strict\"</code>).</p> <code>'strict'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The decoded text content of the file or artifact.</p> load_json(uri) <p>Load and parse a JSON file from the specified URI.</p> <p>This asynchronous method retrieves the file contents as text, then parses the text into a Python object using the standard <code>json</code> library. It is typically accessed via <code>context.artifacts().load_json(...)</code>.</p> <p>Examples:</p> <p>Basic usage to load a JSON file: <pre><code>data = await context.artifacts().load_json(\"file:///path/to/data.json\")\n</code></pre></p> <p>Specifying a custom encoding: <pre><code>data = await context.artifacts().load_json(\n    \"file:///path/to/data.json\",\n    encoding=\"utf-16\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The URI of the JSON file to load. Supports local and remote paths.</p> required <code>encoding</code> <code>str</code> <p>The text encoding to use when reading the file (default: \"utf-8\").</p> <code>'utf-8'</code> <code>errors</code> <code>str</code> <p>The error handling scheme for decoding (default: \"strict\").</p> <code>'strict'</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The parsed Python object loaded from the JSON file.</p>"},{"location":"reference/context-artifacts/#5-helpers","title":"5. Helpers","text":"as_local_dir(artifact_or_uri, *, must_exist) <p>Ensure an artifact representing a directory is available as a local path.</p> <p>This method provides a backend-agnostic way to access directory artifacts as local filesystem paths. For local filesystems, it returns the underlying CAS directory. For remote backends (e.g., S3), it downloads the directory contents to a staging location and returns the path.</p> <p>Examples:</p> <p>Basic usage to access a local directory artifact: <pre><code>local_dir = await context.artifacts().as_local_dir(\"file:///tmp/output_dir\")\nprint(local_dir)\n</code></pre></p> <p>Handling missing directories: <pre><code>try:\n    local_dir = await context.artifacts().as_local_dir(\"s3://bucket/data_dir\")\nexcept FileNotFoundError:\n    print(\"Directory not found.\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>artifact_or_uri</code> <code>str | Path | Artifact</code> <p>The artifact object, URI string, or Path representing the directory.</p> required <code>must_exist</code> <code>bool</code> <p>If True, raises FileNotFoundError if the local path does not exist.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The resolved local filesystem path to the directory artifact.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the resolved local directory does not exist and <code>must_exist</code> is True.</p> as_local_file(artifact_or_uri, *, must_exist) <p>This method transparently handles local and remote artifact URIs, downloading remote files to a staging location if necessary. It is typically accessed via <code>context.artifacts().as_local_file(...)</code>.</p> <p>Examples:</p> <p>Using a local file path: <pre><code>local_path = await context.artifacts().as_local_file(\"/tmp/data.csv\")\n</code></pre></p> <p>Using an S3 URI: <pre><code>local_path = await context.artifacts().as_local_file(\"s3://bucket/key.csv\")\n</code></pre></p> <p>Using an Artifact object: <pre><code>local_path = await context.artifacts().as_local_file(artifact)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>artifact_or_uri</code> <code>str | Path | Artifact</code> <p>The artifact to resolve, which may be a string URI, Path, or Artifact object.</p> required <code>must_exist</code> <code>bool</code> <p>If True, raises FileNotFoundError if the file does not exist or is not a file.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The absolute path to the local file containing the artifact's data.</p>"},{"location":"reference/context-channel/","title":"<code>context.channel()</code> \u2013 ChannelSession API Reference","text":"<p>A <code>ChannelSession</code> provides message I/O, user prompts, streaming text, and progress updates through the configured channel (console/Slack/\u2026). It also manages continuation tokens to avoid race conditions.</p>"},{"location":"reference/context-channel/#channel-resolution-defaults","title":"Channel Resolution &amp; Defaults","text":"<ul> <li>Channel selection priority: explicit <code>channel</code> arg \u2192 session override (from <code>context.channel(channel_key)</code>) \u2192 bus default \u2192 <code>console:stdin</code>.</li> <li>Events are published after alias \u2192 canonical key resolution.</li> </ul>"},{"location":"reference/context-channel/#0-channel-setup","title":"0. Channel Setup","text":"context.channel(channel_key) <p>Set up a new ChannelSession for the current node context.</p> <p>Parameters:</p> Name Type Description Default <code>channel_key</code> <code>str | None</code> <p>An optional key to specify a particular channel.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ChannelSession</code> <p>An instance representing the session for the specified channel.</p> Notes <p>Supported channel key formats include:</p> Channel Type Format Example Notes Console <code>console:stdin</code> Console input/output Slack <code>slack:team/{team_id}:chan/{channel_id}</code> Needs additional configuration Telegram <code>tg:chat/{chat_id}</code> Needs additional configuration UI Session <code>ui:session/{session_id}</code> Requires AG web UI UI Run <code>ui:run/{run_id}</code> Requires AG web UI Webhook <code>webhook:{unique_identifier}</code> For Slack, Discord, Zapier, etc. File-based channel <code>file:path/to/directory</code> File system based channels context.ui_run_channel() <p>Creates a new ChannelSession for the current node context with session key as <code>ui:run/&lt;run_id&gt;</code>.</p> <p>This method is a convenience helper for the AG UI to get the default run channel.</p> <p>Returns:</p> Name Type Description <code>ChannelSession</code> <code>ChannelSession</code> <p>The channel session associated with the current run.</p> context.ui_session_channel() <p>Creates a new ChannelSession for the current node context with session key as <code>ui:session/&lt;session_id&gt;</code>.</p> <p>This method is a convenience helper for the AG UI to get the default session channel.</p> <p>Returns:</p> Name Type Description <code>ChannelSession</code> <code>ChannelSession</code> <p>The channel session associated with the current session.</p>"},{"location":"reference/context-channel/#1-send-methods","title":"1. Send Methods","text":"send(event, *, channel) <p>Send a single outbound event to the configured channel.</p> <p>This method ensures the event is associated with the correct channel, merges context-derived metadata, and publishes the event via the channel bus. This is the core low-level send method; higher-level convenience methods (e.g., <code>send_text</code>, <code>send_rich</code>, etc.) build on top of this and are recommended for common use cases.</p> <p>Examples:</p> <p>Basic usage to send a pre-constructed event: <pre><code>event = OutEvent(type=\"agent.message\", text=\"Hello!\", channel=None)\nawait context.channel().send(event)\n</code></pre></p> <p>Sending to a specific channel: <pre><code>await context.channel().send(event, channel=\"web:chat\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>OutEvent</code> <p>The <code>OutEvent</code> instance to send. If <code>event.channel</code> is not set, it will be resolved automatically.</p> required <code>channel</code> <code>str | None</code> <p>Optional explicit channel key to override the default or event's channel.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> <p>Notes: for AG WebUI, you can set meta with <pre><code>    {\n        \"agent_id\": \"agent-123\",\n        \"name\": \"Analyst\",\n    }\n</code></pre> to override the sender's display name and avatar in the chat.</p> send_text(text, *, meta, channel) <p>Send a plain text message to the configured channel.</p> <p>This method constructs a normalized outbound event, merges context-derived metadata, and dispatches the message via the channel bus.</p> <p>Examples:</p> <p>Basic usage to send a text message: <pre><code>await context.channel().send_text(\"Hello, world!\")\n</code></pre></p> <p>Sending with additional metadata and to a specific channel: <pre><code>await context.channel().send_text(\n    \"Status update.\",\n    meta={\"priority\": \"high\"},\n    channel=\"web:chat\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The primary text content to send.</p> required <code>meta</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to include with the event.</p> <code>None</code> <code>channel</code> <code>str | None</code> <p>Optional explicit channel key to override the default or session-bound channel.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> <p>Notes: for AG WebUI, you can set meta with <pre><code>    {\n        \"agent_id\": \"agent-123\",\n        \"name\": \"Analyst\",\n    }\n</code></pre></p> send_image(url, *, alt, title, channel) <p>Send an image message to the configured channel.</p> <p>This method constructs and dispatches an outbound event containing image metadata, including the image URL, alternative text, and an optional title. Context-derived metadata is automatically merged, and the event is published via the channel bus.</p> <p>Examples:</p> <p>Basic usage to send an image: <pre><code>await context.channel().send_image(\n    url=\"https://example.com/image.png\",\n    alt=\"Sample image\"\n)\n</code></pre></p> <p>Sending with a custom title and to a specific channel: <pre><code>await context.channel().send_image(\n    url=\"https://example.com/photo.jpg\",\n    alt=\"User profile photo\",\n    title=\"Profile\",\n    channel=\"web:chat\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str | None</code> <p>The URL of the image to send. If None, an empty string is used.</p> <code>None</code> <code>alt</code> <code>str</code> <p>Alternative text describing the image (for accessibility).</p> <code>'image'</code> <code>title</code> <code>str | None</code> <p>Optional title to display with the image.</p> <code>None</code> <code>channel</code> <code>str | None</code> <p>Optional explicit channel key to override the default or session-bound channel.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Notes <p>The capability to render images depends on the client adapter.</p> send_file(url, *, file_bytes, filename, title, channel) <p>Send a file to the configured channel in a normalized format.</p> <p>This method constructs and dispatches an outbound event containing file metadata, including the file URL, raw bytes, filename, and an optional title. Context-derived metadata is automatically merged, and the event is published via the channel bus.</p> <p>Examples:</p> <p>Basic usage to send a file by URL: <pre><code>await context.channel().send_file(\n    url=\"https://example.com/report.pdf\",\n    filename=\"report.pdf\",\n    title=\"Monthly Report\"\n)\n</code></pre></p> <p>Sending a file from bytes: <pre><code>await context.channel().send_file(\n    file_bytes=b\"binarydata...\",\n    filename=\"data.bin\",\n    title=\"Raw Data\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str | None</code> <p>The URL of the file to send. If None, only file_bytes will be used.</p> <code>None</code> <code>file_bytes</code> <code>bytes | None</code> <p>Optional raw bytes of the file to send.</p> <code>None</code> <code>filename</code> <code>str</code> <p>The display name of the file (defaults to \"file.bin\").</p> <code>'file.bin'</code> <code>title</code> <code>str | None</code> <p>Optional title to display with the file.</p> <code>None</code> <code>channel</code> <code>str | None</code> <p>Optional explicit channel key to override the default or session-bound channel.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Notes <p>The capability to handle file uploads depends on the client adapter. If both <code>url</code> and <code>file_bytes</code> are provided, both will be included in the event.</p> send_buttons(text, buttons, *, meta, channel) <p>Send a message with interactive buttons to the configured channel.</p> <p>This method constructs and dispatches an outbound event containing a text prompt and a list of interactive buttons. Context-derived metadata is automatically merged, and the event is published via the channel bus.</p> <p>Examples:</p> <p>Basic usage to send a button prompt: <pre><code>from aethergraph import Button\nawait context.channel().send_buttons(\n    \"Choose an option:\",\n    [Button(label=\"Yes\", value=\"yes\"), Button(label=\"No\", value=\"no\")]\n)\n</code></pre></p> <p>Sending with additional metadata and to a specific channel: <pre><code>await context.channel().send_buttons(\n    \"Select your role:\",\n    [Button(label=\"Admin\", value=\"admin\"), Button(label=\"User\", value=\"user\")],\n    meta={\"priority\": \"high\"},\n    channel=\"web:chat\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The primary text content to display above the buttons.</p> required <code>buttons</code> <code>list[Button]</code> <p>A list of <code>Button</code> objects representing the interactive options.</p> required <code>meta</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to include with the event.</p> <code>None</code> <code>channel</code> <code>str | None</code> <p>Optional explicit channel key to override the default or session-bound channel.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p>"},{"location":"reference/context-channel/#2-ask-methods","title":"2. Ask Methods","text":"<p>Ask method automatic implemented continuation so that the agent will resume running when triggered from external channels. </p> ask_text(prompt, *, timeout_s, silent, channel) <p>Prompt the user for a text response in a normalized format.</p> <p>This method sends a prompt to the configured channel, waits for a user reply, and returns the text input. It automatically handles context metadata, timeout, and channel resolution.</p> <p>Examples:</p> <p>Basic usage to prompt for user input: <pre><code>reply = await context.channel().ask_text(\"What is your name?\")\n</code></pre></p> <p>Prompting with a custom timeout and silent mode: <pre><code>reply = await context.channel().ask_text(\n    \"Enter your feedback.\",\n    timeout_s=120,\n    silent=True\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str | None</code> <p>The text prompt to display to the user. If None, a generic prompt may be shown.</p> required <code>timeout_s</code> <code>int</code> <p>Maximum time in seconds to wait for a response (default: 3600).</p> <code>3600</code> <code>silent</code> <code>bool</code> <p>If True, suppresses prompt display in some adapters (back-compat; default: False).</p> <code>False</code> <code>channel</code> <code>str | None</code> <p>Optional explicit channel key to override the default or session-bound channel.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The user's text response, or an empty string if no input was received.</p> wait_text(*, timeout_s, silent, channel) <p>Wait for a single text response from the user in a normalized format.</p> <p>This method prompts the user for input (with no explicit prompt), waits for a reply, and returns the text. It automatically handles context metadata, timeout, and channel resolution.</p> <p>Examples:</p> <p>Basic usage to wait for user input: <pre><code>reply = await context.channel().wait_text()\n</code></pre></p> <p>Waiting with a custom timeout and specific channel: <pre><code>reply = await context.channel().wait_text(\n    timeout_s=120,\n    channel=\"web:chat\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>timeout_s</code> <code>int</code> <p>Maximum time in seconds to wait for a response (default: 3600).</p> <code>3600</code> <code>channel</code> <code>str | None</code> <p>Optional explicit channel key to override the default or session-bound channel.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The user's text response, or an empty string if no input was received.</p> ask_approval(prompt, options, *, timeout_s, channel) <p>Prompt the user for approval or rejection in a normalized format.</p> <p>This method sends an approval prompt with customizable options (buttons) to the configured channel, waits for the user's selection, and returns a normalized result indicating approval status and choice. Context metadata, timeout, and channel resolution are handled automatically.</p> <p>Examples:</p> <p>Basic usage to prompt for approval: <pre><code>result = await context.channel().ask_approval(\"Do you approve this action?\")\n# result: { \"approved\": True/False, \"choice\": \"Approve\"/\"Reject\" }\n</code></pre></p> <p>Prompting with custom options and timeout: <pre><code>result = await context.channel().ask_approval(\n    \"Proceed with deployment?\",\n    options=[\"Yes\", \"No\", \"Defer\"],\n    timeout_s=120\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The text prompt to display to the user.</p> required <code>options</code> <code>Iterable[str]</code> <p>Iterable of button labels for user choices (defaults to \"Approve\" and \"Reject\").</p> <code>('Approve', 'Reject')</code> <code>timeout_s</code> <code>int</code> <p>Maximum time in seconds to wait for a response (default: 3600).</p> <code>3600</code> <code>channel</code> <code>str | None</code> <p>Optional explicit channel key to override the default or session-bound channel.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>A dictionary containing: - \"approved\": bool indicating if the first option was selected (True if approved, False otherwise). - \"choice\": The label of the button selected by the user (str or None).</p> Warning <p>The returned \"choices\" are determined by the external adapter and may vary. To be robust, make sure to use <code>choices.lower()</code> and strip whitespace when comparing.</p> ask_files(prompt, *, accept, multiple, timeout_s, channel) <p>Prompt the user to upload one or more files, optionally with a text comment.</p> <p>This method sends a file upload request to the configured channel, allowing the user to select files and optionally enter accompanying text. The <code>accept</code> parameter provides hints to the client UI about which file types are preferred, but is not enforced server-side. The method waits for the user's response and returns a normalized result containing both text and file references.</p> <p>Examples:</p> <p>Basic usage to prompt for file upload: <pre><code>result = await context.channel().ask_files(\n    prompt=\"Please upload your report.\"\n)\n# result: { \"text\": \"...\", \"files\": [FileRef(...), ...] }\n</code></pre></p> <p>Restricting to images and allowing multiple files: <pre><code>result = await context.channel().ask_files(\n    prompt=\"Upload images for review.\",\n    accept=[\"image/png\", \".jpg\"],\n    multiple=True\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The text prompt to display to the user above the file picker.</p> required <code>accept</code> <code>list[str] | None</code> <p>Optional list of MIME types or file extensions to suggest allowed file types (e.g., \"image/png\", \".pdf\", \".jpg\").</p> <code>None</code> <code>multiple</code> <code>bool</code> <p>If True, allows the user to select multiple files (default: True).</p> <code>True</code> <code>timeout_s</code> <code>int</code> <p>Maximum time in seconds to wait for a response (default: 3600).</p> <code>3600</code> <code>channel</code> <code>str | None</code> <p>Optional explicit channel key to override the default or session-bound channel.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing: - \"text\": str, the user's comment or description (may be empty). - \"files\": List[FileRef], references to the uploaded files (empty if none).</p> Notes <p>On console adapters, file upload is not supported; only text will be returned. The <code>accept</code> parameter is a UI hint and does not guarantee file type enforcement.</p>"},{"location":"reference/context-channel/#3-streaming","title":"3. Streaming","text":"stream(channel) <p>Stream a sequence of text deltas to the configured channel in a normalized format.</p> <p>This method provides a context manager for streaming incremental message updates (such as LLM generation) to the target channel. It automatically handles context metadata, upsert keys, and dispatches start, update, and end events to the channel bus. The caller is responsible for sending deltas and ending the stream.</p> <p>Examples:</p> <p>Basic usage to stream LLM output: <pre><code>async with context.channel().stream() as s:\n    await s.delta(\"Hello, \")\n    await s.delta(\"world!\")\n    await s.end()\n</code></pre></p> <p>Streaming to a specific channel: <pre><code>async with context.channel().stream(channel=\"web:chat\") as s:\n    await s.delta(\"Generating results...\")\n    await s.end(full_text=\"Results complete.\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>channel</code> <code>str | None</code> <p>Optional explicit channel key to target a specific channel for this stream. If None, uses the session-bound or default channel.</p> <code>None</code> <p>Returns:</p> Type Description <code>AsyncIterator[_StreamSender]</code> <p>AsyncIterator[_StreamSender]: An async context manager yielding a <code>_StreamSender</code> object for sending deltas and ending the stream.</p> Notes <p>The caller must explicitly call <code>end()</code> to finalize the stream. No auto-end is performed. The adapter may have specific behaviors for rendering streamed content (update vs. append).</p>"},{"location":"reference/context-channel/#4-utilities","title":"4. Utilities","text":"get_latest_uploads(*, clear) <p>Retrieve the latest uploaded files from this channel's inbox in a normalized format.</p> <p>This method accesses the ephemeral KV store to fetch file uploads associated with the current channel. By default, it clears the inbox after retrieval to prevent duplicate processing. If the KV service is unavailable in the context, a RuntimeError is raised. This method allows the fetch the files user uploaded not from an ask_files prompt, but from any prior upload event.</p> <p>Examples:</p> <p>Basic usage to fetch and clear uploaded files: <pre><code>files = await context.channel().get_latest_uploads()\n</code></pre></p> <p>Fetching files without clearing the inbox: <pre><code>files = await context.channel().get_latest_uploads(clear=False)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>clear</code> <code>bool</code> <p>If True (default), removes files from the inbox after retrieval. If False, files are returned but remain in the inbox.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[FileRef]</code> <p>List[FileRef]: A list of <code>FileRef</code> objects representing the uploaded files. Returns an empty list if no files are present.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the ephemeral KV service is not available in the current context.</p>"},{"location":"reference/context-extension/","title":"Custom Context Services \u2013 API Guide (using <code>Service</code> alias)","text":"<p>AetherGraph lets you attach your own runtime helpers onto <code>context.*</code> as first\u2011class services (e.g., <code>context.trainer()</code>, <code>context.materials()</code>).</p> <p>Use <code>Service</code> (alias of the underlying <code>BaseContextService</code>) for all subclasses:</p> <pre><code>from aethergraph import Service  # alias of BaseContextService\n</code></pre> <p>Register your service after the sidecar starts, then access it anywhere via the context: <code>context.&lt;name&gt;()</code>.</p>"},{"location":"reference/context-extension/#minimal-flow","title":"Minimal Flow","text":"<pre><code>from aethergraph import start_server, Service\nfrom aethergraph.runtime import register_context_service\n\nclass MyCache(Service):\n    def __init__(self):\n        super().__init__()\n        self._data = {}\n\n    def get(self, k, default=None):\n        return self._data.get(k, default)\n\n    def set(self, k, v):\n        self._data[k] = v\n\n# 1) Start sidecar\nstart_server(port=0)\n\n# 2) Register a **singleton** instance for all contexts\nregister_context_service(\"cache\", MyCache())\n\n# 3) Use inside a graph/tool\nasync def do_work(*, context):\n    cache = context.cache()        # no\u2011arg call \u21d2 bound service instance\n    cache.set(\"foo\", 42)\n    return cache.get(\"foo\")\n</code></pre> <p>Why it works:</p> <ul> <li><code>context.__getattr__(\"cache\")</code> resolves the registered service and binds the active <code>NodeContext</code> via <code>Service.bind(...)</code>.</li> <li>The returned <code>_ServiceHandle</code> forwards attribute access and calls. A no\u2011arg call returns the underlying service instance for convenience.</li> </ul>"},{"location":"reference/context-extension/#api-reference","title":"API Reference","text":""},{"location":"reference/context-extension/#1-register-external-services","title":"1. Register External Services","text":"register_context_service(name, service) <p>Register an external service for NodeContext access.</p> <p>This function attaches an external service to the current service container under the specified name. If no container is installed yet, the service is stashed in a pending registry and will be attached automatically when install_services() is called.</p> <p>Examples:</p> <p>Register a custom database service: <pre><code>register_context_service(\"mydb\", MyDatabaseService())\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique string identifier for the external service.</p> required <code>service</code> <code>Service</code> <p>The service instance to register.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Notes <ul> <li>If called before install_services(), the service will be attached later.</li> <li>Services are accessible via NodeContext.ext_services[name].</li> </ul> get_ext_context_service(name) <p>Retrieve an external context service by name.</p> <p>This function returns the external service registered under the given name from the current service container's ext_services registry.</p> <p>Examples:</p> <p>Access a registered service: <pre><code>mydb = get_ext_context_service(\"mydb\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The string name of the external service to retrieve.</p> required <p>Returns:</p> Type Description <code>Service</code> <p>The service instance registered under the given name, or None if not found.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no services container is installed.</p> list_ext_context_services() <p>List all registered external context service names.</p> <p>This function returns a list of all names for services currently registered in the ext_services registry of the current service container.</p> <p>Examples:</p> <p>List all available external services: <pre><code>services = list_ext_context_services()\nprint(services)\n</code></pre></p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of strings representing the names of all registered external services.</p> <code>list[str]</code> <p>Returns an empty list if no services are registered.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no services container is installed.</p>"},{"location":"reference/context-extension/#2-access-external-services","title":"2. Access External Services","text":"context.&lt;service_name&gt;(*args) <p>Retrieve and bind an external context service by name. This allows accessing services as attributes on the context object.</p> <p>This method overrides attribute access to dynamically resolve external services registered in the context. If a service with the requested name exists, it is retrieved and wrapped in a <code>_ServiceHandle</code> for ergonomic access. The returned handle allows attribute access, direct retrieval, and call forwarding if the service is callable.</p> <p>Examples:</p> <pre><code># Retrieve a database service and run a query\ndb = context.database()\ndb.query(\"SELECT * FROM users\")\n\n# Access a logger service and log a message\ncontext.logger.info(\"Hello from node!\")\n\n# Forward arguments to a callable service\nresult = context.some_tool(\"input text\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the service to resolve as an attribute.</p> required <p>Returns:</p> Name Type Description <code>_ServiceHandle</code> <code>Any</code> <p>A callable handle to the resolved service.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If no service with the given name exists in the context.</p> Usage <ul> <li> <p>You can access external services directly as attributes on the context object. For example, if you have registered a service named \"my_service\", you can use:</p> <pre><code># Get the service instance\nsvc = context.my_service()\n\n# Call the service if it's callable\nresult = context.my_service(arg1, arg2)\n\n# Access service attributes\nvalue = context.my_service.some_attribute\n</code></pre> </li> <li> <p>In your Service, you can use <code>self.ctx</code> to access the node context if needed. For example:     <pre><code>class MyService:\n    ...\n    def my_method(self, ...):\n        context = self.ctx  # Access the NodeContext\n        # Use context information as needed\n        context.channel.send(\"Hello from MyService!\")\n</code></pre></p> </li> </ul> Notes <ul> <li>If the service is not registered, an AttributeError is raised.</li> <li>If the service is callable, calling <code>context.service_name(args)</code> will forward the call.</li> <li>If you call <code>context.service_name()</code> with no arguments, you get the underlying service instance.</li> <li>Attribute access (e.g., <code>context.service_name.some_attr</code>) is delegated to the service.</li> </ul> context.svc(name) <p>Retrieve and bind an external context service by name. This method is equivalent to <code>context.&lt;service_name&gt;()</code>. User can use either <code>context.svc(\"service_name\")</code> or <code>context.service_name()</code> to access the service.</p> <p>This method accesses a registered external service, optionally binding it to the current node context if the service supports context binding via a <code>bind</code> method.</p> <p>Examples:</p> <p>Basic usage to access a service: <pre><code>db = context.svc(\"database\")\n</code></pre></p> <p>Accessing a service that requires context binding: <pre><code>logger = context.svc(\"logger\")\nlogger.info(\"Node started.\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique string identifier of the external service to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The external service instance, bound to the current context if applicable.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the requested service is not registered in the external context.</p>"},{"location":"reference/context-extension/#patterns","title":"Patterns","text":""},{"location":"reference/context-extension/#1-contextaware-logging-artifacts","title":"1) Context\u2011Aware Logging &amp; Artifacts","text":"<pre><code>from aethergraph import Service\n\nclass Reporter(Service):\n    def log_metric(self, name: str, value: float):\n        self.ctx().logger().info(\"metric\", extra={\"name\": name, \"value\": value})\n\n    async def save_text(self, text: str) -&gt; str:\n        art = await self.ctx().artifacts().save_text(text)\n        return art.uri\n</code></pre>"},{"location":"reference/context-extension/#2-threadsafe-mutations","title":"2) Thread\u2011safe Mutations","text":"<pre><code>from aethergraph import Service\n\nclass Counter(Service):\n    def __init__(self):\n        super().__init__()\n        self._n = 0\n        self.inc = self.critical()(self.inc)\n\n    def inc(self, k: int = 1) -&gt; int:\n        self._n += k\n        return self._n\n</code></pre>"},{"location":"reference/context-extension/#3-wrapping-external-clients","title":"3) Wrapping External Clients","text":"<pre><code>from aethergraph import Service\nimport httpx\n\nclass Weather(Service):\n    def __init__(self, base: str):\n        super().__init__()\n        self._base = base\n        self._http = httpx.Client(timeout=10)\n\n    async def close(self):\n        await self.run_blocking(self._http.close)\n\n    def get_temp_c(self, city: str) -&gt; float:\n        r = self._http.get(f\"{self._base}/temp\", params={\"city\": city})\n        r.raise_for_status()\n        return float(r.json()[\"c\"])  # noqa\n</code></pre>"},{"location":"reference/context-extension/#4-using-in-graphstools","title":"4) Using in Graphs/Tools","text":"<pre><code>from aethergraph import Service\nfrom aethergraph.runtime import register_context_service\n\nclass Greeter(Service):\n    def greet(self, name: str) -&gt; str:\n        return f\"Hello, {name}! (run={self.ctx().run_id})\"\n\nregister_context_service(\"greeter\", Greeter())\n\n# Inside a node\nasync def hello(*, context):\n    g = context.greeter()               # bound service\n    return {\"msg\": g.greet(\"World\")}\n</code></pre>"},{"location":"reference/context-extension/#faq","title":"FAQ","text":"<p>Q: Where should I create the service instance? Usually at process boot, after <code>start_server(...)</code>. Register exactly once (singleton). If you need per\u2011tenant services, implement your own map inside the service keyed by tenant.</p> <p>Q: How do I access other context services from my service? Use <code>self.ctx().&lt;other_service&gt;()</code>, e.g., <code>self.ctx().artifacts()</code> or <code>self.ctx().logger()</code>.</p> <p>Q: Can a service be async? Yes\u2014methods can be <code>async</code> and you may leverage <code>run_blocking(...)</code> for sync IO.</p> <p>Q: How do I store configuration/keys? Provide them to your service constructor, or pull from env. For dynamic secrets, wire a secrets service and read from it inside your custom service.</p>"},{"location":"reference/context-extension/#gotchas-tips","title":"Gotchas &amp; Tips","text":"<ul> <li>Name collisions: Avoid names of built\u2011ins (<code>channel</code>, <code>memory</code>, <code>artifacts</code>, etc.).</li> <li>No context at import time: <code>Service.ctx()</code> works after binding; do not call it in <code>__init__</code>.</li> <li>Thread safety: Use <code>@Service.critical</code> for shared state.</li> <li>Long\u2011running IO: Prefer <code>await self.run_blocking(...)</code> for blocking clients.</li> <li>Testing: You can construct a <code>Service</code> and call <code>bind(context=FakeContext(...))</code> directly in unit tests.</li> </ul>"},{"location":"reference/context-kv/","title":"<code>context.kv()</code> \u2013 EphemeralKV API Reference","text":"<p><code>EphemeralKV</code> is a process\u2011local, transient key\u2013value store for small JSON\u2011serializable values and short\u2011lived coordination. It is thread\u2011safe (RLock), supports TTLs, and provides a few list helpers. Do not use for large blobs or durable state.</p> <p>The API below is consistent across KV backends. In <code>context</code>, this KV is ephemeral; other implementations may be pluggable later.</p>"},{"location":"reference/context-kv/#concepts-defaults","title":"Concepts &amp; Defaults","text":"<ul> <li>Scope: In\u2011process only; cleared on restart. Not replicated.</li> <li>Thread\u2011safety: Internal reads/writes guarded by <code>RLock</code>.</li> <li>TTL: Expiry is checked lazily on access and via <code>purge_expired()</code>.</li> <li>Prefixing: Instances may prepend a <code>prefix</code> to all keys for namespacing.</li> </ul>"},{"location":"reference/context-kv/#1-core-methods","title":"1. Core Methods","text":"set(key, value, *, ttl_s) <p>Store a value in the ephemeral key-value store with optional expiration.</p> <p>This method inserts or updates the value for a given key, optionally setting a time-to-live (TTL) in seconds. If a TTL is provided, the entry will expire and be automatically removed after the specified duration.</p> <p>Examples:</p> <p>Basic usage to store a value: <pre><code>await context.kv().set(\"session_token\", \"abc123\")\n</code></pre></p> <p>Storing a value with a 10-minute expiration: <pre><code>await context.kv().set(\"user_id\", 42, ttl_s=600)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The string key under which to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store (any serializable object).</p> required <code>ttl_s</code> <code>int | None</code> <p>Optional expiration time in seconds. If None, the value does not expire.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> get(key, default) <p>Retrieve the value associated with a key from the ephemeral key-value store.</p> <p>This method checks for key existence and expiration, automatically removing expired entries. If the key does not exist or is expired, the provided <code>default</code> value is returned.</p> <p>Examples:</p> <p>Basic usage to fetch a value: <pre><code>value = await context.kv().get(\"session_token\")\n</code></pre></p> <p>Providing a default if the key is missing or expired: <pre><code>user_id = await context.kv().get(\"user_id\", default=None)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The string key to look up.</p> required <code>default</code> <code>Any</code> <p>The value to return if the key is not found or has expired.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The stored value if present and not expired; otherwise, the <code>default</code>.</p> delete(key) <p>Remove a key and its associated value from the ephemeral key-value store.</p> <p>This method deletes the specified key from the store if it exists. If the key does not exist, the operation is a no-op and does not raise an error.</p> <p>Examples:</p> <p>Basic usage to delete a key: <pre><code>await context.kv().delete(\"session_token\")\n</code></pre></p> <p>Deleting a user-specific cache entry: <pre><code>await context.kv().delete(f\"user_cache:{user_id}\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The string key to remove from the store.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> list_append_unique(key, items, *, id_key, ttl_s) <p>Append unique dictionary items to a list stored in the ephemeral key-value store.</p> <p>This method ensures that only items with unique <code>id_key</code> values are added to the list associated with the given key. If the key does not exist, a new list is created. Optionally, a time-to-live (TTL) can be set for the entry.</p> <p>Examples:</p> <p>Basic usage to append unique items: <pre><code>await context.kv().list_append_unique(\"recent_users\", [{\"id\": 1, \"name\": \"Alice\"}])\n</code></pre></p> <p>Appending multiple items with a custom ID key and expiration: <pre><code>await context.kv().list_append_unique(\n    \"tasks\",\n    [{\"task_id\": 42, \"desc\": \"Review PR\"}],\n    id_key=\"task_id\",\n    ttl_s=3600\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The string key under which the list is stored.</p> required <code>items</code> <code>list[dict]</code> <p>A list of dictionaries to append. Only items with unique <code>id_key</code> values (not already present in the list) will be added.</p> required <code>id_key</code> <code>str</code> <p>The dictionary key used to determine uniqueness (default: <code>\"id\"</code>).</p> <code>'id'</code> <code>ttl_s</code> <code>int | None</code> <p>Optional expiration time in seconds for the updated list. If None, the list does not expire.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: The updated list of dictionaries after appending unique items.</p> Notes <ul> <li>This method is used for lists of dictionaries where each dictionary has a unique identifier. For example,     it can be used to maintain a list of recent user actions, ensuring no duplicates based on user ID.</li> <li>Example of the stored list structure:     <pre><code>[\n    {\"id\": 1, \"name\": \"Alice\"},\n    {\"id\": 2, \"name\": \"Bob\"},\n    ...\n]\n</code></pre></li> </ul> list_pop_all(key) <p>Atomically remove and return all items from a list stored in the ephemeral key-value store.</p> <p>This method retrieves the entire list associated with the given key and removes the key from the store. If the key does not exist or does not contain a list, an empty list is returned. This operation is atomic and ensures no items are left behind after the call.</p> <p>Examples:</p> <p>Basic usage to pop all items from a list: <pre><code>items = await context.kv().list_pop_all(\"recent_events\")\n</code></pre></p> <p>Handling the case where the key may not exist: <pre><code>logs = await context.kv().list_pop_all(\"logs\")  # returns [] if \"logs\" is missing\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The string key under which the list is stored.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>The list of items that were stored under the key, or an empty list if the key was not found.</p>"},{"location":"reference/context-kv/#2-optional-helpers","title":"2. Optional Helpers","text":"mset(kv, *, ttl_s) <p>Set multiple key-value pairs in the ephemeral key-value store.</p> <p>This asynchronous method iterates over the provided dictionary and sets each key-value pair in the store, optionally applying a time-to-live (TTL) to each entry. If a TTL is specified, each key will expire after the given number of seconds.</p> <p>Examples:</p> <p>Basic usage to set multiple values: <pre><code>await context.kv().mset({\"foo\": 1, \"bar\": \"baz\"})\n</code></pre></p> <p>Setting multiple values with a TTL of 60 seconds: <pre><code>await context.kv().mset({\"session\": \"abc\", \"count\": 42}, ttl_s=60)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>kv</code> <code>dict[str, Any]</code> <p>A dictionary mapping string keys to values to be stored.</p> required <code>ttl_s</code> <code>int | None</code> <p>Optional; the time-to-live for each key in seconds. If None, keys do not expire.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> mget(keys) <p>Retrieve multiple values from the ephemeral key-value store in a single call.</p> <p>This method fetches the values associated with each key in the provided list, preserving the order of the input. If a key does not exist or is expired, <code>None</code> is returned in its place.</p> <p>Examples:</p> <p>Basic usage to fetch several values: <pre><code>values = await context.kv().mget([\"user_id\", \"session_token\", \"profile\"])\n</code></pre></p> <p>Handling missing or expired keys: <pre><code>results = await context.kv().mget([\"foo\", \"bar\"])\n# results might be [None, \"bar_value\"] if \"foo\" is missing or expired\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>A list of string keys to retrieve from the store.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>list[Any]: A list of values corresponding to the input keys. If a key is not found or has expired, its position in the list will be <code>None</code>.</p> expire(key, ttl_s) <p>Set or update the expiration time (TTL) for a key in the ephemeral key-value store.</p> <p>This method updates the expiration timestamp for an existing key, causing it to expire and be automatically removed after the specified number of seconds. If the key does not exist, this operation is a no-op.</p> <p>Examples:</p> <p>Basic usage to set a 5-minute expiration: <pre><code>await context.kv().expire(\"session_token\", ttl_s=300)\n</code></pre></p> <p>Updating the TTL for a cached user profile: <pre><code>await context.kv().expire(\"user_profile:42\", ttl_s=60)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The string key whose expiration time should be set or updated.</p> required <code>ttl_s</code> <code>int</code> <p>The time-to-live in seconds from now. After this duration, the key will expire.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> purge_expired(limit) <p>Remove expired key-value entries from the ephemeral store.</p> <p>This method scans the internal data store for entries whose expiration timestamp has passed and removes them, up to the specified limit. It is intended to be called periodically to keep the store clean and efficient.</p> <p>Examples:</p> <p>Purge up to 1000 expired entries: <pre><code>removed = await context.kv().purge_expired()\n</code></pre></p> <p>Purge a custom number of expired entries: <pre><code>removed = await context.kv().purge_expired(limit=500)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of expired entries to remove in a single call.</p> <code>1000</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of expired entries that were removed from the store.</p>"},{"location":"reference/context-llm/","title":"<code>context.llm()</code> \u2013 LLM Client &amp; Profiles API Reference","text":"<p><code>context.llm()</code> returns an LLM client (profile\u2011aware) with a consistent API across providers (OpenAI, Azure OpenAI, Anthropic, Google, OpenRouter, LM Studio, Ollama). Use it for chat, embeddings, and raw HTTP calls with built\u2011in retries and sane defaults.</p> <p>See LLM Setup &amp; Providers for configuring providers, base URLs, and API keys.</p>"},{"location":"reference/context-llm/#profiles-configuration","title":"Profiles &amp; Configuration","text":"<ul> <li>Profiles: Named client configs (default: <code>\"default\"</code>).</li> <li>Get existing: <code>client = context.llm()</code> or <code>context.llm(profile=\"myprofile\")</code>.</li> <li>Override/update: Pass any of <code>provider/model/base_url/api_key/azure_deployment/timeout</code> to create or update a profile at runtime.</li> <li>Quick set key: <code>context.llm_set_key(provider, model, api_key, profile=\"default\")</code>.</li> </ul> <p>Supported providers: <code>openai</code>, <code>azure</code>, <code>anthropic</code>, <code>google</code>, <code>openrouter</code>, <code>lmstudio</code>, <code>ollama</code>.</p>"},{"location":"reference/context-llm/#0-llm-setup","title":"0. LLM Setup","text":"context.llm(profile, *, provider, model, base_url, ...) <p>Retrieve or configure an LLM client for this context.</p> <p>This method allows you to access a language model client by profile name, or dynamically override its configuration at runtime.</p> <p>Examples:</p> <p>Get the default LLM client: <pre><code>llm = context.llm()\nresponse = await llm.complete(\"Hello, world!\")\n</code></pre></p> <p>Use a custom profile: <pre><code>llm = context.llm(profile=\"my-profile\")\n</code></pre></p> <p>Override provider and model for a one-off call: <pre><code>llm = context.llm(\n    provider=Provider.OpenAI,\n    model=\"gpt-4-turbo\",\n    api_key=\"sk-...\",\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>profile</code> <code>str</code> <p>The profile name to use (default: \"default\"). Set up in <code>.env</code> or <code>register_llm_client()</code> method.</p> <code>'default'</code> <code>provider</code> <code>Provider | None</code> <p>Optionally override the provider (e.g., <code>Provider.OpenAI</code>).</p> <code>None</code> <code>model</code> <code>str | None</code> <p>Optionally override the model name.</p> <code>None</code> <code>base_url</code> <code>str | None</code> <p>Optionally override the base URL for the LLM API.</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>Optionally override the API key for authentication.</p> <code>None</code> <code>azure_deployment</code> <code>str | None</code> <p>Optionally specify an Azure deployment name.</p> <code>None</code> <code>timeout</code> <code>float | None</code> <p>Optionally set a request timeout (in seconds).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LLMClientProtocol</code> <code>LLMClientProtocol</code> <p>The configured LLM client instance for this context.</p> context.llm_set_key.channel(provider, model, api_key, profile) <p>Quickly configure or override the LLM provider, model, and API key for a given profile.</p> <p>This method allows you to update the credentials and model configuration for a specific LLM profile at runtime. It is useful for dynamically switching providers or rotating keys without restarting the application.</p> <p>Examples:</p> <p>Set the OpenAI API key for the default profile: <pre><code>context.llm_set_key(\n    provider=\"openai\",\n    model=\"gpt-4-turbo\",\n    api_key=\"sk-...\",\n)\n</code></pre></p> <p>Configure a custom profile for Anthropic: <pre><code>context.llm_set_key(\n    provider=\"anthropic\",\n    model=\"claude-3-opus\",\n    api_key=\"sk-ant-...\",\n    profile=\"anthropic-profile\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>The LLM provider name (e.g., \"openai\", \"anthropic\").</p> required <code>model</code> <code>str</code> <p>The model name or identifier to use.</p> required <code>api_key</code> <code>str</code> <p>The API key or credential for the provider.</p> required <code>profile</code> <code>str</code> <p>The profile name to update (default: \"default\").</p> <code>'default'</code> <p>Returns:</p> Type Description <p>None. The profile is updated in-place and will be used for subsequent calls</p> <p>to <code>context.llm(profile=...)</code>.</p>"},{"location":"reference/context-llm/#1-main-apis","title":"1. Main APIs","text":"chat(messages, *, reasoning_effort, max_output_tokens, ...) <p>Send a chat request to the LLM provider and return the response in a normalized format. This method handles provider-specific dispatch, output postprocessing, rate limiting, and usage metering. It supports structured output via JSON schema validation and flexible output formats.</p> <p>Examples:</p> <p>Basic usage with a list of messages: <pre><code>response, usage = await context.llm().chat([\n    {\"role\": \"user\", \"content\": \"Hello, assistant!\"}\n])\n</code></pre></p> <p>Requesting structured output with a JSON schema: <pre><code>response, usage = await context.llm().chat(\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this text.\"}],\n    output_format=\"json\",\n    json_schema={\"type\": \"object\", \"properties\": {\"summary\": {\"type\": \"string\"}}}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[dict[str, Any]]</code> <p>List of message dicts, each with \"role\" and \"content\" keys.</p> required <code>reasoning_effort</code> <code>str | None</code> <p>Optional string to control model reasoning depth.</p> <code>None</code> <code>max_output_tokens</code> <code>int | None</code> <p>Optional maximum number of output tokens.</p> <code>None</code> <code>output_format</code> <code>ChatOutputFormat</code> <p>Output format, e.g., \"text\" or \"json\".</p> <code>'text'</code> <code>json_schema</code> <code>dict[str, Any] | None</code> <p>Optional JSON schema for validating structured output.</p> <code>None</code> <code>schema_name</code> <code>str</code> <p>Name for the root schema object (default: \"output\").</p> <code>'output'</code> <code>strict_schema</code> <code>bool</code> <p>If True, enforce strict schema validation.</p> <code>True</code> <code>validate_json</code> <code>bool</code> <p>If True, validate JSON output against schema.</p> <code>True</code> <code>fail_on_unsupported</code> <code>bool</code> <p>If True, raise error for unsupported features.</p> <code>True</code> <code>**kw</code> <code>Any</code> <p>Additional provider-specific keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[str, dict[str, int]]</code> <p>tuple[str, dict[str, int]]: The model response (text or structured output) and usage statistics.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the provider is not supported.</p> <code>RuntimeError</code> <p>For various errors including invalid JSON output or rate limit violations.</p> <code>LLMUnsupportedFeatureError</code> <p>If a requested feature is unsupported by the provider.</p> Notes <ul> <li>This method centralizes handling of different LLM providers, ensuring consistent behavior.</li> <li>Structured output support allows for robust integration with downstream systems.</li> <li>Rate limiting and metering help manage resource usage effectively.</li> </ul> generate_image(prompt, *, model, n, ...) <p>Generate images from a text prompt using the configured LLM provider.</p> <p>This method supports provider-agnostic image generation, including OpenAI, Azure, and Google Gemini. It automatically handles rate limiting, usage metering, and provider-specific options.</p> <p>Examples:</p> <p>Basic usage with a prompt: <pre><code>result = await context.llm().generate_image(\"A cat riding a bicycle\")\n</code></pre></p> <p>Requesting multiple images with custom size and style: <pre><code>result = await context.llm().generate_image(\n    \"A futuristic cityscape\",\n    n=3,\n    size=\"1024x1024\",\n    style=\"vivid\"\n)\n</code></pre></p> <p>Supplying input images for edit-style generation (Gemini): <pre><code>result = await context.llm().generate_image(\n    \"Make this image brighter\",\n    input_images=[my_data_url]\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The text prompt describing the desired image(s).</p> required <code>model</code> <code>str | None</code> <p>Optional model name to override the default.</p> <code>None</code> <code>n</code> <code>int</code> <p>Number of images to generate (default: 1).</p> <code>1</code> <code>size</code> <code>str | None</code> <p>Image size, e.g., \"1024x1024\".</p> <code>None</code> <code>quality</code> <code>str | None</code> <p>Image quality setting (provider-specific).</p> <code>None</code> <code>style</code> <code>str | None</code> <p>Artistic style (provider-specific).</p> <code>None</code> <code>output_format</code> <code>ImageFormat | None</code> <p>Desired image format, e.g., \"png\", \"jpeg\".</p> <code>None</code> <code>response_format</code> <code>ImageResponseFormat | None</code> <p>Response format, e.g., \"url\" or \"b64_json\".</p> <code>None</code> <code>background</code> <code>str | None</code> <p>Background setting, e.g., \"transparent\".</p> <code>None</code> <code>input_images</code> <code>list[str] | None</code> <p>List of input images (as data URLs) for edit-style generation.</p> <code>None</code> <code>azure_api_version</code> <code>str | None</code> <p>Azure-specific API version override.</p> <code>None</code> <code>**kw</code> <code>Any</code> <p>Additional provider-specific keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ImageGenerationResult</code> <code>ImageGenerationResult</code> <p>An object containing generated images, usage statistics, and raw response data.</p> <p>Raises:</p> Type Description <code>LLMUnsupportedFeatureError</code> <p>If the provider does not support image generation.</p> <code>RuntimeError</code> <p>For provider-specific errors or invalid configuration.</p> Notes <ul> <li>This method is accessed via <code>context.llm().generate_image(...)</code>.</li> <li>Usage metering and rate limits are enforced automatically. However, token usage is typically not reported for image generation.</li> <li>The returned <code>ImageGenerationResult</code> includes both images and metadata.</li> </ul> embed(texts, ...) <p>Generate vector embeddings for a batch of texts using the configured LLM provider.</p> <p>This method provides a provider-agnostic interface for embedding text, automatically handling model selection, batching, and provider-specific API quirks. It ensures the output shape matches the input and raises informative errors for configuration issues.</p> <p>Examples:</p> <p>Basic usage with a list of texts: <pre><code>embeddings = await context.llm().embed([\n    \"The quick brown fox.\",\n    \"Jumped over the lazy dog.\"\n])\n</code></pre></p> <p>Specifying a custom embedding model: <pre><code>embeddings = await context.llm().embed(\n    [\"Hello world!\"],\n    model=\"text-embedding-3-large\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>list[str]</code> <p>List of input strings to embed.</p> required <code>model</code> <p>Optional model name to override the default embedding model.</p> required <code>azure_api_version</code> <p>Optional Azure API version override.</p> required <code>extra_body</code> <p>Optional dict of extra fields to pass to the provider.</p> required <code>**kw</code> <p>Additional provider-specific keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[list[float]]</code> <p>list[list[float]]: List of embedding vectors, one per input text.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>texts</code> is not a list of strings.</p> <code>RuntimeError</code> <p>For provider/model/configuration errors or shape mismatches.</p> <code>NotImplementedError</code> <p>If embeddings are not supported for the provider.</p> Notes <ul> <li>For Google Gemini, uses batch embedding if available, otherwise falls back to per-item embedding.</li> <li>For Azure, requires <code>azure_deployment</code> to be set.</li> <li>The returned list always matches the length of <code>texts</code>.</li> </ul>"},{"location":"reference/context-llm/#2-raw-api","title":"2. Raw API","text":"raw(*, method, path, url, ...) <p>Send a low-level HTTP request using the configured LLM provider\u2019s client.</p> <p>This method provides direct access to the underlying HTTP transport, automatically applying provider-specific authentication, base URL resolution, and retry logic. It is intended for advanced use cases where you need to call custom endpoints or experiment with provider APIs not covered by higher-level methods.</p> <p>Examples:</p> <p>Basic usage with a relative path: <pre><code>result = await context.llm().raw(\n    method=\"POST\",\n    path=\"/custom/endpoint\",\n    json={\"foo\": \"bar\"}\n)\n</code></pre></p> <p>Sending a GET request to an absolute URL: <pre><code>response = await context.llm().raw(\n    method=\"GET\",\n    url=\"https://api.openai.com/v1/models\",\n    return_response=True\n)\n</code></pre></p> <p>Overriding headers and query parameters: <pre><code>result = await context.llm().raw(\n    path=\"/v1/special\",\n    headers={\"X-Custom\": \"123\"},\n    params={\"q\": \"search\"}\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>HTTP method to use (e.g., \"POST\", \"GET\").</p> <code>'POST'</code> <code>path</code> <code>str | None</code> <p>Relative path to append to the provider\u2019s base URL.</p> <code>None</code> <code>url</code> <code>str | None</code> <p>Absolute URL to call (overrides <code>path</code> and <code>base_url</code>).</p> <code>None</code> <code>json</code> <code>Any | None</code> <p>JSON-serializable body to send with the request.</p> <code>None</code> <code>params</code> <code>dict[str, Any] | None</code> <p>Dictionary of query parameters.</p> <code>None</code> <code>headers</code> <code>dict[str, str] | None</code> <p>Dictionary of HTTP headers to override defaults.</p> <code>None</code> <code>return_response</code> <code>bool</code> <p>If True, return the raw <code>httpx.Response</code> object; otherwise, return the parsed JSON response.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The parsed JSON response by default, or the raw <code>httpx.Response</code></p> <code>Any</code> <p>if <code>return_response=True</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>url</code> nor <code>path</code> is provided.</p> <code>RuntimeError</code> <p>For HTTP errors or provider-specific failures.</p> Notes <ul> <li>This method is accessed via <code>context.llm().raw(...)</code>.</li> <li>Provider authentication and retry logic are handled automatically.</li> <li>Use with caution; malformed requests may result in provider errors.</li> </ul>"},{"location":"reference/context-logger/","title":"<code>context.logger()</code> \u2013 Contextual Logger API Reference","text":"<p>The <code>context.logger()</code> function returns a Logger instance automatically bound to the current run, node, and, when available, graph context. This logger provides the standard Python logging interface, while automatically injecting structured fields such as <code>run_id</code>, <code>node_id</code>, and <code>graph_id</code> into every log record.</p> <p>Configuration: Set logging options when starting the sidecar: <pre><code>from aethergraph import start_server\n\nstart_server(log_level=\"warning\", ...)\n</code></pre> The runtime also respects environment and application settings (e.g., JSON vs. text output, log rotation, per-namespace log levels).</p>"},{"location":"reference/context-logger/#log-record-fields","title":"Log Record Fields","text":"<p>Each log record includes:</p> <ul> <li><code>run_id</code>, <code>node_id</code>, <code>graph_id</code> (when available)</li> <li>Logger namespace (e.g., <code>aethergraph.node.&lt;id&gt;</code>)</li> <li>Message, level, timestamp</li> <li>Optional <code>metrics</code> or custom fields via <code>extra={...}</code></li> </ul>"},{"location":"reference/context-logger/#quick-reference-logger-methods","title":"Quick Reference: Logger Methods","text":"<p>Use the logger as you would a standard Python <code>Logger</code>:</p> Method Purpose Inputs Notes <code>debug(msg, *args, **kwargs)</code> Verbose diagnostics <code>msg: str</code>, <code>*args</code>, <code>**kwargs</code> For high-volume internal logs. <code>info(msg, *args, **kwargs)</code> General information <code>msg: str</code>, <code>*args</code>, <code>**kwargs</code> Default level; typically visible. <code>warning(msg, *args, **kwargs)</code> Non-fatal issues <code>msg: str</code>, <code>*args</code>, <code>**kwargs</code> Alias: <code>warn()</code> (deprecated in stdlib). <code>error(msg, *args, **kwargs)</code> Recoverable errors <code>msg: str</code>, <code>*args</code>, <code>**kwargs</code> Use <code>exc_info=True</code> for tracebacks. <code>exception(msg, *args, **kwargs)</code> Error with traceback <code>msg: str</code>, <code>*args</code>, <code>**kwargs</code> Same as <code>error(..., exc_info=True)</code>. <code>critical(msg, *args, **kwargs)</code> Severe/fatal conditions <code>msg: str</code>, <code>*args</code>, <code>**kwargs</code> May trigger alerts depending on sink. <p>Common <code>kwargs</code>:</p> <ul> <li><code>extra: dict</code> \u2014 Attach additional structured fields (merged with contextual fields).</li> <li><code>exc_info: bool | BaseException</code> \u2014 Include exception traceback.</li> </ul>"},{"location":"reference/context-mcp/","title":"<code>context.mcp()</code> \u2013 Model Context Protocol (MCP) API Reference","text":"<p><code>context.mcp()</code> gives you access to the MCP service, which manages named MCP clients (stdio / WebSocket / HTTP), handles lifecycle (open/close), and offers thin call helpers.</p> <p>Register clients after the sidecar starts. Example below uses the helper <code>register_mcp_client()</code>.</p>"},{"location":"reference/context-mcp/#1-mcp-clients","title":"1. MCP Clients","text":"<p>Three MCP clients can be established: </p> HttpMCPClient(name, client) <p>               Bases: <code>MCPClientProtocol</code></p> <p>Initialize the HTTP client service with base URL, headers, and timeout.</p> <p>This constructor sets up the base URL for all requests, applies default and custom headers, and configures the request timeout. It also initializes internal state for the asynchronous HTTP client and concurrency control.</p> <p>Examples:</p> <p>Basic usage with default headers: <pre><code>from aethergraph.services.mcp import HttpMCPClient\nclient = HttpMCPClient(\"https://api.example.com\")\n</code></pre></p> <p>Custom headers and timeout: <pre><code>from aethergraph.services.mcp import HttpMCPClient\nclient = HttpMCPClient(\n    \"https://api.example.com\",\n    headers={\"Authorization\": \"Bearer &lt;token&gt;\"},\n    timeout=30.0\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>The root URL for all HTTP requests (e.g., \"https://api.example.com\").</p> required <code>headers</code> <code>dict[str, str] | None</code> <p>Optional dictionary of additional HTTP headers to include with each request.</p> <code>None</code> <code>The \"Content-Type</code> <p>application/json\" header is always set by default.</p> required <code>timeout</code> <code>float</code> <p>The maximum time (in seconds) to wait for a response before timing out.</p> <code>60.0</code> <p>Returns:</p> Name Type Description <code>None</code> <p>Initializes the HttpMCPClient instance.</p> Notes <ul> <li>Ensure that the base_url does not have a trailing slash; it will be added automatically.</li> <li>The client uses asynchronous HTTP requests for non-blocking operations.</li> </ul> WsMCPClient(name, client) <p>               Bases: <code>MCPClientProtocol</code></p> <p>Initialize the WebSocket MCP client with URL, headers, and connection parameters. This class manages a WebSocket connection to an MCP (Modular Control Protocol) server, handling connection lifecycle, pinging for keepalive, and concurrency for sending JSON-RPC requests. It provides methods to list tools, call tools, list resources, and read resources via the MCP protocol.</p> <p>Examples:</p> <p>Basic usage with default headers: <pre><code>from aethergraph.services.mcp import WsMCPClient\nclient = WsMCPClient(\"wss://mcp.example.com/ws\")\nawait client.open()\ntools = await client.list_tools()\nawait client.close()\n</code></pre></p> <p>Custom headers and ping interval: <pre><code>from aethergraph.services.mcp import WsMCPClient\nclient = WsMCPClient(\n    \"wss://mcp.example.com/ws\",\n    headers={\"Authorization\": \"Bearer &lt;token&gt;\"},\n    ping_interval=10.0,\n    timeout=30.0\nawait client.open()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The WebSocket URL of the MCP server (e.g., \"wss://mcp.example.com/ws\").</p> required <code>headers</code> <code>dict[str, str] | None</code> <p>Optional dictionary of additional HTTP headers to include in the WebSocket handshake.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Maximum time (in seconds) to wait for connection and responses.</p> <code>60.0</code> <code>ping_interval</code> <code>float</code> <p>Interval (in seconds) between WebSocket ping frames for keepalive.</p> <code>20.0</code> <code>ping_timeout</code> <code>float</code> <p>Maximum time (in seconds) to wait for a ping response before considering the connection dead.</p> <code>10.0</code> <p>Returns:</p> Name Type Description <code>None</code> <p>Initializes the WsMCPClient instance and prepares internal state.</p> StdioMCPClient(name, client) <p>               Bases: <code>MCPClientProtocol</code></p> <p>Initialize the MCP client service to communicate with a subprocess over stdio using JSON-RPC 2.0.</p> <p>This class launches a subprocess (typically an MCP server), manages its lifecycle, and provides asynchronous methods to interact with it using JSON-RPC 2.0 over standard input/output streams. It handles command execution, environment setup, request/response serialization, and concurrency control for safe multi-call usage.</p> <p>Examples:</p> <p>Basic usage with default environment: <pre><code>from aethergraph.services.mcp import StdioMCPClient\nclient = StdioMCPClient([\"python\", \"mcp_server.py\"])\nawait client.open()\ntools = await client.list_tools()\nawait client.close()\n</code></pre></p> <p>Custom environment and timeout: <pre><code>from aethergraph.services.mcp import StdioMCPClient\nclient = StdioMCPClient(\n    [\"python\", \"mcp_server.py\"],\n    env={\"MY_ENV_VAR\": \"value\"},\n    timeout=30.0\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <code>list[str]</code> <p>Command to start the MCP server subprocess (list of str).</p> required <code>env</code> <code>dict[str, str] | None</code> <p>Optional dictionary of environment variables for the subprocess.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Timeout in seconds for each RPC call.</p> <code>60.0</code> <p>Returns:</p> Name Type Description <code>None</code> <p>Initializes the StdioMCPClient instance.</p> Notes <ul> <li>The subprocess should adhere to the JSON-RPC 2.0 specification over stdio.</li> <li>Ensure proper error handling in the subprocess to avoid deadlocks.</li> </ul>"},{"location":"reference/context-mcp/#2-registration","title":"2. Registration","text":"register_mcp_client(name, client) <p>Register a new MCP client with the current MCP service.</p> <p>This function adds a client instance to the MCP service under the specified name, allowing it to be accessed and managed by the MCP infrastructure.</p> <p>Examples:</p> <pre><code>from aethergraph.runtime import register_mcp_client\nfrom aethergraph.services.mcp import HttpMCPClient\nmy_client = HttpMCPClient(\"https://mcp.example.com\", ...)\nregister_mcp_client(\"myclient\", my_client)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name to associate with the MCP client.</p> required <code>client</code> <code>Any</code> <p>The client instance to register.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no MCP service has been installed via set_mcp_service().</p> list_mcp_clients(name, client) <p>List all registered MCP client names in the current MCP service.</p> <p>This function returns a list of all client names that have been registered with the MCP service, allowing for discovery and management of available clients.</p> <p>Examples:</p> <pre><code>from aethergraph.runtime import list_mcp_clients\nclients = list_mcp_clients()\nprint(clients)\n</code></pre> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of strings representing the names of registered MCP clients.</p> <code>list[str]</code> <p>Returns an empty list if no MCP service is installed or no clients are registered.</p>"},{"location":"reference/context-mcp/#3-service-methods","title":"3. Service Methods","text":"register(name, client) <p>Register a new MCP client under a given name.</p> <p>Examples:</p> <pre><code>context.mcp().register(\"myserver\", my_client)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to register the client under.</p> required <code>client</code> <code>MCPClientProtocol</code> <p>The MCPClientProtocol instance to register.</p> required remove(name) <p>Remove a registered MCP client by name.</p> <p>Examples:</p> <pre><code>context.mcp().remove(\"myserver\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the client to remove.</p> required has(name) <p>Check if a client with the given name is registered.</p> <p>Examples:</p> <pre><code>if context.mcp().has(\"default\"):\n    print(\"Client exists\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the client exists, False otherwise.</p> names() <p>Get a list of all registered client names.</p> <p>Examples:</p> <pre><code>names = context.mcp().names()\n</code></pre> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of registered client names.</p> list_clients() <p>List all registered client names.</p> <p>Examples:</p> <pre><code>clients = context.mcp().list_clients()\n</code></pre> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of registered client names.</p> get(name='default') <p>Retrieve a registered MCP client by name.</p> <p>Examples:</p> <pre><code>client = context.mcp().get(\"default\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the client to retrieve.</p> <code>'default'</code> <p>Returns:</p> Name Type Description <code>MCPClientProtocol</code> <code>MCPClientProtocol</code> <p>The registered client.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the client is not found.</p> open(name) <p>Open the connection for a specific MCP client.</p> <p>Examples:</p> <pre><code>await context.mcp().open(\"default\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the client to open.</p> required close(name) <p>Close the connection for a specific MCP client.</p> <p>Examples:</p> <pre><code>await context.mcp().close(\"default\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the client to close.</p> required open_all() <p>Open all registered MCP client connections.</p> <p>Examples:</p> <pre><code>await context.mcp().open_all()\n</code></pre> close_all() <p>Close all registered MCP client connections.</p> <p>Examples:</p> <pre><code>await context.mcp().close_all()\n</code></pre> call(name, tool, params=None) <p>Call a tool on a specific MCP client, opening the connection if needed.</p> <p>Examples:</p> <pre><code>result = await context.mcp().call(\"default\", \"sum\", {\"a\": 1, \"b\": 2})\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the client to use.</p> required <code>tool</code> <code>str</code> <p>The tool name to call.</p> required <code>params</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of parameters for the tool.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The result from the tool call.</p> list_tools(name) <p>List all tools available on a specific MCP client.</p> <p>Examples:</p> <pre><code>tools = await context.mcp().list_tools(\"default\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the client.</p> required <p>Returns:</p> Type Description <code>list[MCPTool]</code> <p>list[MCPTool]: List of available tools.</p> list_resources(name) <p>List all resources available on a specific MCP client.</p> <p>Examples:</p> <pre><code>resources = await context.mcp().list_resources(\"default\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the client.</p> required <p>Returns:</p> Type Description <code>list[MCPResource]</code> <p>list[MCPResource]: List of available resources.</p> read_resource(name, uri) <p>Read a resource from a specific MCP client.</p> <p>Examples:</p> <pre><code>data = await context.mcp().read_resource(\"default\", \"resource://foo/bar\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the client.</p> required <code>uri</code> <code>str</code> <p>The URI of the resource to read.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The resource data.</p> set_header(name, key, value) <p>Set or override a header for a websocket client at runtime.</p> <p>Examples:</p> <pre><code>context.mcp().set_header(\"default\", \"Authorization\", \"Bearer token\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the client.</p> required <code>key</code> <code>str</code> <p>The header key.</p> required <code>value</code> <code>str</code> <p>The header value.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the client does not support headers.</p> persist_secret(secret_name, value) <p>Persist a secret using the configured secrets provider.</p> <p>Examples:</p> <pre><code>context.mcp().persist_secret(\"API_KEY\", \"my-secret-value\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>The name of the secret.</p> required <code>value</code> <code>str</code> <p>The value to persist.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the secrets provider is not writable.</p>"},{"location":"reference/context-mcp/#registering-clients-after-sidecar-starts","title":"Registering Clients (after sidecar starts)","text":"<p>Start the sidecar, then register one or more MCP clients:</p> <pre><code>from aethergraph import start_server\nfrom aethergraph.runtime import register_mcp_client\nfrom aethergraph.plugins.mcp import StdioMCPClient, WsMCPClient, HttpMCPClient\nimport sys\n\n# 1) Start sidecar (choose your port/logging as you prefer)\nstart_server(port=0)\n\n# 2) Register a local stdio MCP (no auth/network)\nregister_mcp_client(\n    \"local\",\n    client=StdioMCPClient(cmd=[sys.executable, \"-m\", \"aethergraph.plugins.mcp.fs_server\"]),\n)\n\n# 3) (Optional) Register a WebSocket MCP\nregister_mcp_client(\n    \"ws\",\n    client=WsMCPClient(\n        url=\"wss://mcp.example.com/ws\",\n        headers={\"Authorization\": \"Bearer &lt;token&gt;\"},\n        timeout=60.0,\n    ),\n)\n\n# 4) (Optional) Register an HTTP MCP\nregister_mcp_client(\n    \"http\",\n    client=HttpMCPClient(\n        base_url=\"https://mcp.example.com/api\",\n        headers={\"Authorization\": \"Bearer &lt;token&gt;\"},\n        timeout=60.0,\n    ),\n)\n</code></pre> <p>Client types:</p> <ul> <li><code>StdioMCPClient(cmd, env=None, timeout=60.0)</code> \u2013 JSON\u2011RPC over stdio to a subprocess.</li> <li><code>WsMCPClient(url, headers=None, timeout=60.0, ping_interval=20.0, ping_timeout=10.0)</code> \u2013 JSON\u2011RPC over WebSocket.</li> <li><code>HttpMCPClient(base_url, headers=None, timeout=60.0)</code> \u2013 JSON\u2011RPC over HTTP.</li> </ul>"},{"location":"reference/context-mcp/#examples","title":"Examples","text":"<pre><code># Get tool list and call a tool\nmcp = context.mcp()\n\ntools = await mcp.list_tools(\"local\")\nres = await mcp.call(\"local\", tool=\"fs.read_text\", params={\"path\": \"/etc/hosts\"})\n\n# Read a resource listing from WS backend\nawait mcp.open(\"ws\")\nresources = await mcp.list_resources(\"ws\")\n\n# Set a header on the WS client (e.g., late\u2011bound token)\nmcp.set_header(\"ws\", \"Authorization\", \"Bearer NEW_TOKEN\")\n\n# Clean up\nawait mcp.close_all()\n</code></pre> <p>Notes:</p> <ul> <li>Clients lazy\u2011open for operations; you may still call <code>open()</code> explicitly.</li> <li>Errors from the server propagate; inspect tool/resource contracts on the MCP server side for required params and shapes.</li> </ul>"},{"location":"reference/context-memory/","title":"<code>context.memory()</code> \u2013 MemoryFacade API Reference","text":"<p><code>MemoryFacade</code> coordinates HotLog (fast recent events), Persistence (durable JSONL), and Indices (derived KV views).</p> <p>It is accessed via <code>node_context.memory</code> but aggregates functionality from several internal mixins.</p> <p>Methods for vector-memory storage and search is under development.</p>"},{"location":"reference/context-memory/#1-core-recording","title":"1. Core Recording","text":"<p>Basic event logging and raw data access for general messages/memory.  </p> record_raw(*, base, text, ...) <p>Record an unstructured event with optional preview text and metrics.</p> <p>This method generates a stable event ID, populates standard fields (e.g., <code>run_id</code>, <code>scope_id</code>, <code>severity</code>, <code>signal</code>), and appends the event to both the HotLog and Persistence layers. Additionally, it records a metering event for tracking purposes.</p> <p>Examples:</p> <p>Basic usage with minimal fields: <pre><code>await context.memory().record_raw(\n    base={\"kind\": \"user_action\", \"severity\": 2},\n    text=\"User clicked a button.\"\n)\n</code></pre></p> <p>Including metrics and additional fields: <pre><code>await context.memory().record_raw(\n    base={\"kind\": \"tool_call\", \"stage\": \"execution\", \"severity\": 3},\n    text=\"Tool executed successfully.\",\n    metrics={\"latency\": 0.123, \"tokens_used\": 45}\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>dict[str, Any]</code> <p>A dictionary containing event fields such as <code>kind</code>, <code>stage</code>, <code>data</code>, <code>tags</code>, <code>severity</code>, etc.</p> required <code>text</code> <code>str | None</code> <p>Optional preview text for the event. If None, it is derived from the <code>data</code> field in <code>base</code>.</p> <code>None</code> <code>metrics</code> <code>dict[str, float] | None</code> <p>Optional dictionary of numeric metrics (e.g., latency, token usage) to include in the event.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Event</code> <code>Event</code> <p>The fully constructed and persisted <code>Event</code> object.</p> record(kind, data, tags, ...) <p>Record an event with common fields.</p> <p>This method standardizes event creation by populating fields such as <code>kind</code>, <code>severity</code>, <code>tags</code>, and <code>metrics</code>. It also supports optional references for inputs and outputs, and allows for signal strength overrides.</p> <p>Examples:</p> <p>Basic usage for a user action: <pre><code>await context.memory().record(\n    kind=\"user_action\",\n    data={\"action\": \"clicked_button\"},\n    tags=[\"ui\", \"interaction\"]\n)\n</code></pre></p> <p>Recording a tool execution with metrics: <pre><code>await context.memory().record(\n    kind=\"tool_call\",\n    data={\"tool\": \"search\", \"query\": \"weather\"},\n    metrics={\"latency\": 0.123, \"tokens_used\": 45},\n    severity=3\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>Logical kind of event (e.g., <code>\"user_msg\"</code>, <code>\"tool_call\"</code>, <code>\"chat_turn\"</code>).</p> required <code>data</code> <code>Any</code> <p>JSON-serializable content or string providing event details.</p> required <code>tags</code> <code>list[str] | None</code> <p>A list of string labels for categorization. Defaults to None.</p> <code>None</code> <code>severity</code> <code>int</code> <p>An integer (1-3) indicating importance. Defaults to 2.</p> <code>2</code> <code>stage</code> <code>str | None</code> <p>Optional stage of the event (e.g., <code>\"user\"</code>, <code>\"assistant\"</code>, <code>\"system\"</code>). Defaults to None.</p> <code>None</code> <code>inputs_ref</code> <p>Optional references for input values. Defaults to None.</p> <code>None</code> <code>outputs_ref</code> <p>Optional references for output values. Defaults to None.</p> <code>None</code> <code>metrics</code> <code>dict[str, float] | None</code> <p>A dictionary of numeric metrics (e.g., latency, token usage). Defaults to None.</p> <code>None</code> <code>signal</code> <code>float | None</code> <p>Manual override for the signal strength (0.0 to 1.0). If None, it is calculated heuristically.</p> <code>None</code> <code>text</code> <code>str | None</code> <p>Optional preview text override. If None, it is derived from <code>data</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Event</code> <code>Event</code> <p>The fully constructed and persisted <code>Event</code> object.</p> recent(kinds, limit) <p>Retrieve recent events.</p> <p>This method fetches a list of recent events, optionally filtered by kinds.</p> <p>Parameters:</p> Name Type Description Default <code>kinds</code> <code>list[str] | None</code> <p>A list of event kinds to filter by. Defaults to None.</p> <code>None</code> <code>limit</code> <code>int</code> <p>The maximum number of events to retrieve. Defaults to 50.</p> <code>50</code> <p>Returns:</p> Type Description <code>list[Event]</code> <p>list[Event]: A list of recent events.</p> Notes <p>This method interacts with the underlying HotLog service to fetch events associated with the current timeline. The events are returned in chronological order, with the most recent events appearing last in the list. Memory out of the limit will be discarded in the HotLog layer (but persistent in the Persistence layer). Memory in persistence cannot be retrieved via this method.</p> recent_data(kinds, limit) <p>Retrieve recent event data.</p> <p>This method fetches the data or text of recent events, optionally filtered by kinds and tags. Unlike <code>recent()</code>, which returns full Event objects, this method extracts and returns only the data or text content of the events. This is useful for scenarios where only the event payloads are needed.</p> <p>Parameters:</p> Name Type Description Default <code>kinds</code> <code>list[str] | None</code> <p>A list of event kinds to filter by. Defaults to None.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>A list of tags to filter events by. Defaults to None.</p> <code>None</code> <code>limit</code> <code>int</code> <p>The maximum number of events to retrieve. Defaults to 50.</p> <code>50</code> <p>Returns:</p> Type Description <code>list[Any]</code> <p>list[Any]: A list of event data or text.</p> Notes <p>This method first retrieves recent events using the <code>recent()</code> method and then filters them based on the provided tags. It extracts the <code>data</code> attribute if available; otherwise, it attempts to parse the <code>text</code> attribute as JSON. If parsing fails, the raw text is returned.</p> <p>Memory out of the limit will be discarded in the HotLog layer (but persistent in the Persistence layer). Memory in persistence cannot be retrieved via this method.</p> search(query, kinds, ...) <p>Search for events based on a query.</p> <p>This method searches for events that match a query, optionally filtered by kinds and tags. Note that this implementation currently performs a lexical search. Embedding-based search is planned for future development.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query string.</p> required <code>kinds</code> <code>list[str] | None</code> <p>A list of event kinds to filter by. Defaults to None.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>A list of tags to filter events by. Defaults to None.</p> <code>None</code> <code>limit</code> <code>int</code> <p>The maximum number of events to retrieve. Defaults to 100.</p> <code>100</code> <code>use_embedding</code> <code>bool</code> <p>Whether to use embedding-based search. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[Event]</code> <p>list[Event]: A list of events matching the query.</p> Notes <p>This method retrieves recent events using the <code>recent()</code> method and filters them based on the provided tags. It performs a simple lexical search on the event text. Embedding-based search functionality is not yet implemented.</p> <p>Memory out of the limit will be discarded in the HotLog layer (but persistent in the Persistence layer). Memory in persistence cannot be retrieved via this method.</p>"},{"location":"reference/context-memory/#2-chat-operations","title":"2. Chat Operations","text":"<p>Convenience method for recording chat-related memory events.</p> record_chat(role, text, ...) <p>Record a single chat turn in a normalized format.</p> <p>This method automatically handles timestamping, standardizes the <code>role</code>, and dispatches the event to the configured persistence layer.</p> <p>Examples:</p> <p>Basic usage for a user message: <pre><code>await context.memory().record_chat(\"user\", \"Hello graph!\")\n</code></pre></p> <p>Recording a tool output with extra metadata: <pre><code>await context.memory().record_chat(\n    \"tool\",\n    \"Search results found.\",\n    data={\"query\": \"weather\", \"hits\": 5}\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>Literal['user', 'assistant', 'system', 'tool']</code> <p>The semantic role of the speaker. Must be one of: <code>\"user\"</code>, <code>\"assistant\"</code>, <code>\"system\"</code>, or <code>\"tool\"</code>.</p> required <code>text</code> <code>str</code> <p>The primary text content of the message.</p> required <code>tags</code> <code>list[str] | None</code> <p>A list of string labels for categorization. The tag <code>\"chat\"</code> is automatically appended to this list.</p> <code>None</code> <code>data</code> <code>dict[str, Any] | None</code> <p>Arbitrary JSON-serializable dictionary containing extra context (e.g., token counts, model names).</p> <code>None</code> <code>severity</code> <code>int</code> <p>An integer (1-3) indicating importance. (1=Low, 2=Normal, 3=High).</p> <code>2</code> <code>signal</code> <code>float | None</code> <p>Manual override for the signal strength (0.0 to 1.0). If None, it is calculated heuristically.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Event</code> <code>Event</code> <p>The fully persisted <code>Event</code> object containing the generated ID and timestamp.</p> record_chat_user(text, *, ...) <p>Record a user chat turn in a normalized format.</p> <p>This method automatically handles timestamping, standardizes the <code>role</code>, and dispatches the event to the configured persistence layer.</p> <p>Examples:</p> <p>Basic usage for a user message: <pre><code>await context.memory().record_chat_user(\"Hello, how are you doing?\")\n</code></pre></p> <p>Recording a user message with extra metadata: <pre><code>await context.memory().record_chat_user(\n    \"I need help with my account.\",\n    tags=[\"support\", \"account\"],\n    data={\"issue\": \"login failure\"}\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The primary text content of the user's message.</p> required <code>tags</code> <code>list[str] | None</code> <p>A list of string labels for categorization. The tag <code>\"chat\"</code> is automatically appended to this list.</p> <code>None</code> <code>data</code> <code>dict[str, Any] | None</code> <p>Arbitrary JSON-serializable dictionary containing extra context (e.g., user metadata, session details).</p> <code>None</code> <code>severity</code> <code>int</code> <p>An integer (1-3) indicating importance. (1=Low, 2=Normal, 3=High). Defaults to 2.</p> <code>2</code> <code>signal</code> <code>float | None</code> <p>Manual override for the signal strength (0.0 to 1.0). If None, it is calculated heuristically.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Event</code> <code>Event</code> <p>The fully persisted <code>Event</code> object containing the generated ID and timestamp.</p> record_chat_assistant(text, *, ...) <p>Record an assistant chat turn in a normalized format.</p> <p>This method automatically handles timestamping, standardizes the <code>role</code>, and dispatches the event to the configured persistence layer.</p> <p>Examples:</p> <p>Basic usage for an assistant message: <pre><code>await context.memory().record_chat_assistant(\"How can I assist you?\")\n</code></pre></p> <p>Recording an assistant message with extra metadata: <pre><code>await context.memory().record_chat_assistant(\n    \"Here are the search results.\",\n    tags=[\"search\", \"response\"],\n    data={\"query\": \"latest news\", \"results_count\": 10}\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The primary text content of the assistant's message.</p> required <code>tags</code> <code>list[str] | None</code> <p>A list of string labels for categorization. The tag <code>\"chat\"</code> is automatically appended to this list.</p> <code>None</code> <code>data</code> <code>dict[str, Any] | None</code> <p>Arbitrary JSON-serializable dictionary containing extra context (e.g., token counts, model names).</p> <code>None</code> <code>severity</code> <code>int</code> <p>An integer (1-3) indicating importance. (1=Low, 2=Normal, 3=High).</p> <code>2</code> <code>signal</code> <code>float | None</code> <p>Manual override for the signal strength (0.0 to 1.0). If None, it is calculated heuristically.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Event</code> <code>Event</code> <p>The fully persisted <code>Event</code> object containing the generated ID and timestamp.</p> record_chat_system(text, *, ...) <p>Record a system message in a normalized format.</p> <p>This method automatically handles timestamping, standardizes the <code>role</code>, and dispatches the event to the configured persistence layer.</p> <p>Examples:</p> <p>Basic usage for a system message: <pre><code>await context.memory().record_chat_system(\"System initialized.\")\n</code></pre></p> <p>Recording a system message with extra metadata: <pre><code>await context.memory().record_chat_system(\n    \"Configuration updated.\",\n    tags=[\"config\", \"update\"],\n    data={\"version\": \"1.2.3\"}\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The primary text content of the system message.</p> required <code>tags</code> <code>list[str] | None</code> <p>A list of string labels for categorization. The tag <code>\"chat\"</code> is automatically appended to this list.</p> <code>None</code> <code>data</code> <code>dict[str, Any] | None</code> <p>Arbitrary JSON-serializable dictionary containing extra context (e.g., configuration details, system state).</p> <code>None</code> <code>severity</code> <code>int</code> <p>An integer (1-3) indicating importance. (1=Low, 2=Normal, 3=High). Defaults to 1.</p> <code>1</code> <code>signal</code> <code>float | None</code> <p>Manual override for the signal strength (0.0 to 1.0). If None, it is calculated heuristically.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Event</code> <code>Event</code> <p>The fully persisted <code>Event</code> object containing the generated ID and timestamp.</p> recent_chat(*, limit, roles) <p>Retrieve the most recent chat turns as a normalized list.</p> <p>This method fetches the last <code>limit</code> chat events of type <code>chat.turn</code> and returns them in a standardized format. Each item in the returned list contains the timestamp, role, text, and tags associated with the chat event.</p> <p>Examples:</p> <p>Fetch the last 10 chat turns: <pre><code>recent_chats = await context.memory().recent_chat(limit=10)\n</code></pre></p> <p>Fetch the last 20 chat turns for specific roles: <pre><code>recent_chats = await context.memory().recent_chat(\n    limit=20, roles=[\"user\", \"assistant\"]\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of chat events to retrieve. Defaults to 50.</p> <code>50</code> <code>roles</code> <code>Sequence[str] | None</code> <p>An optional sequence of roles to filter by (e.g., <code>[\"user\", \"assistant\"]</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>list[dict[str, Any]]: A list of chat events, each represented as a dictionary</p> <code>list[dict[str, Any]]</code> <p>with the following keys: - \"ts\": The timestamp of the event. - \"role\": The role of the speaker (e.g., \"user\", \"assistant\"). - \"text\": The text content of the chat message. - \"tags\": A list of tags associated with the event.</p>"},{"location":"reference/context-memory/#3-tool-related-memory","title":"3. Tool-related Memory","text":"record_tool_result(tool, inputs, ...) <p>Record the result of a tool execution in a normalized format.</p> <p>This method provides the method to log tool execution results with standardized metadata. Interally, it constructs an <code>Event</code> object encapsulating details about the tool execution, including inputs, outputs, tags, metrics, and a descriptive message.</p> <p>Examples:</p> <p>Recording a tool result with inputs and outputs: <pre><code>await context.memory().record_tool_result(\n    tool=\"data_cleaner\",\n    inputs=[{\"raw_data\": \"some raw input\"}],\n    outputs=[{\"cleaned_data\": \"processed output\"}],\n    tags=[\"data\", \"cleaning\"],\n    metrics={\"execution_time\": 1.23},\n    message=\"Tool executed successfully.\",\n    severity=2,\n)\n</code></pre></p> <p>Logging a tool result with minimal metadata: <pre><code>await context.memory().record_tool_result(\n    tool=\"simple_logger\",\n    message=\"Logged an event.\",\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>str</code> <p>The name of the tool that generated the result.</p> required <code>inputs</code> <code>list[dict[str, Any]] | None</code> <p>A list of dictionaries representing the tool's input data.</p> <code>None</code> <code>outputs</code> <code>list[dict[str, Any]] | None</code> <p>A list of dictionaries representing the tool's output data.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>A list of string labels for categorization.</p> <code>None</code> <code>metrics</code> <code>dict[str, float] | None</code> <p>A dictionary of numerical metrics (e.g., execution time, accuracy).</p> <code>None</code> <code>message</code> <code>str | None</code> <p>A descriptive message about the tool's execution or result.</p> <code>None</code> <code>severity</code> <code>int</code> <p>An integer (1-5) indicating the importance or severity of the result.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>Event</code> <code>Event</code> <p>The fully persisted <code>Event</code> object containing the generated ID and timestamp.</p> recent_tool_results(*, tool, limit, ...) <p>Retrieve recent tool execution results for a specific tool.</p> <p>This method filters and returns the most recent <code>tool_result</code> events associated with the specified tool, allowing you to analyze or process the results of tool executions.</p> <p>Examples:</p> <p>Fetching the 5 most recent results for a tool: <pre><code>recent_results = await context.memory().recent_tool_results(\n    tool=\"data_cleaner\",\n    limit=5,\n)\nfor result in recent_results:\n    print(result)\n</code></pre></p> <p>Retrieving all available results for a tool (up to the default limit): <pre><code>recent_results = await context.memory().recent_tool_results(\n    tool=\"simple_logger\",\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>str</code> <p>The name of the tool whose results are being queried.</p> required <code>limit</code> <code>int</code> <p>The maximum number of results to return (default is 10).</p> <code>10</code> <p>Returns:</p> Type Description <code>list[Event]</code> <p>list[Event]: A list of <code>Event</code> objects representing the recent</p> <code>list[Event]</code> <p><code>tool_result</code> events for the specified tool, ordered by recency.</p> recent_tool_result_data(*, tool, limit, ...) <p>Return a simplified view over recent tool_result events.</p> <p>This method provides a developer-friendly way to retrieve recent tool execution results in a normalized format, including metadata such as timestamps, inputs, outputs, and tags.</p> <p>Examples:</p> <p>Fetching recent tool result data: <pre><code>recent_data = await context.memory().recent_tool_result_data(\n    tool=\"data_cleaner\",\n    limit=5,\n)\nfor entry in recent_data:\n    print(entry)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>str</code> <p>The name of the tool whose results are being queried.</p> required <code>limit</code> <code>int</code> <p>The maximum number of recent results to retrieve.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>list[dict[str, Any]]: A list of dictionaries, each containing: - \"ts\": The timestamp of the event. - \"tool\": The name of the tool. - \"message\": A descriptive message about the tool's execution. - \"inputs\": The input data provided to the tool. - \"outputs\": The output data generated by the tool. - \"tags\": A list of string labels associated with the event.</p>"},{"location":"reference/context-memory/#4-memory-distillation","title":"4. Memory Distillation","text":"distill_long_term(scope_id, *, summary_tag, ...) <p>Distill long-term memory summaries based on specified criteria. This method generates a long-term memory summary by either using a Long-Term Summarizer or an LLM-based Long-Term Summarizer, depending on the <code>use_llm</code> flag. The summaries are filtered and configured based on the provided arguments.</p> <p>Examples:</p> <p>Using the default summarizer: <pre><code>result = await context.memory().distill_long_term(\n    scope_id=\"scope_123\",\n    include_kinds=[\"note\", \"event\"],\n    max_events=100\n)\n</code></pre> Using an LLM-based summarizer: <pre><code>result = await context.memory().distill_long_term(\n    scope_id=\"scope_456\",\n    use_llm=True,\n    summary_tag=\"custom_summary\",\n    min_signal=0.5\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>scope_id</code> <code>str | None</code> <p>The scope ID for the memory to summarize. If None, defaults to the instance's <code>memory_scope_id</code>.</p> <code>None</code> <code>summary_tag</code> <code>str</code> <p>A tag to categorize the generated summary. Defaults to <code>\"session\"</code>.</p> <code>'session'</code> <code>summary_kind</code> <code>str</code> <p>The kind of summary to generate. Defaults to <code>\"long_term_summary\"</code>.</p> <code>'long_term_summary'</code> <code>include_kinds</code> <code>list[str] | None</code> <p>A list of memory kinds to include in the summary. If None, all kinds are included.</p> <code>None</code> <code>include_tags</code> <code>list[str] | None</code> <p>A list of tags to filter the memories. If None, no tag filtering is applied.</p> <code>None</code> <code>max_events</code> <code>int</code> <p>The maximum number of events to include in the summary. Defaults to 200.</p> <code>200</code> <code>min_signal</code> <code>float | None</code> <p>The minimum signal threshold for filtering events. If None, the default signal threshold is used.</p> <code>None</code> <code>use_llm</code> <code>bool</code> <p>Whether to use an LLM-based summarizer. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the generated summary.</p> Example return value <pre><code>{\n    \"uri\": \"file://mem/scope_123/summaries/long_term/2023-10-01T12:00:00Z.json\",\n    \"summary_kind\": \"long_term_summary\",\n    \"summary_tag\": \"session\",\n    \"time_window\": {\"start\": \"2023-09-01\", \"end\": \"2023-09-30\"},\n    \"num_events\": 150,\n    \"included_kinds\": [\"note\", \"event\"],\n    \"included_tags\": [\"important\", \"meeting\"],\n}\n</code></pre> distill_meta_summary(scope_id, *, summary_kind, ...) <p>Generate a meta-summary by distilling existing summary events.</p> <p>This method creates a meta-summary by processing existing long-term summaries. It uses an LLM-based summarizer to generate a higher-level summary based on the provided arguments.</p> <p>Examples:</p> <p>Using the default configuration: <pre><code>result = await context.memory().distill_meta_summary(\n    scope_id=\"scope_123\",\n    source_kind=\"long_term_summary\",\n    source_tag=\"session\",\n)\n</code></pre></p> <p>Customizing the summary kind and tag: <pre><code>result = await context.memory().distill_meta_summary(\n    scope_id=\"scope_456\",\n    summary_kind=\"meta_summary\",\n    summary_tag=\"weekly\",\n    max_summaries=10,\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>scope_id</code> <code>str | None</code> <p>The scope ID for the memory to summarize. If None, defaults to the instance's <code>memory_scope_id</code>.</p> <code>None</code> <code>source_kind</code> <code>str</code> <p>The kind of source summaries to process. Defaults to <code>\"long_term_summary\"</code>.</p> <code>'long_term_summary'</code> <code>source_tag</code> <code>str</code> <p>A tag to filter the source summaries. Defaults to <code>\"session\"</code>.</p> <code>'session'</code> <code>summary_kind</code> <code>str</code> <p>The kind of meta-summary to generate. Defaults to <code>\"meta_summary\"</code>.</p> <code>'meta_summary'</code> <code>summary_tag</code> <code>str</code> <p>A tag to categorize the generated meta-summary. Defaults to <code>\"meta\"</code>.</p> <code>'meta'</code> <code>max_summaries</code> <code>int</code> <p>The maximum number of source summaries to process. Defaults to 20.</p> <code>20</code> <code>min_signal</code> <code>float | None</code> <p>The minimum signal threshold for filtering summaries. If None, the default signal threshold is used.</p> <code>None</code> <code>use_llm</code> <code>bool</code> <p>Whether to use an LLM-based summarizer. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the generated meta-summary.</p> Example return value <pre><code>{\n    \"uri\": \"file://mem/scope_123/summaries/meta/2023-10-01T12:00:00Z.json\",\n    \"summary_kind\": \"meta_summary\",\n    \"summary_tag\": \"meta\",\n    \"time_window\": {\"start\": \"2023-09-01\", \"end\": \"2023-09-30\"},\n    \"num_source_summaries\": 15,\n}\n</code></pre> load_recent_summaries(scope_id, *,  summary_tag, limit) <p>Load the most recent JSON summaries for the specified scope and tag.</p> <p>This method retrieves up to <code>limit</code> summaries from the <code>DocStore</code> based on the provided <code>scope_id</code> and <code>summary_tag</code>. Summaries are identified using the following pattern: <code>mem/{scope_id}/summaries/{summary_tag}/{ts}</code>.</p> <p>Examples:</p> <p>Load the last three session summaries: <pre><code>summaries = await context.memory().load_recent_summaries(\n    scope_id=\"user123\",\n    summary_tag=\"session\",\n    limit=3\n)\n</code></pre></p> <p>Load the last two project summaries: <pre><code>summaries = await context.memory().load_recent_summaries(\n    scope_id=\"project456\",\n    summary_tag=\"project\",\n    limit=2\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>scope_id</code> <code>str | None</code> <p>The memory scope ID. If None, defaults to the current memory scope.</p> <code>None</code> <code>summary_tag</code> <code>str</code> <p>The tag used to filter summaries (e.g., \"session\", \"project\"). Defaults to \"session\".</p> <code>'session'</code> <code>limit</code> <code>int</code> <p>The maximum number of summaries to return. Defaults to 3.</p> <code>3</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>list[dict[str, Any]]: A list of summary dictionaries, ordered from oldest to newest.</p> load_last_summary(scope_id, *,  summary_tag) <p>Load the most recent JSON summary for the specified memory scope and tag.</p> <p>This method retrieves the latest summary document from the <code>DocStore</code> based on the provided <code>scope_id</code> and <code>summary_tag</code>. Summaries are identified using the following pattern: <code>mem/{scope_id}/summaries/{summary_tag}/{ts}</code>.</p> <p>Examples:</p> <p>Load the last session summary: <pre><code>summary = await context.memory().load_last_summary(scope_id=\"user123\", summary_tag=\"session\")\n</code></pre></p> <p>Load the last project summary: <pre><code>summary = await context.memory().load_last_summary(scope_id=\"project456\", summary_tag=\"project\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>scope_id</code> <code>str | None</code> <p>The memory scope ID. If None, defaults to the current memory scope.</p> <code>None</code> <code>summary_tag</code> <code>str</code> <p>The tag used to filter summaries (e.g., \"session\", \"project\"). Defaults to \"session\".</p> <code>'session'</code> <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>dict[str, Any] | None: The most recent summary as a dictionary, or None if no summary is found.</p> soft_hydrate_last_summary(scope_id, *, summary_tag, summary_kind) <p>Load the most recent summary for the specified scope and tag, and log a hydrate event.</p> <p>This method retrieves the latest summary document from the <code>DocStore</code> based on the provided <code>scope_id</code> and <code>summary_tag</code>. If a summary is found, it logs a hydrate event into the current run's HotLog and Persistence layers.</p> <p>Examples:</p> <p>Hydrate the last session summary: <pre><code>summary = await context.memory().soft_hydrate_last_summary(\n    scope_id=\"user123\",\n    summary_tag=\"session\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>scope_id</code> <code>str | None</code> <p>The memory scope ID. If None, defaults to the current memory scope.</p> <code>None</code> <code>summary_tag</code> <code>str</code> <p>The tag used to filter summaries (e.g., \"session\", \"project\"). Defaults to \"session\".</p> <code>'session'</code> <code>summary_kind</code> <code>str</code> <p>The kind of summary (e.g., \"long_term_summary\", \"project_summary\"). Defaults to \"long_term_summary\".</p> <code>'long_term_summary'</code> <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>dict[str, Any] | None: The loaded summary dictionary if found, otherwise None.</p> Side Effects <p>Appends a hydrate event to HotLog and Persistence for the current timeline.</p>"},{"location":"reference/context-memory/#5-vector-memory","title":"5. Vector Memory","text":"rag_remember_events(*, key='default', where=None, policy=None) <p>Bind a RAG corpus by logical key and promote events into it.</p> <p>This method allows you to associate a logical key with a RAG corpus and promote events into it based on filtering criteria.</p> <p>Examples:</p> <p>Promote events into a session corpus: <pre><code>await context.memory().rag_remember_events(\n    key=\"session\",\n    where={\"kinds\": [\"tool_result\"], \"limit\": 200},\n    policy={\"min_signal\": 0.25},\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Logical key for the RAG corpus. Defaults to <code>\"default\"</code>.</p> <code>'default'</code> <code>where</code> <code>dict | None</code> <p>Filtering criteria for selecting events.</p> <code>None</code> <code>policy</code> <code>dict | None</code> <p>Promotion policy, such as minimum signal threshold.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the promotion results.</p> rag_remember_docs(docs, *, key='default', labels=None) <p>Bind a RAG corpus by key and upsert documents into it.</p> <p>This method allows you to associate a logical key with a RAG corpus and upsert a sequence of documents into it.</p> <p>Examples:</p> <p>Upsert documents into a corpus: <pre><code>await context.memory().rag_remember_docs(\n    docs=[{\"text\": \"Document 1\"}, {\"text\": \"Document 2\"}],\n    key=\"knowledge_base\",\n    labels={\"category\": \"reference\"},\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Sequence[dict[str, Any]]</code> <p>A sequence of documents to upsert.</p> required <code>key</code> <code>str</code> <p>Logical key for the RAG corpus. Defaults to <code>\"default\"</code>.</p> <code>'default'</code> <code>labels</code> <code>dict | None</code> <p>Metadata labels to associate with the corpus.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>A dictionary containing the upsert results.</p> rag_search_by_key(*, key='default', query, k=8, filters=None, mode='hybrid') <p>Resolve a corpus by logical key and perform a search query.</p> <p>This method allows you to search within a RAG corpus identified by a logical key.</p> <p>Examples:</p> <p>Perform a search query: <pre><code>results = await context.memory().rag_search_by_key(\n    key=\"knowledge_base\",\n    query=\"What is the capital of France?\",\n    k=5,\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Logical key for the RAG corpus. Defaults to <code>\"default\"</code>.</p> <code>'default'</code> <code>query</code> <code>str</code> <p>The search query string.</p> required <code>k</code> <code>int</code> <p>Number of top results to return. Defaults to 8.</p> <code>8</code> <code>filters</code> <code>dict | None</code> <p>Additional filters for the search.</p> <code>None</code> <code>mode</code> <code>Literal['hybrid', 'dense']</code> <p>Search mode, either <code>\"hybrid\"</code> or <code>\"dense\"</code>. Defaults to <code>\"hybrid\"</code>.</p> <code>'hybrid'</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of search results.</p> rag_answer_by_key(*, key='default', question, style='concise', with_citations=True, k=6) <p>Perform RAG QA over a corpus referenced by a logical key.</p> <p>This method allows you to ask a question and retrieve an answer from a RAG corpus identified by a logical key.</p> <p>Examples:</p> <p>Ask a question: <pre><code>answer = await context.memory().rag_answer_by_key(\n    key=\"knowledge_base\",\n    question=\"What is the capital of France?\",\n    style=\"detailed\",\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Logical key for the RAG corpus. Defaults to <code>\"default\"</code>.</p> <code>'default'</code> <code>question</code> <code>str</code> <p>The question to ask.</p> required <code>style</code> <code>Literal['concise', 'detailed']</code> <p>Answer style, either <code>\"concise\"</code> or <code>\"detailed\"</code>. Defaults to <code>\"concise\"</code>.</p> <code>'concise'</code> <code>with_citations</code> <code>bool</code> <p>Whether to include citations in the answer. Defaults to <code>True</code>.</p> <code>True</code> <code>k</code> <code>int</code> <p>Number of top results to consider. Defaults to 6.</p> <code>6</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the answer and related metadata.</p>"},{"location":"reference/context-memory/#6-utilities","title":"6. Utilities","text":"chat_history_for_llm(*, limit, include_system_summary, ...) <p>Build a ready-to-send OpenAI-style chat message list.</p> <p>This method constructs a dictionary containing a summary of previous context and a list of chat messages formatted for use with OpenAI-style chat models. It includes options to limit the number of messages and incorporate long-term summaries.</p> <p>Examples:</p> <p>Basic usage with default parameters: <pre><code>history = await context.memory().chat_history_for_llm()\n</code></pre></p> <p>Including a system summary and limiting messages: <pre><code>history = await context.memory().chat_history_for_llm(\n    limit=10, include_system_summary=True\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of recent chat messages to include. Defaults to 20.</p> <code>20</code> <code>include_system_summary</code> <code>bool</code> <p>Whether to include a system summary of previous context. Defaults to True.</p> <code>True</code> <code>summary_tag</code> <code>str</code> <p>The tag used to filter summaries. Defaults to \"session\".</p> <code>'session'</code> <code>summary_scope_id</code> <code>str | None</code> <p>An optional scope ID for filtering summaries. Defaults to None.</p> <code>None</code> <code>max_summaries</code> <code>int</code> <p>The maximum number of summaries to load. Defaults to 3.</p> <code>3</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary with the following structure: - \"summary\": A combined long-term summary or an empty string. - \"messages\": A list of chat messages, each represented as a dictionary   with \"role\" and \"content\" keys.</p> <p>Example of returned structure: <pre><code>    {\n        \"summary\": \"Summary of previous context...\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"Summary of previous context...\"},\n            {\"role\": \"user\", \"content\": \"Hello!\"},\n            {\"role\": \"assistant\", \"content\": \"Hi there! How can I help?\"}\n        ]\n    }\n</code></pre></p> build_prompt_segments(*, recent_chat_limit, include_long_term, ...) <p>Assemble memory context for prompts, including long-term summaries, recent chat history, and recent tool usage.</p> <p>Examples:</p> <p>Build prompt segments with default settings: <pre><code>segments = await context.memory().build_prompt_segments()\n</code></pre></p> <p>Include recent tool usage and filter by a specific tool: <pre><code>segments = await context.memory().build_prompt_segments(\n    include_recent_tools=True,\n    tool=\"search\",\n    tool_limit=5\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>recent_chat_limit</code> <code>int</code> <p>The maximum number of recent chat messages to include. Defaults to 12.</p> <code>12</code> <code>include_long_term</code> <code>bool</code> <p>Whether to include long-term memory summaries. Defaults to True.</p> <code>True</code> <code>summary_tag</code> <code>str</code> <p>The tag used to filter long-term summaries. Defaults to \"session\".</p> <code>'session'</code> <code>max_summaries</code> <code>int</code> <p>The maximum number of long-term summaries to include. Defaults to 3.</p> <code>3</code> <code>include_recent_tools</code> <code>bool</code> <p>Whether to include recent tool usage. Defaults to False.</p> <code>False</code> <code>tool</code> <code>str | None</code> <p>The specific tool to filter recent tool usage. Defaults to None.</p> <code>None</code> <code>tool_limit</code> <code>int</code> <p>The maximum number of recent tool events to include. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the following keys:</p> <ul> <li> <p>\"long_term\" (str): Combined long-term summary text or an empty   string if not included.</p> </li> <li> <p>\"recent_chat\" (list[dict[str, Any]]): A list of recent chat   messages, each represented as a dictionary with the following keys:</p> <ul> <li>\"ts\" (str): Timestamp of the message.</li> <li>\"role\" (str): Role of the sender (e.g., \"user\", \"assistant\").</li> <li>\"text\" (str): The content of the message.</li> <li>\"tags\" (list[str]): Tags associated with the message.</li> </ul> </li> <li> <p>\"recent_tools\" (list[dict[str, Any]]): A list of recent tool   usage events, each represented as a dictionary with the following keys:</p> <ul> <li>\"ts\" (str): Timestamp of the tool event.</li> <li>\"tool\" (str): Name of the tool used.</li> <li>\"message\" (str): Message or description of the tool event.</li> <li>\"inputs\" (Any): Inputs provided to the tool.</li> <li>\"outputs\" (Any): Outputs generated by the tool.</li> <li>\"tags\" (list[str]): Tags associated with the tool event.</li> </ul> </li> </ul>"},{"location":"reference/context-rag/","title":"<code>RAGFacade</code> \u2013 Retrieval\u2011Augmented Generation API","text":"<p>Manages corpora, document ingestion (text/files), chunking + embeddings, vector indexing, retrieval, and QA.</p> <p>Backends: defaults to a lightweight SQLite vector index. FAISS is supported locally if installed via pip. See LLM &amp; Index Setup for provider/model/index configuration.</p>"},{"location":"reference/context-rag/#1-core-methods","title":"1. Core Methods","text":"<p>These method are bounded to NodeContext with specific scope (run_id, session_id, user_id etc)</p> bind_corpus(corpus_id, key, create_if_missing, labels, scope_id) <p>Bind or create a RAG corpus for the current node scope.</p> <p>This method ensures a corpus exists for the given scope and key, creating it if necessary. It automatically injects scope labels and metadata, and returns the resolved corpus ID.</p> <p>Examples:</p> <p>Bind a default corpus for the current node: <pre><code>corpus_id = await context.rag().bind_corpus()\n</code></pre></p> <p>Bind or create a corpus with a custom key and extra labels: <pre><code>corpus_id = await context.rag().bind_corpus(\n    key=\"my-data\",\n    labels={\"source\": \"user-upload\"}\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>corpus_id</code> <code>str | None</code> <p>Optional explicit corpus identifier. If not provided, one is generated from the scope and key.</p> <code>None</code> <code>key</code> <code>str | None</code> <p>Optional string to distinguish corpora within the same scope (e.g., \"default\", \"my-data\").</p> <code>None</code> <code>create_if_missing</code> <code>bool</code> <p>If True (default), create the corpus if it does not exist.</p> <code>True</code> <code>labels</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of additional metadata to attach to the corpus.</p> <code>None</code> <code>scope_id</code> <code>str | None</code> <p>Optional override for the scope identifier. Defaults to the current node's scope.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The resolved corpus ID, guaranteed to exist if <code>create_if_missing</code> is True.</p> Notes <ul> <li>The corpus ID is derived from the scope and key if not explicitly provided.</li> <li>Scope labels are automatically merged into the corpus metadata.</li> </ul> upsert_docs(corpus_id, docs, *, scope_id) <p>Ingest and index a list of documents into the specified corpus for the current node scope.</p> <p>This method ensures the corpus exists for the given scope, merges scope labels into each document, and handles both file-based and inline text documents. Documents are chunked, embedded, and indexed for retrieval.</p> <p>Examples:</p> <p>Ingest a list of inline documents: <pre><code>await context.rag().upsert_docs(\n    corpus_id=\"my-corpus\",\n    docs=[\n        {\"text\": \"Document content...\", \"title\": \"Doc Title\"},\n        {\"text\": \"Another doc\", \"labels\": {\"source\": \"user-upload\"}}\n    ]\n)\n</code></pre></p> <p>Ingest a PDF file with custom labels: <pre><code>await context.rag().upsert_docs(\n    corpus_id=\"my-corpus\",\n    docs=[{\"path\": \"/path/to/file.pdf\", \"labels\": {\"type\": \"pdf\"}}]\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>corpus_id</code> <code>str</code> <p>The target corpus identifier.</p> required <code>docs</code> <code>list[dict[str, Any]]</code> <p>A list of document specifications. Each document can be: - File-based: {\"path\": \"/path/to/doc.pdf\", \"labels\": {...}} - Inline text: {\"text\": \"Document content...\", \"title\": \"Doc Title\", \"labels\": {...}}</p> required <code>scope_id</code> <code>str | None</code> <p>Optional override for the scope identifier. Defaults to the current node's scope.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Summary of the ingestion, including number of documents and chunks added.</p> Notes <ul> <li>Scope labels are merged into each document's labels.</li> <li>File-based documents are read and chunked automatically.</li> <li>Inline text documents are chunked based on configured chunk size.</li> </ul> search(corpus_id, query, *,  k, filters, scope_id, mode) <p>Search the specified RAG corpus for relevant chunks matching a query.</p> <p>This method performs a dense or hybrid (dense + lexical) search over the corpus, automatically injecting node scope filters. It returns the top-k most relevant results as <code>SearchHit</code> objects, including chunk text, metadata, and scores.</p> <p>Examples:</p> <p>Basic usage to search a corpus: <pre><code>hits = await context.rag().search(\n    corpus_id=\"my-corpus\",\n    query=\"What is the capital of France?\"\n)\n</code></pre></p> <p>Search with custom filters and top-3 results: <pre><code>hits = await context.rag().search(\n    corpus_id=\"my-corpus\",\n    query=\"project roadmap\",\n    k=3,\n    filters={\"type\": \"meeting-notes\"}\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>corpus_id</code> <code>str</code> <p>The target corpus identifier to search within.</p> required <code>query</code> <code>str</code> <p>The search query string.</p> required <code>k</code> <code>int</code> <p>The number of top results to return (default: 8).</p> <code>8</code> <code>filters</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata filters to apply (merged with scope filters).</p> <code>None</code> <code>scope_id</code> <code>str | None</code> <p>Optional override for the scope identifier. Defaults to the current node's scope.</p> <code>None</code> <code>mode</code> <code>str</code> <p>Search mode, either <code>\"dense\"</code> or <code>\"hybrid\"</code> (default: \"hybrid\").</p> <code>'hybrid'</code> <p>Returns:</p> Type Description <code>list[SearchHit]</code> <p>list[SearchHit]: A list of matching <code>SearchHit</code> objects, each containing chunk text,</p> <code>list[SearchHit]</code> <p>metadata, score, and identifiers.</p> Notes <ul> <li>Scope filters are automatically merged with any provided filters.</li> <li>Hybrid mode fuses dense and lexical search for improved relevance.</li> <li>Results are sorted by descending relevance score.</li> </ul> answer(corpus_id, question, *, llm, style, with_citations, k, scope_id) <p>Answer a question using retrieved context from a specified corpus.</p> <p>This method retrieves relevant context chunks from the target corpus, constructs a prompt for the language model, and generates an answer. Citations to the retrieved chunks are included if requested. The function is accessed via <code>context.rag().answer(...)</code>.</p> <p>Examples:</p> <p>Basic usage to answer a question: <pre><code>result = await context.rag().answer(\n    corpus_id=\"my-corpus\",\n    question=\"What is the capital of France?\"\nprint(result[\"answer\"])\n</code></pre></p> <p>Requesting a detailed answer with citations: <pre><code>result = await context.rag().answer(\n    corpus_id=\"my-corpus\",\n    question=\"Explain the process of photosynthesis.\",\n    style=\"detailed\",\n    with_citations=True,\n    k=8\n)\nprint(\"Answer:\", result[\"answer\"])\nfor cite in result[\"citations\"]:\n    print(f\"Citation: {cite['text']} (Score: {cite['score']})\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>corpus_id</code> <code>str</code> <p>Identifier of the target corpus to search for context.</p> required <code>question</code> <code>str</code> <p>The question to be answered.</p> required <code>llm</code> <code>str | None</code> <p>Optional language model client to use for answer generation. If None, the default LLM is used.</p> <code>None</code> <code>style</code> <code>str</code> <p>The style of the answer, either \"concise\" (default) or \"detailed\".</p> <code>'concise'</code> <code>with_citations</code> <code>bool</code> <p>Whether to include citations to the retrieved context chunks in the answer (default: True).</p> <code>True</code> <code>k</code> <code>int</code> <p>Number of context chunks to retrieve for answering (default: 6).</p> <code>6</code> <code>scope_id</code> <code>str | None</code> <p>Optional identifier to restrict retrieval to a specific scope.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the generated answer, citations, usage statistics, and optionally resolved citation metadata.</p> Notes <ul> <li> <p>the generated dictionary includes:</p> <ul> <li><code>answer</code>: The generated answer text.</li> <li><code>citations</code>: List of retrieved context chunks used as citations.</li> <li><code>usage</code>: LLM usage statistics (tokens, time, etc.).</li> <li><code>resolved_citations</code>: Optional metadata for citations if available.</li> </ul> </li> <li> <p>Example response: <pre><code>{\n    \"answer\": \"The capital of France is Paris.\",\n    \"citations\": [\n        {\"text\": \"Paris is the capital city of France...\", \"score\": 0.95, ...},\n        ...\n    ],\n    \"usage\": {\"prompt_tokens\": 150, \"completion_tokens\": 50, \"total_tokens\": 200, ...},\n    \"resolved_citations\": [\n        {\"doc_id\": \"doc123\", \"title\": \"Geography of France\", ...},\n        ...\n    ]\n}\n</code></pre></p> </li> </ul>"},{"location":"reference/context-rag/#2-storage-management","title":"2. Storage Management","text":"<p>These method are direct operations in <code>RAGFacade</code> to manage all corpura and docs</p> list_corpora() <p>List all available corpora managed by this RAGFacade.</p> <p>This method scans the corpus root directory, loads metadata for each corpus, and returns a list of corpus records with their logical IDs and metadata.</p> <p>Examples:</p> <p>Basic usage to enumerate corpora: <pre><code>corpora = await context.rag().list_corpora()\nfor c in corpora:\n    print(c[\"corpus_id\"], c[\"meta\"].get(\"created_at\"))\n</code></pre></p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of dictionaries, each containing:</p> <ul> <li>\"corpus_id\": The logical identifier for the corpus.</li> <li>\"meta\": The metadata dictionary loaded from corpus.json (may be empty).</li> </ul> list_docs(corpus_id, limit, after) <p>List documents from a corpus in a paginated fashion.</p> <p>This method reads documents from the <code>docs.jsonl</code> file associated with the given <code>corpus_id</code>, returning up to <code>limit</code> documents after the specified <code>after</code> document ID. It is typically accessed via <code>context.rag().list_docs(...)</code>.</p> <p>Examples:</p> <p>Basic usage to list the first 100 documents: <pre><code>docs = await context.rag().list_docs(\"my-corpus\", limit=100)\n</code></pre></p> <p>Paginating after a specific document: <pre><code>docs = await context.rag().list_docs(\"my-corpus\", after=\"doc_123\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>corpus_id</code> <code>str</code> <p>The unique identifier for the corpus whose documents are to be listed.</p> required <code>limit</code> <code>int</code> <p>The maximum number of documents to return (default: 200).</p> <code>200</code> <code>after</code> <code>str | None</code> <p>If provided, only documents after this document ID will be returned.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of document objects, each represented as a dictionary.</p> delete_docs(corpus_id, doc_ids) <p>Remove one or more documents and their associated chunks from a corpus.</p> <p>This method deletes all records for the specified <code>doc_ids</code> from both the <code>docs.jsonl</code> and <code>chunks.jsonl</code> files within the given corpus. It also instructs the vector index backend to remove any vectors associated with the deleted chunks, if supported.</p> <p>Examples:</p> <p>Basic usage to delete a single document: <pre><code>await context.rag().delete_docs(\"my-corpus\", [\"doc_123\"])\n</code></pre></p> <p>Deleting multiple documents at once: <pre><code>await context.rag().delete_docs(\"my-corpus\", [\"doc_1\", \"doc_2\", \"doc_3\"])\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>corpus_id</code> <code>str</code> <p>The unique identifier for the corpus from which documents will be removed.</p> required <code>doc_ids</code> <code>list[str]</code> <p>A list of document IDs to delete. All chunks belonging to these documents will also be removed.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing: - \"removed_docs\": The number of documents removed. - \"removed_chunks\": The number of chunks removed from the index and storage.</p> reembed(corpus_id, *, doc_ids, batch) <p>Re-embed vectors for selected documents (or all) in a corpus.</p> <p>This method re-computes embeddings for all chunks belonging to the specified <code>doc_ids</code> (or for all documents if <code>doc_ids</code> is None) and updates the vector index accordingly. It uses the currently configured embedding client and can be accessed via <code>context.rag().reembed(...)</code>.</p> <p>Examples:</p> <p>Re-embed all documents in a corpus: <pre><code>await context.rag().reembed(\"my-corpus\")\n</code></pre></p> <p>Re-embed only specific documents: <pre><code>await context.rag().reembed(\"my-corpus\", doc_ids=[\"doc_123\", \"doc_456\"])\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>corpus_id</code> <code>str</code> <p>The unique identifier for the corpus whose vectors will be re-embedded.</p> required <code>doc_ids</code> <code>list[str] | None</code> <p>Optional list of document IDs to re-embed. If None, all documents are processed.</p> <code>None</code> <code>batch</code> <code>int</code> <p>The number of chunks to embed per batch (default: 64).</p> <code>64</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing: - \"reembedded\": The number of chunks re-embedded. - \"model\": The embedding model used (if available).</p> stats(corpus_id) <p>Retrieve summary statistics for a given corpus.</p> <p>This method counts the number of documents and chunks in the specified corpus, and loads the associated corpus metadata. It is typically accessed via <code>context.rag().stats(...)</code>.</p> <p>Examples:</p> <p>Basic usage to get corpus statistics: <pre><code>stats = await context.rag().stats(\"my-corpus\")\nprint(stats[\"docs\"], stats[\"chunks\"])\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>corpus_id</code> <code>str</code> <p>The unique identifier for the corpus whose statistics are to be retrieved.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing: - \"corpus_id\": The logical identifier for the corpus. - \"docs\": The number of documents in the corpus. - \"chunks\": The number of text chunks in the corpus. - \"meta\": The metadata dictionary loaded from corpus.json (may be empty).</p>"},{"location":"reference/context-runner/","title":"Launching Agent Runs with the <code>context</code> Method","text":"<p>Aethergraph provides a flexible interface for managing agent runs through the <code>NodeContext</code> object. This approach goes beyond simple nested calls to <code>graph_fn</code>, allowing you to spawn, monitor, and control runs with fine-grained operations. The <code>context</code> method is designed for general use with any agent, and it supplies metadata that supports seamless integration with the AG web UI.</p>"},{"location":"reference/context-runner/#async-run","title":"Async Run","text":"<p>You can initiate an agent run asynchronously using the <code>context.spawn_run</code> method. This \"fire and forget\" approach lets you start a run without waiting for its completion.</p> context.spawn_run(graph_id, *, inputs, session_id, tags, ...) <p>Launch a new run from within the current node or graph context.</p> <p>This method creates and schedules a new run for the specified graph, using the provided inputs and optional metadata. It does not wait for the run to complete; instead, it returns immediately with the new run's ID. The run is managed asynchronously in the background, and is tracked and persisted via the configured RunManager.</p> <p>Examples:</p> <p>Basic usage to spawn a run for a graph: <pre><code>run_id = await context.spawn_run(\n    \"my-graph-id\",\n    inputs={\"x\": 1, \"y\": 2}\n)\n</code></pre></p> <p>Spawning a run with custom tags and agent context: <pre><code>from aethergraph.runtime import RunVisibility\nrun_id = await context.spawn_run(\n    \"my-graph-id\",\n    inputs={\"foo\": \"bar\"},\n    tags=[\"experiment\", \"priority\"],\n    agent_id=\"agent-123\",    # associate with an agent if applicable\n    visibility=RunVisibility.inline, # not shown in UI\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>graph_id</code> <code>str</code> <p>The unique identifier of the graph to execute. i.e. the <code>name</code> field of a registered graph.</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>Dictionary of input values to pass to the graph.</p> required <code>session_id</code> <code>str | None</code> <p>Optional session identifier. Defaults to the current context's session if not provided.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>Optional list of string tags for categorization and tracking.</p> <code>None</code> <code>visibility</code> <code>RunVisibility | None</code> <p>Optional visibility setting for the run (e.g., public, private, normal).</p> <code>None</code> <code>origin</code> <code>RunOrigin | None</code> <p>Optional indicator of the run's origin (e.g., agent, app). Defaults based on agent_id.</p> <code>None</code> <code>importance</code> <code>RunImportance | None</code> <p>Optional importance level for the run (e.g., normal, high).</p> <code>None</code> <code>agent_id</code> <code>str | None</code> <p>Optional agent identifier if the run is associated with an agent.</p> <code>None</code> <code>app_id</code> <code>str | None</code> <p>Optional application identifier if the run is associated with an app.</p> <code>None</code> <code>run_id</code> <code>str | None</code> <p>Optional explicit run identifier. If not provided, one is generated.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The unique run_id of the newly created run.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the RunManager service is not configured in the context.</p> Notes <ul> <li>The spawned run inherits the context's identity for provenance tracking.</li> <li>Metadata <code>tags</code>, <code>visibility</code>, <code>origin</code>, <code>importance</code>, <code>agent_id</code>, <code>app_id</code>, help manage and monitor the run in AG UI,     but do not affect the execution logic of the graph itself. If you are not using AG UI, these fields can be omitted.</li> </ul> <p>To monitor the progress or completion of an asynchronous run, use the <code>context.wait_run</code> method. This allows you to block until the run finishes or a timeout occurs.</p> context.wait_run(run_id, *, timeout_s) <p>Wait for a run to complete and retrieve its final record.</p> <p>This method waits the RunManager for the specified run until it finishes, then returns the completed RunRecord. Optionally, a timeout (in seconds) can be set to limit how long to wait.</p> <p>Examples:</p> <p>Basic usage to wait for a run: <pre><code>run_id = await context.spawn_run(\"my-graph-id\", inputs={\"x\": 1})\nrecord = await context.wait_run(run_id)\n</code></pre></p> <p>Waiting with a timeout: <pre><code>record = await context.wait_run(run_id, timeout_s=30)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>The unique identifier of the run to wait for.</p> required <code>timeout_s</code> <code>float | None</code> <p>Optional timeout in seconds. If set, the method will raise a TimeoutError if the run does not complete in time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>RunRecord</code> <code>RunRecord</code> <p>The final record of the completed run.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the RunManager service is not configured in the context.</p> <code>TimeoutError</code> <p>If the run does not complete within the specified timeout.</p> Notes <ul> <li>This method is useful for orchestration patterns where you need to   synchronize on the completion of child runs.</li> <li>For high-concurrency scenarios, prefer using <code>spawn_run</code> and <code>wait_run</code>   in combination rather than <code>run_and_wait</code>.</li> </ul>"},{"location":"reference/context-runner/#kick-off-a-run-and-wait","title":"Kick off a Run and Wait","text":"<p>If you need to start a run and wait for its result in a single step, the <code>context.run_and_wait</code> method provides a convenient solution. Note this method blocks the process and does not honor the concurrency limit. </p> context.run_and_wait(graph_id, *, inputs, session_id, tags, ...) <p>Run a child graph as a first-class RunManager run and wait for completion.</p> <p>This method launches a new run for the specified graph, waits for it to finish, and returns its outputs and metadata. The run is tracked and visualized in the UI, and all status updates are persisted via the RunManager.</p> <p>Examples:</p> <p>Basic usage to run and wait for a graph: <pre><code>run_id, outputs, has_waits, continuations = await context.run_and_wait(\n    \"my-graph-id\",\n    inputs={\"x\": 1, \"y\": 2}\n)\n</code></pre></p> <p>Running with custom tags and agent context: <pre><code>run_id, outputs, has_waits, continuations = await context.run_and_wait(\n    \"my-graph-id\",\n    inputs={\"foo\": \"bar\"},\n    tags=[\"experiment\", \"priority\"],\n    agent_id=\"agent-123\",\n    visibility=RunVisibility.inline,\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>graph_id</code> <code>str</code> <p>The unique identifier of the graph to execute.</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>Dictionary of input values to pass to the graph.</p> required <code>session_id</code> <code>str | None</code> <p>Optional session identifier. Defaults to the current context's session.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>Optional list of string tags for categorization and tracking.</p> <code>None</code> <code>visibility</code> <code>RunVisibility | None</code> <p>Optional visibility setting for the run (e.g., public, private, normal).</p> <code>None</code> <code>origin</code> <code>RunOrigin | None</code> <p>Optional indicator of the run's origin (e.g., agent, app).</p> <code>None</code> <code>importance</code> <code>RunImportance | None</code> <p>Optional importance level for the run (e.g., normal, high).</p> <code>None</code> <code>agent_id</code> <code>str | None</code> <p>Optional agent identifier if the run is associated with an agent.</p> <code>None</code> <code>app_id</code> <code>str | None</code> <p>Optional application identifier if the run is associated with an app.</p> <code>None</code> <code>run_id</code> <code>str | None</code> <p>Optional explicit run identifier. If not provided, one is generated.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>run_id</code> <code>str</code> <p>The unique run ID of the completed run.</p> <code>outputs</code> <code>dict | None</code> <p>The outputs returned by the graph.</p> <code>has_waits</code> <code>bool</code> <p>True if the run contained any wait nodes. [Not currently used]</p> <code>continuations</code> <code>list[dict]</code> <p>List of continuation metadata, if any. [Not currently used]</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the RunManager service is not configured in the context.</p> Notes <ul> <li>The run is fully tracked and visualized in the AG UI.</li> <li>Use this method for orchestration patterns where you need to await child runs.</li> <li>Metadata fields help with monitoring and provenance, but do not affect graph logic.</li> </ul> Warning <ul> <li>This method blocks until the child run completes.</li> <li>This method will not honor the concurrency limits of the parent run, and may lead to deadlocks if the parent run is waiting on resources held by the child run.</li> <li>Avoid using this method in high-concurrency scenarios to prevent deadlocks.   For such cases, consider using <code>spawn_run</code> followed by <code>wait_run</code> instead.</li> </ul>"},{"location":"reference/context-runner/#cancellation","title":"Cancellation","text":"<p>Aethergraph also supports run cancellation, allowing you to terminate a run that is no longer needed or is taking too long.</p> context.cancel_run(run_id) <p>Cancel a scheduled or running child run by its unique ID.</p> <p>This method requests cancellation of a run managed by the RunManager. The cancellation is propagated to the run's execution context, and any in-progress tasks will be interrupted if possible.</p> <p>Examples:</p> <p>Basic usage to cancel a spawned run: <pre><code>run_id = await context.spawn_run(\"my-graph-id\", inputs={\"x\": 1})\nawait context.cancel_run(run_id)\n</code></pre></p> <p>Cancel a run after waiting for a condition: <pre><code>if should_abort:\n    await context.cancel_run(run_id)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>The unique identifier of the run to cancel.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None. The cancellation request is dispatched to the RunManager.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the RunManager service is not configured in the context.</p> Notes <ul> <li>Cancellation is best-effort and may not immediately terminate all tasks.</li> <li>Use this method for orchestration patterns where you need to abort child runs.</li> <li>The run's status will be updated to \"cancelled\" in the UI and persistence layer.</li> </ul>"},{"location":"reference/context-viz/","title":"<code>context.viz()</code> \u2013 WebUI Visualization API","text":"<p>The visualization service offers a convenient way to log data from agents into persistent storage. It is typically used in conjunction with AG's WebUI to automatically visualize each run. For manual generation of visualization data, it is recommended to use the corresponding Python package.</p>"},{"location":"reference/context-viz/#1-data-visualization","title":"1. Data Visualization","text":"scalar(track_id, *, step, value, ...) <p>Record a single scalar value for visualization in the Aethergraph UI.</p> <p>This method standardizes the event format, auto-fills provenance fields, and dispatches the scalar data to the configured storage backend.</p> <p>Examples:</p> <p>Basic usage to log a loss metric: <pre><code>await context.viz().scalar(\"loss\", step=iteration, value=loss)\n</code></pre></p> <p>Logging a scalar with extra metadata and custom tags: <pre><code>await context.viz().scalar(\n    \"accuracy\",\n    step=42,\n    value=0.98,\n    figure_id=\"metrics\",\n    meta={\"model\": \"resnet\"},\n    tags=[\"experiment:baseline\"]\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>track_id</code> <code>str</code> <p>Unique identifier for the scalar track (e.g., \"loss\").</p> required <code>step</code> <code>int</code> <p>Integer step or iteration number for the data point.</p> required <code>value</code> <code>float</code> <p>The scalar value to record (float).</p> required <code>figure_id</code> <code>str | None</code> <p>Optional figure grouping for UI display.</p> <code>None</code> <code>mode</code> <code>VizMode</code> <p>Storage mode, typically \"append\".</p> <code>'append'</code> <code>meta</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of extra metadata.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>Optional list of string labels. The tag \"type:scalar\" is automatically appended.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None. The event is persisted for later visualization.</p> vector(track_id, *, step, values, ...) <p>Record a single vector (1D array) for visualization in the Aethergraph UI.</p> <p>This method standardizes the event format, auto-fills provenance fields, and dispatches the vector data to the configured storage backend.</p> <p>Examples:</p> <p>Basic usage to log a vector: <pre><code>await context.viz().vector(\"embedding\", step=iteration, values=[0.1, 0.2, 0.3])\n</code></pre></p> <p>Logging a vector with extra metadata and custom tags: <pre><code>await context.viz().vector(\n    \"features\",\n    step=42,\n    values=[1.0, 2.5, 3.7],\n    figure_id=\"feature_tracks\",\n    meta={\"source\": \"encoder\"},\n    tags=[\"experiment:baseline\"]\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>track_id</code> <code>str</code> <p>Unique identifier for the vector track (e.g., \"embedding\").</p> required <code>step</code> <code>int</code> <p>Integer step or iteration number for the data point.</p> required <code>values</code> <code>Sequence[float]</code> <p>Sequence of float values representing the vector.</p> required <code>figure_id</code> <code>str | None</code> <p>Optional figure grouping for UI display.</p> <code>None</code> <code>mode</code> <code>VizMode</code> <p>Storage mode, typically \"append\".</p> <code>'append'</code> <code>meta</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of extra metadata.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>Optional list of string labels. The tag \"type:vector\" is automatically appended.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None. The event is persisted for later visualization.</p> matrix(track_id, *, step, matrix, ...) <p>Record a single matrix (2D array) for visualization in the Aethergraph UI.</p> <p>This method standardizes the event format, auto-fills provenance fields, and dispatches the matrix data to the configured storage backend.</p> <p>Examples:</p> <p>Basic usage to log a matrix: <pre><code>await context.viz().matrix(\"confusion\", step=iteration, matrix=[[1, 2], [3, 4]])\n</code></pre></p> <p>Logging a matrix with extra metadata and custom tags: <pre><code>await context.viz().matrix(\n    \"heatmap\",\n    step=42,\n    matrix=[[0.1, 0.2], [0.3, 0.4]],\n    figure_id=\"metrics\",\n    meta={\"source\": \"model\"},\n    tags=[\"experiment:baseline\"]\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>track_id</code> <code>str</code> <p>Unique identifier for the matrix track (e.g., \"confusion\").</p> required <code>step</code> <code>int</code> <p>Integer step or iteration number for the data point.</p> required <code>matrix</code> <code>Sequence[Sequence[float]]</code> <p>Sequence of sequences of float values representing the 2D matrix.</p> required <code>figure_id</code> <code>str | None</code> <p>Optional figure grouping for UI display.</p> <code>None</code> <code>mode</code> <code>VizMode</code> <p>Storage mode, typically \"append\".</p> <code>'append'</code> <code>meta</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of extra metadata.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>Optional list of string labels. The tag \"matrix\" is automatically appended.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None. The event is persisted for later visualization.</p>"},{"location":"reference/context-viz/#2-artifact-visualization-image-etc","title":"2. Artifact Visualization (image etc.)","text":"image_from_artifact(track_id, *, step, artifact, ...) <p>Record a reference to an existing image Artifact for visualization in the Aethergraph UI.</p> <p>This method standardizes the event format, auto-fills provenance fields, and dispatches the image reference to the configured storage backend.</p> <p>Examples:</p> <p>Basic usage to log an image artifact: <pre><code>await context.viz().image_from_artifact(\n    \"design_shape\",\n    step=17,\n    artifact=artifact,\n    figure_id=\"design\"\n)\n</code></pre></p> <p>Logging an image with extra metadata and custom tags: <pre><code>await context.viz().image_from_artifact(\n    \"output_frame\",\n    step=42,\n    artifact=artifact,\n    figure_id=\"frames\",\n    meta={\"source\": \"simulation\"},\n    tags=[\"experiment:baseline\"]\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>track_id</code> <code>str</code> <p>Unique identifier for the image track (e.g., \"design_shape\").</p> required <code>step</code> <code>int</code> <p>Integer step or iteration number for the data point.</p> required <code>artifact</code> <code>Artifact</code> <p>The Artifact object referencing the stored image.</p> required <code>figure_id</code> <code>str | None</code> <p>Optional figure grouping for UI display.</p> <code>None</code> <code>mode</code> <code>VizMode</code> <p>Storage mode, typically \"append\".</p> <code>'append'</code> <code>meta</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of extra metadata.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>Optional list of string labels. The tag \"image\" is automatically appended.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None. The event is persisted for later visualization.</p> image_from_bytes(track_id, *, step, data, ...) <p>Save image bytes as an Artifact and log a visualization event.</p> <p>This convenience method is accessed via <code>context.viz().image_from_bytes(...)</code> and is used by the Aethergraph UI to persist image data to storage. It stores the image as an Artifact using the configured ArtifactFacade, then logs a visualization event referencing the saved artifact.</p> <p>Examples:</p> <p>Saving a PNG image to the current visualization track: <pre><code>await context.viz().image_from_bytes(\n    track_id=\"experiment-123\",\n    step=42,\n    data=image_bytes,\n    mime=\"image/png\",\n    labels={\"type\": \"output\", \"stage\": \"inference\"},\n    tags=[\"result\", \"png\"]\n)\n</code></pre></p> <p>Saving an image with custom metadata and figure association: <pre><code>await context.viz().image_from_bytes(\n    track_id=\"demo-track\",\n    step=7,\n    data=img_bytes,\n    figure_id=\"fig-1\",\n    meta={\"caption\": \"Sample output\"},\n    mode=\"replace\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>track_id</code> <code>str</code> <p>The identifier for the visualization track to associate with the image.</p> required <code>step</code> <code>int</code> <p>The step or index within the track for this image.</p> required <code>data</code> <code>bytes</code> <p>Raw image bytes to be saved.</p> required <code>mime</code> <code>str</code> <p>The MIME type of the image (default: \"image/png\").</p> <code>'image/png'</code> <code>kind</code> <code>str</code> <p>The artifact kind (default: \"image\").</p> <code>'image'</code> <code>figure_id</code> <code>str | None</code> <p>Optional identifier for the figure this image belongs to.</p> <code>None</code> <code>mode</code> <code>VizMode</code> <p>Visualization mode, e.g., \"append\" or \"replace\".</p> <code>'append'</code> <code>labels</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of labels to attach to the artifact.</p> <code>None</code> <code>meta</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata for the visualization event.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>Optional list of string tags for categorization.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Artifact</code> <code>Artifact</code> <p>The persisted <code>Artifact</code> object representing the saved image.</p> Notes <ul> <li>This method requires that <code>self.artifacts</code> is set to an <code>ArtifactFacade</code> instance.</li> <li>The saved artifact is automatically linked to the visualization event.</li> <li>To change the name of the saved artifact in UI, use <code>labels</code> to set a \"filename\" label.</li> </ul>"},{"location":"reference/decorators/","title":"Decorator API \u2014 <code>@graph_fn</code>, <code>@graphify</code>, <code>@tool</code>","text":"<p>A single reference page for the three core decorators you\u2019ll use to build with AetherGraph.</p>"},{"location":"reference/decorators/#quick-chooser","title":"Quick chooser","text":"Use this when\u2026 Pick Why You want the quickest way to make a Python function runnable as a graph entrypoint and get a <code>context</code> for services <code>@graph_fn</code> Small, ergonomic, ideal for tutorials, notebooks, single\u2011entry tools/agents You need to expose reusable steps with typed I/O that can run standalone or as graph nodes <code>@tool</code> Dual\u2011mode decorator; gives you fine control of inputs/outputs; portable and composable Your function body is mostly tool wiring (fan\u2011in/fan\u2011out) and you want a static graph spec from Python syntax <code>@graphify</code> Author graphs declaratively; returns a <code>TaskGraph</code> factory; great for orchestration patterns"},{"location":"reference/decorators/#graph_fn","title":"<code>@graph_fn</code>","text":"<p>Wrap a normal async function into a runnable graph with optional <code>context</code> injection.</p>"},{"location":"reference/decorators/#signature","title":"Signature","text":"<pre><code>@graph_fn(name: str, *, inputs: list[str] | None = None, outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\nasync def my_fn(..., *, context: NodeContext): ...\n</code></pre>"},{"location":"reference/decorators/#description","title":"Description","text":"<ul> <li>Builds a fresh <code>TaskGraph</code> under the hood and executes it immediately.</li> <li>If your function signature includes <code>context: NodeContext</code>, AetherGraph injects a <code>NodeContext</code> so you can call <code>context.channel()</code>, <code>context.memory()</code>, <code>context.artifacts()</code>, <code>context.llm()</code>, etc.</li> <li>Ideal for single\u2011file demos, CLI/notebook usage, and simple agents.</li> </ul>"},{"location":"reference/decorators/#parameters","title":"Parameters","text":"<ul> <li>name (str, required) \u2014 Graph ID and human\u2011readable name.</li> <li>inputs (list[str], optional) \u2014 Declared input keys. Purely declarative; your function still gets normal Python args.</li> <li>outputs (list[str], optional) \u2014 Declared output keys. If you return a single literal, declare exactly one.</li> <li>version (str, optional) \u2014 Semantic version for registry.</li> <li>agent (str, optional) \u2014 If provided, registers this graph function as an agent under the given name.</li> </ul>"},{"location":"reference/decorators/#returns","title":"Returns","text":"<ul> <li>The decorator returns a <code>GraphFunction</code> object. Calling/awaiting it executes the graph and returns a <code>dict</code> of outputs keyed by <code>outputs</code>.</li> </ul>"},{"location":"reference/decorators/#minimal-example","title":"Minimal example","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"hello.world\", inputs=[\"name\"], outputs=[\"greeting\"], version=\"0.1.0\")\nasync def hello_world(name: str, *, context: NodeContext):\n    await context.channel().send_text(f\"\ud83d\udc4b Hello {name}\")\n    return {\"greeting\": f\"Hello, {name}!\"}\n\n# Run (async)\nres = await hello_world(name=\"Aether\")\nprint(res[\"greeting\"])  # \u2192 \"Hello, Aether!\"\n</code></pre>"},{"location":"reference/decorators/#tips","title":"Tips","text":"<ul> <li>Return a <code>dict</code> where keys match <code>outputs</code>. If you return a single literal, declare one output.</li> <li>You can call <code>@tool</code> functions inside a <code>@graph_fn</code> (they\u2019ll run immediately, not build nodes). Use this for small, fast helper steps.</li> <li>For complex orchestration (fan\u2011in/fan\u2011out), prefer <code>@graphify</code> so <code>@tool</code> calls become nodes.</li> </ul>"},{"location":"reference/decorators/#tool","title":"<code>@tool</code>","text":"<p>Dual\u2011mode decorator for reusable steps with explicit inputs/outputs.</p>"},{"location":"reference/decorators/#signature_1","title":"Signature","text":"<pre><code>@tool(outputs: list[str], *, inputs: list[str] | None = None, name: str | None = None, version: str = \"0.1.0\")\ndef/async def my_tool(...): ...\n</code></pre>"},{"location":"reference/decorators/#description_1","title":"Description","text":"<ul> <li>Immediate mode (no builder/interpreter active): calling the function executes it now and returns a <code>dict</code> of outputs.</li> <li>Graph mode (inside a <code>graph(...)</code> / <code>@graphify</code> body or during <code>@graph_fn</code> build): calling the proxy adds a node to the current graph and returns a <code>NodeHandle</code> with typed outputs.</li> <li>Registers the underlying implementation in the runtime registry for portability.</li> </ul>"},{"location":"reference/decorators/#parameters_1","title":"Parameters","text":"<ul> <li>outputs (list[str], required) \u2014 Names of output values (e.g., <code>[\"result\"]</code>, <code>[\"image\", \"stats\"]</code>).</li> <li>inputs (list[str], optional) \u2014 Input names (auto\u2011inferred from signature if omitted).</li> <li>name (str, optional) \u2014 Registry/display name; defaults to function name.</li> <li>version (str, optional) \u2014 Semantic version for registry.</li> </ul>"},{"location":"reference/decorators/#returns_1","title":"Returns","text":"<ul> <li>In immediate mode: <code>dict</code> of outputs.</li> <li>In graph mode: <code>NodeHandle</code> with <code>.out_key</code> attributes (e.g., <code>node.result</code>).</li> </ul>"},{"location":"reference/decorators/#example-reusable-step","title":"Example \u2014 reusable step","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"sum\", \"count\"])\ndef aggregate(xs: list[int]):\n    return {\"sum\": sum(xs), \"count\": len(xs)}\n\n# Immediate mode\nprint(aggregate([1,2,3]))  # {\"sum\": 6, \"count\": 3}\n</code></pre>"},{"location":"reference/decorators/#example-using-tool-inside-graph_fn-immediate-execution","title":"Example \u2014 using <code>@tool</code> inside <code>@graph_fn</code> (immediate execution)","text":"<pre><code>from aethergraph import graph_fn, tool, NodeContext\n\n@tool(outputs=[\"sum\"])  \ndef add(x: int, y: int):\n    return {\"sum\": x + y}\n\n@graph_fn(name=\"calc.pipeline\", inputs=[\"a\",\"b\"], outputs=[\"total\"])\nasync def calc(a: int, b: int, *, context: NodeContext):\n    out = add(a, b)                 # immediate mode here\n    await context.channel().send_text(f\"sum = {out['sum']}\")\n    return {\"total\": out[\"sum\"]}\n</code></pre>"},{"location":"reference/decorators/#tips_1","title":"Tips","text":"<ul> <li>Use <code>@tool</code> to make steps portable and inspectable (typed I/O makes graphs predictable).</li> <li>In <code>@graph_fn</code> the <code>@tool</code> call executes immediately; in <code>@graphify</code> the same call becomes a graph node.</li> <li>Control\u2011flow knobs like <code>_after</code>, <code>_id</code>, <code>_alias</code> apply only in graph\u2011building contexts (e.g., <code>@graphify</code>), not in <code>@graph_fn</code> bodies.</li> </ul>"},{"location":"reference/decorators/#graphify","title":"<code>@graphify</code>","text":"<p>Author a static TaskGraph by writing normal Python that calls <code>@tool</code>s. The function body executes during build to register nodes and edges; returned node handles/literals define graph outputs.</p>"},{"location":"reference/decorators/#signature_2","title":"Signature","text":"<pre><code>@graphify(*, name: str = \"default_graph\", inputs: Iterable[str] | dict = (), outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\ndef my_graph(...):\n    ...  # body calls @tool proxies (graph mode)\n    return {...}  # NodeHandle(s) and/or literal refs\n</code></pre>"},{"location":"reference/decorators/#description_2","title":"Description","text":"<ul> <li>The decorated function becomes a factory: calling <code>my_graph.build()</code> returns a <code>TaskGraph</code> spec.</li> <li>When the body runs under the builder, calls to <code>@tool</code> proxies add nodes to the graph and return <code>NodeHandle</code>s.</li> <li>Perfect for fan\u2011out (parallel branches) and fan\u2011in (join/aggregate) patterns.</li> </ul>"},{"location":"reference/decorators/#parameters_2","title":"Parameters","text":"<ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (iterable[str] or dict) \u2014 Required/optional input names. If dict, keys are optional names with defaults in the body.</li> <li>outputs (list[str], optional) \u2014 Names of exposed boundary outputs. If body returns a single literal, declare exactly one.</li> <li>version (str) \u2014 Semantic version.</li> <li>agent (str, optional) \u2014 Register this graph as an agent (factory registered).</li> </ul>"},{"location":"reference/decorators/#returns_2","title":"Returns","text":"<ul> <li> <p>The decorator returns a builder function with:</p> </li> <li> <p><code>.build() -&gt; TaskGraph</code></p> </li> <li><code>.spec() -&gt; TaskGraphSpec</code></li> <li><code>.io() -&gt; IO signature</code></li> </ul>"},{"location":"reference/decorators/#example-fanout-fanin","title":"Example \u2014 fan\u2011out + fan\u2011in","text":"<pre><code>from aethergraph import graphify, tool\n\n@tool(outputs=[\"y\"])\ndef f(x: int):\n    return {\"y\": x * x}\n\n@tool(outputs=[\"z\"])\ndef g(x: int):\n    return {\"z\": x + 1}\n\n@tool(outputs=[\"sum\"])  \ndef add(a: int, b: int):\n    return {\"sum\": a + b}\n\n@graphify(name=\"fan_in_out\", inputs=[\"x\"], outputs=[\"total\"]) \ndef pipe(x):\n    a = f(x=x)          # node A (graph mode)  \u2510\n    b = g(x=x)          # node B (graph mode)  \u2518  \u2190 fan\u2011out\n    c = add(a=a.y, b=b.z)   # node C depends on A,B \u2190 fan\u2011in\n    return {\"total\": c.sum}\n\nG = pipe.build()\n</code></pre>"},{"location":"reference/decorators/#example-ordering-with-_after-and-aliasing","title":"Example \u2014 ordering with <code>_after</code> and aliasing","text":"<pre><code>@tool(outputs=[\"out\"]) \ndef step(name: str):\n    return {\"out\": name}\n\n@graphify(name=\"ordered\", inputs=[]) \ndef ordered():\n    a = step(name=\"A\", _alias=\"first\")\n    b = step(name=\"B\", _after=a)\n    c = step(name=\"C\", _after=[a, b], _id=\"third\")\n    return {\"final\": c.out}\n\nG = ordered.build()\n</code></pre>"},{"location":"reference/decorators/#using-tool-inside-graph_fn-vs-graphify","title":"Using <code>@tool</code> inside <code>@graph_fn</code> vs <code>@graphify</code>","text":"<ul> <li>Inside <code>@graph_fn</code>: <code>@tool</code> calls execute immediately (no <code>_after</code>/alias). Great for quick helpers.</li> <li>Inside <code>@graphify</code>: <code>@tool</code> calls define nodes (support <code>_after</code>, <code>_alias</code>, <code>_id</code>, <code>_labels</code>). Ideal for orchestration.</li> </ul>"},{"location":"reference/decorators/#interop-best-practices","title":"Interop &amp; best practices","text":"<ol> <li>Start simple with <code>@graph_fn</code> \u2014 it\u2019s the easiest way to get <code>context</code> and ship a working demo.</li> <li>Extract reusable steps with <code>@tool</code> \u2014 typed I/O makes debugging, tracing, and promotion to graphs trivial.</li> <li> <p>Promote to <code>@graphify</code> when you need:</p> </li> <li> <p>Parallel branches (fan\u2011out), joins (fan\u2011in)</p> </li> <li>Explicit ordering with <code>_after</code></li> <li>Reuse via <code>NodeHandle</code> composition and aliasing</li> <li> <p>Context access:</p> </li> <li> <p><code>@graph_fn</code> gives you <code>context: NodeContext</code> directly.</p> </li> <li>In <code>@graphify</code>, nodes don\u2019t get <code>context</code>; tools run with context at execution time when the graph is interpreted. Use <code>@tool</code> implementations to call <code>context.*</code>.</li> <li>Outputs discipline \u2014 keep outputs small and typed (e.g., <code>{ \"image\": ref, \"metrics\": {\u2026} }</code>).</li> <li>Registry \u2014 all three decorators register artifacts (graph fn as runnable, tool impls, graph factories) so you can call by name later.</li> </ol>"},{"location":"reference/decorators/#see-also","title":"See also","text":"<ul> <li>Quick Start: install, start server, first <code>@graph_fn</code>.</li> <li>**Contex</li> </ul>"},{"location":"reference/graph-graph-fn/","title":"<code>@graph_fn</code> \u2013 Graph Function API","text":"<p><code>@graph_fn</code> turns a plain Python function into a GraphFunction object \u2013 a named, versioned graph entry that still feels like a normal Python callable.</p> <ul> <li>You write a regular function (usually <code>async</code>) with whatever parameters you want.</li> <li>If the function\u2019s signature includes a <code>context</code> parameter, a <code>NodeContext</code> is injected automatically.</li> <li>Inside the body, calls to <code>@tool</code> functions are recorded as nodes, while plain Python calls just run inline.</li> </ul> <p>This section focuses on the API surface and contracts, not on execution details or schedulers.</p>"},{"location":"reference/graph-graph-fn/#1-graphfunction-overview","title":"1. GraphFunction Overview","text":"<p>The decorator builds a <code>GraphFunction</code> instance:</p> <pre><code>class GraphFunction:\n    def __init__(\n        self,\n        name: str,\n        fn: Callable,\n        inputs: list[str] | None = None,\n        outputs: list[str] | None = None,\n        version: str = \"0.1.0\",\n    ):\n        self.graph_id = name\n        self.name = name\n        self.fn = fn\n        self.inputs = inputs or []\n        self.outputs = outputs or []\n        self.version = version\n        self.registry_key: str | None = None\n        self.last_graph = None\n        self.last_context = None\n        self.last_memory_snapshot = None\n</code></pre>"},{"location":"reference/graph-graph-fn/#key-attributes","title":"Key attributes","text":"Attribute Type Meaning / contract <code>graph_id</code> <code>str</code> Stable identifier for the graph, usually equal to <code>name</code>. Used for specs, visualization, and provenance. <code>name</code> <code>str</code> Human\u2011readable name of the graph function. Shows up in logs/registry. <code>fn</code> <code>Callable</code> The original Python function body you wrote. Used to build the graph and evaluate return values. <code>inputs</code> <code>list[str]</code> Declared graph input names. Optional; can be empty. <code>outputs</code> <code>list[str]</code> Declared graph output names. Optional; used for output normalization/validation. <code>version</code> <code>str</code> Version tag for this graph. Included in registry entries and provenance. <code>registry_key</code> <code>str \\| None</code> Internal hook for registry bookkeeping. Not part of the public contract. <code>last_graph</code> <code>TaskGraph \\| None</code> Last built graph (for inspection and debugging). May be <code>None</code> if not built yet. <code>last_context</code> <code>Any</code> Reserved for runtime use (not a stable API). <code>last_memory_snapshot</code> <code>Any</code> Reserved for runtime use (not a stable API). <p>Treat <code>graph_id</code>, <code>name</code>, <code>inputs</code>, <code>outputs</code>, <code>version</code>, and <code>last_graph</code> as the main public surface; the other fields are implementation details that can change.</p>"},{"location":"reference/graph-graph-fn/#2-graph_fn-decorator-definition","title":"2. <code>@graph_fn</code> Decorator \u2013 Definition","text":"<pre><code>def graph_fn(\n    name: str,\n    inputs: list[str] | None = None,\n    outputs: list[str] | None = None,\n    version: str = \"0.1.0\",\n    agent: str | None = None,\n) -&gt; Callable[[Callable], GraphFunction]:\n    ...\n</code></pre>"},{"location":"reference/graph-graph-fn/#parameters","title":"Parameters","text":"Parameter Type Required? Description <code>name</code> <code>str</code> Yes Logical ID for the graph. Also used as the <code>graphfn</code> registry name. <code>inputs</code> <code>list[str] \\| None</code> No Optional list of named graph inputs. If omitted, the graph can still accept <code>**inputs</code>, but there is no typed input list in the spec. <code>outputs</code> <code>list[str] \\| None</code> No Optional list of declared graph outputs. Used to normalize and validate the function\u2019s return value. <code>version</code> <code>str</code> No Semantic version for this graph. Defaults to <code>\"0.1.0\"</code>. <code>agent</code> <code>str \\| None</code> No If provided, additionally registers this <code>GraphFunction</code> in the registry under the <code>agent</code> namespace with the given name."},{"location":"reference/graph-graph-fn/#what-the-decorator-returns","title":"What the decorator returns","text":"<p>Using <code>@graph_fn</code> on a function returns a <code>GraphFunction</code> instance, not the raw function:</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"example\", inputs=[\"x\"], outputs=[\"y\"])\nasync def example(x: int, *, context: NodeContext):\n    ...\n\n# `example` is now a GraphFunction\nassert isinstance(example, GraphFunction)\n</code></pre> <p>Internally, the decorator:</p> <ol> <li>Constructs <code>GraphFunction(name, fn, inputs, outputs, version)</code>.</li> <li>Looks up the current registry via <code>current_registry()</code>.</li> <li> <p>If a registry is available, registers the object as:</p> <ul> <li><code>nspace=\"graphfn\"</code>, <code>name=name</code>, <code>version=version</code>, <code>obj=gf</code>.</li> </ul> </li> <li> <p>If <code>agent</code> is provided, also registers:</p> <ul> <li><code>nspace=\"agent\"</code>, <code>name=agent</code>, <code>version=version</code>, <code>obj=gf</code>.</li> </ul> </li> </ol> <p>The <code>agent</code> registration currently serves as metadata for higher\u2011level agent frameworks; it does not change how the graph function runs.</p>"},{"location":"reference/graph-graph-fn/#3-function-shape-context-injection","title":"3. Function Shape &amp; Context Injection","text":"<p>A <code>graph_fn</code> is written like a normal Python function. The only special convention is the optional <code>context</code> parameter:</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"example\")\nasync def example(x: int, *, context: NodeContext):\n    # `context` is injected automatically\n    await context.channel().send_text(f\"x={x}\")\n    return {\"y\": x + 1}\n</code></pre> <p>Rules:</p> <ul> <li>You can define any positional/keyword parameters you want (<code>x</code>, <code>y</code>, <code>options</code>, etc.).</li> <li>If the function signature contains a parameter named <code>context</code>, the runtime injects a <code>NodeContext</code> instance when building/executing the graph.</li> <li>If <code>context</code> is not present in the signature, nothing special is injected.</li> </ul> <p>The injected <code>NodeContext</code> gives you access to runtime services like <code>channel()</code>, <code>memory()</code>, <code>artifacts()</code>, <code>logger()</code>, etc. This is the main way <code>graph_fn</code>s interact with the outside world.</p>"},{"location":"reference/graph-graph-fn/#4-tools-vs-plain-callables-inside-a-graph_fn","title":"4. Tools vs Plain Callables Inside a <code>graph_fn</code>","text":"<p>Inside a graph function body, you can freely mix:</p> <ol> <li>Plain Python code and callables \u2013 executed immediately as normal Python.</li> <li><code>@tool</code> functions \u2013 these create graph nodes and tracked edges when invoked.</li> </ol>"},{"location":"reference/graph-graph-fn/#plain-callables","title":"Plain callables","text":"<p>Regular functions or methods (not decorated with <code>@tool</code>) behave exactly as in standard Python:</p> <pre><code>def local_scale(x: int, factor: int = 2) -&gt; int:\n    return x * factor\n\n@graph_fn(name=\"mixed_body\")\nasync def mixed_body(x: int, *, context: NodeContext):\n    y = local_scale(x)          # plain Python call, no node created\n    context.logger().info(\"scaled\", extra={\"y\": y})\n    return {\"y\": y}\n</code></pre>"},{"location":"reference/graph-graph-fn/#tool-functions-nodes-on-the-fly","title":"<code>@tool</code> functions \u2013 nodes on the fly","text":"<p>When you call a <code>@tool</code> inside a <code>graph_fn</code>, the decorator\u2019s proxy detects that a graph builder is active, execute the <code>@tool</code>, and return the <code>NodeHandler</code>. You can access the output of the tool as regular <code>@tool</code>. </p> <p>Conceptually:</p> <pre><code>from aethergraph import tool\n\n@tool(outputs=[\"total\"])\ndef sum_vec(xs: list[float]) -&gt; dict:\n    return {\"total\": sum(xs)}\n\n@graph_fn(name=\"tool_usage\")\nasync def tool_usage(xs: list[float], *, context: NodeContext):\n    stats = {\"n\": len(xs)}           # plain Python\n    out = sum_vec(xs=xs)             # becomes a node (NodeHandle)\n    await context.channel().send_text(\n        f\"n={stats['n']}, total={out['total']}\"\n    )\n    return {\"total\": out[\"total\"]}\n</code></pre> <p>API contract:</p> <ul> <li>Plain callables inside a <code>graph_fn</code> execute immediately and are not tracked as graph nodes.</li> <li><code>@tool</code> calls inside a <code>graph_fn</code> become tracked nodes with inputs/outputs wired into the <code>TaskGraph</code>.</li> <li>You can still treat the <code>@tool</code> call\u2019s result like a dict in your function body; the NodeHandle provides field\u2011like accessors that match the tool outputs.</li> </ul>"},{"location":"reference/graph-graph-fn/#5-output-contract-high-level","title":"5. Output Contract (High Level)","text":"<p>The <code>outputs</code> list given to <code>@graph_fn</code> defines the expected graph outputs. After your function body returns, the runtime normalizes the return value and enforces this contract.</p> <p>Supported return shapes (high\u2011level):</p> <ul> <li>A <code>dict[str, Any]</code> \u2013 keys may map to literals, <code>NodeHandle</code>s, or internal <code>Ref</code> objects.</li> <li>A single <code>NodeHandle</code> \u2013 its outputs will be exposed as graph outputs.</li> <li>A single literal / <code>Ref</code> \u2013 only valid when exactly one output is declared.</li> </ul> <p>If <code>outputs</code> is not <code>None</code>:</p> <ul> <li>The normalized result is restricted to exactly those keys (in order).</li> <li>Missing keys cause a <code>ValueError</code>.</li> </ul>"},{"location":"reference/graph-graph-fn/#6-registry-and-agent-metadata","title":"6. Registry and <code>agent</code> Metadata","text":"<p>When a registry is active (<code>current_registry() is not None</code>), <code>@graph_fn</code> registers the created <code>GraphFunction</code> automatically:</p> <pre><code>registry.register(\n    nspace=\"graphfn\",\n    name=name,\n    version=version,\n    obj=gf,\n)\n</code></pre> <p>If you pass <code>agent=\"my_agent\"</code>, the same object is additionally registered as:</p> <pre><code>registry.register(\n    nspace=\"agent\",\n    name=agent,\n    version=version,\n    obj=gf,\n)\n</code></pre> <p>Current usage:</p> <ul> <li><code>graphfn</code> entries: used to discover graphs by name/version and to attach specs/visualizations.</li> <li><code>agent</code> entries: reserved for higher\u2011level agent orchestration (e.g., routing, multi\u2011agent systems). At present, it is a metadata hook only; the core <code>GraphFunction</code> API does not change.</li> </ul>"},{"location":"reference/graph-graph-fn/#7-summary","title":"7. Summary","text":"<ul> <li><code>@graph_fn</code> wraps a Python function into a GraphFunction: a named, versioned graph entry with optional inputs/outputs.</li> <li>The function signature is normal Python; an optional <code>context</code> parameter triggers  <code>NodeContext</code> injection. </li> <li> <p>Inside the body:</p> <ul> <li>Plain Python calls execute inline.</li> <li><code>@tool</code> calls become nodes on the fly, tracked in the underlying <code>TaskGraph</code>.</li> </ul> </li> <li> <p>Declared <code>outputs</code> define the expected graph outputs and are enforced by the runtime\u2019s normalization logic.</p> </li> <li> <p>When a registry is active, each <code>graph_fn</code> is registered under <code>graphfn</code>, and optionally under <code>agent</code> when <code>agent=\"...\"</code> is provided.</p> </li> </ul>"},{"location":"reference/graph-graphify/","title":"<code>graphify</code> \u2013 Static Graph Builder Decorator","text":"<p><code>graphify</code> turns a plain Python function into a static graph factory: a callable that, when invoked, builds and returns a <code>TaskGraph</code> using the graph builder context.</p> <p>Key ideas:</p> <ul> <li>The decorated function is evaluated at build time, not at run time.</li> <li>Inputs are injected as graph refs (via <code>arg(\"&lt;name&gt;\")</code>), not real values.</li> <li>Calls to <code>@tool</code> inside the function body become tool nodes in the <code>TaskGraph</code>.</li> <li>The function\u2019s return value defines which refs / literals are exposed as graph outputs.</li> <li>There is no <code>NodeContext</code> injection in <code>graphify</code>; if you need context services, put that logic inside a <code>@tool</code> and call the tool from the graphified function.</li> </ul> <p>This section focuses on the API and contracts of <code>graphify</code>. Details of <code>TaskGraph</code> structure and the runner live on dedicated pages.</p>"},{"location":"reference/graph-graphify/#1-decorator-signature","title":"1. Decorator Signature","text":"<pre><code>def graphify(\n    *,\n    name: str,\n    outputs: list[str],\n    inputs = (),\n    version: str = \"0.1.0\",\n    agent: str | None = None,\n):\n    ...\n</code></pre>"},{"location":"reference/graph-graphify/#parameters","title":"Parameters","text":"Parameter Type Required? Description <code>name</code> <code>str</code> Yes Graph ID / name for the built <code>TaskGraph</code>. Used in specs, logs, and registry. <code>inputs</code> <code>Iterable[str]</code> or <code>dict[str, Any]</code> No (default <code>()</code> ) Graph input declaration. Can be a list/tuple of required input names, or a dict of optional inputs with default values. <code>outputs</code> <code>list[str] \\| None</code> Yes List of declared graph output names. <code>version</code> <code>str</code> No Version tag for this graph. Included in registry entries and provenance. <code>agent</code> <code>str \\| None</code> No If provided, also registers this graph under the <code>agent</code> namespace in the registry. <p>Inputs semantics</p> <ul> <li> <p>If <code>inputs</code> is a sequence (e.g. <code>(\"x\", \"y\")</code>):</p> <ul> <li>All listed names are treated as required inputs.</li> </ul> </li> <li> <p>If <code>inputs</code> is a dict (e.g. <code>{ \"x\": 0.0, \"y\": 1.0 }</code>):</p> <ul> <li>All keys are treated as optional inputs, and the dict values can be used as default metadata.</li> </ul> </li> </ul> <p>The function parameters determine which inputs are injected (see below); <code>inputs</code> defines the graph-level input signature (required/optional) via <code>g.declare_inputs(...)</code>.</p>"},{"location":"reference/graph-graphify/#2-what-graphify-returns","title":"2. What <code>graphify</code> Returns","text":"<p>Applying <code>graphify</code> to a function wraps it into a builder function:</p> <pre><code>@graphify(name=\"my_graph\", inputs=[\"x\"], outputs=[\"y\"])\ndef my_graph(x):\n    ...\n</code></pre> <p>The decorated object (e.g. <code>my_graph</code>) is not the original function anymore. Instead, it is a zero-argument callable that builds a new <code>TaskGraph</code> each time you call it:</p> <pre><code>g = my_graph()     # builds and returns a TaskGraph\n</code></pre> <p>For convenience, the builder also exposes a few attributes:</p> Attribute Type Meaning <code>my_graph()</code> <code>() -&gt; TaskGraph</code> Calling the object builds a fresh <code>TaskGraph</code>. <code>my_graph.build</code> <code>() -&gt; TaskGraph</code> Alias to the same build function. <code>my_graph.graph_name</code> <code>str</code> Graph name used when constructing the <code>TaskGraph</code> (from <code>name</code>). <code>my_graph.version</code> <code>str</code> Version tag passed via <code>version</code>. <code>my_graph.spec()</code> <code>() -&gt; TaskGraphSpec</code> Builds a graph and returns only the spec (<code>g.spec</code>). <code>my_graph.io()</code> <code>() -&gt; dict</code> Builds a graph and returns <code>g.io_signature()</code>. <p>In other words: <code>graphify</code> gives you a graph factory with lightweight helpers to inspect the spec and I/O signature. The original function body is used only to describe how the graph should be built.</p>"},{"location":"reference/graph-graphify/#3-function-shape-input-injection","title":"3. Function Shape &amp; Input Injection","text":"<p>The function you decorate with <code>graphify</code> is written like a regular synchronous function. Always return a dictionary of outputs specified in the decorator. </p> <pre><code>from aethergraph import graphify\n\n@graphify(name=\"sum_graph\", inputs=[\"xs\"], outputs=[\"total\"])\ndef sum_graph(xs):\n    # body uses NodeHandles / refs\n    ...\n    return {\"total\": some_handle}\n</code></pre>"},{"location":"reference/graph-graphify/#parameter-matching","title":"Parameter matching","text":"<p>Contract:</p> <ul> <li>Only parameters whose name appears in <code>inputs</code> are injected.</li> <li>Injected values are <code>arg(\"&lt;name&gt;\")</code> refs, not concrete Python values.</li> <li>Extra parameters in the function signature that are not listed in <code>inputs</code> are not passed by <code>graphify</code>.</li> <li>There is no <code>context</code> parameter here \u2013 if you need <code>NodeContext</code> services (channel, memory, artifacts, logger, etc.), move that logic into a <code>@tool</code> and call the tool from the graphified function.</li> </ul> <p>This means:</p> <ul> <li>Use <code>inputs</code> to define which parameters are part of the graph I/O.</li> <li>Inside the function body, treat injected parameters as graph references \u2013 suitable for passing into <code>@tool</code> calls and other graph APIs.</li> </ul>"},{"location":"reference/graph-graphify/#4-tools-vs-plain-code-inside-a-graphify-body","title":"4. Tools vs Plain Code Inside a <code>graphify</code> Body","text":"<p>When the builder runs your function inside <code>with graph(name=...) as g</code>, the graph builder is active (<code>current_builder()</code> is not <code>None</code>). This affects how <code>@tool</code> calls behave.</p>"},{"location":"reference/graph-graphify/#plain-python-code","title":"Plain Python code","text":"<p>Non-tool code runs normally and is not recorded as a node:</p> <pre><code>def local_scale(xs, factor=2):\n    return [x * factor for x in xs]\n\n@graphify(name=\"scaled_sum\", inputs=[\"xs\"], outputs=[\"total\"])\ndef scaled_sum(xs):\n    ys = local_scale(xs)   # plain Python; not a node\n    ...\n</code></pre>"},{"location":"reference/graph-graphify/#tool-calls-nodes","title":"<code>@tool</code> calls \u2192 nodes","text":"<p>When you call a <code>@tool</code> inside a <code>graphify</code> body, the <code>tool</code> proxy uses <code>call_tool(...)</code>:</p> <ul> <li>It detects the active <code>GraphBuilder</code> via <code>current_builder()</code>.</li> <li>It creates a tool node via <code>builder.add_tool_node(...)</code>.</li> <li>It returns a NodeHandle (static build-mode handle) with <code>node_id</code> and <code>output_keys</code>.</li> </ul> <p>Example shape:</p> <pre><code>from aethergraph import tool\n\n@tool(outputs=[\"total\"])\ndef sum_vec(xs: list[float]) -&gt; dict:\n    return {\"total\": sum(xs)}\n\n@graphify(name=\"sum_graph\", inputs=[\"xs\"], outputs=[\"total\"])\ndef sum_graph(xs):\n    node = sum_vec(xs=xs)     # \u2190 NodeHandle, not a concrete result\n    return {\"total\": node}   # expose this node\u2019s outputs\n</code></pre> <p>Contract:</p> <ul> <li>Plain Python functions: execute at build time, do not affect the graph structure (unless they call graph APIs themselves).</li> <li><code>@tool</code> calls: always become nodes in the resulting <code>TaskGraph</code> when used inside a <code>graphify</code> body.</li> </ul>"},{"location":"reference/graph-graphify/#control-kwargs-for-node-ordering-ids","title":"Control kwargs for node ordering &amp; IDs","text":"<p>Calling tools inside a <code>@graphify</code> also supports a small set of control-plane keyword arguments that do not become data inputs but affect node metadata and ordering. </p> <p>You can pass them when calling a <code>@tool</code> inside a <code>graphify</code> body:</p> <ul> <li><code>_after</code>: one node or a list of nodes/IDs this node should run after.</li> <li><code>_name</code>: human-readable display name for the node (stored in metadata as <code>display_name</code>).</li> <li><code>_alias</code>: short, unique alias for the node; can be used for later lookup via <code>find_by_alias</code>.</li> <li><code>_id</code>: hard override for the underlying <code>node_id</code> (must be unique within the graph).</li> <li><code>_labels</code>: one or more string labels (as a str or list[str]) to tag the node; used for later lookup and grouping via <code>find_by_label</code>.</li> </ul> <p>Example:</p> <pre><code>n1 = sum_vec(xs=xs, _alias=\"sum1\")\nn2 = sum_vec(xs=n1.total, _after=[n1], _name=\"sum_again\", _id=\"sum_again_node\")\n</code></pre> <p>These control fields are optional and are taken out of <code>kwargs</code> before computing tool inputs, so they never appear as data edges.</p>"},{"location":"reference/graph-graphify/#5-registry","title":"5. Registry","text":"<p>At decoration time, <code>graphify</code> checks the current registry:</p> <p>Behavior:</p> <ul> <li> <p>If a registry is active:</p> <ul> <li>A <code>TaskGraph</code> instance is registered under <code>nspace=\"graph\"</code> with the given <code>name</code> and <code>version</code>.</li> <li>If <code>agent</code> is provided, another <code>TaskGraph</code> instance is registered under <code>nspace=\"agent\"</code> with that name.</li> </ul> </li> <li> <p>If no registry is active:</p> <ul> <li>The builder still works; registration is simply skipped.</li> </ul> </li> </ul> <p>Current implementation registers a concrete <code>TaskGraph</code> instance, not the factory itself.</p>"},{"location":"reference/graph-graphify/#6-summary","title":"6. Summary","text":"<ul> <li><code>graphify</code> decorates a synchronous function and turns it into a TaskGraph factory: calling the decorated object builds a fresh <code>TaskGraph</code>.</li> <li><code>inputs</code> defines graph-level input names (required vs optional) and determines which parameters are injected as <code>arg(\"&lt;name&gt;\")</code> refs.</li> <li> <p>Inside the body:</p> <ul> <li>Plain Python code runs at build time and is not recorded as nodes.</li> <li><code>@tool</code> calls become tool nodes via the active <code>GraphBuilder</code> (<code>current_builder()</code>), with optional control-plane kwargs (<code>_after</code>, <code>_name</code>, <code>_alias</code>, <code>_id</code>) for ordering and IDs.</li> </ul> </li> <li> <p>There is no <code>NodeContext</code> in <code>graphify</code>; use a <code>@tool</code> when you need context services and call that tool from the graphified function.</p> </li> <li>When a registry is active, <code>graphify</code> registers a built <code>TaskGraph</code> under <code>graph</code> and optionally under <code>agent</code> when <code>agent=\"...\"</code> is provided.</li> <li>When executed, <code>graphify</code> graphs automatically take advantage of concurrent execution under a configurable concurrency cap for nodes that are ready at the same level.</li> </ul>"},{"location":"reference/graph-task-graph/","title":"<code>TaskGraph</code> \u2013 Runtime Graph Representation","text":"<p><code>TaskGraph</code> is the runtime representation of a graph in AetherGraph. It combines:</p> <ul> <li>A structural spec (<code>TaskGraphSpec</code>) \u2013 nodes, dependencies, metadata.</li> <li>A mutable state (<code>TaskGraphState</code>) \u2013 node statuses, outputs, patches.</li> <li>Ephemeral runtime nodes (<code>TaskNodeRuntime</code>) \u2013 convenience wrappers used by the scheduler and tools.</li> </ul> <p>This page documents the most commonly used APIs on <code>TaskGraph</code>. </p> <p>You typically will not directly use <code>TaskGraph</code> method except for inspection. Using <code>@graph_fn</code> and <code>graphify</code> to create graph is preferred.</p>"},{"location":"reference/graph-task-graph/#1-construction-core-attributes","title":"1. Construction &amp; Core Attributes","text":""},{"location":"reference/graph-task-graph/#classmethods","title":"Classmethods","text":"<pre><code>TaskGraph.new_run(spec: TaskGraphSpec, *, run_id: str | None = None, **kwargs) -&gt; TaskGraph\nTaskGraph.from_spec(spec: TaskGraphSpec, *, state: TaskGraphState | None = None) -&gt; TaskGraph\n</code></pre> <ul> <li><code>new_run(...)</code> \u2013 convenience to create a fresh run with a new <code>run_id</code> and an empty <code>TaskGraphState</code> (all nodes start in <code>PENDING</code> except the inputs node, which is set to <code>DONE</code>).</li> <li><code>from_spec(...)</code> \u2013 construct a <code>TaskGraph</code> from an existing spec and optional state (used for resuming or inspecting previous runs).</li> </ul> <p>Key attributes</p> <ul> <li><code>graph.spec: TaskGraphSpec</code> \u2013 structural definition.</li> <li><code>graph.state: TaskGraphState</code> \u2013 statuses, outputs, patches, bound inputs.</li> <li><code>graph.graph_id: str</code> \u2013 alias for <code>spec.graph_id</code>.</li> <li><code>graph.nodes: list[TaskNodeRuntime]</code> \u2013 list of runtime node wrappers.</li> <li><code>graph._runtime_nodes: dict[str, TaskNodeRuntime]</code> \u2013 internal node table (id \u2192 runtime node).</li> </ul> Typical construction: new run vs resume <pre><code># New run from a spec\nspec = ...  # TaskGraphSpec from graphify / storage\nG = TaskGraph.new_run(spec)\n\n# Resume with existing state\nstate = ...  # TaskGraphState loaded from storage\nG_resumed = TaskGraph.from_spec(spec, state=state)\n</code></pre> <p>You rarely instantiate <code>TaskGraph</code> directly; use <code>new_run</code> or <code>from_spec</code> (or runner helpers) instead.</p>"},{"location":"reference/graph-task-graph/#2-node-access-selection","title":"2. Node Access &amp; Selection","text":""},{"location":"reference/graph-task-graph/#direct-access","title":"Direct access","text":"<pre><code>node(self, node_id: str) -&gt; TaskNodeRuntime\n@property\nnodes(self) -&gt; list[TaskNodeRuntime]\n\nnode_ids(self) -&gt; list[str]\nget_by_id(self, node_id: str) -&gt; str\n</code></pre> <ul> <li><code>node(node_id)</code> \u2013 get the runtime node (raises if not found).</li> <li><code>nodes</code> \u2013 list of all runtime nodes.</li> <li><code>node_ids()</code> \u2013 list of node IDs.</li> <li><code>get_by_id()</code> \u2013 returns the same ID or raises if missing (useful when normalizing selectors).</li> </ul>"},{"location":"reference/graph-task-graph/#indexed-finders","title":"Indexed finders","text":"<pre><code>get_by_alias(alias: str) -&gt; str\nfind_by_label(label: str) -&gt; list[str]\nfind_by_logic(logic_prefix: str, *, first: bool = False) -&gt; list[str] | str | None\nfind_by_display(name_prefix: str, *, first: bool = False) -&gt; list[str] | str | None\n</code></pre> <p>These use metadata created at build time (e.g., via <code>call_tool(..., _alias=..., _labels=[...], _name=...)</code>).</p> <ul> <li><code>get_by_alias(\"sum1\")</code> \u2192 node id for alias <code>sum1</code> or <code>KeyError</code>.</li> <li><code>find_by_label(\"critical\")</code> \u2192 all node ids tagged with that label.</li> <li><code>find_by_logic(\"tool_name\")</code> \u2192 nodes whose logic name equals or starts with <code>tool_name</code>.</li> <li><code>find_by_display(\"My Step\")</code> \u2192 nodes whose display name equals or starts with <code>\"My Step\"</code>.</li> </ul>"},{"location":"reference/graph-task-graph/#unified-selector-dsl","title":"Unified selector DSL","text":"<pre><code>select(selector: str, *, first: bool = False) -&gt; str | list[str] | None\npick_one(selector: str) -&gt; str\npick_all(selector: str) -&gt; list[str]\n</code></pre> <p>Selector mini\u2011DSL:</p> <ul> <li><code>\"@alias\"</code> \u2192 by alias.</li> <li><code>\"#label\"</code> \u2192 by label (may return many).</li> <li><code>\"id:&lt;id&gt;\"</code> \u2192 exact id.</li> <li><code>\"logic:&lt;prefix&gt;\"</code> \u2192 logic name prefix.</li> <li><code>\"name:&lt;prefix&gt;\"</code> \u2192 display name prefix.</li> <li><code>\"/regex/\"</code> \u2192 regex on <code>node_id</code>.</li> <li>anything else \u2192 prefix match on <code>node_id</code>.</li> </ul> Selector examples <pre><code># Single node by alias\ntarget_id = graph.pick_one(\"@sum1\")\n\n# All nodes with a label\ncritical_ids = graph.pick_all(\"#critical\")\n\n# First node whose logic name starts with \"normalize_\"\nnid = graph.select(\"logic:normalize_\", first=True)\n\n# Regex on node id\ndebug_nodes = graph.pick_all(\"/debug_.*/\")\n</code></pre> <p>Use selectors when building debug tooling, partial resets, or visualization filters.</p>"},{"location":"reference/graph-task-graph/#3-readonly-views","title":"3. Read\u2011only Views","text":"<pre><code>view(self) -&gt; GraphView\nlist_nodes(self, exclude_internal: bool = True) -&gt; list[str]\n</code></pre> <ul> <li> <p><code>view()</code> \u2013 returns a <code>GraphView</code> with:</p> </li> <li> <p><code>graph_id</code>,</p> </li> <li><code>nodes</code> (specs),</li> <li><code>node_status</code> (derived map: node id \u2192 <code>NodeStatus</code>),</li> <li><code>metadata</code>.</li> <li><code>list_nodes(exclude_internal=True)</code> \u2013 list node ids, optionally excluding internal nodes (ids starting with <code>_</code>).</li> </ul> Inspecting a graph view <pre><code>v = graph.view()\nprint(v.graph_id)\nprint(v.node_status)  # {\"node_1\": NodeStatus.DONE, ...}\n</code></pre> <p><code>GraphView</code> is a snapshot for inspection / APIs; it does not expose mutation methods.</p>"},{"location":"reference/graph-task-graph/#4-graph-mutation-patches","title":"4. Graph Mutation (Patches)","text":"<p>Dynamic graph edits are represented as patches in <code>TaskGraphState</code>.</p> <pre><code>patch_add_or_replace_node(node_spec: dict[str, Any]) -&gt; None\npatch_remove_node(node_id: str) -&gt; None\npatch_add_dependency(node_id: str, dependency_id: str) -&gt; None\n</code></pre> <ul> <li><code>patch_add_or_replace_node</code> \u2013 add a new node or replace an existing one (payload is a plain dict convertible to <code>TaskNodeSpec</code>).</li> <li><code>patch_remove_node</code> \u2013 remove a node by id.</li> <li><code>patch_add_dependency</code> \u2013 add a new dependency edge.</li> </ul> <p>Each method:</p> <ul> <li>appends a <code>GraphPatch</code> entry to <code>state.patches</code> and increments <code>state.rev</code>,</li> <li>notifies observers via <code>on_patch_applied</code>,</li> <li>rebuilds <code>_runtime_nodes</code> for the effective view.</li> </ul> <p>These APIs are intended for advanced dynamic graph editing and patch flows; many users won\u2019t need them directly.</p>"},{"location":"reference/graph-task-graph/#5-topology-subgraphs","title":"5. Topology &amp; Subgraphs","text":"<pre><code>dependents(node_id: str) -&gt; list[str]\ntopological_order() -&gt; list[str]\nget_subgraph_nodes(start_node_id: str) -&gt; list[str]\nget_upstream_nodes(start_node_id: str) -&gt; list[str]\n</code></pre> <ul> <li><code>dependents(nid)</code> \u2013 all nodes that list <code>nid</code> as a dependency.</li> <li><code>topological_order()</code> \u2013 a topological sort of all nodes (raises on cycles).</li> <li><code>get_subgraph_nodes(start)</code> \u2013 <code>start</code> plus all nodes reachable downstream (dependents).</li> <li><code>get_upstream_nodes(start)</code> \u2013 <code>start</code> plus all nodes it depends on (upstream).</li> </ul> Working with subgraphs <pre><code># All nodes that can be affected if you change `node_a`\nforward = graph.get_subgraph_nodes(\"node_a\")\n\n# All nodes that must run before `node_b`\nupstream = graph.get_upstream_nodes(\"node_b\")\n</code></pre> <p>These helpers are typically used for partial reset, impact analysis, or visualization filters.</p>"},{"location":"reference/graph-task-graph/#6-state-mutation-reset","title":"6. State Mutation &amp; Reset","text":"<pre><code>async def set_node_status(self, node_id: str, status: NodeStatus) -&gt; None\nasync def set_node_outputs(self, node_id: str, outputs: dict[str, Any]) -&gt; None\n\nasync def reset_node(self, node_id: str, *, preserve_outputs: bool = False)\nasync def reset(\n    self,\n    node_ids: list[str] | None = None,\n    *,\n    recursive: bool = True,\n    direction: str = \"forward\",\n    preserve_outputs: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <ul> <li><code>set_node_status</code> \u2013 update a node\u2019s status and notify observers (<code>on_node_status_change</code>).</li> <li><code>set_node_outputs</code> \u2013 update a node\u2019s outputs and notify observers (<code>on_node_output_change</code>).</li> <li><code>reset_node</code> \u2013 reset a single node to <code>PENDING</code>, optionally keeping outputs.</li> <li> <p><code>reset</code> \u2013 reset all or part of the graph:</p> </li> <li> <p><code>node_ids=None</code> \u2192 reset all nodes (except the synthetic inputs node).</p> </li> <li><code>recursive=True, direction=\"forward\"</code> \u2192 also reset all dependents.</li> <li><code>recursive=True, direction=\"backward\"</code> \u2192 reset upstream dependencies.</li> </ul> Partial reset patterns <pre><code># Reset a node and everything that depends on it\nawait graph.reset(node_ids=[\"step_3\"], recursive=True, direction=\"forward\")\n\n# Reset only a single node, keeping its outputs\nawait graph.reset(node_ids=[\"step_3\"], recursive=False, preserve_outputs=True)\n\n# Reset entire graph (except inputs)\nawait graph.reset(node_ids=None)\n</code></pre> <p>These methods are used by runners / UIs to implement retry, rerun from here, and what-if operations.</p>"},{"location":"reference/graph-task-graph/#7-io-definition-binding","title":"7. IO Definition &amp; Binding","text":""},{"location":"reference/graph-task-graph/#io-apis","title":"IO APIs","text":"<pre><code>declare_inputs(\n    *,\n    required: Iterable[str] | None = None,\n    optional: dict[str, Any] | None = None,\n) -&gt; None\n\nexpose(name: str, value: Ref | Any) -&gt; None\nrequire_outputs(*names: str) -&gt; None\n\nio_signature(include_values: bool = False) -&gt; dict[str, Any]\n</code></pre> <ul> <li> <p><code>declare_inputs(...)</code> \u2013 declares graph-level inputs:</p> </li> <li> <p><code>required</code> \u2013 names that must be provided when binding inputs.</p> </li> <li><code>optional</code> \u2013 names with default values (modeled via <code>ParamSpec</code>).</li> <li> <p><code>expose(name, value)</code> \u2013 declare a graph output:</p> </li> <li> <p><code>value</code> can be a Ref (to node outputs or inputs) or a literal.</p> </li> <li><code>require_outputs(...)</code> \u2013 sanity check for required outputs (uses internal <code>_io_outputs</code>).</li> <li> <p><code>io_signature(include_values=False)</code> \u2013 summarized IO description:</p> </li> <li> <p><code>inputs.required</code> / <code>inputs.optional</code> (names and defaults).</p> </li> <li><code>outputs.keys</code> \u2013 names of exposed outputs.</li> <li><code>outputs.bindings</code> \u2013 raw bindings (Refs or literals).</li> <li><code>outputs.values</code> \u2013 optional resolved values (when <code>include_values=True</code>).</li> </ul> <p>Binding of actual input values happens via the runner, which calls the internal <code>_validate_and_bind_inputs(...)</code> helper.</p> Inspect IO signature <pre><code>sig = graph.io_signature()\nprint(sig[\"inputs\"][\"required\"])\nprint(sig[\"outputs\"][\"keys\"])\n\n# After a run, you can inspect resolved output values\nfull = graph.io_signature(include_values=True)\nprint(full[\"outputs\"][\"values\"])\n</code></pre> <p>The IO signature is useful for APIs, UIs, and tooling that needs to describe how to call a graph without inspecting internals.</p>"},{"location":"reference/graph-task-graph/#8-observers-notifications","title":"8. Observers &amp; Notifications","text":"<pre><code>add_observer(observer: Any) -&gt; None\n</code></pre> <p>Observers are objects that can implement any of the following methods:</p> <ul> <li><code>on_node_status_change(runtime_node)</code></li> <li><code>on_node_output_change(runtime_node)</code></li> <li><code>on_inputs_bound(graph)</code></li> <li><code>on_patch_applied(graph, patch)</code></li> </ul> <p>They are invoked whenever the corresponding events occur.</p> Lightweight observer usage <pre><code>class PrintObserver:\n    def on_node_status_change(self, node):\n        print(\"status\", node.node_id, node.state.status)\n\ngraph.add_observer(PrintObserver())\n</code></pre> <p>Observers are the main extension point for logging, metrics, and live UI updates.</p>"},{"location":"reference/graph-task-graph/#9-diff-persistence-helpers","title":"9. Diff &amp; Persistence Helpers","text":""},{"location":"reference/graph-task-graph/#diffing","title":"Diffing","text":"<pre><code>diff(other: TaskGraph) -&gt; dict[str, Any]\n</code></pre> <ul> <li>Compare two graphs with the same <code>graph_id</code>.</li> <li> <p>Returns a dict with:</p> </li> <li> <p><code>\"added\"</code>: list of node ids present only in <code>other</code>.</p> </li> <li><code>\"removed\"</code>: list of node ids present only in <code>self</code>.</li> <li><code>\"modified\"</code>: node ids whose dependencies or metadata differ.</li> </ul> Basic diff usage <pre><code>d = graph_v2.diff(graph_v1)\nprint(\"added\", d[\"added\"])\nprint(\"modified\", d[\"modified\"])\n</code></pre> <p>Useful for visualizing evolution, reviewing patches, or migration tooling.</p>"},{"location":"reference/graph-task-graph/#spec-serialization","title":"Spec serialization","text":"<pre><code>spec_json(self) -&gt; dict[str, Any]\n</code></pre> <ul> <li>Returns a JSON\u2011safe representation of the spec (<code>TaskGraphSpec</code>) using <code>_dataclass_to_plain</code>.</li> <li>Storage/layout is left to callers (file, DB, etc.).</li> </ul>"},{"location":"reference/graph-task-graph/#10-debug-visualization","title":"10. Debug &amp; Visualization","text":""},{"location":"reference/graph-task-graph/#humanreadable-summary","title":"Human\u2011readable summary","text":"<pre><code>pretty(self, *, max_nodes: int = 20, max_width: int = 100) -&gt; str\n__str__(self) -&gt; str\n</code></pre> <ul> <li> <p><code>pretty(...)</code> \u2013 a compact, human\u2011friendly summary including:</p> </li> <li> <p>graph id, node count, observer count;</p> </li> <li>IO signature summary;</li> <li>state summary;</li> <li>a small table of nodes with id, type, status, dependencies count, and logic.</li> <li><code>__str__</code> \u2013 uses <code>pretty(max_nodes=12, max_width=96)</code> for <code>print(graph)</code>.</li> </ul> Quick debug print <pre><code>print(graph)          # uses __str__\nprint(graph.pretty()) # full summary\n</code></pre> <p>This is the fastest way to get an overview of a graph in a REPL or log.</p>"},{"location":"reference/graph-task-graph/#visualization-helpers","title":"Visualization helpers","text":"<p>At the bottom of the module, these are attached as methods:</p> <pre><code>TaskGraph.to_dot = to_dot\nTaskGraph.visualize = visualize\nTaskGraph.ascii_overview = ascii_overview\n</code></pre> <ul> <li><code>graph.to_dot(...)</code> \u2013 export a DOT representation.</li> <li><code>graph.visualize(...)</code> \u2013 high\u2011level helper for rich visualizations (see Visualization docs).</li> <li><code>graph.ascii_overview(...)</code> \u2013 ASCII summary for terminals / logs.</li> </ul> High\u2011level usage (shape only) <pre><code>dot_str = graph.to_dot()\nprint(graph.ascii_overview())\n# graph.visualize(...)  # see visualization docs for options\n</code></pre> <p>Exact options and rendering backends are described on the Visualization page.</p>"},{"location":"reference/graph-task-graph/#11-summary","title":"11. Summary","text":"<ul> <li><code>TaskGraph</code> ties together spec, state, and runtime node table.</li> <li>Use <code>new_run</code> / <code>from_spec</code> to construct graphs; use selectors (<code>pick_one</code>, <code>pick_all</code>) to locate nodes.</li> <li>IO is declared via <code>declare_inputs</code> / <code>expose</code> and inspected via <code>io_signature</code>.</li> <li>Topology helpers (<code>dependents</code>, <code>get_subgraph_nodes</code>, <code>get_upstream_nodes</code>) support partial reset and analysis.</li> <li>State mutation APIs (<code>set_node_status</code>, <code>set_node_outputs</code>, <code>reset</code>) underpin runners and interactive tooling.</li> <li>Patches, diff, observers, and visualization helpers are advanced tools for dynamic graphs, UIs, and diagnostics.</li> </ul>"},{"location":"reference/graph-tool/","title":"<code>@tool</code> \u2013 Dual\u2011mode Tool Decorator","text":"<p>The <code>@tool</code> decorator turns a plain Python callable into an AetherGraph tool:</p> <ul> <li>Immediate mode (no active graph): calling the decorated function executes it directly and returns a dict of outputs (or an awaitable for async tools).</li> <li>Graph mode (inside <code>@graph_fn</code> / <code>@graphify</code>): calling the decorated function builds a node and returns a <code>NodeHandle</code>; nothing is executed yet.</li> <li>Registry integration: when a registry is active, the implementation is automatically registered under the <code>tool</code> namespace.</li> <li>Context injection: if the tool signature includes a <code>context</code> parameter, a <code>NodeContext</code> is injected automatically at run time when the tool runs as a node in a graph.</li> </ul>"},{"location":"reference/graph-tool/#signature","title":"Signature","text":"<pre><code>@tool(\n    outputs: list[str],\n    inputs: list[str] | None = None,\n    *,\n    name: str | None = None,\n    version: str = \"0.1.0\",\n)\n</code></pre>"},{"location":"reference/graph-tool/#required-vs-optional","title":"Required vs optional","text":"Parameter Type Required? Description <code>outputs</code> <code>list[str]</code> Yes Declares the named outputs produced by the tool. Every call must return a dict containing exactly these keys. <code>inputs</code> <code>list[str] \\| None</code> No Optional explicit input names. If <code>None</code>, they are inferred from the implementation\u2019s signature (excluding <code>*args</code>, <code>**kwargs</code>). <code>name</code> <code>str \\| None</code> (keyword\u2011only) No Optional registry / UI name. Defaults to the underlying implementation\u2019s <code>__name__</code>. <code>version</code> <code>str</code> (keyword\u2011only) No Semantic version used for registry and provenance. Defaults to <code>\"0.1.0\"</code>. <p>Notes</p> <ul> <li><code>outputs</code> is always required and defines the contract enforced at runtime.</li> <li><code>inputs</code> is usually optional \u2013 in most cases, you can let AetherGraph infer it from the function\u2019s parameters.</li> <li><code>name</code> and <code>version</code> matter when you later want to look up tools in a registry or inspect runs.</li> </ul>"},{"location":"reference/graph-tool/#behavior-overview","title":"Behavior Overview","text":"<ul> <li> <p>Sync vs async</p> </li> <li> <p>If the implementation is synchronous, the decorated function returns a dict in immediate mode.</p> </li> <li> <p>If the implementation is async, the decorated function returns an awaitable in immediate mode.</p> </li> <li> <p>Graph vs immediate mode</p> </li> <li> <p>When a graph builder is active (<code>current_builder() is not None</code>), calling the tool returns a <code>NodeHandle</code> (graph node), not data.</p> </li> <li> <p>When no builder is active, calling the tool executes the implementation and returns results.</p> </li> <li> <p>Context injection</p> </li> <li> <p>If the tool\u2019s signature includes a parameter named <code>context</code>, it is treated as a reserved injectable rather than a normal data input.</p> </li> <li>You do not list <code>\"context\"</code> in the <code>inputs</code> list; <code>inputs</code> is for data\u2011flow arguments only.</li> <li>When the tool runs as a node in a graph (via <code>@graph_fn</code> or <code>graphify</code>), AetherGraph automatically injects a <code>NodeContext</code> instance for that node. Callers do not pass it manually when running the graph.</li> </ul>"},{"location":"reference/graph-tool/#runtime-contracts","title":"Runtime Contracts","text":"<p>The decorator enforces a strict I/O contract for every tool call:</p> <ul> <li> <p>The implementation must return either:</p> </li> <li> <p>a <code>dict</code> with exactly the declared <code>outputs</code> keys, or</p> </li> <li> <p>a value that <code>_normalize_result</code> can convert into such a dict.</p> </li> <li> <p><code>_check_contract(outputs, out, impl)</code> validates that:</p> </li> <li> <p>all <code>outputs</code> keys are present,</p> </li> <li>no unexpected keys are produced (where applicable).</li> </ul> <p>On violation, the runtime raises a clear error pointing at the original implementation.</p>"},{"location":"reference/graph-tool/#contextaware-tools","title":"Context\u2011aware Tools","text":"<p>To use runtime services inside a tool (channel, memory, artifacts, logger, etc.), add a <code>context</code> parameter:</p> <pre><code>from aethergraph import tool, NodeContext\n\n@tool(outputs=[\"y\"], inputs=[\"x\"])\nasync def double_with_log(x: int, *, context: NodeContext) -&gt; dict:\n    # `context` is injected automatically when this tool runs as a graph node\n    context.logger().info(\"doubling\", extra={\"x\": x})\n    return {\"y\": x * 2}\n</code></pre> <p>Contracts:</p> <ul> <li><code>context</code> is not part of the data\u2011flow; it never becomes an edge in the graph.</li> <li>In graph mode, callers simply write <code>double_with_log(x=42)</code> in a <code>@graph_fn</code> / <code>graphify</code> body; the runner injects <code>NodeContext</code> when executing the node.</li> <li>In immediate mode (outside any graph), you can optionally pass a <code>context</code> argument manually if you want to test the tool with a synthetic context, but typical usage is inside graphs.</li> </ul>"},{"location":"reference/graph-tool/#usage-patterns","title":"Usage Patterns","text":"<p>Below are common ways to define and call tools. These examples focus on shape and contracts; see the main Graph docs for deeper patterns.</p> 1. Simple synchronous tool <pre><code>from aethergraph import tool\n\n@tool(outputs=[\"y\"])\ndef double(x: int) -&gt; dict:\n    return {\"y\": x * 2}\n\n# Immediate mode (no active graph)\nresult = double(x=21)\nassert result == {\"y\": 42}\n</code></pre> <p>Key points</p> <ul> <li><code>outputs=[\"y\"]</code> is required.</li> <li><code>inputs</code> is omitted \u2192 inferred as <code>[\"x\"]</code> from the function signature.</li> <li>In immediate mode, <code>double(...)</code> runs immediately and returns a dict.</li> </ul> 2. Async tool <pre><code>from aethergraph import tool\n\n@tool(outputs=[\"text\"])\nasync def fetch_text(url: str) -&gt; dict:\n    data = await some_async_http_get(url)\n    return {\"text\": data}\n\n# Immediate mode\nresult = await fetch_text(url=\"https://example.com\")\nprint(result[\"text\"])\n</code></pre> <p>Key points</p> <ul> <li>Implementation is async \u2192 in immediate mode, calling <code>fetch_text(...)</code> returns an awaitable.</li> <li><code>inputs</code> again inferred from the signature (<code>[\"url\"]</code>).</li> </ul> 3. Explicit inputs (with non\u2011data params) <pre><code>from aethergraph import tool, NodeContext\n\n@tool(outputs=[\"out\"], inputs=[\"a\", \"b\"])\nasync def add(a: int, b: int = 0, *, scale: int = 1, context: NodeContext | None = None) -&gt; dict:\n    # `a`, `b` are data inputs; `scale` and `context` are not graph edges\n    if context is not None:\n        context.logger().info(\"adding\", extra={\"a\": a, \"b\": b, \"scale\": scale})\n    return {\"out\": (a + b) * scale}\n</code></pre> <p>Key points</p> <ul> <li><code>inputs=[\"a\", \"b\"]</code> means only <code>a</code> and <code>b</code> are considered data\u2011inputs from upstream nodes.</li> <li> <p><code>scale</code> and <code>context</code> are non\u2011data parameters:</p> </li> <li> <p><code>context</code> is a reserved injectable (runtime will supply it in graph mode).</p> </li> <li><code>scale</code> can be passed as a literal or configured inside the graph; it does not appear as an edge unless you model it explicitly.</li> </ul> 4. Graph mode \u2013 building nodes <pre><code>from aethergraph import graph_fn, tool, NodeContext\n\n@tool(outputs=[\"y\"])\ndef double(x: int):\n    return {\"y\": x * 2}\n\n@tool(outputs=[\"y\"], inputs=[\"x\"])\nasync def double_with_log(x: int, *, context: NodeContext) -&gt; dict:\n    context.logger().info(\"doubling\", extra={\"x\": x})\n    return {\"y\": x * 2}\n\n@graph_fn(name=\"pipeline\", outputs=[\"z\"])\nasync def pipeline(*, context: NodeContext):\n    # Inside a graph_fn, calling tools builds nodes\n    n1 = double(x=21)                 # NodeHandle, not the numeric 42\n    n2 = double_with_log(x=n1.y)      # NodeHandle with NodeContext injection at run time\n    return {\"z\": n2.y}\n</code></pre> <p>Key points</p> <ul> <li>Inside <code>@graph_fn</code>, both <code>double(...)</code> and <code>double_with_log(...)</code> do not run immediately.</li> <li>Instead, <code>NodeHandle</code>s are returned and the builder records tool nodes and their dependencies.</li> <li> <p>When the graph is executed:</p> </li> <li> <p>The runtime calls the underlying implementation with concrete values for data inputs.</p> </li> <li>If a <code>context</code> parameter is present, a <code>NodeContext</code> instance is injected automatically for that node.</li> </ul>"},{"location":"reference/graph-tool/#registry-behavior-advanced","title":"Registry Behavior (Advanced)","text":"<p>When a registry is active (<code>current_registry() is not None</code>), the decorator automatically registers the underlying implementation:</p> <pre><code>registry.register(\n    nspace=\"tool\",\n    name=name or impl.__name__,\n    version=version,\n    obj=impl,\n)\n</code></pre> <p>This enables:</p> <ul> <li>Discovery \u2013 listing available tools by <code>(name, version)</code>.</li> <li>Provenance \u2013 runs can record which version of which tool was used.</li> <li>Hot reload / development \u2013 registries can swap implementations without changing graph code.</li> </ul> <p>If no registry is active, the decorator still works normally; registration is simply skipped.</p>"},{"location":"reference/graph-tool/#summary","title":"Summary","text":"<ul> <li>Use <code>@tool(outputs=[...])</code> on any function to make it part of the graph runtime.</li> <li>Required: <code>outputs</code> \u2013 define the contract.</li> <li>Optional: <code>inputs</code>, <code>name</code>, <code>version</code> \u2013 control graph wiring and registry metadata.</li> <li>Immediate calls return concrete data (or awaitables); calls inside graphs create nodes and wire dependencies.</li> <li> <p>To use runtime services inside a tool, add a <code>context</code> parameter:</p> </li> <li> <p>It is automatically injected as <code>NodeContext</code> when the tool runs as a node in a graph.</p> </li> <li>Callers never pass <code>context</code> manually when running <code>@graph_fn</code> or <code>graphify</code> graphs; the runner does it for them.</li> </ul>"},{"location":"reference/graphify-graphfn/","title":"<code>@graphify</code> and <code>graph_fn</code> \u2014 Convert Python Functions to Graphs","text":""},{"location":"reference/graphify-graphfn/#introduction","title":"Introduction","text":"<p>This document explains how to use the <code>@graphify</code> decorator and the <code>graph_fn</code> function to convert Python functions into executable graphs within the Aethergraph framework. These tools enable you to define, orchestrate, and manage complex workflows as graphs, making it easier to build modular agents and applications. You will also learn how to provide metadata for agents and apps, and how these components interact with the Aethergraph UI.</p>"},{"location":"reference/graphify-graphfn/#1-core-apis","title":"1. Core APIs","text":"<p>The <code>@graphify</code> decorator defines a <code>TaskGraph</code>, where each node must be annotated with <code>@tool</code>. When a <code>TaskGraph</code> is triggered, all execution traces are preserved, and the graph can be resumed automatically if a <code>run_id</code> is provided. Note that direct invocation of subgraphs or nested graphs within a <code>TaskGraph</code> is not supported; instead, use <code>context.spawn_run(...)</code> or other context runner methods to launch nested graphs.</p> <p>In contrast, <code>@graph_fn</code> offers a more intuitive way to define a graph from a standard Python function. It converts a Python function\u2014typically one that accepts a <code>context</code> parameter\u2014into an executable graph. You can run the resulting graph as you would any async Python function, and you can nest subgraphs using <code>run</code>, <code>async_run</code>, or <code>context.spawn_run(...)</code>. However, be aware that concurrency limits may not be enforced if you invoke the graph directly using native Python calls (e.g., <code>await my_fn(...)</code>).</p> @graphify(name, inputs, outputs, version, ... ) <p>Decorator to define a <code>TaskGraph</code> and optionally register it as an agent or app.</p> <p>This decorator wraps a Python function as a <code>TaskGraph</code>, enabling it to be executed as a node-based graph with runtime context, retry policy, and concurrency controls. It also supports rich metadata registration for agent and app discovery.</p> <p>Examples:</p> <p>Basic usage: <pre><code>@graphify(\n    name=\"add_numbers\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"sum\"],\n)\nasync def add_numbers(a: int, b: int):\n    return {\"sum\": a + b}\n</code></pre></p> <p>Registering as an agent with metadata: <pre><code>@graphify(\n    name=\"chat_agent\",\n    inputs=[\"message\", \"files\", \"context_refs\", \"session_id\", \"user_meta\"],\n    outputs=[\"response\"],\n    as_agent={\n        \"id\": \"chatbot\",\n        \"title\": \"Chat Agent\",\n        \"description\": \"Conversational AI agent.\",\n        \"mode\": \"chat_v1\",\n        \"icon\": \"chat\",\n        \"tags\": [\"chat\", \"nlp\"],\n    },\n)\nasync def chat_agent(...):\n    ...\n</code></pre></p> <p>Registering as an app: <pre><code>@graphify(\n    name=\"summarizer\",\n    inputs=[],\n    outputs=[\"summary\"],\n    as_app={\n        \"id\": \"summarizer-app\",\n        \"name\": \"Text Summarizer\",\n        \"description\": \"Summarizes input text.\",\n        \"category\": \"Productivity\",\n        \"tags\": [\"nlp\", \"summary\"],\n    },\n)\nasync def summarizer():\n    ...\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Unique name for the graph function.</p> <code>'default_graph'</code> <code>inputs</code> <p>List of input parameter names. If <code>as_agent</code> is provided with <code>mode=\"chat_v1\"</code>, this must match <code>[\"message\", \"files\", \"context_refs\", \"session_id\", \"user_meta\"]</code>.</p> <code>()</code> <code>outputs</code> <p>List of output keys returned by the function.</p> <code>None</code> <code>version</code> <p>Version string for the graph function (default: \"0.1.0\").</p> <code>'0.1.0'</code> <code>entrypoint</code> <code>bool</code> <p>If True, marks this graph as the main entrypoint for a flow.  [Currently unused]</p> <code>False</code> <code>flow_id</code> <code>str | None</code> <p>Optional flow identifier for grouping related graphs.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>List of string tags for discovery and categorization.</p> <code>None</code> <code>as_agent</code> <code>dict[str, Any] | None</code> <p>Optional dictionary defining agent metadata. Used when running through Aethergraph UI. See additional information below.</p> <code>None</code> <code>as_app</code> <code>dict[str, Any] | None</code> <p>Optional dictionary defining app metadata. Used when running through Aethergraph UI. See additional information below.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TaskGraph</code> <p>A decorator that transforms a function into a TaskGraph with the specified configuration.</p> Notes <ul> <li>as_agent and as_app are not needed to define a graph; they are only for registration purposes for use in Aethergraph UI.</li> <li>When registering as an agent, the <code>as_agent</code> dictionary should include at least an \"id\" key.</li> <li>When registering as an app, the <code>as_app</code> dictionary should include at least an \"id\" key.</li> <li>The decorated function is a sync function (generate the TaskGraph), despite the underlying <code>@tool</code> can be async.</li> <li>Fields <code>inputs</code> and <code>outputs</code> are can be inferred from the function signature if not explicitly provided, but it's recommended to declare them for clarity.</li> </ul> @graph_fn(name, inputs, outputs, version, *, ...) <p>Decorator to define a graph function and optionally register it as an agent or app.</p> <p>This decorator wraps a Python function as a <code>GraphFunction</code>, enabling it to be executed as a node-based graph with runtime context, retry policy, and concurrency controls. It also supports rich metadata registration for agent and app discovery.</p> <p>Examples:</p> <p>Basic usage: <pre><code>@graph_fn(\n    name=\"add_numbers\",\n    inputs=[\"a\", \"b\"],\n    outputs=[\"sum\"],\n)\nasync def add_numbers(a: int, b: int):\n    return {\"sum\": a + b}\n</code></pre></p> <p>Registering as an agent with metadata: <pre><code>@graph_fn(\n    name=\"chat_agent\",\n    inputs=[\"message\", \"files\", \"context_refs\", \"session_id\", \"user_meta\"],\n    outputs=[\"response\"],\n    as_agent={\n        \"id\": \"chatbot\",\n        \"title\": \"Chat Agent\",\n        \"description\": \"Conversational AI agent.\",\n        \"mode\": \"chat_v1\",\n        \"icon\": \"chat\",\n        \"tags\": [\"chat\", \"nlp\"],\n    },\n)\nasync def chat_agent(...):\n    ...\n</code></pre></p> <p>Registering as an app: <pre><code>@graph_fn(\n    name=\"summarizer\",\n    inputs=[],\n    outputs=[\"summary\"],\n    as_app={\n        \"id\": \"summarizer-app\",\n        \"name\": \"Text Summarizer\",\n        \"description\": \"Summarizes input text.\",\n        \"category\": \"Productivity\",\n        \"tags\": [\"nlp\", \"summary\"],\n    },\n)\nasync def summarizer():\n    ...\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name for the graph function.</p> required <code>inputs</code> <code>list[str] | None</code> <p>List of input parameter names. If <code>as_agent</code> is provided with <code>mode=\"chat_v1\"</code>, this must match <code>[\"message\", \"files\", \"context_refs\", \"session_id\", \"user_meta\"]</code>.</p> <code>None</code> <code>outputs</code> <code>list[str] | None</code> <p>List of output keys returned by the function.</p> <code>None</code> <code>version</code> <code>str</code> <p>Version string for the graph function (default: \"0.1.0\").</p> <code>'0.1.0'</code> <code>entrypoint</code> <code>bool</code> <p>If True, marks this graph as the main entrypoint for a flow.  [Currently unused]</p> <code>False</code> <code>flow_id</code> <code>str | None</code> <p>Optional flow identifier for grouping related graphs.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>List of string tags for discovery and categorization.</p> <code>None</code> <code>as_agent</code> <code>dict[str, Any] | None</code> <p>Optional dictionary defining agent metadata. Used when running through Aethergraph UI. See additional information below.</p> <code>None</code> <code>as_app</code> <code>dict[str, Any] | None</code> <p>Optional dictionary defining app metadata. Used when running through Aethergraph UI. See additional information below.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[[Callable], GraphFunction]</code> <p>A decorator that wraps the function as a <code>GraphFunction</code> and registers it</p> <code>Callable[[Callable], GraphFunction]</code> <p>in the runtime registry, with agent/app metadata if provided.</p> Notes <ul> <li>as_agent and as_app are not needed to define a graph; they are only for registration purposes for use in Aethergraph UI.</li> <li>When registering as an agent, the <code>as_agent</code> dictionary should include at least an \"id\" key.</li> <li>When registering as an app, the <code>as_app</code> dictionary should include at least an \"id\" key.</li> <li>The decorated function can be either synchronous or asynchronous.</li> <li>Fields <code>inputs</code> and <code>outputs</code> are can be inferred from the function signature if not explicitly provided, but it's recommended to declare them for clarity.</li> </ul>"},{"location":"reference/graphify-graphfn/#2-agentapp-metadata","title":"2. Agent/App Metadata","text":"<p>When using agents with Aethergraph, you must declare the entry graph as either an <code>agent</code> or an <code>app</code>.</p> <p>An agent is triggered by a Workspace message and requires the following input signature:</p> <pre><code>@graph_fn(\n    name=\"my_agent\",\n    inputs=[\"message\", \"files\", \"context_refs\", \"session_id\", \"user_meta\"],\n    # outputs can be arbitrary\n    as_agent={\n        id=\"...\",\n        title=\"My Agent\",         # Displayed in the Avatar\n        mode=\"chat_v1\",           # Specifies as a session workspace agent\n        run_visibility=\"inline\",  # \"inline\" hides run status\n        memory_level=\"session\",   # \"user\" | \"session\" | \"run\", etc.\n    }\n)\n</code></pre> <p>You do not need to start the agent manually using <code>run</code> or <code>run_async</code>; the UI will automatically trigger the agent with the following default attributes:</p> <ul> <li><code>message</code>: The message sent from the UI.</li> <li><code>files</code>: Attached files from the UI, parsed as <code>FileRef</code>.</li> <li><code>context_refs</code>: Artifact links from the UI, typically a list of dicts with <code>artifact_id</code>.</li> <li><code>session_id</code>: Convenient access to the session ID (same as <code>context.session_id</code>).</li> <li><code>user_meta</code>: Currently not in use.</li> </ul> <p>It is recommended to use <code>graph_fn</code> for agents to enable dynamic routing and orchestration. For better UI responsiveness, use <code>context.spawn_run(...)</code> to launch other agents or graphs. You do not need to invoke <code>app</code> graphs from within agents.</p> <p>An app is a reusable graph listed in the Aethergraph UI. Apps are typically single graphs without direct inputs. If you need to collect inputs, use <code>context.ui_run_channel().ask_text(...)</code> or other <code>ask_*</code> methods to prompt for arguments.</p> <p>It is recommended to use <code>graphify</code> for apps. The <code>graphify</code> decorator allows all node execution statuses to be displayed in the UI, and the app can be terminated manually.</p> <code>as_agent</code> meta keys <p>               Bases: <code>TypedDict</code></p> <p>Configuration metadata for an agent. Register an agent with <code>as_agent</code> parameter in <code>@graphify</code> or <code>@graph_fn</code>.</p> <p>All fields are optional except <code>id</code> in practice; anything omitted gets reasonable defaults in build_agent_meta.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the agent. Defaults to graph name.</p> <code>title</code> <code>str</code> <p>Display name of the agent. Optional, shown in the UI.</p> <code>description</code> <code>str</code> <p>Brief description of the agent. Optional, shown in the UI.</p> <code>short_description</code> <code>str</code> <p>Shorter summary (used in cards). Optional.</p> <code>icon_key</code> <code>str</code> <p>Icon key used in the UI (e.g. \"message-circle\").</p> <code>color</code> <code>str</code> <p>Accent color token (e.g. \"emerald\").</p> <code>badge</code> <code>str</code> <p>Badge label, e.g. \"Chat Agent\".</p> <code>category</code> <code>str</code> <p>Category, e.g. \"Core\", \"R&amp;D Lab\", \"Infra\", \"Productivity\".</p> <code>status</code> <code>str</code> <p>\"available\" | \"coming-soon\" | \"hidden\" | \"error\" | ...</p> <code>mode</code> <code>str</code> <p>Operational mode. Defaults to \"chat_v1\" for chat agents.</p> <code>session_kind</code> <code>str</code> <p>Session type, e.g. \"chat\". Defaults to \"chat\".</p> <code>flow_id</code> <code>str</code> <p>Flow identifier for wiring. Defaults to graph name.</p> <code>tags</code> <code>list[str]</code> <p>Tags used for search / grouping.</p> <code>tool_graphs</code> <code>list[str]</code> <p>Related tool graph identifiers.</p> <code>features</code> <code>list[str]</code> <p>Optional feature bullets for UI.</p> <code>run_visibility</code> <code>RunVisibility</code> <p>\"normal\" | \"inline\" | ...</p> <code>run_importance</code> <code>RunImportance</code> <p>\"normal\" | \"high\" | ...</p> <code>memory_level</code> <code>Literal['user', 'session', 'run']</code> <p>Memory scope level.</p> <code>memory_scope</code> <code>str</code> <p>Logical scope, e.g. \"session.global\", \"user.all\".</p> <code>github_url</code> <code>str</code> <p>Optional GitHub link.</p> <code>as_app</code> meta keys <p>               Bases: <code>TypedDict</code></p> <p>Configuration metadata for an application. Register an app with <code>as_app</code> parameter in <code>@graphify</code> or <code>@graph_fn</code>.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the app. Defaults to graph name.</p> <code>name</code> <code>str</code> <p>Human-readable name of the app. Defaults to \"App for \". <code>badge</code> <code>str</code> <p>Short badge or label for the app. Optional, shown in the UI.</p> <code>short_description</code> <code>str</code> <p>Brief summary of the app's purpose. Optional.</p> <code>description</code> <code>str</code> <p>Detailed description of the app. Optional.</p> <code>category</code> <code>str</code> <p>Category, e.g. \"Core\", \"R&amp;D Lab\", \"Infra\", \"Productivity\".</p> <code>status</code> <code>str</code> <p>\"available\" | \"coming-soon\" | \"hidden\" | \"error\" | ...</p> <code>icon_key</code> <code>str</code> <p>Icon key for the app.</p> <code>color</code> <code>str</code> <p>Accent color token.</p> <code>mode</code> <code>str</code> <p>App mode, e.g. \"no_input_v1\". Defaults to \"no_input_v1\".</p> <code>tags</code> <code>list[str]</code> <p>Tags for search / grouping.</p> <code>features</code> <code>list[str]</code> <p>Notable features for the app.</p> <code>run_visibility</code> <code>RunVisibility</code> <p>\"normal\" | \"inline\" | ...</p> <code>run_importance</code> <code>RunImportance</code> <p>\"normal\" | \"high\" | ...</p> <code>flow_id</code> <code>str</code> <p>Flow identifier. Defaults to graph name.</p> <code>github_url</code> <code>str</code> <p>Optional GitHub link.</p>"},{"location":"reference/rest-api/","title":"REST API","text":"<ul> <li><code>GET /health</code> \u2192 200 OK</li> <li><code>POST /execute</code> \u2192 Execute a graph function</li> <li><code>GET/PUT /artifacts/*</code> \u2192 Retrieve/store artifacts</li> </ul> <p>(Add OpenAPI/Redoc when ready.)</p>"},{"location":"reference/runner-api/","title":"Runner API \u2013 <code>run_async</code> &amp; <code>run</code>","text":"<p>The runner serves as the unified entry point for executing:</p> <ul> <li>a <code>GraphFunction</code> (created with <code>@graph_fn</code>), or</li> <li>a static <code>TaskGraph</code> (constructed via <code>graphify</code>, builder, or storage).</li> </ul> <p>Internally, it initializes a <code>RuntimeEnv</code>, configures services, and manages a <code>ForwardScheduler</code> with customizable retry and concurrency options.</p>"},{"location":"reference/runner-api/#runner-apis","title":"Runner APIs","text":"<p>Use these APIs to start new runs. For nested execution of graphs or agents, refer to the Context Run API.</p> run_async(target, inputs, identity, **rt_overrides) <p>Execute a TaskGraph or GraphFunction asynchronously with optional persistence and resumability.</p> <p>This method handles environment setup, cold-resume from persisted state (if available), input validation, scheduling, and output resolution. It supports both fresh runs and resuming incomplete runs, automatically wiring up persistence observers and enforcing snapshot policies.</p> <p>Examples:</p> <p>Running a graph function: <pre><code>result = await run_async(my_graph_fn, {\"x\": 1, \"y\": 2})\n</code></pre></p> <p>Running a TaskGraph with custom run ID and identity: <pre><code>result = await run_async(\n    my_task_graph,\n    {\"input\": 42},\n    run_id=\"custom-run-123\",\n    identity=my_identity,     # Only used with API requests. Ignored when running locally.\n    max_concurrency=8\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>target</code> <p>The TaskGraph, GraphFunction, or builder to execute.</p> required <code>inputs</code> <code>dict[str, Any] | None</code> <p>Dictionary of input values for the graph.</p> <code>None</code> <code>identity</code> <code>RequestIdentity | None</code> <p>Optional RequestIdentity for user/session context.</p> <code>None</code> <code>**rt_overrides</code> <p>Optional runtime overrides for environment and execution. Recognized runtime overrides include:</p> <ul> <li>run_id (str): Custom run identifier.</li> <li>session_id (str): Session identifier for grouping runs.</li> <li>agent_id (str): Agent identifier for provenance.</li> <li>app_id (str): Application identifier for provenance.</li> <li>retry (RetryPolicy): Custom retry policy.</li> <li>max_concurrency (int): Maximum number of concurrent tasks.</li> <li>Any additional container attributes supported by your environment.</li> </ul> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The resolved outputs of the graph, or a status dict if waiting on continuations.</p> <p>Raises:</p> Type Description <code>GraphHasPendingWaits</code> <p>If the graph is waiting on external events and outputs are not ready.</p> <code>TypeError</code> <p>If the target is not a valid TaskGraph or GraphFunction.</p> Notes <ul> <li>Speficially for GraphFunctions, you can directly use <code>await graph_fn(**inputs)</code> without needing <code>run_async</code>.</li> <li><code>graph_fn</code> is not resumable; use TaskGraphs for persistence and recovery features.</li> <li> <p>when using <code>graph</code> for persistence/resumability, ensure your outputs are JSON-serializable, for examples:</p> <ul> <li>primitive types (str, int, float, bool, None)</li> <li> <p>lists/dicts of primitive types</p> </li> <li> <p>graph that can be resumed with JSON-serializable outputs: <pre><code>@graphify(...)\ndef my_graph(...):\n    ...\n    return {\"result\": 42, \"data\": [1, 2, 3], \"info\": None} # valid JSON-serializable output\n</code></pre></p> </li> <li>graph that cannot be resumed due to non-JSON-serializable outputs: <pre><code>@graphify(...)\ndef my_graph(...):\n    ...\n    return {\"chekpoint\": torch.pt, \"file\": open(\"data.bin\", \"rb\")} # invalid outputs for resuming (but valid for fresh runs)\n</code></pre></li> <li>Despite this, you can still use <code>graph</code> without persistence features; just avoid resuming such graphs.</li> </ul> </li> </ul> run(target, inputs, identity, **rt_overrides) <p>Execute a target graph node synchronously with the provided inputs.</p> <p>This function submits the execution of a target node (or graph) to the event loop, allowing for asynchronous execution while providing a synchronous interface. Runtime configuration overrides can be supplied as keyword arguments.</p> <p>Examples:</p> <p>Running a graph node with default inputs: <pre><code>future = run(my_node)\nresult = future.result()\n</code></pre></p> <p>Running with custom inputs and runtime overrides: <pre><code>future = run(my_node, inputs={\"x\": 42}, timeout=10)\noutput = future.result()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>target</code> <p>The graph node or callable to execute.</p> required <code>inputs</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of input values to pass to the node.</p> <code>None</code> <code>identity</code> <code>RequestIdentity | None</code> <p>Optional RequestIdentity for user/session context.</p> <code>None</code> <code>**rt_overrides</code> <p>Additional keyword arguments to override runtime configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <p>concurrent.futures.Future: A future representing the asynchronous execution of the node.</p> Notes <ul> <li>This function is suitable for use in synchronous contexts where asynchronous execution is desired.</li> <li>It is recommended to use asynchronous execution directly when possible for better performance and responsiveness.</li> </ul>"},{"location":"reference/runtime-api/","title":"Runtime Services API \u2013 Global Services &amp; Helpers","text":"<p>AetherGraph keeps core runtime services (channels, LLM, RAG, logger, external services, MCP, etc.) in a services container. These helpers give you a consistent way to:</p> <ul> <li>Install or swap the global services container.</li> <li>Access channel/LLM/RAG/logger services outside of a <code>NodeContext</code>.</li> <li>Register extra context services reachable via <code>context.&lt;name&gt;()</code>.</li> <li>Configure RAG backends and MCP clients.</li> </ul> <p>In most applications you don\u2019t call these directly. When you start the sidecar server via <code>start_server()</code> / <code>start(...)</code>, it builds a default container (using <code>build_default_container</code>) and installs it so that <code>current_services()</code> and friends have something to return. The low\u2011level APIs below are mainly for advanced setups, tests, and custom hosts.</p>"},{"location":"reference/runtime-api/#1-core-services-container","title":"1. Core Services Container","text":"<p>These functions manage the process\u2011wide services container. Very likely user does not need to manage these manually.</p> install_services(services) -&gt; None <p>Description: Install a services container globally and in the current <code>ContextVar</code>.</p> <p>Inputs:</p> <ul> <li><code>services: Any</code> \u2013 Typically the result of <code>build_default_container()</code> or your own container.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes:</p> <ul> <li>Call this once at app startup if you\u2019re not using the sidecar server.</li> <li><code>current_services()</code> will fail until either <code>install_services(...)</code> or <code>ensure_services_installed(...)</code> has been used.</li> </ul> ensure_services_installed(factory) -&gt; Any <p>Description: Lazily create and install a services container if none exists, using <code>factory()</code>.</p> <p>Inputs:</p> <ul> <li><code>factory: Callable[[], Any]</code> \u2013 Function that returns a new services container.</li> </ul> <p>Returns:</p> <ul> <li><code>Any</code> \u2013 The active services container.</li> </ul> <p>Notes:</p> <ul> <li>Used internally by the runner to ensure a container exists (<code>build_default_container</code>).</li> <li>If a container is already installed, it is reused and just bound into the current context.</li> </ul> current_services() -&gt; Any <p>Description: Get the currently active services container.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>Any</code> \u2013 The services container instance.</li> </ul> <p>Notes:</p> <ul> <li>Raises <code>RuntimeError</code> if no services have been installed yet.</li> <li>Under normal use, this is wired automatically when the server starts.</li> </ul> use_services(services) -&gt; context manager <p>Description: Temporarily override the services container within a <code>with</code> block.</p> <p>Inputs:</p> <ul> <li><code>services: Any</code> \u2013 Services container to use inside the context.</li> </ul> <p>Returns:</p> <ul> <li>Context manager \u2013 Restores the previous services value on exit.</li> </ul> <p>Notes:</p> <ul> <li>Handy for tests or one\u2011off experiments where you want an isolated container.</li> </ul>"},{"location":"reference/runtime-api/#2-channel-service-helpers","title":"2. Channel Service Helpers","text":"<p>Channel helpers give you direct access to the channel bus (the same system behind <code>context.channel()</code>), and let you configure defaults and aliases.</p> get_channel_service() -&gt; Any <p>Description: Return the channel bus from the current services container.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>ChannelBus</code> (typed as <code>Any</code> here)</li> </ul> set_default_channel(key) -&gt; None <p>Description: Set the default channel key used when no explicit channel is specified.</p> <p>Inputs:</p> <ul> <li><code>key: str</code> \u2013 Channel key (e.g., <code>\"console\"</code>, <code>\"slack:my-bot\"</code>).</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> get_default_channel() -&gt; str <p>Description: Get the current default channel key.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>str</code> \u2013 Default channel key.</li> </ul> set_channel_alias(alias, channel_key) -&gt; None <p>Description: Register a human\u2011friendly alias for a channel key.</p> <p>Inputs:</p> <ul> <li><code>alias: str</code> \u2013 Short name to use in configs / code.</li> <li><code>channel_key: str</code> \u2013 Real channel key (e.g., full Slack route).</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> register_channel_adapter(name, adapter) -&gt; None <p>Description: Register a channel adapter implementation (e.g., Slack, Telegram, custom UI) under a name.</p> <p>Inputs:</p> <ul> <li><code>name: str</code> \u2013 Identifier for the adapter.</li> <li><code>adapter: Any</code> \u2013 Adapter instance implementing the channel interface.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes:</p> <ul> <li>Adapters are usually wired by the server configuration; you rarely need this in everyday graph code.</li> </ul>"},{"location":"reference/runtime-api/#3-llm-rag-service-helpers","title":"3. LLM &amp; RAG Service Helpers","text":"<p>These helpers configure the process\u2011wide LLM client profiles and the RAG backend.</p> get_llm_service() -&gt; Any <p>Description: Return the LLM service from the current container (the same one backing <code>context.llm()</code>).</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>Any</code> \u2013 LLM service.</li> </ul> register_llm_client(profile, provider, model, embed_model=None, base_url=None, api_key=None, timeout=None) -&gt; None <p>Description: Configure or update an LLM profile on the global LLM service.</p> <p>Inputs:</p> <ul> <li><code>profile: str</code> \u2013 Profile name (e.g., <code>\"default\"</code>, <code>\"fast\"</code>).</li> <li><code>provider: str</code> \u2013 Provider ID (<code>\"openai\"</code>, <code>\"anthropic\"</code>, etc.).</li> <li><code>model: str</code> \u2013 Chat/completion model name.</li> <li><code>embed_model: str | None</code> \u2013 Optional embeddings model.</li> <li><code>base_url: str | None</code></li> <li><code>api_key: str | None</code></li> <li><code>timeout: float | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code> (under the hood the configured client is created and stored).</li> </ul> <p>Notes:</p> <ul> <li>Alias: <code>set_llm_client = register_llm_client</code>.</li> <li>Equivalent to calling <code>svc.llm.configure_profile(...)</code> on the container.</li> </ul> set_rag_llm_client(client=None, *, provider=None, model=None, embed_model=None, base_url=None, api_key=None, timeout=None) -&gt; LLMClientProtocol <p>Description: Set the LLM client used by the RAG service.</p> <p>Inputs:</p> <ul> <li><code>client: LLMClientProtocol | None</code> \u2013 Existing client instance. If <code>None</code>, a new <code>GenericLLMClient</code> is created.</li> <li><code>provider: str | None</code></li> <li><code>model: str | None</code></li> <li><code>embed_model: str | None</code></li> <li><code>base_url: str | None</code></li> <li><code>api_key: str | None</code></li> <li><code>timeout: float | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>LLMClientProtocol</code> \u2013 The RAG LLM client actually installed.</li> </ul> <p>Notes:</p> <ul> <li>If <code>client</code> is <code>None</code>, you must provide <code>provider</code>, <code>model</code>, and <code>embed_model</code> to construct a <code>GenericLLMClient</code>.</li> <li>The chosen client is stored via <code>svc.rag.set_llm_client(...)</code>.</li> </ul> set_rag_index_backend(*, backend=None, index_path=None, dim=None) -&gt; Any <p>Description: Configure the vector index backend for the RAG service.</p> <p>Inputs:</p> <ul> <li><code>backend: str | None</code> \u2013 e.g., <code>\"sqlite\"</code> or <code>\"faiss\"</code> (defaults from <code>settings.rag.backend</code>).</li> <li><code>index_path: str | None</code> \u2013 Relative or absolute path (defaults from <code>settings.rag.index_path</code>).</li> <li><code>dim: int | None</code> \u2013 Embedding dimension (defaults from <code>settings.rag.dim</code>).</li> </ul> <p>Returns:</p> <ul> <li><code>Any</code> \u2013 The created vector index instance.</li> </ul> <p>Notes:</p> <ul> <li>Uses <code>create_vector_index(...)</code> under the hood and registers it via <code>svc.rag.set_index_backend(...)</code>.</li> <li>If <code>backend=\"faiss\"</code> but FAISS isn\u2019t available, the factory may log a warning and fall back to SQLite.</li> </ul>"},{"location":"reference/runtime-api/#4-logger-helper","title":"4. Logger Helper","text":"current_logger_factory() -&gt; Any <p>Description: Get the logger factory from the services container.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>Any</code> \u2013 Logger factory; typical usage is <code>current_logger_factory().for_scheduler()</code> or similar.</li> </ul>"},{"location":"reference/runtime-api/#5-external-context-services","title":"5. External Context Services","text":"<p>External context services are extra objects you register on the container and then access via <code>context.&lt;name&gt;()</code> inside nodes and tools.</p> register_context_service(name, service) -&gt; None <p>Description: Register a custom service in <code>svc.ext_services</code>.</p> <p>Inputs:</p> <ul> <li><code>name: str</code> \u2013 Name under which it will be exposed (e.g., <code>\"trainer\"</code>, <code>\"materials\"</code>).</li> <li><code>service: Any</code> \u2013 Service instance.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes:</p> <ul> <li>Once registered, your <code>NodeContext</code> can expose it using a helper like <code>context.trainer()</code> (depending on your <code>NodeContext</code> implementation).</li> </ul> get_ext_context_service(name) -&gt; Any <p>Description: Get a previously registered external context service.</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>Any | None</code> \u2013 The service instance or <code>None</code> if not present.</li> </ul> list_ext_context_services() -&gt; list[str] <p>Description: List all registered external context service names.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>list[str]</code> \u2013 Names registered in <code>svc.ext_services</code>.</li> </ul>"},{"location":"reference/runtime-api/#6-mcp-service-helpers","title":"6. MCP Service Helpers","text":"<p>These functions configure the Model Context Protocol (MCP) integration on the services container.</p> set_mcp_service(mcp_service) -&gt; None <p>Description: Install an MCP service object on the container.</p> <p>Inputs:</p> <ul> <li><code>mcp_service: Any</code> \u2013 Service implementing MCP coordination.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> get_mcp_service() -&gt; Any <p>Description: Get the current MCP service instance.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>Any</code> \u2013 The MCP service instance.</li> </ul> register_mcp_client(name, client) -&gt; None <p>Description: Register an MCP client with the active MCP service.</p> <p>Inputs:</p> <ul> <li><code>name: str</code> \u2013 Logical client name.</li> <li><code>client: Any</code> \u2013 MCP client instance.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes:</p> <ul> <li>Raises <code>RuntimeError</code> if no MCP service has been installed (<code>set_mcp_service(...)</code>).</li> </ul> list_mcp_clients() -&gt; list[str] <p>Description: List all registered MCP client names.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>list[str]</code> \u2013 Client names; returns <code>[]</code> if no MCP service or clients.</li> </ul>"},{"location":"reference/runtime-api/#7-summary","title":"7. Summary","text":"<ul> <li>The services container is the central place where channels, LLM, RAG, logger, external services, and MCP live.</li> <li>The sidecar server normally installs this container for you; otherwise use <code>install_services(...)</code> / <code>ensure_services_installed(...)</code>.</li> <li>Channel, LLM, RAG, and MCP helpers let you configure global behavior without touching individual graphs.</li> <li><code>register_context_service(...)</code> is the main hook for extending <code>NodeContext</code> with custom domain services (e.g., trainers, simulators, material DBs).</li> </ul>"},{"location":"reference/runtime-server/","title":"Sidecar Server \u2013 <code>start_server</code> and <code>stop_server</code>","text":"<p>The sidecar server is a lightweight FastAPI+Uvicorn process that:</p> <ul> <li>Boots the services container (channels, LLM, RAG, artifacts, state store, etc.).</li> <li>Exposes a small HTTP/WebSocket API for adapters, web UI, and continuations.</li> <li>Installs a web channel so you can talk to agents via a browser.</li> </ul> <p>For most applications without UI:</p> <ol> <li>Call <code>start_server(...)</code> once at startup (or in a notebook cell).</li> <li>Use <code>@graph_fn</code>, <code>run_async</code>, and <code>context.*</code> as usual.</li> <li>Optionally call <code>stop_server()</code> in tests or when shutting down.</li> </ol> <p>When using Aethergraph UI, it is suggested to</p> <ol> <li>Use CLI <code>aethergraph serve</code> with your modules and scripts </li> <li>Use <code>--reload</code> to auto reload changes when you make changes on graphs/agents </li> </ol> <p>When using UI, you will need specific <code>as_agent</code> and <code>as_app</code> parameters in <code>@graph_fn</code> or <code>@graphify</code>; agents and apps are triggered from UI; you do not need to manually start the run in Python.</p>"},{"location":"reference/runtime-server/#1-start_server-sync-sidecar-starter","title":"1. <code>start_server</code> \u2013 sync sidecar starter","text":"start_server(*, workspace, host, port, log_level, ...) <p>Start (or reuse) the AetherGraph sidecar server in a normalized and flexible way.</p> <p>This method manages server lifecycle, workspace locking, and dynamic loading of user code. It supports both in-process and cross-process server reuse, and can return handles for advanced control or integration.</p> <p>Examples:</p> <p>Basic usage to start a server and get its URL: <pre><code>url = start_server(workspace=\"./aethergraph_data\", port=0)\n</code></pre></p> <p>Loading user graphs before starting: <pre><code>url = start_server(\n  workspace=\"./aethergraph_data\",\n  port=0,\n  load_paths=[\"./my_graphs.py\"],\n  project_root=\".\",\n)\n</code></pre></p> <p>Starting and blocking until server exit (notebook/script mode): <pre><code>url, handle = start_server(workspace=\"./aethergraph_data\", port=0, return_handle=True)\nprint(\"Server running at\", url)\ntry:\n  handle.block()\nexcept KeyboardInterrupt:\n  print(\"Stopping server...\")\n  handle.stop()\n</code></pre></p> <p>Returning the dependency injection container for advanced use: <pre><code>url, container = start_server(workspace=\"./aethergraph_data\", return_container=True)\n</code></pre></p> <p>Returning both container and handle: <pre><code>url, container, handle = start_server(\n  workspace=\"./aethergraph_data\",\n  return_container=True,\n  return_handle=True,\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>workspace</code> <code>str</code> <p>Persistent storage directory for server state and data.</p> <code>'./aethergraph_data'</code> <code>host</code> <code>str</code> <p>Host address to bind the server (default \"127.0.0.1\").</p> <code>'127.0.0.1'</code> <code>port</code> <code>int</code> <p>Port to bind the server (0 for auto-pick, or specify a fixed port).</p> <code>8745</code> <code>log_level</code> <code>str</code> <p>Logging level for the application.</p> <code>'warning'</code> <code>unvicorn_log_level</code> <code>str</code> <p>Logging level for the Uvicorn server.</p> <code>'warning'</code> <code>return_container</code> <code>bool</code> <p>If True, also return the app's dependency injection container.</p> <code>False</code> <code>return_handle</code> <code>bool</code> <p>If True, return a ServerHandle for programmatic control (block/stop).</p> <code>False</code> <code>load_modules</code> <code>list[str] | None</code> <p>List of Python modules to import before server start.</p> <code>None</code> <code>load_paths</code> <code>list[str] | None</code> <p>List of Python file paths to import before server start.</p> <code>None</code> <code>project_root</code> <code>str | None</code> <p>Path to add to sys.path for module resolution during loading.</p> <code>None</code> <code>strict_load</code> <code>bool</code> <p>If True, raise on import/load errors; otherwise, record errors in loader report.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str | tuple[str, Any] | tuple[str, ServerHandle] | tuple[str, Any, ServerHandle]</code> <p>The server URL (e.g., \"http://127.0.0.1:53421\").</p> <code>tuple</code> <code>str | tuple[str, Any] | tuple[str, ServerHandle] | tuple[str, Any, ServerHandle]</code> <p>Optionally, (url, container), (url, handle), or (url, container, handle) depending on the flags set and whether the server was started in-process.</p> Notes <ul> <li>Workspace is a dedicated directory for server data, including logs, caches, and runtime state; multiple processes using the same   workspace will coordinate to reuse a single server instance. Delete the workspace to reset state.</li> <li>Use handle.block() to wait for server exit when you need to keep the server running in a script or notebook. This is typically not needed   when using the server in client mode.</li> <li>When you are using Aethergraph UI, use handle.block() to keep the server running so that the UI can connect to it and discover agents/apps.</li> </ul>"},{"location":"reference/runtime-server/#2-start-server-via-cli","title":"2. start server via CLI","text":"CLI start server <p>Start the AetherGraph server via CLI.</p> <p>This entrypoint launches the persistent sidecar server for your workspace, enabling API access for frontend/UI clients. It supports automatic port selection, workspace isolation, and dynamic loading of user graphs/apps.</p> <p>Examples:</p> <p>Basic usage with default workspace and port: <pre><code>python -m aethergraph serve --workspace # only default agents/apps show up\n</code></pre></p> <p>load user graphs from a file and autoreload on changes: <pre><code>python -m aethergraph serve --load-path ./graphs.py --reload\n</code></pre></p> <p>Load multiple modules and set a custom project root: <pre><code>python -m aethergraph serve --load-module mygraphs --project-root .\n</code></pre></p> <p>Reuse detection (print URL if already running): <pre><code>python -m aethergraph serve --reuse\n</code></pre></p> <p>Customize workspace and port: <pre><code>python -m aethergraph serve --workspace ./my_workspace --port 8000  # this will not show previous runs/artifacts unless reused\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>argv</code> <code>list[str] | None</code> <p>Optional list of CLI arguments. If None, uses sys.argv[1:].</p> <code>None</code> Required keywords <ul> <li><code>serve</code>: Command to start the AetherGraph server. If no other command is given, the server will only load default built-in agents/apps.</li> </ul> Optional keywords <ul> <li><code>workspace</code>: Path to the workspace folder (default: ./aethergraph_data).</li> <li><code>host</code>: Host address to bind (default: 127.0.0.1).</li> <li><code>port</code>: Port to bind (default: 8745; use 0 for auto-pick).</li> <li><code>log-level</code>: App log level (default: warning).</li> <li><code>uvicorn-log-level</code>: Uvicorn log level (default: warning).</li> <li><code>project-root</code>: Temporarily added to sys.path for local imports.</li> <li><code>load-module</code>: Python module(s) to import before server starts (repeatable).</li> <li><code>load-path</code>: Python file(s) to load before server starts (repeatable).</li> <li><code>strict-load</code>: Raise error if graph loading fails.</li> <li><code>reuse</code>: If server already running for workspace, print URL and exit.</li> <li><code>reload</code>: Enable auto-reload (dev mode).</li> </ul> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Exit code (0 for success, 2 for unknown command).</p> Notes <ul> <li>Launching the server via CLI keeps it running persistently for API clients to connect like AetherGraph UI.</li> <li>In local mode, the server port will automatically be consistent with UI connections.</li> <li>use <code>--reload</code> for development to auto-restart on code changes. This will use uvicorn's reload feature.</li> <li>When switching ports, the UI will not show previous runs/artifacts unless the server is reused. This is     because the server URL is tied to the frontend hash. Keep the server in a same port (default 8745) for local dev.     Later the UI can support dynamic port discovery via server.json.</li> </ul>"},{"location":"reference/tool-channel/","title":"Built-in Channel Tools (<code>@tool</code>)","text":"<p>AetherGraph ships a small set of built-in channel tools that wrap the active <code>context.channel()</code> service. They are all defined as <code>@tool(...)</code> so that:</p> <ul> <li>In graph mode (<code>@graph_fn</code>, <code>@graphify</code>), each call becomes a tool node with proper provenance.</li> <li>The <code>ask_*</code> tools support resumable waits (user input, approvals, file uploads).</li> <li>In immediate mode (no active graph), they behave like plain async functions returning dicts.</li> </ul> <p>Important: All <code>ask_*</code> tools are meant to be called from a graph (<code>@graph_fn</code> or <code>@graphify</code>), not from inside another <code>@tool</code> implementation. They rely on a <code>NodeContext</code> and wait/resume semantics that only exist at the graph level.</p> <p>All tools here assume <code>context</code> is injected automatically by the runtime; you typically do not pass <code>context</code> yourself.</p>"},{"location":"reference/tool-channel/#1-ask_text-prompt-wait-for-free-form-reply","title":"1. <code>ask_text</code> \u2013 prompt + wait for free-form reply","text":"<pre><code>@tool(name=\"ask_text\", outputs=[\"text\"])\nasync def ask_text(\n    *,\n    resume=None,\n    context=None,\n    prompt: str | None = None,\n    silent: bool = False,\n    timeout_s: int = 3600,\n    channel: str | None = None,\n):\n    ...\n</code></pre> ask_text(*, resume=None, context=None, prompt=None, silent=False, timeout_s=3600, channel=None) -&gt; {\"text\": str} <p>Description:</p> <p>Send an optional <code>prompt</code> message via the active channel and wait for a text reply. Under the hood this uses a dual-stage tool (<code>ask_text_ds</code>) so the node can enter a WAITING state and be resumed later when the user responds.</p> <p>Inputs (data/control):</p> <ul> <li><code>prompt: str | None</code> \u2013 Text shown to the user. If <code>None</code>, some channels may only show a generic input request.</li> <li><code>silent: bool</code> \u2013 If <code>True</code>, do not send a visible prompt; only wait for incoming text.</li> <li><code>timeout_s: int</code> \u2013 Max seconds to wait before timing out.</li> <li><code>channel: str | None</code> \u2013 Optional channel key or alias (Slack thread, web session, etc.). If <code>None</code>, uses the default channel.</li> <li><code>resume: Any</code> \u2013 Continuation payload used internally on resume. You do not set this manually in normal usage.</li> <li><code>context</code> \u2013 Injected <code>NodeContext</code>, used internally via <code>context.channel()</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>{\"text\": str}</code> \u2013 The captured user reply as plain text.</li> </ul> <p>Notes:</p> <ul> <li>Use this inside <code>@graph_fn</code> / <code>@graphify</code> for resumable user input.</li> <li>Not intended to be called from within another <code>@tool</code> implementation.</li> </ul>"},{"location":"reference/tool-channel/#2-wait_text-wait-for-a-reply-without-sending-a-prompt","title":"2. <code>wait_text</code> \u2013 wait for a reply without sending a prompt","text":"<pre><code>@tool(name=\"wait_text\", outputs=[\"text\"])\nasync def wait_text(\n    *, resume=None, context=None, timeout_s: int = 3600, channel: str | None = None\n):\n    ...\n</code></pre> wait_text(*, resume=None, context=None, timeout_s=3600, channel=None) -&gt; {\"text\": str} <p>Description:</p> <p>Wait for the next incoming text message on a given channel without sending a new prompt. Useful when a prior node already sent a message, and you only need to block until the user responds.</p> <p>Inputs:</p> <ul> <li><code>timeout_s: int</code> \u2013 Max seconds to wait.</li> <li><code>channel: str | None</code> \u2013 Channel key/alias; defaults to the current/default channel.</li> <li><code>resume</code>, <code>context</code> \u2013 Internal, handled by the runtime.</li> </ul> <p>Returns:</p> <ul> <li><code>{\"text\": str}</code> \u2013 The received message.</li> </ul> <p>Notes:</p> <ul> <li>Like <code>ask_text</code>, this is a waitable tool node \u2013 only use at graph level.</li> </ul>"},{"location":"reference/tool-channel/#3-ask_approval-buttons-approval-flow","title":"3. <code>ask_approval</code> \u2013 buttons / approval flow","text":"<pre><code>@tool(name=\"ask_approval\", outputs=[\"approved\", \"choice\"])\nasync def ask_approval(\n    *,\n    resume=None,\n    context=None,\n    prompt: str,\n    options: list[str] | tuple[str, ...] = (\"Approve\", \"Reject\"),\n    timeout_s: int = 3600,\n    channel: str | None = None,\n):\n    ...\n</code></pre> ask_approval(*, prompt, options=(\"Approve\",\"Reject\"), timeout_s=3600, channel=None, ...) -&gt; {\"approved\": bool, \"choice\": str} <p>Description:</p> <p>Send a message with button options (e.g., Approve / Reject) and wait for the user to click one. Ideal for human-in-the-loop approvals in a workflow.</p> <p>Inputs:</p> <ul> <li><code>prompt: str</code> \u2013 Text shown above the buttons.</li> <li><code>options: list[str] | tuple[str, ...]</code> \u2013 Labels for buttons (first is typically the \"approve\" action).</li> <li><code>timeout_s: int</code> \u2013 Max seconds to wait.</li> <li><code>channel: str | None</code> \u2013 Optional channel key/alias.</li> <li><code>resume</code>, <code>context</code> \u2013 Internal; managed by the runtime.</li> </ul> <p>Returns:</p> <ul> <li> <p><code>{\"approved\": bool, \"choice\": str}</code></p> </li> <li> <p><code>approved</code> \u2013 <code>True</code> if the chosen label is considered positive (by current policy; typically the first option), <code>False</code> otherwise.</p> </li> <li><code>choice</code> \u2013 The raw string label clicked by the user.</li> </ul> <p>Notes:</p> <ul> <li>Implemented as a dual-stage waitable tool; use from <code>@graph_fn</code> / <code>@graphify</code>.</li> </ul>"},{"location":"reference/tool-channel/#4-ask_files-prompt-for-uploads","title":"4. <code>ask_files</code> \u2013 prompt for uploads","text":"<pre><code>@tool(name=\"ask_files\", outputs=[\"text\", \"files\"])\nasync def ask_files(\n    *,\n    resume=None,\n    context=None,\n    prompt: str,\n    accept: list[str] | None = None,\n    multiple: bool = True,\n    timeout_s: int = 3600,\n    channel: str | None = None,\n):\n    ...\n</code></pre> ask_files(*, prompt, accept=None, multiple=True, timeout_s=3600, channel=None, ...) -&gt; {\"text\": str, \"files\": list[FileRef]} <p>Description:</p> <p>Ask the user to upload one or more files, optionally constraining allowed types, and wait until they respond.</p> <p>Inputs:</p> <ul> <li><code>prompt: str</code> \u2013 Text requesting the upload.</li> <li><code>accept: list[str] | None</code> \u2013 Optional list of accepted types (MIME types or extensions), depending on channel implementation.</li> <li><code>multiple: bool</code> \u2013 If <code>True</code>, allow multiple files; otherwise require a single upload.</li> <li><code>timeout_s: int</code> \u2013 Max seconds to wait.</li> <li><code>channel: str | None</code> \u2013 Optional channel key/alias.</li> <li><code>resume</code>, <code>context</code> \u2013 Internal.</li> </ul> <p>Returns:</p> <ul> <li> <p><code>{\"text\": str, \"files\": list[FileRef]}</code></p> </li> <li> <p><code>text</code> \u2013 Optional message text the user sent along with the files.</p> </li> <li><code>files</code> \u2013 List of <code>FileRef</code> objects pointing at uploaded files.</li> </ul> <p>Notes:</p> <ul> <li>Files are typically stored via the artifact service behind the scenes; <code>FileRef</code> carries enough info to retrieve them.</li> <li>Use in graph-level code only.</li> </ul>"},{"location":"reference/tool-channel/#5-send_text-fire-and-forget-text-message","title":"5. <code>send_text</code> \u2013 fire-and-forget text message","text":"<pre><code>@tool(name=\"send_text\", outputs=[\"ok\"])\nasync def send_text(\n    *, text: str, meta: dict[str, Any] | None = None, channel: str | None = None, context=None\n):\n    ...\n</code></pre> send_text(*, text, meta=None, channel=None, context=None) -&gt; {\"ok\": bool} <p>Description:</p> <p>Send a text message to the selected channel and return immediately (no wait).</p> <p>Inputs:</p> <ul> <li><code>text: str</code> \u2013 Message body.</li> <li><code>meta: dict[str, Any] | None</code> \u2013 Optional metadata for the channel adapter (thread IDs, tags, etc.).</li> <li><code>channel: str | None</code> \u2013 Target channel key/alias; defaults to the current/default channel.</li> <li><code>context</code> \u2013 Injected <code>NodeContext</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>{\"ok\": True}</code> on success.</li> </ul> <p>Notes:</p> <ul> <li>Non-waiting tool: useful for notifications, logging, or streaming intermediate updates.</li> </ul>"},{"location":"reference/tool-channel/#6-send_image-post-an-image","title":"6. <code>send_image</code> \u2013 post an image","text":"<pre><code>@tool(name=\"send_image\", outputs=[\"ok\"])\nasync def send_image(\n    *,\n    url: str | None = None,\n    alt: str = \"image\",\n    title: str | None = None,\n    channel: str | None = None,\n    context=None,\n):\n    ...\n</code></pre> send_image(*, url=None, alt=\"image\", title=None, channel=None, context=None) -&gt; {\"ok\": bool} <p>Description:</p> <p>Send an image message to the channel, typically by URL.</p> <p>Inputs:</p> <ul> <li><code>url: str | None</code> \u2013 Public or internally resolvable image URL.</li> <li><code>alt: str</code> \u2013 Alt text.</li> <li><code>title: str | None</code> \u2013 Optional title/caption.</li> <li><code>channel: str | None</code> \u2013 Target channel key/alias.</li> <li><code>context</code> \u2013 Injected.</li> </ul> <p>Returns:</p> <ul> <li><code>{\"ok\": True}</code> on success.</li> </ul>"},{"location":"reference/tool-channel/#7-send_file-attach-a-file","title":"7. <code>send_file</code> \u2013 attach a file","text":"<pre><code>@tool(name=\"send_file\", outputs=[\"ok\"])\nasync def send_file(\n    *,\n    url: str | None = None,\n    file_bytes: bytes | None = None,\n    filename: str = \"file.bin\",\n    title: str | None = None,\n    channel: str | None = None,\n    context=None,\n):\n    ...\n</code></pre> send_file(*, url=None, file_bytes=None, filename=\"file.bin\", title=None, channel=None, context=None) -&gt; {\"ok\": bool} <p>Description:</p> <p>Send a file attachment to the channel, either by URL or raw bytes.</p> <p>Inputs:</p> <ul> <li><code>url: str | None</code> \u2013 If provided, the channel may fetch the file from this URL.</li> <li><code>file_bytes: bytes | None</code> \u2013 Raw file contents; used when you already have the bytes in memory.</li> <li><code>filename: str</code> \u2013 Name to show to the user.</li> <li><code>title: str | None</code> \u2013 Optional human-friendly label.</li> <li><code>channel: str | None</code> \u2013 Target channel key/alias.</li> <li><code>context</code> \u2013 Injected.</li> </ul> <p>Returns:</p> <ul> <li><code>{\"ok\": True}</code> on success.</li> </ul> <p>Notes:</p> <ul> <li>Channel adapters decide how to handle <code>url</code> vs <code>file_bytes</code>.</li> </ul>"},{"location":"reference/tool-channel/#8-send_buttons-message-with-interactive-buttons","title":"8. <code>send_buttons</code> \u2013 message with interactive buttons","text":"<pre><code>@tool(name=\"send_buttons\", outputs=[\"ok\"])\nasync def send_buttons(\n    *,\n    text: str,\n    buttons: list[Button],\n    meta: dict[str, Any] | None = None,\n    channel: str | None = None,\n    context=None,\n):\n    ...\n</code></pre> send_buttons(*, text, buttons, meta=None, channel=None, context=None) -&gt; {\"ok\": bool} <p>Description:</p> <p>Send a message with interactive buttons, without waiting for a response in this node. Useful for UI-only affordances when another node will handle the actual click.</p> <p>Inputs:</p> <ul> <li><code>text: str</code> \u2013 Message text.</li> <li><code>buttons: list[Button]</code> \u2013 Channel-specific button descriptors.</li> <li><code>meta: dict[str, Any] | None</code> \u2013 Optional metadata.</li> <li><code>channel: str | None</code> \u2013 Target channel key/alias.</li> <li><code>context</code> \u2013 Injected.</li> </ul> <p>Returns:</p> <ul> <li><code>{\"ok\": True}</code> on success.</li> </ul> <p>Notes:</p> <ul> <li>To wait on a button click, use <code>ask_approval</code> instead.</li> </ul>"},{"location":"reference/tool-channel/#9-get_latest_uploads-retrieve-recent-file-uploads","title":"9. <code>get_latest_uploads</code> \u2013 retrieve recent file uploads","text":"<pre><code>@tool(name=\"get_lastest_uploads\", outputs=[\"files\"])\nasync def get_latest_uploads(*, clear: bool = True, context) -&gt; list[FileRef]:\n    ...\n</code></pre> get_latest_uploads(*, clear=True, context) -&gt; {\"files\": list[FileRef]} <p>Description:</p> <p>Fetch the most recent file uploads associated with the current channel session. This is a convenience around <code>channel.get_latest_uploads</code>.</p> <p>Inputs:</p> <ul> <li><code>clear: bool = True</code> \u2013 If <code>True</code>, clear the internal buffer after reading so subsequent calls only see newer uploads.</li> <li><code>context</code> \u2013 Injected <code>NodeContext</code>; used to access <code>context.channel()</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>{\"files\": list[FileRef]}</code> \u2013 List of file references.</li> </ul> <p>Notes:</p> <ul> <li>Tool is registered under the name <code>\"get_lastest_uploads\"</code> (note the spelling) but the Python symbol is <code>get_latest_uploads</code>.</li> <li>Any channel session that supports uploads will expose the same upload buffer.</li> </ul>"},{"location":"reference/tool-channel/#usage-resumption","title":"Usage &amp; Resumption","text":"<ul> <li>All of these are tools, so in <code>@graph_fn</code> and <code>@graphify</code> they appear as nodes in the <code>TaskGraph</code>.</li> <li><code>ask_text</code>, <code>wait_text</code>, <code>ask_approval</code>, and <code>ask_files</code> are waitable tool nodes \u2014 they can pause a run and be resumed via continuations (Slack/web UI/etc.).</li> <li>Do not call these from inside another <code>@tool</code> implementation; they depend on graph-level scheduling and <code>NodeContext</code>.</li> <li>For simple, non-graph scripts, you can still <code>await</code> them directly as async functions, but you will not get resumability or persisted state unless running under the sidecar / scheduler.</li> </ul>"},{"location":"reference/tools-facade/","title":"Tools Facade","text":""},{"location":"reference/tools-facade/#registerfunc-namenone-inputsnone-outputsnone-str","title":"register(func, *, name=None, inputs=None, outputs=None) \u2192 str","text":"<p>Registers a tool and returns its name/id.</p>"},{"location":"reference/tools-facade/#callname-args-dict-dict","title":"call(name, args: dict) \u2192 dict","text":"<p>Invokes a tool by name with validated args.</p>"},{"location":"tutorials/t0-plan/","title":"Agentic R&amp;D in 60 Seconds \u2013 with AetherGraph","text":"<p>Short, practical demos of agentic R&amp;D workflows in plain Python, using AetherGraph.</p> <ul> <li>Format: ~60s video + LinkedIn post + example link</li> <li>Focus: \u201cWhat can I do THIS WEEK that makes R&amp;D less painful?\u201d</li> <li> <p>Core building blocks:</p> </li> <li> <p>Channels \u2013 Python that talks back</p> </li> <li>Artifacts \u2013 stop losing results</li> <li>Memory \u2013 pipeline as a lab notebook</li> <li>External services \u2013 glue for your stack</li> <li>Bonus: LLMs \u2013 reasoning inside the flow</li> </ul>"},{"location":"tutorials/t0-plan/#series-overview-for-intro-guides-page","title":"Series Overview (for intro / guides page)","text":"<p>Most \u201cagentic frameworks\u201d promise the world. I just want my R&amp;D scripts to talk back, remember things, and organize results \u2014 without leaving Python.</p> <p>\u201cAgentic R&amp;D in 60 Seconds \u2013 with AetherGraph\u201d is a short series of ~1-minute demos where I slowly build up practical \u201cagentic\u201d behaviors on top of normal Python code:</p> <ul> <li>Ep.1 \u2013 Channels: Python that talks back.</li> <li>Ep.2 \u2013 Artifacts: stop losing results in random folders.</li> <li>Ep.3 \u2013 Memory: your pipeline as a lightweight lab notebook.</li> <li>Ep.4 \u2013 External services: glue for your existing stack.</li> <li>Ep.5 (Bonus) \u2013 LLMs: let the model look at your data and suggest next steps.</li> </ul> <p>AetherGraph can be used to build much more complex agentic systems, but this series stays deliberately practical:</p> <p>\u2192 What can I do this week that makes my R&amp;D life less painful?</p>"},{"location":"tutorials/t0-plan/#episode-1-python-that-talks-back-channels","title":"Episode 1 \u2013 Python That Talks Back (Channels)","text":""},{"location":"tutorials/t0-plan/#linkedin-post-draft","title":"LinkedIn Post Draft","text":"<p>Most \u201cagentic frameworks\u201d promise the world. I just want my R&amp;D scripts to talk back, remember things, and organize results \u2014 without leaving Python.</p> <p>Over the next few weeks I\u2019m sharing a short mini-series:</p> <p>\u201cAgentic R&amp;D in 60 Seconds \u2013 with AetherGraph\u201d</p> <p>Each post = a ~1 minute demo + a small code snippet, focused on one capability in AetherGraph:</p> <ul> <li>Channels \u2013 Python that talks back</li> <li>Artifacts \u2013 stop losing results in random folders</li> <li>Memory \u2013 your pipeline as a lightweight lab notebook</li> <li>External services \u2013 glue for your existing stack</li> </ul> <p>AetherGraph can be used to build much fancier agentic systems, but in this series I want to stay very practical:</p> <p>\u2192 \u201cWhat can I do this week that makes my R&amp;D life less painful?\u201d</p> <p>Ep.1 \u2013 Python that talks back (channels)</p> <p>In this first demo, I turn a plain Python script into a tiny \u201cexperiment setup wizard\u201d.</p> <p>The script:</p> <ul> <li>Asks for a project name</li> <li>Asks how many steps to run</li> <li>Lets you enable or skip an \u201cadvanced mode\u201d</li> <li>Then prints the final config and starts the run</li> </ul> <p>All of this is powered by AetherGraph\u2019s channel API, but it still looks and feels like normal async Python.</p> <p>Code sketch:</p> <p>from aethergraph import graph_fn from aethergraph.core.runtime import ExecutionContext</p> <p>@graph_fn(name=\"channel_wizard\") async def channel_wizard(context: ExecutionContext): chan = context.channel()</p> <pre><code>project = await chan.ask_text(\"Project name?\")\nsteps = int(\n    await chan.ask_text(\"Number of steps (e.g. 10)?\")\n)\n\nadvanced_choice = await chan.ask_approval(\n    \"Enable advanced mode?\",\n    options=[\"Yes\", \"No\"],\n)\n\nconfig = {\n    \"project\": project,\n    \"steps\": steps,\n    \"advanced\": (advanced_choice == \"Yes\"),\n}\n\nawait chan.send_text(f\"Running experiment with config: {config}\")\n\n# Replace this with your real workload\n# e.g. run_training(config), submit_job(config), etc.\nawait chan.send_text(\"Experiment started \u2705\")\n</code></pre> <p>Beyond this demo</p> <p>Once you\u2019re comfortable with context.channel(), you can:</p> <ul> <li>Turn any CLI script into a chat-like wizard (console today, Slack / others later).</li> <li>Ask for approvals before expensive steps (e.g. \u201cSubmit this job to the cluster?\u201d).</li> <li>Build interactive workflows where the system asks you for missing pieces instead of failing.</li> </ul> <p>Links you can add:</p> <ul> <li>Examples repo</li> <li>Main repo</li> <li>Intro page</li> </ul>"},{"location":"tutorials/t0-plan/#episode-2-stop-losing-your-results-artifacts","title":"Episode 2 \u2013 Stop Losing Your Results (Artifacts)","text":"<p>Most R&amp;D projects eventually become a graveyard of:</p> <ul> <li>final_results_v7_really_final.json</li> <li>new_final_results_latest.csv</li> </ul> <p>In Ep.2 of \u201cAgentic R&amp;D in 60 Seconds \u2013 with AetherGraph\u201d, I show how to use artifacts so your workflow always knows where its outputs live.</p> <p>Demo idea:</p> <p>A tiny analysis script:</p> <ul> <li>Loads a CSV</li> <li>Computes a small summary (e.g. min / max / mean)</li> <li>Saves it as a versioned artifact tied to an experiment scope</li> <li>Prints the artifact URI so you can retrieve it later</li> </ul> <p>Code sketch:</p> <p>from aethergraph import graph_fn from aethergraph.core.runtime import ExecutionContext</p> <p>import pandas as pd</p> <p>@graph_fn(name=\"summarize_csv\") async def summarize_csv(context: ExecutionContext, path: str, scope: str = \"demo_exp\"): df = pd.read_csv(path)</p> <pre><code>summary = {\n    \"rows\": int(len(df)),\n    \"cols\": int(len(df.columns)),\n    \"col_names\": list(df.columns),\n    \"mean_of_first_col\": float(df[df.columns[0]].mean()),\n}\n\nref = await context.artifacts.save_json(\n    data=summary,\n    scope=scope,\n    tag=\"summary\",\n)\n\nawait context.channel().send_text(\n    f\"Saved summary artifact:\\n{ref.uri}\"\n)\n\nreturn {\"artifact_uri\": ref.uri}\n</code></pre> <p>A second graph to load the last summary:</p> <p>@graph_fn(name=\"load_last_summary\") async def load_last_summary(context: ExecutionContext, scope: str = \"demo_exp\"): last = await context.artifacts.load_last( scope=scope, tag=\"summary\", ) await context.channel().send_text(f\"Last summary:\\n{last}\")</p> <p>Beyond this demo</p> <p>With artifacts you can:</p> <ul> <li>Version reports, plots, and models per experiment or per run.</li> <li>Build \u201clatest report\u201d and \u201clatest metrics\u201d helpers instead of guessing file names.</li> <li>Connect artifacts with memory (e.g. log events that reference artifact URIs).</li> </ul>"},{"location":"tutorials/t0-plan/#episode-3-pipelines-with-a-memory-memory","title":"Episode 3 \u2013 Pipelines With a Memory (Memory)","text":"<p>Most R&amp;D work gets lost in log files and screenshots. The process \u2014 which configs you tried, what worked, what failed \u2014 is rarely captured in a way you can query.</p> <p>In Ep.3 of \u201cAgentic R&amp;D in 60 Seconds \u2013 with AetherGraph\u201d, I treat the pipeline itself as a tiny lab notebook using the memory layer.</p> <p>Demo idea:</p> <p>Each run:</p> <ul> <li>Logs its config and metric as a memory event</li> <li>Tags events by experiment scope</li> <li>Then we ask memory to summarize the last few runs</li> </ul> <p>Code sketch for logging a run:</p> <p>from aethergraph import graph_fn from aethergraph.core.runtime import ExecutionContext</p> <p>@graph_fn(name=\"train_once\") async def train_once( context: ExecutionContext, lr: float, steps: int, scope: str = \"exp1\", ): chan = context.channel() mem = context.memory()</p> <pre><code>metric = 0.75 + (lr * 0.05)  # fake metric\n\nawait chan.send_text(f\"Run finished: lr={lr}, steps={steps}, metric={metric:.3f}\")\n\nawait mem.log_event(\n    scope_id=scope,\n    kind=\"run\",\n    payload={\n        \"lr\": lr,\n        \"steps\": steps,\n        \"metric\": metric,\n    },\n    tags=[\"demo\", \"train_run\"],\n)\n\nreturn {\"metric\": metric}\n</code></pre> <p>Code sketch for summarizing recent runs:</p> <p>@graph_fn(name=\"summarize_recent_runs\") async def summarize_recent_runs(context: ExecutionContext, scope: str = \"exp1\"): mem = context.memory() chan = context.channel()</p> <pre><code>summary = await mem.distill_long_term(\n    scope_id=scope,\n    question=\"Summarize the last 3 runs and what changed between them.\",\n    include_kinds=[\"run\"],\n    max_events=3,\n)\n\nawait chan.send_text(\"Summary of recent runs:\")\nawait chan.send_text(summary[\"text\"])\n</code></pre> <p>Beyond this demo</p> <p>With memory you can:</p> <ul> <li>Keep a queryable history of runs (config + results + comments).</li> <li>Ask the system to highlight trends.</li> <li>Combine with channels: ask for human notes mid-run and store them.</li> </ul>"},{"location":"tutorials/t0-plan/#episode-4-glue-for-your-stack-external-services","title":"Episode 4 \u2013 Glue for Your Stack (External Services)","text":"<p>Real R&amp;D rarely lives in one library. You probably have:</p> <ul> <li>a simulation codebase over here</li> <li>a queue / job runner over there</li> <li>some custom Python scripts everywhere</li> </ul> <p>In Ep.4 of \u201cAgentic R&amp;D in 60 Seconds \u2013 with AetherGraph\u201d, I treat one of those tools as a first-class service inside the execution context.</p> <p>Demo idea:</p> <ul> <li>Define a simple SimService that mocks running a simulation.</li> <li>Register it with the AetherGraph runtime as a context service.</li> <li>Call it from a graph function, then save results as artifacts.</li> </ul> <p>Service sketch:</p> <p>class SimService: async def run(self, config: dict) -&gt; dict: return { \"status\": \"ok\", \"config\": config, \"result\": 42.0, }</p>"},{"location":"tutorials/t0-plan/#register-under-the-name-sim-in-your-container-setup","title":"Register under the name \"sim\" in your container / setup","text":"<p>Graph sketch:</p> <p>from aethergraph import graph_fn from aethergraph.core.runtime import ExecutionContext</p> <p>@graph_fn(name=\"run_sim_experiment\") async def run_sim_experiment( context: ExecutionContext, steps: int = 10, param: float = 0.5, scope: str = \"sim_demo\", ): chan = context.channel() arts = context.artifacts() sim = context.ext_service(\"sim\")  # or context.sim</p> <pre><code>config = {\"steps\": steps, \"param\": param}\nawait chan.send_text(f\"Submitting simulation with config: {config}\")\n\nresult = await sim.run(config)\nawait chan.send_text(f\"Simulation finished with result={result['result']}\")\n\nref = await arts.save_json(\n    data=result,\n    scope=scope,\n    tag=\"sim_result\",\n)\n\nawait chan.send_text(f\"Saved result as artifact: {ref.uri}\")\nreturn {\"artifact_uri\": ref.uri}\n</code></pre> <p>Beyond this demo</p> <p>With external services you can:</p> <ul> <li>Wrap existing simulators, API clients, or job runners as context services.</li> <li>Keep your graph logic clean while delegating heavy lifting to those services.</li> <li>Combine with channels, memory, and artifacts to build a coherent workflow.</li> </ul>"},{"location":"tutorials/t0-plan/#episode-5-bonus-let-the-llm-look-at-your-data-contextllm","title":"Episode 5 (Bonus) \u2013 Let the LLM Look at Your Data (context.llm())","text":"<p>We now have:</p> <ul> <li>Channels \u2013 Python that talks back</li> <li>Artifacts \u2013 structured results you can find again</li> <li>Memory \u2013 a lightweight lab notebook</li> <li>External services \u2013 glue to call your existing stack</li> </ul> <p>In this bonus episode of \u201cAgentic R&amp;D in 60 Seconds \u2013 with AetherGraph\u201d, I plug in the last piece:</p> <p>Let an LLM look at your data inside the flow, instead of manually copying logs into a chat box.</p> <p>Demo idea:</p> <ul> <li>Load the last summary artifact for an experiment.</li> <li>Call context.llm(\"default\").chat(...) to ask for an explanation.</li> <li>Have the pipeline print / send the explanation back via the channel.</li> </ul> <p>Code sketch:</p> <p>from aethergraph import graph_fn from aethergraph.core.runtime import ExecutionContext</p> <p>@graph_fn(name=\"explain_last_summary\") async def explain_last_summary( context: ExecutionContext, scope: str = \"demo_exp\", profile: str = \"default\", ): chan = context.channel() arts = context.artifacts() llm = context.llm(profile)</p> <pre><code>last = await arts.load_last(scope=scope, tag=\"summary\")\nif last is None:\n    await chan.send_text(\"No summary artifact found yet.\")\n    return\n\nuser_content = (\n    \"Here is a JSON summary of a recent experiment:\\n\\n\"\n    f\"{last}\\n\\n\"\n    \"Explain what this tells us, and suggest one reasonable next experiment to run.\"\n)\n\ntext, usage = await llm.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are an R&amp;D lab assistant.\"},\n        {\"role\": \"user\", \"content\": user_content},\n    ],\n)\n\nawait chan.send_text(\"LLM analysis of last summary:\")\nawait chan.send_text(text)\n\nreturn {\"analysis\": text, \"usage\": usage}\n</code></pre> <p>Beyond this demo</p> <p>With context.llm() you can:</p> <ul> <li>Explain artifact contents.</li> <li>Summarize memory events.</li> <li>Propose next steps based on metrics and configs.</li> <li>Implement small policy loops where the LLM suggests new parameters and the graph runs them (with human approvals in the loop).</li> </ul> <p>This is where the earlier pieces come together:</p> <ul> <li>Channels \u2192 interactive conversation.</li> <li>Artifacts \u2192 structured results to feed the model.</li> <li>Memory \u2192 history and context.</li> <li>External services \u2192 real simulators and tools.</li> </ul>"},{"location":"tutorials/t1-build-your-first-graph-fn/","title":"Tutorial 1: Build Your First <code>graph_fn</code>","text":"<p>This tutorial walks you through the core API of AetherGraph and helps you build your first reactive agent using the <code>@graph_fn</code> decorator.</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#overview","title":"\ud83d\ude80 Overview","text":"<p>AetherGraph introduces a Python\u2011native way to create agents that can think, wait, and talk \u2014 all inside a normal Python function. With the <code>@graph_fn</code> decorator, you can:</p> <ul> <li>Add context\u2011aware I/O (<code>context.channel()</code>, <code>context.llm()</code>, <code>context.memory()</code>)</li> <li>Run interactively or headlessly</li> <li>Chain, nest, and resume computations without defining a custom graph DSL</li> </ul> <p>In this tutorial, you will:</p> <ol> <li>Start the AetherGraph server (the sidecar)</li> <li>Define your first <code>graph_fn</code></li> <li>Call an LLM and send messages through the channel</li> <li>Run it synchronously and see the result</li> </ol>"},{"location":"tutorials/t1-build-your-first-graph-fn/#1-boot-the-sidecar","title":"1. Boot the Sidecar","text":"<p>Before you run any agent, you must start the sidecar server, which wires up the runtime services such as channel communication, artifact storage, memory, and resumptions.</p> <pre><code>from aethergraph import start_server\n\nurl = start_server()  # launches a lightweight FastAPI server in the background\nprint(\"AetherGraph sidecar server started at:\", url)\n</code></pre> <p>The sidecar is safe to start anywhere \u2014 even in Jupyter or interactive shells. It sets up a workspace under <code>./aethergraph_data</code> by default. Your data, including artifacts, memory, resumption files, will all be exported to workspace for persist access.</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#2-define-a-minimal-agent","title":"2. Define a Minimal Agent","text":"<p>A <code>graph_fn</code> is a context\u2011injected async function that represents a reactive node or agent.</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"hello_world\")\nasync def hello_world(input_text: str, *, context: NodeContext):\n    context.logger().info(\"hello_world started\")\n\n    # Send a message via the default channel (console)\n    await context.channel().send_text(f\"\ud83d\udc4b Hello! You sent: {input_text}\")\n\n    # Optional: Call an LLM directly from the context\n    llm_text, _usage = await context.llm().chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"Be brief.\"},\n            {\"role\": \"user\", \"content\": f\"Say hi back to: {input_text}\"},\n        ]\n    )\n    await context.channel().send_text(f\"LLM replied: {llm_text}\")\n\n    output = input_text.upper()\n    context.logger().info(\"hello_world finished\")\n    return {\"final_output\": output}\n</code></pre> <p>Return value: Although you can return any data type in a dictionary, it is suggested to return a dictionary of JSON-serializable results (e.g. <code>{\"result\": value}</code>). For large data or binary files, save them via <code>context.artifacts().write(...)</code> and return the artifact path/uri instead for later reuse. </p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#key-concepts","title":"Key Concepts","text":"Concept Description <code>@graph_fn</code> Turns a plain async Python function into a context\u2011aware agent. <code>NodeContext</code> Injected automatically. Gives access to channels, memory, LLMs, and logging. <code>context.channel()</code> Sends and receives messages (console, Slack, web UI, etc.). <code>context.llm()</code> Unified interface to language models via environment configuration. <code>context.logger()</code> Node\u2011aware structured logging. <p>See the full API of <code>graph_fn</code> at Graph Function API</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#3-run-the-agent","title":"3. Run the Agent","text":"<p>You can run your <code>graph_fn</code> directly or use helper runners.</p> <pre><code>from aethergraph import run\n\nif __name__ == \"__main__\":\n    result = run(hello_world, inputs={\"input_text\": \"hello world\"})\n    print(\"Result:\", result)\n</code></pre> <p>Output example:</p> <pre><code>[AetherGraph] \ud83d\udc4b Hello! You sent: hello world\n[AetherGraph] LLM replied: Hi there!\nResult: {'final_output': 'HELLO WORLD'}\n</code></pre> <p>You can also <code>await hello_world(...)</code> in any async context \u2014 all <code>graph_fn</code>s are awaitable by design.</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#4-why-this-is-different","title":"4. Why This is Different","text":"<p>AetherGraph\u2019s <code>@graph_fn</code> model is Pythonic yet agentic. Unlike traditional workflow frameworks that require static DAG definitions, AetherGraph lets you:</p> <ul> <li>Run without pre\u2011declaring a graph \u2013 it dynamically builds one as you go.</li> <li>Access unified runtime services \u2013 channels, memory, artifacts, LLMs, and schedulers are all injected via context.</li> <li>Compose natively \u2013 you can <code>await</code> another <code>graph_fn</code>, mix <code>@tool</code>s, or parallelize with <code>asyncio.gather</code>.</li> <li>Stay resumable \u2013 everything you run is automatically backed by a persistent runtime; you can resume mid\u2011flow later.</li> </ul> <p>These traits make AetherGraph unique among Python agent frameworks \u2014 designed not only for chatbots, but also for scientific, engineering, and simulation workflows.</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#5-next-steps","title":"5. Next Steps","text":"<p>In the next tutorial, you\u2019ll learn how to turn these reactive functions into static DAGs using <code>graphify()</code>, enabling resumable and inspectable computation graphs.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/","title":"Tutorial 2: Static Graphs with <code>@graphify</code>","text":"<p><code>@graphify</code> turns a plain Python function into a graph builder. Instead of executing immediately (like <code>@graph_fn</code>), it builds a deterministic TaskGraph from <code>@tool</code> calls \u2014 a DAG you can inspect, persist, and run later.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#mental-model","title":"\ud83e\udded Mental Model","text":"<p><code>@graph_fn</code> \u2192 executes now (reactive, context\u2011rich)</p> <p><code>@graphify</code> \u2192 builds first, runs later (deterministic DAG)</p> <ul> <li>Each <code>@tool</code> call becomes a node in the DAG.</li> <li>Edges are formed by data flow and optional ordering via <code>_after=[\u2026]</code>.</li> <li>You get reproducibility, inspectability, and clean fan\u2011in/fan\u2011out.</li> </ul> <p>Note: Access runtime services (<code>channel</code>, <code>llm</code>, <code>memory</code>) through tools in static graphs. If you need direct <code>context.*</code> calls inline, use <code>@graph_fn</code>.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#1-key-rules-short","title":"1. Key Rules (short)","text":"<ul> <li>Only <code>@tool</code> calls are allowed as steps in a <code>@graphify</code> builder. Use plain Python only to wire values or format the graph (no side\u2011effects); such code will not appear as nodes.</li> <li>Build \u2260 Run. Calling a <code>@graphify</code> function returns a TaskGraph. Use a runner to execute it.</li> <li>Async supported. Tools can be sync or async; the runner provides both sync and async entry points.</li> <li>Resumption requires stable IDs. Give important nodes a fixed <code>*_id</code> and reuse the same <code>run_id</code> when resuming.</li> <li>Outputs: Return a dict of JSON\u2011serializable values for resumption. Large/binary data \u2192 save via <code>artifacts()</code> and return a reference. (Full rules live in the API page.)</li> </ul> <p>Related: <code>@graph_fn</code> can also emit an implicit graph when you call <code>@tool</code>s inside it. Use <code>_after</code> to enforce ordering there too, and inspect the last run\u2019s captured graph with <code>graph_fn.last_graph</code>.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#2-shapes-tools-graphify","title":"2. Shapes (tools &amp; graphify)","text":""},{"location":"tutorials/t2-static-graph-with-graphify/#tool-shape-suggested","title":"<code>@tool</code> shape (suggested)","text":"<pre><code>from aethergraph import tool\n\n@tool(name=\"load_csv\", outputs=[\"rows\"])            # names become handle fields. Name is optional running locally\ndef load_csv(path: str) -&gt; dict:                    # return dict matching outputs\n    # ... load and parse ...\n    return {\"rows\": rows}\n</code></pre> <ul> <li><code>@tool</code> can be sync or async, they all run as async internally. </li> <li>Declare <code>outputs=[...]</code>. Returned dict must contain those keys.</li> </ul>"},{"location":"tutorials/t2-static-graph-with-graphify/#graphify-shape-suggested","title":"<code>@graphify</code> shape (suggested)","text":"<pre><code>from aethergraph import graphify\n\n@graphify(name=\"etl\", inputs=[\"csv_path\"], outputs=[\"nrows\"])  # declarative I/O\ndef etl(csv_path: str):\n    raw = load_csv(path=csv_path)         # node\n    # ... add more tool calls ...\n    return {\"nrows\": len(raw.rows)}      # JSON-serializable outputs\n</code></pre> <ul> <li><code>graphify</code> is always a sync function. No <code>await</code> allowed inside the builder.</li> <li>Use <code>_after=...</code> to force ordering when no data edge exists.</li> <li>Calling <code>etl()</code> builds a <code>TaskGraph</code>; it does not run.</li> <li>Run using <code>run(...)</code> / <code>run_async(...)</code> with <code>inputs={...}</code>.</li> </ul>"},{"location":"tutorials/t2-static-graph-with-graphify/#3-minimal-example-build-run","title":"3. Minimal Example \u2014 Build \u2192 Run","text":"<pre><code>from aethergraph import graphify, tool\nfrom aethergraph.runner import run  \n\n@tool(outputs=[\"doubled\"])  \ndef double(x: int) -&gt; dict:\n    return {\"doubled\": x * 2}\n\n@tool(outputs=[\"shifted\"])  \ndef add_ten(x: int) -&gt; dict:\n    return {\"shifted\": x + 10}\n\n@graphify(name=\"tiny_pipeline\", inputs=[\"x\"], outputs=[\"y\"])\ndef tiny_pipeline(x: int):\n    a = double(x=x)                   # node A\n    b = add_ten(x=a.doubled)         # node B depends on A via data edge\n    return {\"y\": b.shifted}\n\n# Build (no execution yet)\nG = tiny_pipeline()                   # \u2192 TaskGraph\n\n# Run (sync helper, useful in Jupyter notebook)\nresult = run(G, inputs={\"x\": 7})\nprint(result)  # {'y': 24}\n</code></pre> <p>Try <code>max_concurrency=1</code> vs <code>&gt;1</code> in the runner if your tools are async and parallelizable.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#4-ordering-without-data-edges-_after","title":"4. Ordering Without Data Edges \u2014 <code>_after</code>","text":"<pre><code>@tool(outputs=[\"ok\"])  \ndef fetch() -&gt; dict: ...\n\n@tool(outputs=[\"done\"])\ndef train() -&gt; dict: ...\n\n@graphify(name=\"seq\", inputs=[], outputs=[\"done\"])\ndef seq():\n    a = fetch()\n    b = train(_after=a)               # force run-after without wiring data\n    return {\"done\": b.done}\n</code></pre> <ul> <li>Use a single node or a list <code>_after=[a, b]</code>.</li> </ul>"},{"location":"tutorials/t2-static-graph-with-graphify/#5-resume-a-run-stable-_id-run_id","title":"5. Resume a Run \u2014 Stable <code>_id</code> + <code>run_id</code>","text":"<p>Resumption lets you continue a partially-completed graph without redoing finished nodes. This is useful for flaky I/O or long pipelines.</p> <ul> <li>Assign deterministic IDs to nodes with <code>_id=\"...\"</code> in your tools.</li> <li>Reuse the same <code>run_id</code> when invoking the runner again.</li> <li>Indefinite waits (e.g., human input) are supported via dedicated wait tools and are covered in the Channels &amp; Wait Models tutorial\u2014this section uses a non\u2011channel example.</li> </ul> <pre><code>from aethergraph import graphify, tool\nfrom aethergraph.core.runtime.graph_runner import run_async\nimport random\n\n@tool(outputs=[\"ok\"])  \ndef prepare() -&gt; dict:\n    # Pretend to set up workspace/artifacts\n    return {\"ok\": True}\n\n@tool(outputs=[\"value\"])  \ndef flaky_compute(x: int) -&gt; dict:\n    # Simulate a transient failure half the time\n    if random.random() &lt; 0.5:\n        raise RuntimeError(\"transient error \u2014 try resuming\")\n    return {\"value\": x * 2}\n\n@tool(outputs=[\"ok\"])  \ndef finalize(v: int) -&gt; dict:\n    # Commit final result (e.g., write an artifact)\n    return {\"ok\": True}\n\n@graphify(name=\"resumable_pipeline\", inputs=[\"x\"], outputs=[\"y\"]) \ndef resumable_pipeline(x: int):\n    s1 = prepare(_id=\"prepare_1\")\n    s2 = flaky_compute(x=x, _after=s1, _id=\"flaky_2\")  # may fail on first run\n    s3 = finalize(v=s2.value, _after=s2, _id=\"finalize_3\")\n    return {\"y\": s2.value}\n\n# First run may fail while computing 'flaky_2'...\n# await run_async(resumable_pipeline(), inputs={\"x\": 21}, run_id=\"run-abc\")\n\n# Re-run with the SAME run_id to resume from the failed node (prepare_1 is skipped):\n# await run_async(resumable_pipeline(), inputs={\"x\": 21}, run_id=\"run-abc\")\n</code></pre> <p>Keep <code>_id</code>s stable to allow the engine to match nodes. If a node fails or is interrupted, resuming with the same <code>run_id</code> will continue from the last successful checkpoint.</p> <p>Use json-serializable output in <code>@tool</code> so that Aethergraph can reload previous outputs; otherwise resumption may fail.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#6-inspect-beforeafter-running","title":"6. Inspect Before/After Running","text":"<p>Once you have a <code>TaskGraph</code> (e.g., <code>G = tiny_pipeline()</code>), you can:</p> <pre><code>print(G.pretty())           # readable node table\nprint(G.ascii_overview())   # compact topology\nprint(G.topological_order())\n\n# Graph metadata\nsig  = tiny_pipeline.io()   # declared inputs/outputs\nspec = tiny_pipeline.spec() # full GraphSpec (nodes, edges, metadata)\n\n# Export (if enabled)\ndot = G.to_dot()            # Graphviz DOT text\n# G.visualize()             # render to image if your env supports it\n</code></pre>"},{"location":"tutorials/t2-static-graph-with-graphify/#7-practical-tips","title":"7. Practical Tips","text":"<ul> <li>Keep nodes small and typed: expose clear outputs (e.g., <code>outputs=[\"clean\"]</code>).</li> <li>Use JSON\u2011serializable returns; store big/binary as artifacts.</li> <li>Prefer <code>_after</code> for control edges instead of fake data plumb\u2011through.</li> <li>No nested static graphs (don\u2019t call one <code>@graphify</code> from another). Use tools or run graphs separately.</li> <li>Async tools + <code>max_concurrency</code> unlock parallel speedups.</li> </ul>"},{"location":"tutorials/t2-static-graph-with-graphify/#8-summary","title":"8. Summary","text":"<ul> <li><code>@graphify</code> materializes a static DAG from <code>@tool</code> calls.</li> <li>Build with the function call; run with the runner (sync or async).</li> <li>For resumption, use stable <code>_id</code> per node and replay with the same <code>run_id</code>.</li> <li>Inspect graphs via <code>pretty()</code>, <code>ascii_overview()</code>, <code>.io()</code>, <code>.spec()</code>, and <code>to_dot()</code>.</li> </ul> <p>Use <code>@graphify</code> for pipelines and reproducible experiments; stick with <code>@graph_fn</code> for interactive, context\u2011heavy agents.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/","title":"Tutorial 3: Talk to Your Graph \u2014 Channels and Waits","text":"<p>This tutorial explains how your graph talks back \u2014 how agents communicate with the outside world and how different kinds of waits work under the hood. You\u2019ll learn the difference between cooperative waits (for live, stateless agents) and dual-stage waits (for resumable workflows), and how to use each effectively.</p> <p>Goal: Understand how channels unify I/O, and why only <code>@graphify</code> with dual-stage waits can resume safely after a crash.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#1-what-is-a-channel","title":"1. What Is a Channel?","text":"<p>A channel is your agent\u2019s communication route \u2014 Slack, Telegram, Web UI, or Console. It lets your code send messages, request input, and stream updates through a consistent API.</p> <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"greet\")\nasync def greet(*, context):\n    ch = context.channel() \n    await ch.send_text(\"Starting demo\u2026\")\n    name = await ch.ask_text(\"Your name?\")    # cooperative wait\n    await ch.send_text(f\"Nice to meet you, {name}!\")\n    return {\"user\": name}\n</code></pre> <ul> <li>The <code>context.channel()</code> method returns a <code>ChannelSession</code> helper with async methods like <code>send_text</code>, <code>ask_text</code>, <code>ask_approval</code>, <code>ask_files</code>, <code>stream</code>, and <code>progress</code>.</li> <li>If no channel is configured, it falls back to the console (<code>console:stdin</code>).</li> </ul> <p>\ud83d\udca1 Channel setup and adapter configuration (Slack, Telegram, Web) are covered in Channel Setup.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#2-the-two-wait-models","title":"2. The Two Wait Models","text":"<p>AetherGraph supports two wait mechanisms \u2014 cooperative and dual-stage \u2014 both built on the continuation system but with very different lifecycles.</p> <p></p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#cooperative-waits-via-contextchannel-methods","title":"Cooperative waits \u2014 via <code>context.channel()</code> methods","text":"<ul> <li>Implemented by <code>ChannelSession</code> (<code>ask_text</code>, <code>ask_approval</code>, <code>ask_files</code>, etc.).</li> <li>Work inside a running process \u2014 the node suspends, then resumes when the reply arrives.</li> <li>These waits are stateful for inspection, but not resumable; if the process dies, the session is lost.</li> <li>Used mainly in <code>@graph_fn</code> agents, which execute immediately and stay alive.</li> </ul>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#dual-stage-waits-via-built-in-channel-tools","title":"Dual-stage waits \u2014 via built-in channel tools","text":"<ul> <li>Implemented as @tool nodes in <code>aethergraph.tools</code> (<code>ask_text</code>, <code>send_text</code>, etc.).</li> <li>Each wait becomes a graph node stored in the runtime snapshot.</li> <li>Can pause indefinitely and resume after restarts using <code>run_id</code> in <code>@graphify</code>.</li> <li>Used in <code>@graphify</code> graphs, which are strictly persisted and versioned.</li> </ul> <p>\u26a0\ufe0f All built-in dual-stage methods are <code>@tool</code>s \u2014 do not call them inside another tool. They are meant for use in graphify or top-level graph_fn logic, not nested nodes.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#3-lifecycle-and-persistence","title":"3. Lifecycle and Persistence","text":"Concept <code>@graph_fn</code> + Cooperative Waits <code>@graphify</code> + Dual-Stage Waits Execution Runs immediately (reactive) Builds DAG, runs with scheduler State Stateful for in-process waits Snapshot persisted to disk or DB Wait behavior Cooperative (in-process) Dual-stage (resumable) Resume after crash \u274c Lost, consider saving progress in memory and sementic recovery \u2705 Recoverable with <code>run_id</code>  and stable <code>node_id</code>; set up <code>_id</code> when building the graph <p>You can also use the <code>context.channel()</code> method in <code>@graphify</code> for convenience within a <code>@tool</code>, or use dual-stage wait tools in <code>graph_fn</code>. However, these approaches cannot guarantee resumption due to the stateful nature of the method or graph. Caveat for console dual-stage tools: Console input is handled differently, and dual-stage waits do not support resumption for console channels. However, it is rare for a local process using the console to terminate unexpectedly.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#4-cooperative-wait-example","title":"4. Cooperative Wait Example","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"cooperative_wait\")\nasync def cooperative_wait(*, context):\n    ch = context.channel()\n    await ch.send_text(\"Processing...\")\n    ans = await ch.ask_approval(\"Continue?\", options=[\"Yes\", \"No\"])\n    if ans[\"approved\"]:\n        await ch.send_text(\"\u2705 Proceeding.\")\n    else:\n        await ch.send_text(\"\u274c Stopped.\")\n    return {\"ok\": ans[\"approved\"]}\n</code></pre> <ul> <li>Perfect for short-lived interactive runs.</li> <li>Not resumable if interrupted; all state is lost when the process exits.</li> <li>Consider saving states to memory for sementic recovery for non-critical tasks.</li> </ul>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#5-dual-stage-wait-example-resumable","title":"5. Dual-Stage Wait Example (Resumable)","text":"<pre><code>from aethergraph import graphify\nfrom aethergraph.tools import send_text, ask_text # built-in `@tool`. Do not use them in anohter `@tool`\n\n@graphify(name=\"dual_stage_greet\", inputs=[\"channel\"], outputs=[\"greeting\"])\ndef dual_stage_greet(channel: str):\n    a = send_text(text=\"Hello!\", channel=channel, _id=\"start\")\n    b = ask_text(prompt=\"What's your name?\", channel=channel, _after=a, _id=\"wait_name\")\n    c = send_text(text=f\"Hi {b.text}!\", channel=channel, _after=b, _id=\"reply\")\n    return {\"greeting\": c.text}\n</code></pre> <ul> <li>Each step is a tool node with a unique <code>_id</code>.</li> <li>If the process stops after <code>ask_text</code>, simply rerun with the same <code>run_id</code> to resume.</li> <li>The system restores from the last persisted snapshot.</li> </ul>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#key-takeaways","title":"Key Takeaways","text":"<ul> <li><code>context.channel()</code> methods implement cooperative waits \u2014 great for live agents.</li> <li>Built-in channel tools (<code>ask_text</code>, <code>send_text</code>, etc.) implement dual-stage waits \u2014 required for resumable graphs.</li> <li><code>graph_fn</code> is stateless, inspectable via <code>.last_graph</code> but not recoverable.</li> <li><code>graphify</code> uses snapshots to persist progress and enable recovery with <code>run_id</code>.</li> <li>Dual-stage tools are <code>@tool</code> nodes \u2014 never call them inside another tool.</li> </ul> <p>Channels make your graph talk. Wait models decide how long it remembers the conversation.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/","title":"Tutorial 4: Store &amp; Recall \u2014 Artifacts and Memory","text":"<p>This tutorial focus on the how Aethergraph can memorize what happened before. Learn how to save outputs, log results/events, and recall them later using AetherGraph\u2019s two persistence pillars:</p> <ul> <li>Artifacts \u2014 durable assets (files/dirs/JSON/text) stored by content address (CAS URI) with labels &amp; metrics for ranking and search.</li> <li>Memory \u2014 a structured event &amp; result log with fast \u201cwhat\u2019s the latest?\u201d simple recent\u2011history queries.</li> </ul> <p>We\u2019ll build this up step\u2011by\u2011step with short, copy\u2011ready snippets.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#0-what-youll-use","title":"0. What you\u2019ll use","text":"<pre><code># Access services from your NodeContext\narts = context.artifacts()   # artifact store\nmem  = context.memory()      # event &amp; result log\n</code></pre> <p>Mental model: Artifacts hold large, immutable outputs. Memory records what happened and the small named values you need to recall quickly.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#1-save-something-get-a-uri-open-it-later","title":"1. Save something \u2192 get a URI \u2192 open it later","text":""},{"location":"tutorials/t4-store-recall-artifacts-memory/#a-ingest-an-existing-file","title":"A. Ingest an existing file","text":"<pre><code>art = await arts.save_file(\n    path=\"/tmp/report.pdf\",\n    kind=\"report\",                 # a short noun; you\u2019ll filter/rank by this\n    labels={\"exp\": \"A\"},           # 1\u20133 filters you actually plan to query\n    # metrics={\"bleu\": 31.2},      # optional if you\u2019ll rank later\n)\n\n# When you need a real path again:\npath = await arts.as_local_file(art)\n</code></pre> <p>Why CAS? It prevents accidental overwrites and gives you a stable handle you can pass around (in Memory, dashboards, etc.).</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#b-streamwrite-no-temp-file-atomically","title":"B. Stream\u2011write (no temp file), atomically","text":"<pre><code>async with arts.writer(kind=\"plot\", planned_ext=\".png\") as w:\n    w.write(png_bytes)\n# on exit \u2192 the artifact is committed and indexed\n</code></pre> <p>Tip: Prefer <code>writer(...)</code> for programmatically produced bytes.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#2-record-results-youll-want-to-recall-fast","title":"2. Record results you\u2019ll want to recall fast","text":"<p>Use Memory for structured results and lightweight logs.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#a-record-a-typed-result-fast-recall-by-name","title":"A. Record a typed result (fast recall by name)","text":"<pre><code>await mem.record_tool_result(\n    tool=\"train.step\",\n    outputs=[\n        {\"name\": \"val_acc\",  \"kind\": \"number\", \"value\": 0.912},\n        {\"name\": \"ckpt_uri\", \"kind\": \"uri\",    \"value\": uri},\n    ],\n)\n\nrecent = await mem.recent_tool_results(tool=\"train.step\", limit=10) # retrieve the last tool result events\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#b-log-arbitrary-events-structured-but-lightweight","title":"B. Log arbitrary events (structured but lightweight)","text":"<pre><code>await mem.record(\n    kind=\"train_log\",\n    data={\"epoch\": 1, \"loss\": 0.25, \"acc\": 0.91},\n)\n\nrecent = await mem.recent(kinds=[\"train_log\"], limit=3) # recent is an Event\n</code></pre> <p>You will need to load the data from seriazalized <code>recent.text</code> (<code>channel.memory()</code> docs)</p> <p>Need only the decoded payloads?</p> <pre><code>logs = await mem.recent_data(kinds=[\"train_log\"], limit=3) # this returns the `data` saved in memory, not Event\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#3-search-filter-and-rank-artifacts","title":"3. Search, filter, and rank artifacts","text":""},{"location":"tutorials/t4-store-recall-artifacts-memory/#a-search-by-labels-you-saved-earlier","title":"A. Search by labels you saved earlier","text":"<pre><code>hits = await arts.search(\n    kind=\"report\",\n    labels={\"exp\": \"A\"},    # exact\u2011match filter across indexed labels\n)\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#b-pick-best-so-far-by-a-metric","title":"B. Pick \u201cbest so far\u201d by a metric","text":"<pre><code>best = await arts.best(\n    kind=\"checkpoint\",\n    metric=\"val_acc\",   # must exist in artifact.metrics\n    mode=\"max\",         # or \"min\"\n    scope=\"run\",        # limit to current run | graph | node\n)\nif best:\n    best_path = await arts.as_local_file(best)\n</code></pre> <p>Attach <code>metrics={\"val_acc\": ...}</code> when saving to enable ranking later.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#4-practical-recipes","title":"4. Practical recipes","text":""},{"location":"tutorials/t4-store-recall-artifacts-memory/#a-save-small-jsontext-directly","title":"A. Save small JSON/Text directly","text":"<pre><code>cfg_art = await arts.save_json({\"lr\": 1e-3, \"batch\": 64})\nlog_art = await arts.save_text(\"training finished ok\")\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#b-browse-everything-produced-in-this-run","title":"B. Browse everything produced in this run","text":"<pre><code>all_run_outputs = await arts.list(scope=\"run\")\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#c-pin-something-to-keep-forever","title":"C. Pin something to keep forever","text":"<pre><code>await arts.pin(artifact_id=cfg_art.artifact_id)\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#d-keep-memory-lean-but-persistent","title":"D. Keep Memory lean but persistent","text":"<ul> <li>Memory acts like a fixed\u2011length hot queue for fast recall (<code>last_by_name</code>, <code>recent</code>).</li> <li>All events are persisted for later inspection, but only a rolling window stays hot in KV for speed.</li> </ul>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#5-minimal-reference-schemas-helpers","title":"5. Minimal reference (schemas &amp; helpers)","text":"<p>You rarely need all fields. Here are the useful bits to recognize in code and logs.</p> <pre><code>@dataclass\nclass Artifact:\n    artifact_id: str\n    uri: str           # CAS URI\n    kind: str          # short noun (e.g., \"checkpoint\", \"report\")\n    labels: dict[str, Any]\n    metrics: dict[str, Any]\n    preview_uri: str | None = None\n    pinned: bool = False\n</code></pre> <pre><code>@dataclass\nclass Event:\n    event_id: str\n    ts: str\n    kind: str          # e.g., \"tool_result\", \"train_log\"\n    topic: str | None = None\n    inputs: list[Value] | None = None\n    outputs: list[Value] | None = None\n    metrics: dict[str, float] | None = None\n    text: str | None = None   # JSON string or message text\n    version: int = 1\n</code></pre> <p>Helper (already built\u2011in) that returns decoded payloads from <code>recent</code>:</p> <pre><code>async def recent_data(*, kinds: list[str], limit: int = 50) -&gt; list[Any]\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#6-when-to-use-what","title":"6. When to use what","text":"Need Use Why Keep a file/dir for later <code>artifacts.save_file(...)</code> / <code>writer(...)</code> Durable, deduped, indexed Store small JSON/Text <code>artifacts.save_json/text</code> Convenience, still indexed Log structured progress/events <code>memory.record</code> \u2192 <code>recent</code>/<code>recent_data</code> Lightweight trace Pick the best checkpoint/report <code>artifacts.best(kind, metric, mode)</code> Built\u2011in ranking List everything from current run <code>artifacts.list(scope=\"run\")</code> One\u2011liner browse"},{"location":"tutorials/t5-add-intelligence-llm-rag/","title":"Tutorial 5: Add Intelligence \u2014 LLM &amp; RAG","text":"<p>This tutorial adds language models and retrieval\u2011augmented generation (RAG) to your agents. You\u2019ll:</p> <ol> <li>set up an LLM profile</li> <li>chat from a graph function</li> <li>build a searchable RAG corpus from your files/memory</li> <li>answer questions grounded by retrieved context (with optional citations)</li> </ol> <p>Works with OpenAI, Anthropic, Google (Gemini), OpenRouter, LM Studio, and Ollama via a unified GenericLLMClient.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#0-mental-model","title":"0. Mental model","text":"<ul> <li>LLM: a provider\u2011agnostic client you access via <code>context.llm(...)</code> for chat and embeddings.</li> <li>RAG: a corpus of documents (from files and/or Memory events) that are chunked, embedded, and retrieved to ground LLM answers.</li> </ul> <pre><code>llm = context.llm()    # chat &amp; embed &amp; image generation\nrag = context.rag()    # corpora, upsert, search, answer\n</code></pre>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>API keys for the providers you want (e.g., OpenAI, Anthropic, Gemini, OpenRouter).</li> <li>If using local models: LM Studio or Ollama running locally and a base URL.</li> </ul>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#2-configure-llms-profiles","title":"2. Configure LLMs (Profiles)","text":"<p>You can configure profiles in environment variables (recommended) or at runtime. See docs for complete setup method.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#a-env-profiles-recommended","title":"A) <code>.env</code> profiles (recommended)","text":"<p>Profiles are named by the section after <code>LLM__</code>. Example: a profile called <code>MY_OPENAI</code>:</p> <pre><code>AETHERGRAPH_LLM__MY_OPENAI__PROVIDER=openai\nAETHERGRAPH_LLM__MY_OPENAI__MODEL=gpt-4o-mini\nAETHERGRAPH_LLM__MY_OPENAI__TIMEOUT=60\nAETHERGRAPH_LLM__MY_OPENAI__API_KEY=sk-...\nAETHERGRAPH_LLM__MY_OPENAI__EMBED_MODEL=text-embedding-3-small  # needed for llm().embed() or RAG\n</code></pre> <p>Then in code:</p> <pre><code>llm = context.llm(profile=\"my_openai\")\ntext, usage = await llm.chat([...])\n</code></pre> <p>The default profile comes from your container config. Use profiles when you want to switch providers/models per node or per run.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#b-register-at-runtime-programmatic","title":"B) Register at runtime (programmatic)","text":"<p>Useful for notebooks/demos or dynamically wiring services:</p> <pre><code>from aethergraph.llm import register_llm_client, set_rag_llm_client\n\nclient = register_llm_client(\n    profile=\"runtime_openai\",\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    api_key=\"sk-...\",\n)\n\n# RAG can use a dedicated LLM (for embedding + answering). If not set, it uses the default profile.\nset_rag_llm_client(client=client)\n</code></pre> <p>You can also pass parameters directly to <code>set_rag_llm_client(provider=..., model=..., embed_model=..., api_key=...)</code>.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#c-oneoff-key-injection","title":"C) One\u2011off key injection","text":"<p>If you just need to override a key in memory for a demo:</p> <pre><code>context.llm_set_key(provider=\"openai\", api_key=\"sk-...\")\n</code></pre> <p>Sidecar note: If your run needs channels, resumable waits, or shared services, start the sidecar server before using runtime registration.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#3-chat-embed-from-a-graph-function","title":"3. Chat &amp; Embed from a Graph Function","text":""},{"location":"tutorials/t5-add-intelligence-llm-rag/#chat-provideragnostic","title":"Chat (provider\u2011agnostic)","text":"<pre><code>@graph_fn(name=\"ask_llm\")\nasync def ask_llm(question: str, *, context):\n    llm = context.llm(profile=\"my_openai\")  # or omit profile for default\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are concise and helpful.\"},\n        {\"role\": \"user\",   \"content\": question},\n    ]\n    reply, usage = await llm.chat(messages)\n    return {\"answer\": reply, \"usage\": usage}\n</code></pre>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#embeddings","title":"Embeddings","text":"<pre><code>vectors = await context.llm(profile=\"my_openai\").embed([\n    \"First text chunk\", \"Second text chunk\"\n])\n</code></pre> <p>RAG needs an embed model configured on the chosen profile.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#optional-knobs","title":"Optional knobs","text":"<p>The <code>chat()</code> API allows various parameters for reasoning, json output etc. See the API reference for detailed usage. </p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#4-raw-api-escape-hatch","title":"4. Raw API escape hatch","text":"<p>For power users who need endpoints not yet covered by the high\u2011level client (such as low-level inputs, VLM models, custom models):</p> <pre><code>openai = context.llm(profile=\"my_openai\")\npayload = {\n    \"model\": \"gpt-4o-mini\",\n    \"input\": [\n        {\"role\": \"system\", \"content\": \"You are concise.\"},\n        {\"role\": \"user\",   \"content\": \"Explain attention in one sentence.\"}\n    ],\n    \"max_output_tokens\": 128,\n    \"temperature\": 0.3,\n}\nraw = await openai.raw(path=\"/responses\", json=payload)\n</code></pre> <ul> <li><code>raw(path=..., json=...)</code> sends a verbatim request to the provider base URL.</li> <li>You are responsible for parsing the returned JSON shape.</li> </ul> <p>Use this when experimenting with new provider features before first\u2011class support lands in the client.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#5-rag-from-docs-memory-to-grounded-answers","title":"5. RAG: From Docs &amp; Memory to Grounded Answers","text":"<p>Flow: <code>Files/Events \u2192 chunk + embed \u2192 index \u2192 retrieve top\u2011k \u2192 LLM answers with context</code>.</p> <ul> <li>Corpora live behind <code>context.rag()</code>.</li> <li>Ingest files (by path) and inline text, and/or promote Memory events into a corpus.</li> </ul>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#a-backend-storage","title":"A) Backend &amp; storage","text":"<p>Default vector index: SQLite (local, zero\u2011dep) \u2014 great for laptops and small corpora.</p> <p>Switch to FAISS: faster ANN search for larger corpora.</p> <p>Set up RAG backend: </p> <ul> <li>Env:</li> </ul> <pre><code># RAG Settings\nAETHERGRAPH_RAG__BACKEND=faiss        # or sqlite\nAETHERGRAPH_RAG__DIM=1536             # embedding dimension (e.g., OpenAI text-embedding-3-small)\n</code></pre> <ul> <li>Runtime:</li> </ul> <pre><code>from aethergraph.services.rag import set_rag_index_backend\n\nset_rag_index_backend(backend=\"faiss\", dim=1536)\n# If FAISS is not installed, it logs a warning and falls back to SQLite automatically.\n</code></pre> <ul> <li>On\u2011disk layout: each corpus stores <code>corpus.json</code>, <code>docs.jsonl</code>, <code>chunks.jsonl</code>; source files are saved as Artifacts for provenance.</li> </ul>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#b-build-update-a-corpus-from-files-text","title":"B) Build / update a corpus from files &amp; text","text":"<pre><code>await context.rag().upsert_docs(\n    corpus_id=\"my_docs\",\n    docs=[\n        {\"path\": \"data/report.pdf\", \"labels\": {\"type\": \"report\"}},\n        {\"text\": \"Experiment hit 91.2% accuracy on CIFAR-10.\", \"title\": \"exp-log\"},\n    ],\n)\n</code></pre> <ul> <li> <p>Use file docs when you already have a local file: <code>{\"path\": \"/abs/or/relative.ext\", \"labels\": {...}}</code>. Supported \u201csmart-parsed\u201d types are <code>.pdf</code>, <code>.md/markdown</code>, and <code>.txt</code> (others are treated as plain text). The original file is saved as an Artifact for provenance; if your PDF is a scan, run OCR first (we only extract selectable text). </p> </li> <li> <p>Use inline docs when you have content in memory: <code>{\"text\": \"...\", \"title\": \"nice-short-title\", \"labels\": {...}}</code>. Keep titles short and meaningful; add 1\u20133 optional labels you\u2019ll actually filter by (e.g., <code>{\"source\":\"lab\", \"week\":2}</code>).</p> </li> </ul> <p>Behind the scenes: documents are stored as Artifacts, parsed, chunked, embedded, and added to the vector index.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#c-search-retrieve-answer-with-citations","title":"C) Search, retrieve, answer (with citations)","text":"<pre><code>hits = await context.rag().search(\"my_docs\", \"key findings\", k=8, mode=\"hybrid\")\nans  = await context.rag().answer(\n    corpus_id=\"my_docs\",\n    question=\"Summarize the main findings and list key metrics.\",\n    style=\"concise\",\n    with_citations=True,\n    k=6,\n)\n# ans \u2192 { \"answer\": str, \"citations\": [...], \"resolved_citations\": [...], \"usage\": {...} }\n</code></pre> <p>Use <code>resolved_citations</code> to map snippets back to Artifact URIs for auditability.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#d-choosing-the-llm-for-rag","title":"D) Choosing the LLM for RAG","text":"<p>RAG uses a dedicated RAG LLM client that must have both <code>model</code> and <code>embed_model</code> set.</p> <p>Runtime:</p> <pre><code>from aethergraph.llm import set_rag_llm_client\nset_rag_llm_client(provider=\"openai\", model=\"gpt-4o-mini\", embed_model=\"text-embedding-3-small\", api_key=\"sk-\u2026\")\n</code></pre> <p>If you don\u2019t set one, it falls back to the default LLM profile (ensure that profile also has an <code>embed_model</code>).</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#e-corpus-management-ops","title":"E) Corpus management (ops)","text":"<p>For maintenance and ops you can:</p> <ul> <li>List corpora / docs to inspect what\u2019s indexed.</li> <li>Delete docs to remove vectors and records.</li> <li>Re\u2011embed to refresh vectors after changing embed model or chunking.</li> <li>Stats to view counts of docs/chunks and corpus metadata.</li> </ul> <p>These live on the same facade: <code>rag.list_corpora()</code>, <code>rag.list_docs(...)</code>, <code>rag.delete_docs(...)</code>, <code>rag.reembed(...)</code>, <code>rag.stats(...)</code>. See API reference for details.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#6-practical-recipes","title":"6. Practical recipes","text":"<ul> <li>Switch providers by changing <code>profile=</code> in <code>context.llm(...)</code> without touching your code elsewhere.</li> <li>Save docs as Artifacts (e.g., <code>save_text</code>, <code>save(path=...)</code>) and ingest by <code>{\"path\": local_path}</code> so RAG can cite their URIs.</li> </ul>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#summary","title":"Summary","text":"<ul> <li>Configure LLM profiles via <code>.env</code> or runtime registration, then use <code>llm.chat()</code> / <code>llm.embed()</code>.</li> <li>Build RAG corpora from files and Memory events, then call <code>rag.answer(..., with_citations=True)</code> for grounded responses.</li> </ul>"},{"location":"tutorials/t6-use-external-tools-mcp/","title":"Tutorial 6: Use External Tools \u2014 The MCP Example","text":"<p>AetherGraph supports external tool integration via the Model Context Protocol (MCP) \u2014 a simple JSON\u2011RPC 2.0 interface for listing and calling tools, reading resources, and managing structured outputs from remote services. In short, MCP lets your graph talk to anything that can expose a compliant interface: local CLI utilities, web services, or even another AI system.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#1-what-is-mcp","title":"1. What Is MCP?","text":"<p>The Model Context Protocol defines a standard way for an AI or orchestration framework to:</p> <ul> <li>List tools that an external process or service provides.</li> <li>Call those tools with structured arguments.</li> <li>List or read resources, such as files, datasets, or model outputs.</li> </ul> <p>AetherGraph\u2019s <code>MCPService</code> provides a unified layer for managing multiple MCP clients \u2014 e.g. a local subprocess (<code>StdioMCPClient</code>), a WebSocket endpoint (<code>WsMCPClient</code>), or an HTTP service (<code>HttpMCPClient</code>).</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#each-mcp-client-conforms-to-a-minimal-contract","title":"Each MCP client conforms to a minimal contract:","text":"<pre><code>class MCPClientProtocol:\n    async def list_tools(self) -&gt; List[MCPTool]: ...\n    async def call(self, tool: str, params: Dict[str, Any]) -&gt; Dict[str, Any]: ...\n    async def list_resources(self) -&gt; List[MCPResource]: ...\n    async def read_resource(self, uri: str) -&gt; Dict[str, Any]: ...\n</code></pre> <p>You can register many clients under names (e.g. <code>default</code>, <code>local</code>, <code>remote</code>), and access them via <code>context.mcp(name)</code>.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#2-minimal-example-query-a-local-mcp-tool","title":"2. Minimal Example \u2014 Query a Local MCP Tool","text":"<p>Suppose you have a local script or service exposing MCP over stdio. You can wrap it with <code>StdioMCPClient</code>:</p> <pre><code>from aethergraph.services import StdioMCPClient, MCPService\n\n# Initialize the service manually (usually handled by container)\nclient = StdioMCPClient([\"python\", \"my_mcp_server.py\"])\nmcp = MCPService({\"default\": client})\n\n# Example call to a tool named \"summarize_text\"\nasync def main():\n    await mcp.open(\"default\")\n    tools = await mcp.list_tools(\"default\")\n    print(\"Available tools:\", [t.name for t in tools])\n\n    result = await mcp.call(\"default\", \"summarize_text\", {\"text\": \"Hello MCP!\"})\n    print(\"Result:\", result)\n\nasyncio.run(main())\n</code></pre> <p>This works the same if you use a WebSocket or HTTP\u2011based MCP server \u2014 just replace the client class.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#3-inside-aethergraph-access-via-nodecontext","title":"3. Inside AetherGraph \u2014 Access via NodeContext","text":"<p>In real usage, you need to register the MCP service after starting the server:</p> <pre><code>from aethergraph import start_server\nfrom aethergraph.services import StdioMCPClient, MCPService\nfrom aethergraph.runtime import register_mcp_client \n\nstart_server()\nclient = StdioMCPClient([\"python\", \"my_mcp_server.py\"])\n\nregister_mcp_client(\"default\", client=client) # accessed by context.mcp(\"default)\n</code></pre> <p>The <code>NodeContext</code> injects it automatically, so any <code>graph_fn</code> or <code>@tool</code> can access it:</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"mcp_query\")\nasync def mcp_query(*, context: NodeContext):\n    # List tools available on the default MCP server\n    tools = await context.mcp(\"default\").list_tools()\n    context.logger().info(f\"Available MCP tools: {[t.name for t in tools]}\")\n\n    # Call a specific tool\n    result = await context.mcp(\"default\").call(\"summarize_text\", {\"text\": \"Explain MCP in one line.\"})\n    await context.channel().send_text(f\"Summary: {result['summary']}\")\n\n    return {\"result\": result}\n</code></pre> <p>Run this function via <code>run(graph, inputs)</code> or within a larger workflow \u2014 it will connect automatically to the configured MCP client.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#4-when-to-use-mcp","title":"4. When to Use MCP","text":"<p>MCP is useful when you want to:</p> <ul> <li>Bridge external AI systems (like a local LM Studio model or an in\u2011house LLM server) into AetherGraph.</li> <li>Integrate existing Python tools or APIs without writing new wrappers.</li> <li>Query live data services (e.g., weather, finance, or database APIs) through a JSON\u2011RPC layer.</li> </ul> <p>Because MCP uses async JSON\u2011RPC, it can easily be multiplexed across multiple nodes or graphs \u2014 even concurrently.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#5-available-clients","title":"5. Available Clients","text":"Client Type Class Transport Use Case Stdio <code>StdioMCPClient</code> stdin/stdout subprocess Run local CLI tools WebSocket <code>WsMCPClient</code> persistent WS channel Long\u2011lived AI services HTTP <code>HttpMCPClient</code> RESTful endpoint Web APIs with JSON\u2011RPC routes <p>You can register any number of them:</p> <p><pre><code>from aethergraph.services.mcp.ws_client import WsMCPClient\nmcp = MCPService({\n    \"default\": WsMCPClient(\"wss://example.com/mcp\"),\n    \"local\": StdioMCPClient([\"python\", \"my_local_tool.py\"])\n})\n</code></pre> See all MCP-related methods in API reference</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#6-combined-example-http-tool-call-in-graph","title":"6. Combined Example \u2014 HTTP Tool Call in Graph","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"fetch_weather\")\nasync def fetch_weather(city: str, *, context: NodeContext):\n    # Connect to HTTP MCP backend\n    result = await context.mcp(\"default\").call(\"get_weather\", {\"city\": city})\n\n    report = result.get(\"report\", \"No data returned.\")\n    await context.channel().send_text(f\"Weather in {city}: {report}\")\n    return {\"city\": city, \"report\": report}\n</code></pre> <p>This looks like any other AetherGraph node \u2014 but the heavy lifting happens externally. MCP makes any compliant server a first\u2011class citizen in your graphs.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#7-notes-tips","title":"7. Notes &amp; Tips","text":"<ul> <li>Auto\u2011Reconnect: MCP clients auto\u2011reopen when disconnected, so you can safely call <code>context.mcp(\"local\")</code> multiple times.</li> <li>Multiple Servers: You can connect to multiple MCPs simultaneously for different tool domains.</li> </ul> <ul> <li>Chained Tools: Results from MCP calls are just Python dicts \u2014 they can be piped to other <code>@tool</code>s or stored as artifacts for later retrieval.</li> </ul>"},{"location":"tutorials/t6-use-external-tools-mcp/#summary","title":"Summary","text":"<p>MCP integration turns AetherGraph into a universal agent\u2011to\u2011agent protocol bridge. You can:</p> <ol> <li>Connect external AI or data tools via stdio, WebSocket, or HTTP.</li> <li>Access them with one unified API: <code>context.mcp(name)</code>.</li> <li>Call, list, and read resources without writing custom adapters.</li> </ol> <p>In the next section, we\u2019ll explore Extending Services \u2014 showing how to register your own MCP\u2011like service or log LLM prompts for inspection.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/","title":"Tutorial 7: Plug In Your World \u2014 Custom Services","text":"<p>Sometimes your agents need to talk to the rest of your world\u2014clusters, databases, storage systems, internal APIs. Instead of wiring that logic into every <code>graph_fn</code>, AetherGraph lets you attach custom services to the <code>context</code> object:</p> <pre><code># Later inside a graph or tool\nawait context.trainer().submit(spec)\nawait context.storage().put(\"/tmp/report.pdf\")\nstatus = await context.tracker().job_status(job_id)\n</code></pre> <p>This tutorial shows how to:</p> <ol> <li>Define a small service class (just Python).</li> <li>Register it so it appears as <code>context.&lt;name&gt;()</code>.</li> <li>Use it from <code>graph_fn</code> / <code>@tool</code> code.</li> <li>Apply practical patterns (HPC jobs, storage, external APIs).</li> </ol> <p>Goal: keep agent logic clean and move integration glue into reusable, testable services.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#1-what-is-a-custom-service-really","title":"1. What is a Custom Service, Really?","text":"<p>A custom service is a long\u2011lived Python object the runtime injects into every <code>NodeContext</code> under a chosen name.</p> <p>Once registered, it works anywhere:</p> <pre><code>@graph_fn(name=\"demo_trainer\")\nasync def demo_trainer(*, context):\n    job_id = await context.trainer().submit({\"epochs\": 10})\n    return {\"job_id\": job_id}\n</code></pre> <p>Key properties</p> <ul> <li>Named entrypoint \u2014 you choose the accessor (e.g., <code>trainer</code>, <code>storage</code>, <code>models</code>).</li> <li>Shared instance \u2014 one instance reused across nodes/runs (unless you design otherwise).</li> <li>Context\u2011aware \u2014 methods can access the current <code>NodeContext</code> (<code>run_id</code>, <code>graph_id</code>, <code>node_id</code>).</li> <li>Async\u2011first \u2014 works naturally with <code>await</code> and the event loop.</li> </ul> <p>Use a service when you have state or connectivity to share: clients, pools, caches, queues, background workers. For pure functions, a regular module is fine.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#2-minimal-service-from-zero-to-contexttrainer","title":"2. Minimal Service: from Zero to <code>context.trainer()</code>","text":""},{"location":"tutorials/t7-extend-runtime-custom-services/#step-1-define-a-service-class","title":"Step 1: Define a service class","text":"<p>Most custom services inherit from <code>Service</code> (aka <code>BaseContextService</code>) to get handy utilities: access to the current context, a service\u2011wide mutex, and helpers to run blocking code.</p> <pre><code>from aethergraph.services.runtime.base import Service\n\nclass Trainer(Service):\n    async def submit(self, spec: dict) -&gt; str:\n        \"\"\"Submit a training job to your cluster/scheduler.\"\"\"\n        job_id = await self._submit_to_cluster(spec)  # implement backend call\n        return job_id\n\n    async def inspect_job(self, job_id: str) -&gt; dict:\n        status = await self._query_cluster(job_id)    # implement backend call\n        return {\"job_id\": job_id, \"status\": status}\n</code></pre> <p>Notes</p> <ul> <li><code>self.ctx()</code> gives you the current <code>NodeContext</code> at call time\u2014so logs, memory, and artifacts are run\u2011scoped automatically.</li> <li>The service can hold internal state (connection pools, caches) across calls.</li> </ul>"},{"location":"tutorials/t7-extend-runtime-custom-services/#step-2-register-the-service","title":"Step 2: Register the service","text":"<p>Register an instance at startup (e.g., when your sidecar/server boots):</p> <pre><code>from aethergraph import start_server\nfrom aethergraph.services.runtime.registry import register_context_service\n\nstart_server()  # start sidecar so services can be wired\n\ntrainer_service = Trainer()\nregister_context_service(\"trainer\", trainer_service)\n</code></pre> <p>From now on, inside any node:</p> <pre><code>job_id = await context.trainer().submit(spec)\n</code></pre> <p>Pattern: register once \u2192 call anywhere.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#3-using-services-inside-graph_fn-and-tool","title":"3. Using Services Inside <code>graph_fn</code> and <code>@tool</code>","text":"<p>Services behave like built\u2011ins on <code>context</code>.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#example-a-submit-and-track-a-job","title":"Example A \u2014 Submit and track a job","text":"<pre><code>from aethergraph import graph_fn, tool\n\n@graph_fn(name=\"train_and_wait\", outputs=[\"job_id\", \"done\"])\nasync def train_and_wait(spec: dict, *, context):\n    job_id = await context.trainer().submit(spec)\n    ready = await wait_for_training(job_id=job_id, context=context)\n    return {\"job_id\": job_id, \"done\": ready[\"ready\"]}\n\n@tool(name=\"wait_for_training\", outputs=[\"ready\"])\nasync def wait_for_training(job_id: str, *, context) -&gt; dict:\n    info = await context.trainer().inspect_job(job_id)\n    return {\"ready\": info[\"status\"] == \"COMPLETED\"}\n</code></pre> <p>Why this is nice:</p> <ul> <li>Cluster logic in one place (<code>Trainer</code>), not scattered across graphs.</li> <li>Tests can swap in a fake <code>Trainer</code> that returns canned statuses.</li> </ul>"},{"location":"tutorials/t7-extend-runtime-custom-services/#example-b-custom-storage-wrapper","title":"Example B \u2014 Custom storage wrapper","text":"<pre><code>class Storage(Service):\n    async def put(self, local_path: str, key: str) -&gt; str:\n        uri = await self._upload(local_path, key)  # implement upload\n        self.ctx().logger().info(\"storage.put\", extra={\"uri\": uri})\n        return uri\n\n    async def get(self, uri: str, dest: str) -&gt; None:\n        await self._download(uri, dest)\n\n@graph_fn(name=\"upload_report\", outputs=[\"uri\"])\nasync def upload_report(*, context):\n    uri = await context.storage().put(\"/tmp/report.pdf\", key=\"reports/2025-01-01.pdf\")\n    return {\"uri\": uri}\n</code></pre> <p>You can mix <code>context.storage()</code> with core features like <code>artifacts()</code> and <code>memory()</code>\u2014for example, storing the CAS URI next to an external bucket URI.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#4-concurrency-shared-state","title":"4. Concurrency &amp; Shared State","text":"<p>Because a service instance is shared, multiple nodes (or graphs) may hit it concurrently. If you expect concurrent accesses to a service, protect shared state inside the service, not at every call site.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#a-servicewide-mutex-recommended-pattern","title":"A) Service\u2011wide mutex (recommended pattern)","text":"<p>Use the built\u2011in <code>critical()</code> helper to guard a method. The pattern below binds the mutex to an instance method immediately after <code>__init__</code>, ensuring <code>self</code> exists:</p> <pre><code>import asyncio\nfrom aethergraph.services.runtime.base import Service\n\nclass CounterService(Service):\n    def __init__(self):\n        super().__init__()\n        self._value = 0\n        # Decorate incr with the bound service-wide mutex\n        # The entire method runs under a critical section\n        self.incr = self.critical()(self.incr)  # type: ignore\n\n    async def incr(self, n: int = 1) -&gt; int:\n        self._value += n\n        await asyncio.sleep(0)  # yield to event loop\n        return self._value\n</code></pre> <p>If you need finer\u2011grained control (e.g., per\u2011key locks, rate windows), design your own locking scheme inside the service. The point is to centralize concurrency policy in one place.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#b-offload-blocking-work","title":"B) Offload blocking work","text":"<pre><code>class Heavy(Service):\n    async def compute(self, x: int) -&gt; int:\n        return await self.run_blocking(self._slow_cpu_fn, x)  # threadpool offload\n\n    def _slow_cpu_fn(self, x: int) -&gt; int:\n        ...  # pure CPU work\n</code></pre> <p>This keeps agents responsive even when a service must do something synchronous or CPU\u2011heavy (e.g. heavy local simulation, training etc.).</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#5-service-lifecycle-startclose","title":"5. Service Lifecycle (start/close)","text":"<p>Some integrations need setup/teardown\u2014opening DB pools, authenticating SDKs, or warming models. Implement optional hooks on your service:</p> <pre><code>class Tracker(Service):\n    async def start(self):\n        self._client = ...  # open DB/HTTP client\n\n    async def close(self):\n        if getattr(self, \"_client\", None):\n            await self._client.aclose()\n</code></pre> <p>Call these from your process bootstrap/shutdown (sidecar, web server, CLI). The runtime doesn\u2019t force a pattern\u2014choose how you host services.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#6-testing-swapping-implementations","title":"6. Testing &amp; Swapping Implementations","text":"<p>Because services are registered by name, they\u2019re easy to replace in tests:</p> <pre><code>class FakeTrainer(Service):\n    async def submit(self, spec: dict) -&gt; str:\n        return \"job-test-123\"\n\n    async def inspect_job(self, job_id: str) -&gt; dict:\n        return {\"job_id\": job_id, \"status\": \"COMPLETED\"}\n\n# Test setup\nregister_context_service(\"trainer\", FakeTrainer())\n# All code using context.trainer() now talks to the fake.\n</code></pre>"},{"location":"tutorials/t7-extend-runtime-custom-services/#7-design-tips-common-patterns","title":"7. Design Tips &amp; Common Patterns","text":"<p>A few patterns that work well in real projects:</p> <ul> <li> <p>One concept \u2192 one service <code>context.trainer()</code> for orchestration, <code>context.storage()</code> for object stores, <code>context.materials()</code> for domain registries, etc.</p> </li> <li> <p>Keep names explicit   Prefer <code>context.k8s_jobs()</code> or <code>context.minio()</code> over vague <code>context.utils()</code>.</p> </li> <li> <p>Use services for anything stateful   HTTP clients, ORM sessions, caches, in\u2011memory registries, queues, schedulers.</p> </li> <li> <p>Don\u2019t replace built\u2011ins   Leave <code>context.memory()</code>, <code>context.artifacts()</code>, <code>context.channel()</code> alone. If you mirror to another system, create a separate service that consumes those.</p> </li> </ul> <p>More handy service ideas</p> Scenario Accessor What it wraps / does HPC / Training cluster <code>context.trainer()</code> Slurm/K8s jobs, Ray, internal queue External object storage <code>context.storage()</code> S3/GCS/MinIO, signed URLs, lifecycle/pinning Job/run tracking <code>context.tracker()</code> DB for job metadata, status dashboards Feature or embedding store <code>context.vectorstore()</code> Vector DB client, batch upserts, hybrid search Materials/parts registry <code>context.materials()</code> Domain DB + caching (e.g., refractive indices) Metrics/telemetry export <code>context.metrics()</code> Push to Prometheus/OTel/Grafana Lineage/BI export <code>context.lineage()</code> Push run/graph/node metadata to warehouse PDF/Doc processing <code>context.docs()</code> OCR, parsing, chunking utilities Secure secrets broker <code>context.secrets()</code> Rotation, envelope decryption Payment/billing <code>context.billing()</code> Client to your billing/ledger microservice License/Entitlements <code>context.license()</code> Gate features per user/org Remote execution (HPC/VM functions) <code>context.runner()</code> Dispatch Python/CLI jobs to remote workers Caching layer for expensive API calls <code>context.cache()</code> Memoization + TTL + invalidation Model hosting / inference gateway <code>context.predict()</code> Internal inference service with model registry"},{"location":"tutorials/t7-extend-runtime-custom-services/#8-optional-callable-services","title":"8. Optional: Callable Services","text":"<p>If you like compact call sites, implement <code>__call__</code>:</p> <pre><code>class Predictor(Service):\n    async def __call__(self, prompt: str) -&gt; str:\n        return await self.generate(prompt)\n\n    async def generate(self, prompt: str) -&gt; str:\n        ...\n\n# After registration as \"predictor\":\ntext = await context.predictor(\"hello\")               # calls __call__\ntext = await context.predictor().generate(\"hello\")    # explicit method\n</code></pre> <p>Sugar only; explicit method names (<code>submit</code>, <code>inspect_job</code>, <code>upload</code>, <code>generate</code>) are often clearer for teams.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#9-how-this-fits-with-mcp-and-other-integrations","title":"9. How This Fits with MCP and Other Integrations","text":"<p>In the previous section, MCP treated external processes (HTTP/WebSocket/stdio) as tools your agent can call. Custom services are the other half:</p> <ul> <li>MCP: great when the external system already speaks MCP and you want tools/resources auto\u2011described.</li> <li>Custom services: great when you want a plain Python wrapper around internal systems\u2014no extra server, no protocol.</li> </ul> <p>Projects often mix both:</p> <ul> <li>Use an MCP server for generic capabilities (filesystem, SQL, web search).</li> <li>Use services like <code>context.trainer()</code> and <code>context.storage()</code> for tightly\u2011coupled, org\u2011specific infrastructure.</li> </ul> <p>With this pattern in place, you can keep adding capabilities by teaching the runtime new services, while keeping agent code small, readable, and testable.</p>"},{"location":"tutorials/t8-design-your-own-waits/","title":"Design Your Own Waits &amp; Adapters (Coming Soon)","text":"<p>This chapter is a roadmap for a feature that\u2019s not public yet: defining your own dual\u2011stage waits and adapters so external systems can safely pause and resume graphs.</p> <p>You can treat this as a design note: it explains what is coming and how to think about it, without locking you into any final API.</p>"},{"location":"tutorials/t8-design-your-own-waits/#1-why-custom-waits","title":"1. Why Custom Waits?","text":"<p>Today you\u2019ve seen two wait styles:</p> <ul> <li> <p>Cooperative waits via <code>context.channel().ask_*</code> inside <code>@graph_fn</code>.</p> </li> <li> <p>Great for interactive agents while the process is alive.</p> </li> <li>Not resumable after the Python process dies.</li> <li> <p>Dual\u2011stage waits via built\u2011in tools used in <code>@graphify</code> static graphs.</p> </li> <li> <p>The graph can pause indefinitely (e.g., waiting for Slack/Web reply).</p> </li> <li>You can cold\u2011resume with the same <code>run_id</code> much later.</li> </ul> <p>Custom waits are about taking that second pattern and making it available for your own backends:</p> <ul> <li>external approval systems</li> <li>internal job schedulers</li> <li>lab/experiment queues</li> <li>human\u2011in\u2011the\u2011loop tools that live outside channels like Slack/Telegram</li> </ul> <p>The goal: let you say \u201cpause here until X happens in system Y, then resume this node safely\u201d.</p>"},{"location":"tutorials/t8-design-your-own-waits/#2-what-exists-today","title":"2. What Exists Today","text":"<p>You already have:</p> <ul> <li>Cooperative waits on <code>NodeContext.channel()</code> (<code>ask_text</code>, <code>ask_approval</code>, <code>ask_files</code>, etc.).</li> <li> <p>Built\u2011in dual\u2011stage wait tools (used in the channel tutorial) that:</p> </li> <li> <p>create a continuation token</p> </li> <li>emit an outgoing event (e.g. to Slack/Web)</li> <li>persist a snapshot</li> <li>resume when an inbound event matches that token</li> </ul> <p>These built\u2011ins are wired to existing adapters (console, Slack, etc.) and work out of the box for common interaction flows.</p> <p>What\u2019s missing right now is a public, stable API for defining your own dual\u2011stage tools and adapters.</p> <p>That\u2019s what this \u201ccoming soon\u201d chapter is preparing you for.</p>"},{"location":"tutorials/t8-design-your-own-waits/#3-design-shape-of-a-dualstage-tool-conceptual","title":"3. Design Shape of a Dual\u2011Stage Tool (Conceptual)","text":"<p>At a high level, a dual\u2011stage tool will look like:</p> <ol> <li> <p>Stage A \u2013 schedule / emit</p> </li> <li> <p>Construct a request payload.</p> </li> <li>Register a continuation token with the runtime.</li> <li>Send an event to some external system (HTTP, MQ, email, etc.).</li> <li> <p>Return a WAITING status instead of a final result.</p> </li> <li> <p>Stage B \u2013 resume / handle reply</p> </li> <li> <p>An inbound event (webhook, poller, bridge) calls back with the same token.</p> </li> <li>Runtime restores the graph + node and hands you the reply payload.</li> <li>Your tool continues execution and returns a normal result.</li> </ol>"},{"location":"tutorials/t8-design-your-own-waits/#possible-future-shape-pseudocode-not-final","title":"Possible future shape (pseudo\u2011code, not final)","text":"<pre><code>class ApproveJob(DualStageTool):  # name TBD\n    async def build_request(self, spec: dict, *, context):\n        # Stage A: emit request + return a continuation descriptor\n        token = await context.create_continuation(kind=\"job_approval\", payload={\"spec\": spec})\n        await self.emit_request(spec=spec, token=token)\n        return self.wait(token)   # tells runtime: this node is now WAITING\n\n    async def on_resume(self, reply: dict, *, context):\n        # Stage B: this runs when the continuation is resumed\n        approved = bool(reply.get(\"approved\", False))\n        return {\"approved\": approved}\n</code></pre> <p>Again: this is illustrative only \u2014 the real base class and method names will be documented once the API is ready.</p> <p>The important idea is the split between \u201cset up a wait + emit a request\u201d and \u201chandle the resume payload\u201d, wired together by a continuation token.</p>"},{"location":"tutorials/t8-design-your-own-waits/#4-adapters-bridging-external-systems","title":"4. Adapters: Bridging External Systems","text":"<p>Custom waits are only half the story \u2014 you also need a way to bridge an external system to AetherGraph\u2019s continuation store.</p> <p>Conceptually, an adapter will:</p> <ol> <li>Listen for inbound events from your system (webhook, queue consumer, polling loop).</li> <li>Parse them into a payload <code>{token, data}</code>.</li> <li>Call the runtime to resolve the corresponding continuation and resume the graph.</li> </ol> <p>In practice this might look like (pseudo\u2011code):</p> <pre><code># somewhere in your web app / worker\n\n@app.post(\"/callbacks/job-approved\")\nasync def job_approved(req):\n    token = req.json()[\"token\"]\n    payload = {\"approved\": req.json()[\"approved\"]}\n    await runtime.resolve_continuation(token=token, payload=payload)\n    return {\"ok\": True}\n</code></pre> <p>Behind the scenes, the runtime will:</p> <ul> <li>load the snapshot for the relevant <code>run_id</code> / graph</li> <li>mark the waiting node as ready</li> <li>resume execution from that node using the payload</li> </ul> <p>The adapter API that makes this nicer to write will be documented with the dual\u2011stage tool support.</p>"},{"location":"tutorials/t8-design-your-own-waits/#5-how-this-relates-to-graph_fn-and-graphify","title":"5. How This Relates to <code>graph_fn</code> and <code>graphify</code>","text":"<p>It\u2019s useful to remember the three modes:</p> Mode Wait type available today Cold\u2011resume after process exit? <code>graph_fn</code> + <code>context.channel()</code> Cooperative waits (<code>ask_*</code>) \u274c No <code>graphify</code> + built\u2011in dual waits Dual\u2011stage tools \u2705 Yes <code>graphify</code> + custom dual waits Coming soon \u2705 Yes (for your own waits) <p>Once custom dual\u2011stage tools are available, you\u2019ll:</p> <ul> <li>Use <code>graph_fn</code> when you want live, in\u2011process agents. You can certainly combine <code>graph_fn</code> and <code>DualStageTool</code>, it's just <code>graph_fn</code> does not preserve states and resumption is not protected if the process gets interrupted. </li> <li>Use <code>graphify</code> + dual\u2011stage waits when you want hard guarantees about resuming long\u2011running flows.</li> <li>Wrap external systems (HPC, approval workflows, human review portals) as first\u2011class wait nodes.</li> </ul>"},{"location":"tutorials/t8-design-your-own-waits/#6-what-you-can-do-today","title":"6. What You Can Do Today","text":"<p>While the custom API is still baking, you can already:</p> <ul> <li>Use built\u2011in dual\u2011stage waits with channels (Slack/Web) in static graphs to pause and resume runs.</li> <li>Use cooperative waits (<code>context.channel().ask_*</code>) in <code>graph_fn</code> agents when you don\u2019t need cold\u2011resume.</li> <li>Front external systems with simple scripts or services that call existing channel tools (for example, sending a Slack approval that resumes a static graph).</li> </ul> <p>When the public dual\u2011stage API lands, you\u2019ll be able to replace those scripts with:</p> <ul> <li>explicit <code>@tool</code>\u2011style wait nodes, and</li> <li>small, typed adapters that speak your domain\u2019s language.</li> </ul>"},{"location":"tutorials/t8-design-your-own-waits/#7-looking-ahead","title":"7. Looking Ahead","text":"<p>This chapter is intentionally high\u2011level and labeled Coming Soon. The final documentation will include:</p> <ul> <li>A concrete base class or decorator for defining dual\u2011stage wait tools.</li> <li>A small adapter kit for wiring external callbacks into the continuation store.</li> <li>End\u2011to\u2011end examples: long\u2011running jobs, external approval systems, and custom interactive apps.</li> </ul> <p>Until then, you can design your flows with this mental model in mind:</p> <p>\u201cAnything that can emit a token and later send it back can be turned into a resumable node.\u201d</p> <p>Once the API is ready, you\u2019ll drop in the real primitives and your graphs will gain robust, resumable waits across all your systems.</p>"},{"location":"ui/agents-apps/","title":"Defining Apps and Agents for the AetherGraph UI","text":"<p>This page explains:</p> <ul> <li>What agents and apps are in AetherGraph</li> <li>When you need them (only if you\u2019re using the AG UI)</li> <li>How to define them with minimal metadata</li> <li>Best practices for orchestration and memory</li> <li>How the UI passes information (messages, files, context refs) into your backend entrypoints</li> </ul> <p>If you\u2019re only using AetherGraph as a Python library (no UI), you don\u2019t need to define agents or apps at all. You can just call your graphs directly.</p>"},{"location":"ui/agents-apps/#1-concepts-agent-vs-app","title":"1. Concepts: Agent vs App","text":""},{"location":"ui/agents-apps/#11-agent","title":"1.1 Agent","text":"<p>An agent is a chat endpoint exposed to the AG UI.</p> <ul> <li>It shows up in the Agent Gallery.</li> <li>The UI creates a chat session bound to that agent.</li> <li>Each user message is turned into a run of the agent\u2019s graph function, using a fixed \"chat\" signature.</li> <li>Inside the agent, you can:<ul> <li>Call tools and other graphs</li> <li>Spawn background runs</li> <li>Write to memory</li> <li>Log artifacts, etc.</li> </ul> </li> </ul> <p>Key points:</p> <ul> <li>An agent must be a <code>graph_fn</code> (a function-like entrypoint), not a full <code>@graphify</code> flow.</li> <li>That <code>graph_fn</code> uses a fixed input signature for <code>mode=\"chat_v1\"</code> (the default chat agent mode).</li> <li>The agent can internally spawn or run other <code>graphify</code> graphs or <code>graph_fn</code>s.</li> </ul> <p>Think of the agent as the front door for conversation, not as the place to do all heavy work. Use it as a thin router that delegates to other graphs.</p> <p>The structure of <code>files</code> and <code>context_refs</code> may evolve as the UI grows. The idea stays the same: the UI collects what the user selected (files, artifacts, runs) and passes them to your agent.</p>"},{"location":"ui/agents-apps/#12-app","title":"1.2 App","text":"<p>An app is a click-to-run flow exposed to the AG UI.</p> <ul> <li>It shows up in the App Gallery.</li> <li>The user clicks an app card to start a new run.</li> <li>The run is tracked, visualized, and (for graphs) can be resumed in the UI.</li> </ul> <p>Key points:</p> <ul> <li>An app can be backed by either:<ul> <li>a <code>@graphify</code> graph (recommended), or</li> <li>a <code>@graph_fn</code> function.</li> </ul> </li> <li>For observability and resumption, <code>graphify</code> is strongly recommended:<ul> <li>You get full run history and node-level status.</li> <li>Resumption and partial reruns are easier.</li> </ul> </li> </ul> <p>Apps are ideal for flows like:</p> <ul> <li>ETL / batch processing</li> <li>Data analysis pipelines</li> <li>Long-running workflows kicked off by a single click</li> </ul>"},{"location":"ui/agents-apps/#13-when-do-i-need-agentsapps","title":"1.3 When do I need agents/apps?","text":"<p>You only need to define agents and apps if you want your logic to appear in the UI as:</p> <ul> <li>a chat agent, or</li> <li>a launchable app from the galleries.</li> </ul> <p>If you\u2019re building a purely programmatic pipeline with no UI, you can ignore <code>as_agent</code> and <code>as_app</code> entirely.</p>"},{"location":"ui/agents-apps/#2-minimal-definitions-the-happy-path","title":"2. Minimal definitions (the happy path)","text":"<p>AetherGraph is designed so you can start with minimal metadata and still get a usable UI:</p> <ul> <li>Agents: only <code>id</code> and <code>title</code> are required.</li> <li>Apps: only <code>id</code> and <code>name</code> are required.</li> </ul> <p>Everything else is optional and has sensible defaults.</p>"},{"location":"ui/agents-apps/#21-minimal-agent","title":"2.1 Minimal agent","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(\n    name=\"demo_agent\",\n    as_agent={\n        \"id\": \"demo_agent\",\n        \"title\": \"Demo Agent\",\n        # everything else (mode, badge, category, etc.) is optional\n    },\n)\nasync def demo_agent(\n    message: str,\n    files: list[dict] | None,\n    context_refs: list[dict] | None,\n    session_id: str,\n    user_meta: dict | None,\n    *,\n    context,\n) -&gt; str:\n    # Thin conversational entrypoint; do as little as possible here.\n    return f\"You said: {message}\"\n</code></pre> <p>Defaults you get automatically (from backend meta building):</p> <ul> <li><code>mode = \"chat_v1\"</code> (chat-style agent)</li> <li><code>status = \"available\"</code></li> <li><code>session_kind = \"chat\"</code></li> </ul> <p>Category, badge, color, tags, etc. are optional. The UI will still show a functional card in the Agent Gallery and let you open a chat session.</p>"},{"location":"ui/agents-apps/#22-minimal-app","title":"2.2 Minimal app","text":"<pre><code>from aethergraph import graphify\n\n@graphify(\n    name=\"demo_app\",\n    inputs=[],   # recommended: no required inputs; configure via channels/memory\n    as_app={\n        \"id\": \"demo_app\",\n        \"name\": \"Demo App\",\n    },\n)\ndef demo_app():\n    \"\"\"Example of a simple app-backed graph.\"\"\"\n    ...\n</code></pre> <p>Defaults you get automatically:</p> <ul> <li><code>mode = \"no_input_v1\"</code></li> <li><code>status = \"available\"</code></li> </ul> <p>If you don\u2019t specify a category, the app still shows up in the gallery (e.g. under an \"Other Apps\" section). As you grow the project, you can later add richer metadata without touching the core graph code.</p>"},{"location":"ui/agents-apps/#3-agent-requirements-patterns","title":"3. Agent requirements &amp; patterns","text":""},{"location":"ui/agents-apps/#31-fixed-input-signature-for-chat_v1","title":"3.1 Fixed input signature for <code>chat_v1</code>","text":"<p>For the default chat agent mode (<code>mode=\"chat_v1\"</code>):</p> <ul> <li>The agent must be a <code>graph_fn</code>.</li> <li> <p>It must accept the standard chat inputs:</p> </li> <li> <p><code>message</code>: the user\u2019s message text</p> </li> <li><code>files</code>: any uploaded files / attachments for this turn</li> <li><code>context_refs</code>: references to selected files, artifacts, or runs in the UI</li> <li><code>session_id</code>: the current chat session ID</li> <li><code>user_meta</code>: optional metadata about the user</li> <li><code>context</code>: the injected <code>NodeContext</code> (keyword-only argument)</li> </ul> <p>The exact structure of <code>files</code> and <code>context_refs</code> may change, but we keep the signature stable and the intent the same.</p>"},{"location":"ui/agents-apps/#32-use-the-agent-as-a-thin-router","title":"3.2 Use the agent as a thin router","text":"<p>Best practice: keep the agent entrypoint light. It should mostly:</p> <ol> <li>Parse the message and decide what to do.</li> <li>Use the context methods to orchestrate background work.</li> </ol> <p>For example:</p> <ul> <li>Call another graph to process data.</li> <li>Spawn a long-running run in the background.</li> <li>Store or retrieve memory.</li> </ul> <p>This keeps the chat UI responsive and prevents your agent from blocking on long tasks.</p>"},{"location":"ui/agents-apps/#4-apps-graphify-vs-graph_fn","title":"4. Apps: <code>graphify</code> vs <code>graph_fn</code>","text":"<p>You can define an app on:</p> <ul> <li>a <code>@graphify</code> graph (recommended), or</li> <li>a <code>@graph_fn</code> function.</li> </ul> <p>Recommended: use <code>graphify</code> whenever you care about:</p> <ul> <li>Step-by-step observability in the UI</li> <li>Resuming after failures</li> <li>Visualizing the structure of your flow</li> </ul> <p>Example:</p> <pre><code>from aethergraph import graphify\n\n@graphify(\n    name=\"batch_import\",\n    inputs=[],\n    as_app={\"id\": \"batch_import\", \"name\": \"Batch Import\"},\n)\ndef batch_import():\n    \"\"\"Example app that runs as a tracked, resumable graph.\"\"\"\n    ...\n</code></pre> <p>You can still call the same <code>batch_import</code> graph from agents or from code using context methods or normal Python calls.</p>"},{"location":"ui/agents-apps/#5-metadata-minimal-vs-richer-ui","title":"5. Metadata: minimal vs richer UI","text":"<p>You already have a detailed reference page for all metadata keys. Here we keep it high-level and opinionated.</p>"},{"location":"ui/agents-apps/#51-minimal-agent-metadata","title":"5.1 Minimal agent metadata","text":"<pre><code>as_agent={\n    \"id\": \"assistant\",\n    \"title\": \"Assistant\",\n}\n</code></pre> <p>Everything else is optional. When you want nicer cards and better grouping, you can add:</p> <ul> <li><code>badge</code>: small label on the card, e.g. <code>\"Chat Agent\"</code>.</li> <li><code>category</code>: e.g. <code>\"Core\"</code>, <code>\"R&amp;D Lab\"</code>, <code>\"Infra\"</code>, <code>\"Productivity\"</code>.</li> <li><code>short_description</code>: brief summary for the gallery card.</li> <li><code>icon_key</code>: icon to use in the gallery (e.g. <code>\"message-circle\"</code>).</li> <li><code>color</code>: accent color token (e.g. <code>\"emerald\"</code>).</li> <li><code>tags</code>: for filtering and search.</li> <li><code>features</code>: bullet points for the detail panel.</li> <li><code>github_url</code>: link to the agent\u2019s implementation.</li> </ul>"},{"location":"ui/agents-apps/#52-minimal-app-metadata","title":"5.2 Minimal app metadata","text":"<pre><code>as_app={\n    \"id\": \"demo_app\",\n    \"name\": \"Demo App\",\n}\n</code></pre> <p>Same idea as agents: start minimal, add more when you care about presentation and grouping.</p>"},{"location":"ui/agents-apps/#6-best-practices-orchestration-from-agents","title":"6. Best practices: orchestration from agents","text":""},{"location":"ui/agents-apps/#61-use-agents-as-front-doors","title":"6.1 Use agents as \"front doors\"","text":"<p>Good pattern:</p> <ul> <li>Define one or a few well-scoped agents as entrypoints.</li> <li>Inside each agent:</li> <li>Parse the request.</li> <li>Decide which graphs to call.</li> <li>Spawn the actual work as runs.</li> </ul> <p>This keeps:</p> <ul> <li>The chat UI responsive.</li> <li>Runs organized by agent, session, and tags.</li> <li>Your core graphs reusable across agents, apps, and scripts.</li> </ul>"},{"location":"ui/agents-apps/#62-spawn_run-as-the-default-orchestration-primitive","title":"6.2 <code>spawn_run</code> as the default orchestration primitive","text":"<p>From your agent (or any graph function) you can access a <code>context</code> object that offers helpers like <code>spawn_run</code> and <code>wait_run</code>.</p> <p>Example pattern:</p> <pre><code>from aethergraph.runtime import RunVisibility\n\n@graph_fn(\n    name=\"assistant_agent\",\n    as_agent={\"id\": \"assistant\", \"title\": \"Assistant\"},\n)\nasync def assistant_agent(message: str, files, context_refs, session_id: str, user_meta, *, context):\n    if \"analyze\" in message:\n        run_id = await context.spawn_run(\n            \"analysis_graph\",\n            inputs={\"message\": message},\n            visibility=RunVisibility.normal,\n        )\n        return f\"I started an analysis run (run_id={run_id}). I\u2019ll let you know when it\u2019s done.\"\n\n    # Simple inline reply\n    return f\"Quick answer to: {message}\"\n</code></pre> <p>Notes:</p> <ul> <li><code>spawn_run</code> returns quickly and does not block the agent.</li> <li>You only need minimal inputs: graph name, optional <code>inputs</code>, and <code>visibility</code>.</li> <li>Later you can pass tags, parent IDs, or other metadata if needed.</li> </ul> <p>When you do need the result of a run, you can call:</p> <pre><code>record = await context.wait_run(run_id)\n</code></pre> <p>Today, <code>RunRecord</code> contains run metadata (timestamps, status, etc.) but does not expose outputs yet. That API will evolve; see UI notes for up-to-date caveats.</p>"},{"location":"ui/agents-apps/#7-memory-in-agents","title":"7. Memory in agents","text":"<p>Agents often need persistent memory to behave coherently across runs and sessions. You configure this via <code>memory_level</code> and <code>memory_scope</code> in <code>as_agent</code>.</p>"},{"location":"ui/agents-apps/#71-memory-levels-common-pattern","title":"7.1 Memory levels (common pattern)","text":"<p>Typical levels:</p> <ul> <li><code>\"session\"</code> \u2013 memory is scoped per chat session.</li> <li><code>\"user\"</code> \u2013 memory is scoped per user across sessions.</li> </ul> <p>Example:</p> <pre><code>@graph_fn(\n    name=\"assistant_agent\",\n    as_agent={\n        \"id\": \"assistant\",\n        \"title\": \"Assistant\",\n        \"memory_level\": \"session\",   # one memory store per chat session\n        \"memory_scope\": \"assistant.main\",\n    },\n)\nasync def assistant_agent(..., *, context):\n    ...\n</code></pre>"},{"location":"ui/agents-apps/#72-memory-scope-a-namespace-within-the-level","title":"7.2 Memory scope: a namespace within the level","text":"<p><code>memory_scope</code> is a logical namespace inside the chosen level.</p> <p>You can think of it as:</p> <p>Memory = f(memory_level, memory_scope, user/session)</p> <p>Some examples:</p> <ul> <li><code>memory_level=\"session\", memory_scope=\"assistant.main\"</code>   \u2192 Each session gets its own \u201cassistant.main\u201d store.</li> <li><code>memory_level=\"user\", memory_scope=\"assistant.main\"</code>   \u2192 Same assistant memory shared across sessions for the same user.</li> <li><code>memory_level=\"user\", memory_scope=\"assistant.research\"</code>   \u2192 A separate research-specific store per user.</li> </ul> <p>Scopes are just strings; using dotted names (<code>\"assistant.main\"</code>, <code>\"planner.global\"</code>) keeps things tidy.</p> <p>A useful mental model: scope is a subset within a level. For example, multiple agents in the same session can intentionally share a <code>memory_scope</code>, while using <code>memory_level=\"session\"</code> to ensure that memory is not persisted beyond that session.</p>"},{"location":"ui/agents-apps/#8-summary","title":"8. Summary","text":"<ul> <li>Agents: chat entrypoints based on <code>graph_fn</code> with a fixed chat signature. Use them as thin routers that spawn real work into graphs.</li> <li>Apps: click-to-run flows, ideally <code>graphify</code> graphs, for launch-and-observe workflows.</li> <li>Minimal meta:<ul> <li>Agents: <code>as_agent={\"id\": \"...\", \"title\": \"...\"}</code> is enough.</li> <li>Apps: <code>as_app={\"id\": \"...\", \"name\": \"...\"}</code> is enough.</li> </ul> </li> <li>Best practices:<ul> <li>Keep agents lean and orchestrate via <code>spawn_run</code>.</li> <li>Use <code>graphify</code> for flows you want to observe and resume.</li> <li>Configure <code>memory_level</code> and <code>memory_scope</code> so that related agents share the right subset of memory (within a session, or across sessions for a user).</li> <li>Expect the details around file handling and <code>context_refs</code> to evolve; the high-level pattern (UI selects things, agent receives them as parameters) will stay stable.</li> </ul> </li> </ul> <p>For source / deployment caveats and current limitations, see UI notes.</p>"},{"location":"ui/overview/","title":"AetherGraph UI Overview","text":"<p>The AetherGraph UI gives you a visual control panel for your graphs, agents, and runs:</p> <ul> <li>Launch apps from an App Gallery (one-click flows).</li> <li>Chat with agents that orchestrate graphs behind the scenes.</li> <li>Inspect runs, sessions, and artifacts.</li> <li>Resume failed or partial runs (for <code>graphify</code> graphs).</li> </ul> <p>This section of the docs covers:</p> <ol> <li>Start the server with the built-in UI</li> <li>Expose agents and apps to the UI</li> <li>Important notes and troubleshooting tips</li> </ol> <p>If you only use AetherGraph as a Python library (no UI), you can ignore this whole section and just import and run graphs from your own code.</p>"},{"location":"ui/overview/#what-the-ui-actually-does","title":"What the UI actually does","text":"<p>At a high level:</p> <ul> <li> <p>The server exposes:</p> <ul> <li>A REST API: <code>/api/v1/\u2026</code></li> <li>A static UI bundle: <code>/ui</code></li> </ul> </li> <li> <p>The frontend reads metadata about:</p> <ul> <li>Graphs (<code>@graphify</code>) and graph functions (<code>@graph_fn</code>)</li> <li>Anything marked with <code>as_app</code> or <code>as_agent</code></li> </ul> </li> <li> <p>The UI then lets you:</p> <ul> <li>Start runs (apps)</li> <li>Start chat sessions (agents)</li> <li>Inspect runs, sessions, memory, and artifacts</li> </ul> </li> </ul> <p>You control what appears in the UI by how you define your project module, apps, and agents. The next pages walk you through:</p> <ul> <li>Starting the server</li> <li>Defining agents and apps for the UI</li> </ul>"},{"location":"ui/overview/#when-should-i-use-the-ui","title":"When should I use the UI?","text":"<p>Good use cases for the UI:</p> <ul> <li>You want non-engineers (or future you) to use your graphs without reading code.</li> <li>You want a visual \u201clab notebook\u201d: runs, artifacts, and memory all tracked in one workspace.</li> <li>You want to debug complex flows with better observability and resumption.</li> </ul> <p>Cases where the UI is optional:</p> <ul> <li>Fully automated backends where you call AetherGraph purely as a library.</li> <li>One-off scripts or small experiments where a CLI is enough.</li> </ul> <p>If you\u2019re unsure, start with the UI. It\u2019s usually easier to wire up a graph once and then reuse it from both UI and code.</p>"},{"location":"ui/server/","title":"Starting the AetherGraph Server with the Built-in UI","text":"<p>This guide walks through:</p> <ol> <li>Installing / upgrading AetherGraph</li> <li>Defining a project module for your graphs &amp; apps</li> <li>Starting the server from the terminal (recommended, with auto-reload)</li> <li>Optionally starting the server from a Python script</li> </ol> <p>Once the server is running, you\u2019ll be able to open the UI at <code>/ui</code>. From there, you\u2019ll see your App Gallery, Agent Gallery, Runs, Sessions, and Artifacts.</p>"},{"location":"ui/server/#1-install-upgrade-aethergraph","title":"1. Install / upgrade AetherGraph","text":"<p>From your virtual environment:</p> <pre><code>pip install -U aethergraph\n</code></pre> <p>If you are working from the source repo instead:</p> <pre><code># From the repo root\npip install -e .\n</code></pre> <p>Note The published PyPI package includes a prebuilt UI bundle. If you run from source and haven\u2019t built the UI, <code>/ui</code> will return a 501 until you build the frontend and copy the bundle into <code>aethergraph/server/ui_static/</code>. See UI notes for details.</p>"},{"location":"ui/server/#2-define-a-project-module-with-graphs-apps","title":"2. Define a project module with graphs &amp; apps","text":"<p>Create a project folder, for example:</p> <pre><code>my_project/\n  demos/\n    __init__.py\n    chat_demo.py\n  aethergraph_data/   # workspace (created automatically as needed)\n</code></pre> <p>In <code>demos/chat_demo.py</code>, define a graph and expose it as an app so the UI can display it in the App Gallery:</p> <pre><code># demos/chat_demo.py\nfrom aethergraph import graphify\n\n@graphify(\n    name=\"chat_with_memory_demo\",\n    inputs=[],\n    outputs=[\"turns\", \"summary\"],\n    as_app={\n        \"id\": \"chat_with_memory_demo\",\n        \"name\": \"Chat with Memory\",\n    },\n)\ndef chat_with_memory_demo():\n    # Your graph implementation here \u2013 tools, nodes, etc.\n    ...\n</code></pre> <p>Key points:</p> <ul> <li>The module must be importable from your project root (e.g. <code>import demos</code> must work).</li> <li><code>@graphify(..., as_app={...})</code> registers this graph as an app, so it appears in the UI.</li> <li>You can have multiple files and nested modules under <code>demos</code>; everything is imported when you pass <code>--load-module demos</code> to the server.</li> </ul> <p>Later, you can also add agents (chat endpoints) as <code>graph_fn</code> functions. See Agents &amp; Apps for details.</p>"},{"location":"ui/server/#3-start-the-server-from-the-terminal-recommended","title":"3. Start the server from the terminal (recommended)","text":"<p>From your project root (the folder containing <code>demos/</code>), run:</p>"},{"location":"ui/server/#minimal-dev-command","title":"Minimal dev command","text":"<pre><code>aethergraph serve --project-root . --load-module demos --reload\n</code></pre> <p>This will:</p> <ul> <li>Add <code>.</code> to <code>sys.path</code> so <code>demos</code> can be imported.</li> <li>Load all graphs / apps defined in the <code>demos</code> module.</li> <li>Start the API + UI server on <code>http://127.0.0.1:8745</code>.</li> <li>Enable auto-reload: editing your graph files triggers a server restart and reload of your graphs.</li> </ul> <p>You should see output similar to:</p> <pre><code>[AetherGraph] \ud83d\ude80  Server started at: http://127.0.0.1:8745\n[AetherGraph] \ud83d\udda5\ufe0f  UI:                http://127.0.0.1:8745/ui\n[AetherGraph] \ud83d\udce1  API:               http://127.0.0.1:8745/api/v1/\n[AetherGraph] \ud83d\udcc2  Workspace:         ./aethergraph_data\n[AetherGraph] \u267b\ufe0f  Auto-reload:       enabled\n</code></pre> <p>Now open:</p> <ul> <li>UI: <code>http://127.0.0.1:8745/ui</code> \u2013 App Gallery, Runs, Sessions, Artifacts, etc.</li> <li>API: <code>http://127.0.0.1:8745/api/v1/</code> \u2013 for direct API calls.</li> </ul>"},{"location":"ui/server/#31-common-cli-flags","title":"3.1 Common CLI flags","text":"<p>Project / workspace</p> <ul> <li> <p><code>--project-root PATH</code>   Root directory to temporarily add to <code>sys.path</code> when loading user graphs.   Default: <code>.</code> (current directory).</p> </li> <li> <p><code>--workspace PATH</code>   Directory where runs, artifacts, and other state are stored.   Default: <code>./aethergraph_data</code>.</p> </li> </ul> <p>Loading graphs</p> <ul> <li> <p><code>--load-module NAME</code> (repeatable)   Python module(s) to import before the server starts.   Example: <code>--load-module demos</code> imports everything under <code>demos</code>.</p> </li> <li> <p><code>--load-path PATH</code> (repeatable)   Python file(s) to load directly.   Example: <code>--load-path ./examples/my_graph.py</code>.</p> </li> </ul> <p>Network / dev</p> <ul> <li> <p><code>--host HOST</code> / <code>--port PORT</code>   Host and port to bind (e.g. <code>--host 0.0.0.0 --port 8745</code>).   Use <code>--port 0</code> to let the OS pick a free port.</p> </li> <li> <p><code>--reload</code>   Enable uvicorn\u2019s auto-reload. Any <code>.py</code> changes under the project root (and <code>--load-path</code> files) will restart the server and reload your graphs. Strongly recommended for local development and debugging.</p> </li> <li> <p><code>--strict-load</code>   If set, the server will abort on the first graph load error instead of continuing with partial results.</p> </li> </ul> <p>More gotchas and deployment tips: see UI notes.</p>"},{"location":"ui/server/#4-starting-the-server-from-a-python-script-optional","title":"4. Starting the server from a Python script (optional)","text":"<p>You can also start the server from a Python script, for example to embed AetherGraph into your own launcher:</p> <pre><code># start_server.py\nfrom aethergraph import start_server\n\nif __name__ == \"__main__\":\n    start_server(\n        workspace=\"./aethergraph_data\",\n        project_root=\".\",\n        load_module=[\"demos\"],\n        host=\"127.0.0.1\",\n        port=8745,\n    )\n</code></pre> <p>Then run:</p> <pre><code>python start_server.py\n</code></pre> <p>This is convenient when you want to:</p> <ul> <li>Package AetherGraph inside a larger application.</li> <li>Start the server from an IDE run configuration.</li> </ul> <p>For active development and debugging, the CLI with <code>--reload</code> is strongly recommended.</p> <p>The scripted <code>start_server</code> path does not automatically enable uvicorn\u2019s file-watching reload. With <code>aethergraph serve --reload</code>, you get a much smoother loop:</p> <ol> <li>Edit graphs  </li> <li>Uvicorn reloads the server  </li> <li>Refresh the UI and see your changes immediately</li> </ol> <p>Use <code>start_server</code> when you want a simple \u201cembed AetherGraph in my program\u201d story; use <code>aethergraph serve</code> for day-to-day graph and app development.</p>"},{"location":"ui/ui-notes/","title":"UI Notes, Limitations, and Common Pitfalls","text":"<p>This page collects practical notes and \u201cgotchas\u201d around the AetherGraph UI:</p> <ul> <li>Running from source vs PyPI</li> <li>Auto-reload and development workflow</li> <li>Project roots, modules, and imports</li> <li>Memory levels and scopes (quick reference)</li> <li>Current limitations and things that may change</li> </ul>"},{"location":"ui/ui-notes/#1-running-from-source-vs-pypi","title":"1. Running from source vs PyPI","text":"<p>If you install AetherGraph from PyPI:</p> <ul> <li>The package ships with a prebuilt UI bundle.</li> <li><code>/ui</code> should work out of the box.</li> </ul> <p>If you run from source:</p> <ul> <li>The repo expects a built frontend bundle under: <code>aethergraph/server/ui_static/</code></li> <li>Until that bundle exists, hitting <code>/ui</code> will return a <code>501</code> with a message like: <p>\"UI bundle not found. Please build the frontend and copy it to ui_static.\"</p> </li> </ul> <p>In that case, either:</p> <ul> <li>Install from PyPI for local development, or</li> <li>Build the frontend from source and copy the resulting static files into <code>ui_static/</code>.</li> </ul>"},{"location":"ui/ui-notes/#2-auto-reload-and-development-workflow","title":"2. Auto-reload and development workflow","text":"<p>For day-to-day development, prefer:</p> <pre><code>aethergraph serve --project-root . --load-module demos --reload\n</code></pre> <p>This will:</p> <ul> <li>Watch your Python files for changes.</li> <li>Restart the server when graphs change.</li> <li>Let you refresh the UI and see new graphs, agents, and apps without manual restarts.</li> </ul> <p>The scripted <code>start_server(...)</code> path is better for embedding AetherGraph into another app, but does not enable uvicorn\u2019s auto-reload by default.</p>"},{"location":"ui/ui-notes/#3-project-roots-modules-and-imports","title":"3. Project roots, modules, and imports","text":"<p>Common pitfalls when the UI shows \u201cno apps\u201d or \u201cno agents\u201d:</p> <ol> <li> <p>Module not importable </p> <ul> <li>Ensure your <code>--project-root</code> or <code>project_root</code> is correct.</li> <li>Confirm <code>python -c \"import demos_clean\"</code> works from that directory.</li> </ul> </li> <li> <p>Missing <code>__init__.py</code> </p> <ul> <li>For packages like <code>demos_clean</code>, ensure <code>__init__.py</code> exists.</li> </ul> </li> <li> <p>Forgot <code>as_app</code> / <code>as_agent</code> </p> <ul> <li>Graphs only appear in the App Gallery if they have <code>as_app={...}</code>.</li> <li>Agents only appear in the Agent Gallery if they have <code>as_agent={...}</code> and are defined via <code>graph_fn</code>.</li> </ul> </li> <li> <p>Load flags mismatch </p> <ul> <li>If you use <code>--load-path</code>, make sure you are pointing at the actual files that define your graphs/agents.</li> <li>If you use <code>--load-module</code>, ensure the module name matches the package structure.</li> </ul> </li> </ol>"},{"location":"ui/ui-notes/#4-memory-levels-and-scopes-quick-reference","title":"4. Memory levels and scopes (quick reference)","text":"<p>You configure agent memory via <code>as_agent</code>:</p> <pre><code>as_agent={\n    \"id\": \"assistant\",\n    \"title\": \"Assistant\",\n    \"memory_level\": \"session\",   # or \"user\"\n    \"memory_scope\": \"assistant.main\",\n}\n</code></pre> <ul> <li> <p><code>memory_level</code> controls the outer scope:</p> <ul> <li><code>\"session\"</code> \u2192 memory is tied to a single chat session.</li> <li><code>\"user\"</code> \u2192 memory is tied to a user across sessions.</li> </ul> </li> <li> <p><code>memory_scope</code> is a namespace within that level:</p> <ul> <li>Multiple agents can share a <code>memory_scope</code> if you want them to see the same memory subset.</li> <li>Or you can split scopes (<code>\"assistant.main\"</code>, <code>\"assistant.research\"</code>) for the same user/session.</li> <li>You can totally ignore memory_scope if you want all agents see the memory in the same level.</li> </ul> </li> </ul> <p>A good default:</p> <ul> <li>For typical chat agents: <code>memory_level=\"session\"</code>.</li> <li>For long-lived personalization: <code>memory_level=\"user\"</code>.</li> <li>For specialized agents: <code>memory_level=\"sessin\", memory_scope=\"assistant.simulation\"</code> </li> </ul>"},{"location":"ui/ui-notes/#5-current-limitations-and-evolving-apis","title":"5. Current limitations and evolving APIs","text":"<p>A few things are intentionally still evolving:</p>"},{"location":"ui/ui-notes/#51-run-outputs-via-runrecord","title":"5.1 Run outputs via <code>RunRecord</code>","text":"<p>Today:</p> <ul> <li><code>context.spawn_run()</code> returns a <code>run_id</code>.</li> <li><code>record = await context.wait_run(run_id)</code> returns a <code>RunRecord</code> with metadata (status, timestamps, etc.).</li> <li>Outputs are not yet exposed directly from <code>RunRecord</code>.</li> </ul> <p>Workaround patterns:</p> <ul> <li>Use artifacts as the output contract (e.g., JSON, CSV, reports written via the artifact store).</li> <li>Use memory to persist important \u201cresults\u201d that future runs or agents can read.</li> </ul> <p>The API here is expected to expand over time; check the changelog for updates.</p>"},{"location":"ui/ui-notes/#52-files-and-context_refs","title":"5.2 Files and <code>context_refs</code>","text":"<p>The exact structure of:</p> <ul> <li><code>files</code> (upload payloads), and</li> <li><code>context_refs</code> (references to selected files/artifacts/runs)</li> </ul> <p>may change as the UI gains:</p> <ul> <li>Richer file browsing and selection</li> <li>Cross-run artifact linking</li> <li>More flexible context passing</li> </ul> <p>The stable part is:</p> <ul> <li>Agents defined with <code>mode=\"chat_v1\"</code> take these parameters.</li> <li>The UI will always pass in a consistent structure that the backend understands.</li> </ul>"},{"location":"ui/ui-notes/#6-checklist-why-is-nothing-showing-up-in-the-ui","title":"6. Checklist: \u201cWhy is nothing showing up in the UI?\u201d","text":"<p>If you open <code>/ui</code> and see an empty App Gallery or Agent Gallery, check:</p> <ol> <li>Did you start the server with <code>--load-module</code> or <code>--load-path</code> pointing at your code?</li> <li>Is your module importable under <code>--project-root</code>?</li> <li>Did you mark at least one graph with <code>as_app={...}</code>?</li> <li>Did you define at least one <code>graph_fn</code> with <code>as_agent={...}</code> and the correct chat signature?</li> <li>Are there any import errors in the server logs?</li> <li>If running from source, did you build or copy the UI bundle into <code>ui_static/</code>?</li> </ol> <p>If all else fails, try a minimal example:</p> <ul> <li>New project dir</li> <li>Single <code>demos_clean</code> package</li> <li>One <code>@graphify(..., as_app=...)</code></li> <li>Start with the dev command from Start the server</li> </ul> <p>Once that works, gradually move back to your full project.</p>"}]}