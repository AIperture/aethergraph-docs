{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AetherGraph","text":"<p>A Python\u2011first agentic framework for R&amp;D workflows. The core surface is <code>@graph_fn</code> plus Context Services (LLM, memory, channel, artifacts, KV) and Tools for safe capability calls.</p> <p>\ud83d\udc49 Get hands-on in 5 minutes: Quickstart</p> <p>Why AetherGraph? - Python functions as first-class graph nodes (<code>@graph_fn</code>). - Built-in services: no bolt\u2011ons required, but easy to swap. - Artifacts + memory for traceable, reproducible research.</p>"},{"location":"context/","title":"Context Overview","text":"<p>Context is the lightweight runtime handle your tools/agents receive at execution time. It encapsulates the current run/graph/node scope and provides access to built\u2011in services (channels, memory, artifacts, kv, LLM, RAG, MCP, logger, continuations, clock, etc.) via concise helper methods.</p> <p>In short: do your work with plain Python, and when you need IO, state, or orchestration, call <code>context.&lt;service&gt;()</code>.</p>"},{"location":"context/#quick-start","title":"Quick start","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"hello_context\")\nasync def hello_context(*, context):\n    await context.channel().send_text(\"Hello from AetherGraph!\")\n    await context.memory().write_result(\n        topic=\"hello\",\n        outputs=[{\"name\":\"msg\",\"kind\":\"text\",\"value\":\"hello\"}],\n        tags=[\"demo\"],\n    )\n    log = context.logger()\n    log.info(\"done\", extra={\"stage\": \"finish\"})\n</code></pre>"},{"location":"context/#what-the-context-contains","title":"What the Context contains","text":"<p>Each <code>NodeContext</code> carries stable identifiers and bound services for the current execution scope.</p> <pre><code>@dataclass\nclass NodeContext:\n    run_id: str\n    graph_id: str\n    node_id: str\n    services: NodeServices  # wiring for all built\u2011ins\n</code></pre> <p>IDs - run_id \u2014 unique per execution run - graph_id \u2014 which graph this node belongs to - node_id \u2014 unique node invocation id</p>"},{"location":"context/#core-methods-oneliners","title":"Core methods (one\u2011liners)","text":"Method Purpose <code>context.channel(key: str | None = None)</code> Message &amp; interaction bus (text, buttons, files, streaming, progress). Defaults to configured channel; pass a key to target another (e.g., <code>\"slack:#lab\"</code>). <code>context.memory()</code> Session/run memory fa\u00e7ade (record events, write typed results, query recent/indices, rolling &amp; episode summaries, optional RAG helpers). <code>context.artifacts()</code> Artifact store/index fa\u00e7ade (stage/save/write files/dirs, search/best, pin). <code>context.kv()</code> Small, transient key\u2013value store for coordination and short\u2011lived caches. <code>context.llm(profile=\"default\")</code> LLM client for <code>chat()</code> and <code>embed()</code>; switch keys via <code>context.llm_set_key(...)</code>. <code>context.rag()</code> RAG fa\u00e7ade: create corpora, upsert docs, search/retrieve, answer with citations. <code>context.mcp(name)</code> MCP client to call external tool servers (stdio/ws/http), list/read resources. <code>context.logger()</code> Scoped Python logger with <code>{run_id, graph_id, node_id}</code> auto\u2011injected. <code>context.continuations()</code> Access to continuation store (usually used indirectly via <code>channel().ask_*</code>). <code>context.clock()</code> Clock/time helpers (scheduling, timestamps) if bound. <p>If a service is not configured in your runtime, its accessor raises a clear error (e.g., <code>LLMService not available</code>).</p>"},{"location":"context/#typical-patterns","title":"Typical patterns","text":""},{"location":"context/#1-ask-wait-continue","title":"1) Ask \u2192 Wait \u2192 Continue","text":"<pre><code>text = await context.channel().ask_text(\"Provide a dataset path\")\n# runtime yields, persists a continuation; resumes with user input\n</code></pre>"},{"location":"context/#2-stream-progress","title":"2) Stream + progress","text":"<pre><code>async with context.channel().stream() as s:\n    await s.delta(\"Parsing\u2026 \")\n    await s.delta(\"OK. \")\nasync with context.channel().progress(title=\"Training\", total=100) as p:\n    for i in range(0, 101, 5):\n        await p.update(current=i)\n</code></pre>"},{"location":"context/#3-artifacts-memory","title":"3) Artifacts + memory","text":"<pre><code>art = await context.artifacts().save(path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\":\"A\"})\nawait context.memory().write_result(\n    topic=\"report\",\n    outputs=[{\"name\":\"uri\",\"kind\":\"uri\",\"value\": art.uri}],\n)\n</code></pre>"},{"location":"context/#4-rag-answer-using-llm","title":"4) RAG answer using LLM","text":"<pre><code>hits = await context.rag().search(\"notes\", query=\"What is MTF?\", k=5)\nans = await context.rag().answer(\"notes\", question=\"What is MTF?\", style=\"concise\")\nawait context.channel().send_text(ans[\"answer\"]) \n</code></pre>"},{"location":"context/#5-external-tools-via-mcp","title":"5) External tools via MCP","text":"<pre><code>res = await context.mcp(\"ws\").call(\"search\", {\"q\": \"tolerance analysis\", \"k\": 5})\n</code></pre>"},{"location":"context/#choosing-a-channel","title":"Choosing a channel","text":"<ul> <li>By default, <code>context.channel()</code> resolves to the runtime\u2019s configured default (e.g., console).</li> <li>Pass a channel key to target a specific adapter:</li> <li><code>\"console:stdin\"</code></li> <li><code>\"slack:#research\"</code> or <code>\"slack:@alice\"</code></li> <li><code>\"telegram:chat:&lt;id&gt;\"</code></li> <li>You can override per call: <code>context.channel(\"slack:#lab\").send_text(\"hi\")</code> or <code>context.channel().send_text(\"hi\", channel=\"slack:#lab\")</code>.</li> </ul>"},{"location":"context/#error-handling-logging","title":"Error handling &amp; logging","text":"<ul> <li>Service calls raise <code>RuntimeError</code> if the service is missing; wire them at server startup.</li> <li>Use <code>context.logger().exception(\"...\")</code> inside <code>except</code> blocks to capture tracebacks.</li> <li>Prefer logging artifact URIs and metrics over large payloads.</li> </ul>"},{"location":"context/#server-wiring-very-short","title":"Server wiring (very short)","text":"<p>Make services available before running graphs/tools. <pre><code># Pseudo-bootstrap\nservices = build_runtime_services(\n    logger=StdLoggerService.build(),\n    llm=make_llm_service(),\n    memory=make_memory_facade_factory(),\n    artifacts=make_artifact_facade(),\n    kv=EphemeralKV(prefix=\"ag:\"),\n    rag=make_rag_facade(),\n    mcp=make_mcp_service(),\n)\nstart_server(services)\n</code></pre></p>"},{"location":"context/#philosophy","title":"Philosophy","text":"<ul> <li>Python\u2011first: context calls are helpers, not a DSL. Keep your logic in plain Python.</li> <li>Minimal surface: each service exposes a small, composable API.</li> <li>Swappable backends: you can replace services (e.g., LLM provider, KV backend) without changing your tools.</li> </ul>"},{"location":"context/#see-also","title":"See also","text":"<ul> <li><code>context.channel()</code> \u2014 cooperative waits, streaming, progress</li> <li><code>context.memory()</code> \u2014 event log, typed results, summaries, RAG helpers</li> <li><code>context.artifacts()</code> \u2014 CAS storage + search</li> <li><code>context.llm()</code> \u2014 chat &amp; embeddings</li> <li><code>context.rag()</code> \u2014 corpora &amp; QA</li> <li><code>context.mcp()</code> \u2014 external tool bridges</li> <li><code>context.kv()</code> \u2014 transient coordination</li> <li><code>context.logger()</code> \u2014 structured logs</li> </ul>"},{"location":"graph_fn/","title":"Graph Function <code>graph_fn</code> Quickstart &amp; Reference","text":"<p>Make any Python async function a runnable, inspectable Graph Function with a single decorator. You keep normal Python control\u2011flow; AetherGraph wires in runtime services via <code>context</code> and exposes your outputs as graph boundaries.</p>"},{"location":"graph_fn/#tldr","title":"TL;DR","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}! \ud83d\udc4b\")\n    return {\"greeting\": f\"Hello, {name}\"}\n\n# Run (async)\nres = await hello(name=\"Aether\")          # \u2192 {\"greeting\": \"Hello, Aether\"}\n\n# Or run (sync) for quick scripts\nout = hello.sync(name=\"Aether\")            # same result\n</code></pre>"},{"location":"graph_fn/#what-is-a-graph-function","title":"What is a Graph Function?","text":"<p>A Graph Function is a small wrapper around your Python function that:</p> <ul> <li> <p>builds a fresh internal TaskGraph,</p> </li> <li> <p>injects a <code>NodeContext</code> if your function declares <code>*, context</code>,</p> </li> <li> <p>executes your function (awaiting if needed),</p> </li> <li> <p>normalizes the return value into named outputs, and</p> </li> <li> <p>records graph boundary outputs for downstream composition/inspection.</p> </li> </ul> <p>You do not need to learn a new DSL. Write Python; use <code>context.&lt;service&gt;()</code> when you need IO/state.</p>"},{"location":"graph_fn/#decorator-signature","title":"Decorator signature","text":"<pre><code>@graph_fn(\n    name: str,\n    inputs: list[str] | None = None,\n    outputs: list[str] | None = None,\n    version: str = \"0.1.0\",\n    agent: str | None = None,  # optional: also register as an agent name\n)\n</code></pre> <p>Required</p> <ul> <li>name (str) \u2013 Unique identifier for this graph function.</li> </ul> <p>Optional</p> <ul> <li> <p>inputs (list[str]) \u2013 Declares input names for docs/registry (not enforced at call time).</p> </li> <li> <p>outputs (list[str]) \u2013 Declares output names/order; enables single\u2011literal returns.</p> </li> <li> <p>version (str) \u2013 Semantic version for registry/discovery.</p> </li> <li> <p>agent (str) \u2013 Also register in the <code>agent</code> namespace (advanced).</p> </li> </ul>"},{"location":"graph_fn/#function-shape","title":"Function shape","text":"<p><pre><code>@graph_fn(name=\"example\", inputs=[\"x\"], outputs=[\"y\"])\nasync def example(x: int, *, context):\n    # use services via context: channel/memory/artifacts/kv/llm/rag/mcp/logger\n    await context.channel().send_text(f\"x={x}\")\n    return {\"y\": x + 1}\n</code></pre> - Positional/keyword parameters are your API.</p> <ul> <li>Include <code>*, context</code> to receive the <code>NodeContext</code>. If you don\u2019t declare it, nothing is injected.</li> </ul>"},{"location":"graph_fn/#returning-values-normalization-rules","title":"Returning values (normalization rules)","text":"<p>Your return can be:</p> <p>1) Dict of outputs (recommended) <pre><code>return {\"result\": 42, \"note\": \"ok\"}\n</code></pre></p> <p>2) Single literal \u2014 only if you declared exactly one output <pre><code>@graph_fn(name=\"one\", outputs=[\"y\"])\nasync def one(*, context):\n    return 123  # normalized to {\"y\": 123}\n</code></pre></p> <p>3) NodeHandle / Refs (advanced) If you return node handles or refs created by graph utilities, they\u2019re exposed as boundary outputs automatically. For most users, plain dicts/literals are enough.</p> <p>Validation - If <code>outputs</code> are declared, missing keys raise: <code>ValueError(\"Missing declared outputs: ...\")</code>. - Returning a single literal without exactly one declared output raises an error.</p>"},{"location":"graph_fn/#running","title":"Running","text":"<p><pre><code># Async (preferred in apps/servers)\nres = await my_fn(a=1, b=2)\n\n# Sync helper (scripts/CLI/tests)\nout = my_fn.sync(a=1, b=2)\n</code></pre> Internally this builds a fresh runtime environment, constructs a TaskGraph, executes your function in an interpreter, and returns the normalized outputs.</p>"},{"location":"graph_fn/#accessing-context","title":"Accessing Context","text":"<p>Declare <code>*, context</code> to use built\u2011ins: <pre><code>@graph_fn(name=\"report\", outputs=[\"uri\"])\nasync def report(data: dict, *, context):\n    # Log breadcrumbs\n    log = context.logger(); log.info(\"building report\")\n\n    # Save an artifact\n    art = await context.artifacts().save(path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\":\"A\"})\n\n    # Record a typed result in memory\n    await context.memory().write_result(topic=\"report\", outputs=[{\"name\":\"uri\",\"kind\":\"uri\",\"value\": art.uri}])\n\n    # Notify user\n    await context.channel().send_text(f\"Report ready: {art.uri}\")\n    return {\"uri\": art.uri}\n</code></pre></p>"},{"location":"graph_fn/#concurrency-retry-advanced","title":"Concurrency &amp; retry (advanced)","text":"<p><code>GraphFunction.run()</code> accepts knobs used by the interpreter/runtime: <pre><code>await my_fn.run(\n    env=None,                            # supply a prebuilt RuntimeEnv, or let the runner build one\n    retry=RetryPolicy(),                 # backoff/retries for node execution\n    max_concurrency: int | None = None,  # cap parallelism inside the interpreter\n    **inputs,\n)\n</code></pre> For most users, calling <code>await my_fn(...)</code> / <code>.sync(...)</code> is sufficient; the runner chooses sensible defaults.</p>"},{"location":"graph_fn/#minimal-patterns","title":"Minimal patterns","text":"<p>Hello + context <pre><code>@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}\")\n    return {\"greeting\": f\"Hello, {name}\"}\n</code></pre></p> <p>One output (literal) <pre><code>@graph_fn(name=\"square\", outputs=[\"y\"])\nasync def square(x: int, *, context):\n    return x * x\n</code></pre></p> <p>Multi\u2011output dict <pre><code>@graph_fn(name=\"stats\", outputs=[\"mean\",\"std\"])\nasync def stats(xs: list[float], *, context):\n    import statistics as st\n    return {\"mean\": st.mean(xs), \"std\": st.pstdev(xs)}\n</code></pre></p>"},{"location":"graph_fn/#tips-gotchas","title":"Tips &amp; gotchas","text":"<ul> <li>Always include <code>*, context</code> when you need services (channel/memory/llm/etc.).</li> <li>Declare <code>outputs=[...]</code> if you want to return a single literal; otherwise return a dict.</li> <li>Output validation is strict when <code>outputs</code> are declared\u2014return all of them.</li> <li><code>inputs=[...]</code> is for documentation/registry; your Python signature is the source of truth at call time.</li> <li>You can also register the function as an agent by passing <code>agent=\"name\"</code> (covered later).</li> </ul>"},{"location":"graph_fn/#next-steps","title":"Next steps","text":"<ul> <li><code>graphify</code>: combine multiple functions into a larger graph with explicit edges.</li> <li><code>@tool</code>: publish functions as reusable nodes (IO typed), then orchestrate with <code>graphify</code>.</li> <li>Context services: <code>channel</code>, <code>artifacts</code>, <code>memory</code>, <code>kv</code>, <code>llm</code>, <code>rag</code>, `m</li> </ul>"},{"location":"graphify/","title":"AetherGraph \u2014 <code>@graphify</code> (Builder Decorator)","text":"<p><code>@graphify</code> lets you write a plain Python function whose body builds a <code>TaskGraph</code> using tool calls. Instead of executing immediately, the function becomes a graph factory: call <code>.build()</code> to get a concrete graph, <code>.spec()</code> to inspect, and <code>.io()</code> to see its input/output signature.</p>"},{"location":"graphify/#why-graphify-vs-graph_fn","title":"Why <code>graphify</code> vs <code>graph_fn</code>?","text":"Aspect <code>graph_fn</code> <code>graphify</code> Primary purpose Execute now as a single graph node Build a graph (explicit fan\u2011in/fan\u2011out wiring) Return at call Dict of outputs (or awaitable) A builder you later <code>.build()</code> into a graph Control\u2011flow Pythonic, implicit graph behind the scenes Explicit nodes &amp; edges via tool calls (<code>NodeHandle</code>) Best for Orchestration + <code>context.*</code> services Pipelines, DAGs, reusable subgraphs <p>Use <code>graphify</code> when you want:</p> <ul> <li>Multiple tool calls as separate nodes</li> <li>Explicit dependencies (<code>_after</code>) and fan\u2011in/fan\u2011out</li> <li>To inspect/serialize the graph spec for registry/UI</li> <li>To reuse the same pipeline with different inputs</li> </ul> <p>Use <code>graph_fn</code> when you want:</p> <ul> <li>A simple function that runs immediately and returns values</li> <li>Access to <code>context.channel()/memory()/artifacts()/llm()</code> services</li> <li>Minimal ceremony (one decorator and go)</li> </ul>"},{"location":"graphify/#decorator-signature","title":"Decorator Signature","text":"<pre><code>from aethergraph import graphify\n\n@graphify(*, name=\"default_graph\", inputs=(), outputs=None, version=\"0.1.0\", agent: str | None = None)\ndef build_fn(...):\n    ...  # tool calls returning NodeHandles\n    return {\"y\": handle.y}\n</code></pre> <p>Parameters</p> <ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (Iterable[str] or dict) \u2014 Declare required/optional inputs.  </li> <li>If <code>list/tuple</code>: treated as required input names.  </li> <li>If <code>dict</code>: <code>{required_name: ..., ...}</code> for optional mapping; builder will declare required/optional accordingly.</li> <li>outputs (list[str] | None) \u2014 Names to expose. If you return a single literal, you must declare exactly one.</li> <li>version (str) \u2014 Semantic version for registry/spec metadata.</li> <li>agent (str | None) \u2014 Optionally register the built graph under <code>agent</code> namespace.</li> </ul> <p>Return value</p> <p>The decorated symbol becomes a builder function with helpers:</p> <ul> <li><code>.build() -&gt; TaskGraph</code></li> <li><code>.spec() -&gt; GraphSpec</code></li> <li><code>.io() -&gt; IOSignature</code></li> <li>Attributes: <code>.graph_name</code>, <code>.version</code></li> </ul>"},{"location":"graphify/#writing-a-graphify-body","title":"Writing a <code>@graphify</code> Body","text":"<p>Inside the function:</p> <ol> <li>Use <code>arg(\"name\")</code> to reference declared inputs.</li> <li>Call <code>@tool</code> functions (or <code>call_tool(\"pkg.mod:fn\", ...)</code>) \u2014 each returns a <code>NodeHandle</code> in build mode.</li> <li>Return outputs as:</li> <li>A dict mapping names \u2192 <code>NodeHandle</code> outputs or refs/literals, or</li> <li>A single <code>NodeHandle</code> (its outputs will be exposed), or</li> <li>A single literal only if <code>outputs</code> has length 1.</li> </ol> <pre><code>from aethergraph import graphify, tool\nfrom aethergraph.graph import arg\n\n@tool(outputs=[\"embedded\"])\ndef embed(text: str): ...\n\n@tool(outputs=[\"score\"])\ndef score(vec, query_vec): ...\n\n@graphify(name=\"ranker\", inputs=[\"texts\",\"query\"], outputs=[\"scores\"])\ndef ranker(texts, query):\n    q = embed(text=query)\n    # fan\u2011out: call `embed` for each text\n    vecs = [embed(text=t) for t in texts]  # list[NodeHandle]\n    # fan\u2011in: score each against query vec\n    scs = [score(vec=v.embedded, query_vec=q.embedded) for v in vecs]\n    return {\"scores\": [s.score for s in scs]}\n\nG = ranker.build()\n</code></pre>"},{"location":"graphify/#control-dependencies-without-data-edges","title":"Control Dependencies without Data Edges","text":"<p>Use <code>_after</code> when you must enforce order but don\u2019t pass outputs: <pre><code>@tool(outputs=[\"ok\"])\ndef fetch(): return {\"ok\": True}\n\n@tool(outputs=[\"done\"])\ndef train(): return {\"done\": True}\n\n@graphify(name=\"seq\", inputs=[], outputs=[\"done\"])\ndef seq():\n    a = fetch()\n    b = train(_after=a)   # run b after a\n    return {\"done\": b.done}\n</code></pre></p>"},{"location":"graphify/#registration","title":"Registration","text":"<p>If a registry is active, <code>@graphify</code> registers the built graph under <code>nspace=\"graph\"</code> with <code>name</code>/<code>version</code> so it can be listed or launched elsewhere. You can also register it as an <code>agent</code> via the <code>agent=</code> parameter.</p>"},{"location":"graphify/#example-endtoend-pipeline","title":"Example: End\u2011to\u2011End Pipeline","text":"<pre><code>from aethergraph import tool, graphify\nfrom aethergraph.graph import arg\n\n@tool(outputs=[\"rows\"])\ndef load_csv(path: str): ...\n\n@tool(outputs=[\"clean\"])\ndef clean(rows): ...\n\n@tool(outputs=[\"model\"])\ndef train(data): ...\n\n@tool(outputs=[\"uri\"])\ndef report(model): ...\n\n@graphify(name=\"etl_train_report\", inputs=[\"csv_path\"], outputs=[\"uri\"])\ndef etl_train_report(csv_path):\n    raw  = load_csv(path=arg(\"csv_path\"))\n    tidy = clean(rows=raw.rows)\n    mod  = train(data=tidy.clean)\n    rep  = report(model=mod.model)\n    return {\"uri\": rep.uri}\n\nG = etl_train_report.build()\n</code></pre>"},{"location":"graphify/#using-tool-inside-graph_fn-brief","title":"Using <code>@tool</code> Inside <code>@graph_fn</code> (Brief)","text":"<p>While <code>@graph_fn</code> is for immediate execution, you can drop explicit tool nodes inside a <code>graph_fn</code> when you want finer\u2011grained tracing or parallelism:</p> <pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int): return {\"y\": x*x}\n\n@graph_fn(name=\"mix\")\nasync def mix(x: int, *, context):\n    h = square(x=x)                 # schedules a tool node in the implicit graph\n    await context.channel().send_text(\"running square\u2026\")\n    return {\"y\": h.y}               # exposes tool output as graph_fn output\n</code></pre> <p>Prefer <code>@graphify</code> for full pipeline construction; use <code>@graph_fn</code> when you want to orchestrate services (<code>context.*</code>) and run quickly.</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>A 5\u2011minute on\u2011ramp to AetherGraph: install, start the sidecar server, and run your first <code>@graph_fn</code>.</p>"},{"location":"quickstart/#1-install","title":"1) Install","text":"<pre><code>pip install aethergraph\n# or, from source\n# pip install -e .\n</code></pre> <p>Python: 3.10+</p>"},{"location":"quickstart/#2-start-the-sidecar-server-oneliner","title":"2) Start the sidecar server (one\u2011liner)","text":"<p>AetherGraph ships a lightweight sidecar that wires up core services (logger, artifacts, memory, KV, channels, etc.)</p> <pre><code># quickstart_server.py\nfrom aethergraph import start\n\nurl = start(host=\"127.0.0.1\", port=0, log_level=\"warning\")\nprint(\"AetherGraph server:\", url)\n</code></pre> <p>Run it:</p> <pre><code>python quickstart_server.py\n</code></pre> <p>You should see an HTTP URL like <code>http://127.0.0.1:54321</code> printed. (Random free port by default.)</p>"},{"location":"quickstart/#3-your-first-graph-function","title":"3) Your first graph function","text":"<p><code>@graph_fn</code> turns an ordinary async Python function into a runnable graph entrypoint. If you include a <code>context</code> parameter, you get access to built\u2011in services like <code>context.channel()</code> and <code>context.memory()</code>.</p> <pre><code># quickstart_graph_fn.py\nfrom aethergraph import graph_fn, NodeContext\nfrom aethergraph import start\n\n# 1) Start the sidecar so services are available\nstart()\n\n# 2) Define a small graph function\n@graph_fn(name=\"hello.world\", inputs=[\"name\"], outputs=[\"greeting\"], version=\"0.1.0\")\nasync def hello_world(name: str, *, context: NodeContext):\n    # Use the channel to send a message (console by default)\n    await context.channel().send_text(f\"\ud83d\udc4b Hello, {name}! Running graph\u2026\")\n\n    # Do any Python you want here \u2014 call tools, query memory, etc.\n    greeting = f\"Hello, {name}. Nice to meet you from AetherGraph.\"\n\n    # Return outputs as a dict (keys must match `outputs=[...]`)\n    return {\"greeting\": greeting}\n\n# 3) Run it (async wrapper provided)\nif __name__ == \"__main__\":\n    import asyncio\n    async def main():\n        res = await hello_world(name=\"Researcher\")\n        print(\"Result:\", res)\n    asyncio.run(main())\n</code></pre> <p>Run it:</p> <pre><code>python quickstart_graph_fn.py\n</code></pre> <p>You should see a console message from the channel and printed output like:</p> <pre><code>Result: {\"greeting\": \"Hello, Researcher. Nice to meet you from AetherGraph.\"}\n</code></pre>"},{"location":"quickstart/#4-what-just-happened","title":"4) What just happened?","text":"<ul> <li>Sidecar server booted in the background and installed default services (channels, artifacts, memory, KV, logger).</li> <li><code>@graph_fn</code> built a tiny task graph from your function and executed it.</li> <li><code>context.channel()</code> used the default channel (console) to emit a message.</li> </ul> <p>Tip: You can override the channel at call\u2011site with <code>context.channel(\"slack:#research\")</code>, once you\u2019ve configured a Slack adapter.</p>"},{"location":"quickstart/#5-next-steps","title":"5) Next steps","text":"<ul> <li>Add tools with <code>@tool</code> to wrap reusable steps and surface inputs/outputs.</li> <li>Use <code>@graphify</code> for fan\u2011in / fan\u2011out graph construction when the body is mostly tool calls.</li> <li>Explore artifacts (<code>context.artifacts()</code>), memory (<code>context.memory()</code>), and RAG (</li> </ul>"},{"location":"server/","title":"AetherGraph \u2014 Server (Sidecar) Overview","text":"<p>The AetherGraph server is a lightweight sidecar that wires up all runtime services (channels, memory, artifacts, KV, LLM, RAG, MCP, logging, etc.) and exposes a small HTTP/WebSocket surface for adapters and tools. You can run AetherGraph without the server, but the sidecar makes it easy to:</p> <ul> <li>Use GUI/chat adapters (Slack/Telegram/Console UI) that push events back to your runs</li> <li>Host continuation callbacks for <code>ask_text()</code> / <code>ask_approval()</code></li> <li>Centralize service wiring (secrets, paths, corpora, registries)</li> <li>Inspect/trace runs, artifacts, and health in one place</li> </ul> <p>Think of it as your local control plane so your graph functions can stay plain Python.</p>"},{"location":"server/#quick-start","title":"Quick Start","text":"<pre><code>from aethergraph.server import start, stop\n\n# 1) Start the sidecar (in a background thread) and get its base URL\nurl = start(host=\"127.0.0.1\", port=0)   # port=0 \u2192 auto-pick a free port\nprint(\"AetherGraph sidecar:\", url)\n\n# 2) Run your graph functions as usual\nfrom aethergraph import graph_fn\n\n@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}\")\n    return {\"greeting\": f\"Hello, {name}\"}\n\n# ... elsewhere ...\n# res = await hello(name=\"ZC\")\n\n# 3) (Optional) Stop when done (tests/CLI)\nstop()\n</code></pre>"},{"location":"server/#api-start-start_async-stop","title":"API \u2014 <code>start()</code> / <code>start_async()</code> / <code>stop()</code>","text":""},{"location":"server/#start","title":"start","text":"<p><pre><code>start(*, workspace: str = \"./aeg_workspace\", session_id: str | None = None,\n      host: str = \"127.0.0.1\", port: int = 0, log_level: str = \"warning\") -&gt; str\n</code></pre> Start the sidecar in a background thread. Safe to call at the top of scripts or notebook cells.</p> <p>Parameters</p> <ul> <li> <p>workspace (str) \u2013 Root directory for runtime state (artifacts, logs, corpora, temp files). Auto\u2011created.</p> </li> <li> <p>session_id (str, optional) \u2013 Override the logical session. If <code>None</code>, the runtime will create one.</p> </li> <li> <p>host (str) \u2013 Bind address (defaults to loopback).</p> </li> <li> <p>port (int) \u2013 <code>0</code> picks a free port automatically; otherwise bind an explicit port.</p> </li> <li> <p>log_level (str) \u2013 Uvicorn log level (e.g., <code>\"info\"</code>, <code>\"warning\"</code>).</p> </li> </ul> <p>Returns str \u2013 Base URL, e.g., <code>\"http://127.0.0.1:54321\"</code>.</p>"},{"location":"server/#start_async","title":"start_async","text":"<p><pre><code>start_async(**kwargs) -&gt; str\n</code></pre> Async\u2011friendly wrapper that still runs the server in a thread to avoid clashing with your event loop.</p>"},{"location":"server/#stop","title":"stop","text":"<p><pre><code>stop() -&gt; None\n</code></pre> Signal the background server to shut down and join its thread (useful in tests/CI or ephemeral scripts).</p>"},{"location":"server/#why-a-sidecar","title":"Why a sidecar?","text":"<ul> <li>Continuations: <code>context.channel().ask_*</code> creates a continuation token and waits for a resume callback; the server receives user replies (Slack/Telegram/HTTP) and wakes your run.</li> <li>Adapters: chat/file/progress adapters connect over HTTP/WS to publish events (<code>agent.message</code>, <code>agent.progress.*</code>, uploads) into your run.</li> <li>Central config: one place to load settings, secrets, workspace paths, and register services (LLM, RAG, MCP, artifact store, memory backends).</li> <li>Inspection: optional health and tracing endpoints (depending on your app factory) to debug runs locally.</li> </ul>"},{"location":"server/#what-start-actually-does","title":"What <code>start()</code> actually does","text":"<ol> <li>Loads app settings (<code>load_settings()</code>), installs them as current (<code>set_current_settings(...)</code>).</li> <li>Builds a FastAPI app via <code>create_app(workspace=..., cfg=...)</code> \u2014 this registers services and routes.</li> <li>Picks a free port if <code>port=0</code> and launches Uvicorn in a background thread (non\u2011blocking).</li> <li>Returns the base URL so other components (e.g., WS/HTTP MCP clients) can connect.</li> </ol>"},{"location":"server/#typical-usage-patterns","title":"Typical usage patterns","text":""},{"location":"server/#notebooks-quick-scripts","title":"Notebooks &amp; quick scripts","text":"<pre><code>url = start(port=0)\n# \u2026 run several cells that use context.channel()/continuations\n# restart kernel or call stop() when done\n</code></pre>"},{"location":"server/#longrunning-dev-server","title":"Long\u2011running dev server","text":"<ul> <li>Call <code>start(host=\"0.0.0.0\", port=8787, log_level=\"info\")</code> once at process start.</li> <li>Point Slack/Telegram adapters or local tools at <code>http://localhost:8787</code>.</li> </ul>"},{"location":"server/#testsci","title":"Tests/CI","text":"<pre><code>url = start(port=0)\ntry:\n    # run test suite that uses continuations/artifacts\n    ...\nfinally:\n    stop()\n</code></pre>"},{"location":"server/#interop-with-context-services","title":"Interop with context services","text":"<p>Once the sidecar is up, graph functions can rely on bound services:</p> <ul> <li> <p><code>context.channel()</code> \u2013 routes via the server to your chat adapters</p> </li> <li> <p><code>context.artifacts()</code> \u2013 saves to the workspace CAS under the sidecar</p> </li> <li> <p><code>context.memory()</code> \u2013 hotlog/persistence live alongside the server\u2019s config</p> </li> <li> <p><code>context.rag()</code> \u2013 corpora root under workspace; embedders/indices wired here</p> </li> <li> <p><code>context.mcp(...)</code> \u2013 WS/HTTP MCP clients often target sidecar endpoints</p> </li> </ul>"},{"location":"server/#security-notes","title":"Security notes","text":"<ul> <li>Default bind is <code>127.0.0.1</code> (local only). Use <code>0.0.0.0</code> only in trusted networks.</li> <li>Protect WS/HTTP endpoints behind auth headers/tokens if exposing beyond localhost.</li> <li>Never log plaintext API keys; prefer a Secrets store.</li> </ul>"},{"location":"server/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Port already in use: pass <code>port=0</code> or another free port.</li> <li>Nothing happens after ask_text(): ensure the chat adapter posts replies to the sidecar (correct base URL / token).</li> <li>No LLM/kv/rag configured: your <code>create_app()</code> must wire these services (or the accessors will raise \"\u2026 not available\").</li> <li>Jupyter hangs on restart: call <code>stop()</code> before restarting the kernel, or rely on kernel shutdown to terminate the thread.</li> </ul>"},{"location":"server/#minimal-adapter-sketch-optional","title":"Minimal adapter sketch (optional)","text":"<pre><code># Example: WebSocket adapter connecting to sidecar URL\na_sync_ws_client.connect(f\"{url.replace('http','ws')}/events\", headers={\"Authorization\": \"Bearer demo\"})\n# publish OutEvent / listen for Continuation notifications\n</code></pre>"},{"location":"server/#summary","title":"Summary","text":"<p>Run the sidecar server to centralize runtime services, handle continuations/adapters, and keep your graph functions clean. Use <code>start()</code> to launch in\u2011process, <code>start_async()</code> in async apps, and <code>stop()</code> for tests/CI. Configure paths and services once; build everything else in plain Python.</p>"},{"location":"tools/","title":"AetherGraph \u2014 <code>@tool</code> Decorator (Reference &amp; How\u2011to)","text":"<p><code>@tool</code> turns a plain Python function into a tool node that can be executed immediately or added to a graph during build time. You write ordinary Python, declare outputs, and AetherGraph handles result normalization and graph node creation.</p>"},{"location":"tools/#what-is-a-tool","title":"What is a Tool?","text":"<p>A tool is a reusable, IO\u2011typed operation that can be executed on its own or orchestrated inside a graph. Tools are perfect for things like \u201cload CSV\u201d, \u201ctrain model\u201d, \u201cplot chart\u201d, \u201csend_slack\u201d, etc.</p> <ul> <li>Immediate mode (no graph builder active): calling the tool runs the Python function right away and returns a dict of outputs.</li> <li>Graph mode (inside a <code>with graph(...):</code> block or a <code>@graphify</code> body): calling the tool adds a node to the graph and returns a <code>NodeHandle</code> you can wire to other nodes (fan\u2011in/fan\u2011out).</li> <li>Tools automatically register in the runtime registry (<code>nspace=\"tool\"</code>) when a registry is active.</li> </ul> <p>This page covers the simple function form. (The advanced waitable class form is documented separately.)</p>"},{"location":"tools/#decorator-signature","title":"Decorator Signature","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs: list[str], *, inputs: list[str] | None = None,\n      name: str | None = None, version: str = \"0.1.0\")\ndef your_function(...): ...\n</code></pre> <p>Parameters</p> <ul> <li>outputs (list[str], required) \u2014 Declares the output keys your tool will produce.</li> <li>inputs (list[str], optional) \u2014 Explicit input names. Omit to infer from function signature (excluding <code>*args</code>/<code>**kwargs</code>).  </li> <li>name (str, optional) \u2014 Registry/display name. Defaults to the function\u2019s <code>__name__</code>.  </li> <li>version (str, optional) \u2014 Semantic version recorded in the registry (default: <code>\"0.1.0\"</code>).</li> </ul> <p>Return value (call\u2011site dependent)</p> <ul> <li>Immediate mode: returns a <code>dict</code> of outputs.  </li> <li>Graph mode: returns a <code>NodeHandle</code> (or an awaitable handle under an interpreter) to be wired/exposed by the builder.</li> </ul>"},{"location":"tools/#return-normalization","title":"Return Normalization","text":"<p>The wrapped function can return different shapes; the decorator normalizes into a dict that must include every declared output:</p> <ul> <li><code>None</code> \u2192 <code>{}</code></li> <li><code>dict</code> \u2192 used as\u2011is</li> <li><code>tuple</code> \u2192 <code>{\"out0\": v0, \"out1\": v1, ...}</code></li> <li>single value \u2192 <code>{\"result\": value}</code></li> </ul> <p>If any declared <code>outputs</code> are missing from the normalized dict, a <code>ValueError</code> is raised.</p>"},{"location":"tools/#control-keywords-graph-mode","title":"Control Keywords (graph mode)","text":"<p>When calling a tool while building a graph (e.g., inside a <code>with graph(...):</code> or <code>@graphify</code> body), you may pass these special kwargs to influence scheduling/metadata:</p> <ul> <li><code>_after</code> (NodeHandle | list[NodeHandle | node_id]): explicit dependency edges (fan\u2011in).  </li> <li><code>_name</code> (str): display name for UI/spec.  </li> <li><code>_id</code> (str): hard override of the node ID (must be unique in the graph).  </li> <li><code>_alias</code> (str): optional alias for reverse lookups.  </li> <li><code>_labels</code> (Iterable[str]): lightweight tags for search/grouping.</li> </ul> <p>Example:</p> <pre><code>res = my_tool(a=arg_a, b=arg_b, _after=[prev1, prev2], _name=\"preprocess\", _labels=[\"data\",\"prep\"])\n</code></pre> <p>These control keys are stripped before calling your function and only affect graph construction.</p>"},{"location":"tools/#simple-examples","title":"Simple Examples","text":""},{"location":"tools/#1-immediate-execution-no-graph-builder-active","title":"1) Immediate execution (no graph builder active)","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"sum\", \"mean\"])  # outputs you promise to return\ndef stats(xs: list[float]):\n    s = sum(xs)\n    return {\"sum\": s, \"mean\": s / len(xs)}\n\nout = stats([1,2,3,4])   # \u2192 {\"sum\": 10, \"mean\": 2.5}\n</code></pre>"},{"location":"tools/#2-graph-construction-inside-a-builder","title":"2) Graph construction (inside a builder)","text":"<pre><code>from aethergraph import tool\nfrom aethergraph import graphify\nfrom aethergraph.graph import arg  # or from aethergraph.graph.graph_refs import arg\n\n@tool(outputs=[\"y\"])\ndef add(x: int, z: int): return {\"y\": x + z}\n\n@tool(outputs=[\"z\"])\ndef mul(x: int, k: int): return {\"z\": x * k}\n\n@graphify(name=\"pipeline\", inputs=[\"x\"], outputs=[\"y\"])\ndef pipeline(x):\n    a = mul(x=arg(\"x\"), k=2)          # NodeHandle(\"mul_...\")\n    b = add(x=arg(\"x\"), z=a.z)        # depends on `a` automatically via data edge\n    return {\"y\": b.y}\n\nG = pipeline.build()                    # TaskGraph\nspec = pipeline.spec()                  # graph spec for inspection/registry\nio = pipeline.io()                      # IO signature\n</code></pre>"},{"location":"tools/#3-forcing-an-order-with-_after-no-data-edge","title":"3) Forcing an order with <code>_after</code> (no data edge)","text":"<pre><code>@tool(outputs=[\"ok\"])\ndef init(): return {\"ok\": True}\n\n@tool(outputs=[\"ready\"])\ndef warmup(): return {\"ready\": True}\n\n@graphify(name=\"order_demo\", inputs=[], outputs=[\"ready\"])\ndef order_demo():\n    n1 = init()\n    n2 = warmup(_after=n1)   # enforce sequencing without passing data\n    return {\"ready\": n2.ready}\n</code></pre>"},{"location":"tools/#registration-optional","title":"Registration (Optional)","text":"<p>If a runtime registry is active (via <code>current_registry()</code>), the decorator auto\u2011registers your tool under the <code>tool</code> namespace with its <code>name</code> and <code>version</code> so it can be listed and referenced later.</p> <p>You can also call tools by dotted path via <code>call_tool(\"pkg.module:function\", arg1=..., ...)</code> to avoid importing at build sites, but the recommended ergonomic flow is to <code>import</code> the tool and call it directly.</p>"},{"location":"tools/#best-practices","title":"Best Practices","text":"<ul> <li>Keep tools focused and side\u2011effect aware (e.g., write artifacts via <code>context.artifacts()</code> inside <code>@graph_fn</code> wrappers).</li> <li>Always declare <code>outputs</code> and make your function return those keys.</li> <li>Use <code>_after</code> for control dependencies when no data edge exists.</li> <li>Prefer composing tools via <code>@graphify</code> for explicit fan\u2011in/fan\u2011out graphs.</li> <li>Inside <code>@graph_fn</code>, you can call tools to create explicit nodes, but <code>@graph_fn</code> is for immediate orchestration.</li> </ul>"},{"location":"build-graphs/","title":"Build Graphs in AetherGraph","text":"<p>Welcome! This section is the fastest way to grok how to build and run graphs with Python-first ergonomics.</p> <p>We introduce things in the order you will actually use them:</p> <ol> <li><code>@graph_fn</code> \u2014 the on-ramp. Wrap a regular Python function so it runs as a single graph node, with full <code>context.*</code> access. Great for demos, services, notebooks.</li> <li><code>@tool</code> \u2014 make any function a graph node. Use it inside <code>graph_fn</code> for per-step visibility, metrics, artifacts, and reuse.</li> <li><code>@graphify</code> \u2014 build an explicit DAG for fan-out/fan-in, ordering via <code>_after</code>, subgraphs, and reuse.</li> </ol> <p>Tip: Start with <code>@graph_fn</code> (plus a couple of <code>@tool</code> calls). Move to <code>@graphify</code> when you want explicit topology, parallel map/reduce, barriers, or long-lived pipelines.</p>"},{"location":"build-graphs/#what-is-a-graph-here","title":"What is a \"graph\" here?","text":"<ul> <li>AetherGraph executes TaskGraphs \u2014 directed acyclic graphs of nodes.</li> <li>A node can be:</li> <li>a graph function (<code>@graph_fn</code>) \u2014 runs immediately and can call context services.</li> <li>a tool node (<code>@tool</code>) \u2014 a typed, reusable operation with visible inputs/outputs.</li> <li>The Context (<code>context.*</code>) gives every node uniform access to runtime services:   <code>channel()</code>, <code>artifacts()</code>, <code>memory()</code>, <code>kv()</code>, <code>llm()</code>, <code>rag()</code>, <code>mcp()</code>, <code>logger()</code>.</li> </ul>"},{"location":"build-graphs/#quickstart-30-lines","title":"Quickstart (30 lines)","text":"<pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int):\n    return {\"y\": x * x}\n\n@graph_fn(name=\"demo\", outputs=[\"y\"])\nasync def demo(x: int, *, context):\n    await context.channel().send_text(f\"Computing square of {x}\u2026\")\n    h = square(x=x)              # creates a node you can inspect later\n    await context.channel().send_text(\"Done.\")\n    return {\"y\": h.y}            # expose tool output\n</code></pre> <p>Why this design? - You get instant run semantics (like a normal async function), but steps you mark with <code>@tool</code> become visible graph nodes with metrics/artifacts. - When your flow grows and needs explicit fan-out/fan-in or ordering, switch to <code>@graphify</code>.</p>"},{"location":"build-graphs/#next-steps","title":"Next steps","text":"<ul> <li><code>graph_fn</code> (on-ramp) -&gt; graph_fn.md</li> <li><code>@tool</code> reference -&gt; tool.md</li> <li><code>@graphify</code> (explicit DAG + fan-in/out) -&gt; graphify.md</li> <li>Choosing the right approach -&gt; choosing.md</li> </ul>"},{"location":"build-graphs/choosing/","title":"Choosing: <code>graph_fn</code> vs <code>@graphify</code> vs <code>@tool</code>","text":"<p>Use this one-screen guide to pick the right entry point.</p>"},{"location":"build-graphs/choosing/#start-simple","title":"Start simple","text":"<ul> <li><code>@graph_fn</code> \u2014 quickest way to ship a working function with <code>context.*</code>. Add a couple of <code>@tool</code> calls inside if you want visible/inspectable steps.</li> </ul>"},{"location":"build-graphs/choosing/#scale-up-when-needed","title":"Scale up when needed","text":"<ul> <li><code>@graphify</code> \u2014 when you need explicit DAG control:</li> <li>fan-out / fan-in / map-reduce</li> <li><code>_after</code> (barriers) and <code>_alias</code>/<code>_labels</code> for orchestration and UI</li> <li>subgraph reuse and IO/spec inspection</li> </ul>"},{"location":"build-graphs/choosing/#tool-is-a-building-block","title":"<code>@tool</code> is a building block","text":"<ul> <li>Wrap any function to make it a typed node.</li> <li>Works in both: inside <code>@graph_fn</code> (immediate run, visible steps) and in <code>@graphify</code> (adds nodes to DAG).</li> <li>Control kwargs (<code>_after</code>, <code>_alias</code>, <code>_labels</code>, <code>_id</code>) apply only in graph build contexts.</li> </ul>"},{"location":"build-graphs/choosing/#quick-comparison","title":"Quick comparison","text":"Capability <code>@graph_fn</code> <code>@graphify</code> <code>@tool</code> Immediate \"just run\" Yes Build first Yes (outside graph) Full <code>context.*</code> access Yes (via <code>context</code>) via tools/subgraphs when called under <code>graph_fn</code> Visible per-step nodes via <code>@tool</code> calls native yes Fan-out / fan-in (map/reduce) limited (Python loops) Yes (concise) building block Control edges (<code>_after</code>/barrier) No Yes Yes in graph build Graph spec/IO inspection implicit Yes (<code>.spec()/.io()</code>) n/a Best for demos, services pipelines, orchestration atomic operations <p>Rule of thumb: Start with <code>@graph_fn</code>. When you feel the need for explicit topology or orchestration, switch the same steps into <code>@graphify</code> using the exact same <code>@tool</code>s.</p>"},{"location":"build-graphs/graph_fn/","title":"<code>@graph_fn</code> \u2014 Python-first on-ramp","text":"<p>Wrap a normal (async) Python function so it runs as a single graph node with full access to <code>context.*</code> services. Return values are exposed as graph outputs.</p>"},{"location":"build-graphs/graph_fn/#signature","title":"Signature","text":"<pre><code>@graph_fn(name: str, *, inputs: list[str] | None = None, outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\ndef|async def fn(..., *, context: NodeContext) -&gt; dict | value | NodeHandle\n</code></pre> <ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (list[str], optional) \u2014 Declared input keys (used for IO spec; optional for quickstart).</li> <li>outputs (list[str], optional) \u2014 Declared output keys (enables single-value return).</li> <li>version (str) \u2014 SemVer for registry/lineage.</li> <li>agent (str, optional) \u2014 If set, register this graph function as an agent (advanced).</li> </ul>"},{"location":"build-graphs/graph_fn/#return-normalization","title":"Return normalization","text":"<ul> <li>dict -&gt; keys become outputs; NodeHandles/Refs are exposed.</li> <li>single value -&gt; allowed only if exactly one <code>outputs</code> key is declared (collapsed to that name).</li> <li>NodeHandle -&gt; its outputs are exposed (single output collapses).</li> </ul>"},{"location":"build-graphs/graph_fn/#using-tool-inside-graph_fn","title":"Using <code>@tool</code> inside <code>graph_fn</code>","text":"<p>You can call <code>@tool</code> functions to create visible/inspectable nodes while keeping immediate Python control flow:</p> <pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int): return {\"y\": x*x}\n\n@graph_fn(name=\"demo\", outputs=[\"y\"])\nasync def demo(x: int, *, context):\n    h = square(x=x)          # creates a node\n    await context.channel().send_text(\"computed\")\n    return {\"y\": h.y}\n</code></pre> <p>Important: Control kwargs like <code>_after</code>, <code>_alias</code>, <code>_labels</code> are only honored in graph build contexts (e.g., <code>@graphify</code>). Inside <code>graph_fn</code>, execution order follows normal Python semantics. If you need control edges without passing data, use <code>@graphify</code>.</p>"},{"location":"build-graphs/graph_fn/#when-to-use-graph_fn","title":"When to use <code>@graph_fn</code>","text":"<ul> <li>Quick demos, notebooks, service-style tasks.</li> <li>One to a few steps, mostly sequential.</li> <li>You want full <code>context.*</code> access and instant execution, with optional visibility via <code>@tool</code> calls.</li> </ul> <p>See also: tool.md, graphify.md.</p>"},{"location":"build-graphs/graphify/","title":"<code>@graphify</code> \u2014 Build an explicit DAG (fan-out, fan-in, ordering)","text":"<p>Use <code>@graphify</code> when you need clear topology: map/fan-out, reduce/fan-in, barriers via <code>_after</code>, subgraphs, or reusable pipelines.</p>"},{"location":"build-graphs/graphify/#signature","title":"Signature","text":"<p><pre><code>@graphify(*, name=\"default_graph\", inputs=(), outputs=None, version=\"0.1.0\", agent: str | None = None)\ndef body(...):\n    # Use tool calls to add nodes and return NodeHandles/Refs\n    return {...}\n</code></pre> - The decorated function returns a builder: call <code>.build()</code> to get a <code>TaskGraph</code> instance; <code>.spec()</code> for a serializable spec; <code>.io()</code> for IO signature.</p>"},{"location":"build-graphs/graphify/#control-edges-and-labels-graph-build-only","title":"Control edges and labels (graph build only)","text":"<p><code>@tool</code> control kwargs are honored here: - <code>_after</code>, <code>_alias</code>, <code>_labels</code>, <code>_id</code>, <code>_name</code></p>"},{"location":"build-graphs/graphify/#patterns","title":"Patterns","text":""},{"location":"build-graphs/graphify/#fan-out-map-over-inputs","title":"Fan-out (map over inputs)","text":"<pre><code>from aethergraph import tool, graphify\n\n@tool(outputs=[\"vec\"])\ndef embed(text: str): ...\n\n@graphify(name=\"fanout_demo\", inputs=[\"texts\"], outputs=[\"vecs\"])\ndef fanout_demo(texts):\n    handles = [embed(text=t) for t in texts]          # fan-out\n    return {\"vecs\": [h.vec for h in handles]}         # expose list of outputs\n</code></pre>"},{"location":"build-graphs/graphify/#fan-in-reduce","title":"Fan-in (reduce)","text":"<pre><code>@tool(outputs=[\"score\"])\ndef dot(a, b): ...\n\n@graphify(name=\"fanin_demo\", inputs=[\"query\", \"vecs\"], outputs=[\"scores\"])\ndef fanin_demo(query, vecs):\n    q = embed(text=query)\n    scores = [dot(a=v, b=q.vec) for v in vecs]        # fan-in through q\n    return {\"scores\": [s.score for s in scores]}\n</code></pre>"},{"location":"build-graphs/graphify/#control-edge-without-data","title":"Control edge without data","text":"<pre><code>@tool(outputs=[\"ok\"])   def init(): ...\n@tool(outputs=[\"done\"]) def train(): ...\n\n@graphify(name=\"order\", outputs=[\"done\"])\ndef order():\n    a = init()\n    b = train(_after=a)            # sequence a -&gt; b\n    return {\"done\": b.done}\n</code></pre>"},{"location":"build-graphs/graphify/#subgraph-reuse-optional","title":"Subgraph reuse (optional)","text":"<p>You can register graphs and call them as nodes (advanced). For most cases, compose <code>@tool</code>s directly inside <code>@graphify</code>.</p>"},{"location":"build-graphs/graphify/#when-to-use-graphify","title":"When to use <code>@graphify</code>","text":"<ul> <li>You need parallelism (map) or aggregation (reduce).</li> <li>You need ordering without data flow (<code>_after</code>/barriers).</li> <li>You want a reusable / inspectable DAG (e.g., schedule in a UI).</li> </ul> <p>See also: graph_fn.md, tool.md, choosing.md.</p>"},{"location":"build-graphs/tool/","title":"<code>@tool</code> \u2014 Turn any function into a graph node","text":"<p>Make a plain function a typed, reusable node with explicit inputs/outputs. Works in both <code>@graph_fn</code> (immediate run with visible steps) and <code>@graphify</code> (graph build).</p>"},{"location":"build-graphs/tool/#decorator","title":"Decorator","text":"<pre><code>@tool(outputs: list[str], inputs: list[str] | None = None, *, name: str | None = None, version: str = \"0.1.0\")\ndef fn(...): ...\n</code></pre> <ul> <li>outputs (list[str]) \u2014 Output field names this tool produces.</li> <li>inputs (list[str], optional) \u2014 Input names; inferred from signature if omitted.</li> <li>name (str, optional) \u2014 Registry name (defaults to function name).</li> <li>version (str) \u2014 SemVer for registry/lineage.</li> </ul>"},{"location":"build-graphs/tool/#return-normalization","title":"Return normalization","text":"<ul> <li><code>None</code> -&gt; <code>{}</code></li> <li><code>dict</code> -&gt; as-is</li> <li><code>tuple</code> -&gt; <code>{ \"out0\": v0, \"out1\": v1, ... }</code></li> <li>single value -&gt; <code>{ \"result\": value }</code></li> </ul> <p>Contract check: Declared <code>outputs</code> must be present in the normalized return, otherwise a <code>ValueError</code> is raised.</p>"},{"location":"build-graphs/tool/#two-modes-same-decorator","title":"Two modes (same decorator)","text":"Where called from Behavior Outside any graph Runs immediately and returns a dict. Inside <code>@graph_fn</code> Creates a node handle you can expose. Inside <code>@graphify</code> Adds a node to the DAG (honors control kw). <p>Control kwargs (graph build only): - <code>_after</code> (NodeHandle | list) \u2014 add control-edge dependency. - <code>_alias</code> / <code>_id</code> \u2014 override node id / alias. - <code>_labels</code> (list[str]) \u2014 annotate node for UI/search. - <code>_name</code> \u2014 display name hint.</p>"},{"location":"build-graphs/tool/#minimal-example","title":"Minimal example","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"y\"])\ndef square(x:int) -&gt; dict:\n    return {\"y\": x*x}\n</code></pre> <p>Use in <code>graph_fn</code> or <code>@graphify</code> as shown in their pages.</p>"},{"location":"recipes/data-analysis-loop/","title":"Recipe: Iterative Data Analysis","text":"<ul> <li>User asks for analysis \u2192 generate code</li> <li>Run code; store figures &amp; tables in <code>artifacts()</code></li> <li>Record metrics in <code>memory()</code> and summarize at the end</li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"recipes/job-submit-poll/","title":"Recipe: Submit + Poll + Notify","text":"<ul> <li>Submit a long-running job</li> <li>Poll status and send progress via <code>channel()</code></li> <li>On failure, ask user to retry or stop; save logs to <code>artifacts()</code></li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"recipes/memory-rag-mini/","title":"Recipe: Mini Memory\u2011RAG","text":"<ul> <li>Ingest notes as <code>memory().record(kind=\"note\", data=...)</code></li> <li>Simple retrieval via <code>memory().recent()/query()</code> into <code>llm().chat()</code></li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"reference/config/","title":"Config Keys","text":"<ul> <li><code>AETH_PORT</code> \u2014 server port</li> <li><code>LOG_LEVEL</code> \u2014 logging level (e.g., INFO)</li> <li><code>MODEL_NAME</code> \u2014 default LLM</li> <li><code>SLACK_*</code> \u2014 Slack integration (bot token, signing secret)</li> </ul> <p>(Expand with your actual config schema.)</p>"},{"location":"reference/context-artifacts/","title":"AetherGraph \u2014 <code>context.artifacts()</code> Reference","text":"<p>This page documents the ArtifactFacade methods returned by <code>context.artifacts()</code> in a concise format: signature, brief description, parameters, and returns \u2014 plus examples for <code>writer()</code> and scoped search.</p>"},{"location":"reference/context-artifacts/#overview","title":"Overview","text":"<p><code>context.artifacts()</code> returns an ArtifactFacade bound to the current <code>run_id</code>, <code>graph_id</code>, and <code>node_id</code>. It wraps an <code>AsyncArtifactStore</code> for persistence and an <code>AsyncArtifactIndex</code> for search/pinning/metrics. Most mutating ops auto\u2011index and record an occurrence.</p> <p>Typical flow</p> <ol> <li> <p>Stage (optional) \u2192 write \u2192 ingest</p> </li> <li> <p>Or directly save an existing file path</p> </li> <li> <p>Or use the built\u2011in <code>writer()</code> context manager to stream bytes and auto\u2011index</p> </li> </ol>"},{"location":"reference/context-artifacts/#artifactsstage","title":"artifacts.stage","text":"<p><pre><code>stage(ext: str = \"\") -&gt; str\n</code></pre> Plan a staging file path (temporary path) with an optional extension.</p> <p>Parameters</p> <ul> <li>ext (str, optional) \u2013 Suggested extension (e.g., \".png\", \".csv\").</li> </ul> <p>Returns str \u2013 Staging file path.</p>"},{"location":"reference/context-artifacts/#artifactsingest","title":"artifacts.ingest","text":"<p><pre><code>ingest(staged_path: str, *, kind: str, labels=None, metrics=None, suggested_uri: str | None = None, pin: bool = False) -&gt; Artifact\n</code></pre> Ingest a previously staged file into the store, attach metadata, and auto\u2011index.</p> <p>Parameters</p> <ul> <li> <p>staged_path (str) \u2013 Path returned by <code>stage()</code> or <code>stage_dir()</code>.</p> </li> <li> <p>kind (str) \u2013 Logical artifact kind (e.g., \"image\", \"table\", \"model\").</p> </li> <li> <p>labels (dict, optional) \u2013 Arbitrary labels; merged into index filters.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics used for <code>best()</code> queries.</p> </li> <li> <p>suggested_uri (str, optional) \u2013 Hint for final URI; store may ignore.</p> </li> <li> <p>pin (bool) \u2013 Mark artifact as pinned in the index.</p> </li> </ul> <p>Returns Artifact \u2013 Indexed artifact record.</p>"},{"location":"reference/context-artifacts/#artifactssave","title":"artifacts.save","text":"<p><pre><code>save(path: str, *, kind: str, labels=None, metrics=None, suggested_uri: str | None = None, pin: bool = False) -&gt; Artifact\n</code></pre> Save an existing on\u2011disk file to the store with metadata; auto\u2011index and record occurrence. Sets <code>last_artifact</code>.</p> <p>Parameters</p> <ul> <li> <p>path (str) \u2013 Existing file path to persist.</p> </li> <li> <p>kind (str) \u2013 Logical artifact kind.</p> </li> <li> <p>labels (dict, optional) \u2013 Arbitrary labels.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics.</p> </li> <li> <p>suggested_uri (str, optional) \u2013 Hint for final URI; store may ignore.</p> </li> <li> <p>pin (bool) \u2013 Mark artifact as pinned.</p> </li> </ul> <p>Returns Artifact \u2013 Indexed artifact record.</p>"},{"location":"reference/context-artifacts/#artifactswriter","title":"artifacts.writer","text":"<p><pre><code>writer(*, kind: str, planned_ext: str | None = None, pin: bool = False) -&gt; AsyncContextManager[Writer]\n</code></pre> Open a binary writer context that persists bytes as an artifact; auto\u2011indexes on exit. Sets <code>last_artifact</code>.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Logical artifact kind.</p> </li> <li> <p>planned_ext (str, optional) \u2013 Extension hint for underlying temp file.</p> </li> <li> <p>pin (bool) \u2013 Mark resulting artifact as pinned.</p> </li> </ul> <p>Yields Writer \u2013 File\u2011like object; write bytes and close by exiting the context.</p> <p>Example <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"make_png\")\nasync def make_png(*, context):\n    import PIL.Image as Image\n    img = Image.new(\"RGB\", (128, 128), (255, 122, 26))\n    async with context.artifacts().writer(kind=\"image\", planned_ext=\".png\") as w:\n        # writer exposes a real file handle underneath\n        img.save(w, format=\"PNG\")\n    art = context.artifacts().last_artifact\n    await context.channel().send_image(url=art.uri, title=\"Generated PNG\")\n    return {\"uri\": art.uri}\n</code></pre></p>"},{"location":"reference/context-artifacts/#artifactsstage_dir","title":"artifacts.stage_dir","text":"<p><pre><code>stage_dir(suffix: str = \"\") -&gt; str\n</code></pre> Plan a staging directory for multi\u2011file artifacts.</p> <p>Parameters</p> <ul> <li>suffix (str, optional) \u2013 Optional directory suffix.</li> </ul> <p>Returns str \u2013 Staging directory path.</p>"},{"location":"reference/context-artifacts/#artifactsingest_dir","title":"artifacts.ingest_dir","text":"<p><pre><code>ingest_dir(staged_dir: str, **kw) -&gt; Artifact\n</code></pre> Ingest a directory of files as a single logical artifact; forwards extra keyword args to the store.</p> <p>Parameters</p> <ul> <li> <p>staged_dir (str) \u2013 Directory created by <code>stage_dir()</code>.</p> </li> <li> <p>kw \u2013 Store\u2011specific options (e.g., kind/labels/metrics/pin).</p> </li> </ul> <p>Returns Artifact \u2013 Indexed artifact record.</p>"},{"location":"reference/context-artifacts/#artifactstmp_path","title":"artifacts.tmp_path","text":"<p><pre><code>tmp_path(suffix: str = \"\") -&gt; str\n</code></pre> Alias of <code>stage()</code> for convenience.</p> <p>Parameters</p> <ul> <li>suffix (str, optional) \u2013 Extension or suffix.</li> </ul> <p>Returns str \u2013 Staging file path.</p>"},{"location":"reference/context-artifacts/#artifactsload_artifact","title":"artifacts.load_artifact","text":"<p><pre><code>load_artifact(uri: str) -&gt; Any\n</code></pre> Load a previously saved artifact by URI, using the store\u2019s type\u2011specific loader.</p> <p>Parameters</p> <ul> <li>uri (str) \u2013 Artifact URI.</li> </ul> <p>Returns Any \u2013 Decoded object (depends on store &amp; artifact type).</p>"},{"location":"reference/context-artifacts/#artifactsload_artifact_bytes","title":"artifacts.load_artifact_bytes","text":"<p><pre><code>load_artifact_bytes(uri: str) -&gt; bytes\n</code></pre> Load raw bytes for a previously saved artifact by URI.</p> <p>Parameters</p> <ul> <li>uri (str) \u2013 Artifact URI.</li> </ul> <p>Returns bytes \u2013 Artifact content.</p>"},{"location":"reference/context-artifacts/#artifactslist","title":"artifacts.list","text":"<p><pre><code>list(*, scope: Literal[\"node\",\"run\",\"graph\",\"project\",\"all\"] = \"run\") -&gt; list[Artifact]\n</code></pre> Quick listing with implicit scoping (defaults to the current run). Under the hood, this uses the index with reasonable filters for the given scope.</p> <p>Parameters</p> <ul> <li> <p>scope (str) \u2013 One of:</p> </li> <li> <p>\"node\" \u2013 filter by (run_id, graph_id, node_id) </p> </li> <li> <p>\"graph\" \u2013 filter by (run_id, graph_id) </p> </li> <li> <p>\"run\" \u2013 filter by (run_id) (default) </p> </li> <li> <p>\"project\" \u2013 filter by project/org if tracked in labels  </p> </li> <li> <p>\"all\" \u2013 no implicit filters (use sparingly)</p> </li> </ul> <p>Returns list[Artifact] \u2013 Matching artifacts.</p>"},{"location":"reference/context-artifacts/#artifactssearch","title":"artifacts.search","text":"<p><pre><code>search(*, kind: str | None = None, labels: dict | None = None, metric: str | None = None, mode: Literal[\"max\",\"min\"] | None = None, scope: Scope = \"run\", extra_scope_labels: dict | None = None) -&gt; list[Artifact]\n</code></pre> Index search with automatic scoping. Merges your <code>labels</code> with scope\u2011derived labels.</p> <p>Parameters</p> <ul> <li> <p>kind (str, optional) \u2013 Filter by artifact kind.</p> </li> <li> <p>labels (dict, optional) \u2013 Arbitrary label filters.</p> </li> <li> <p>metric (str, optional) \u2013 Metric name for ranking.</p> </li> <li> <p>mode ({\"max\",\"min\"}, optional) \u2013 Ranking direction.</p> </li> <li> <p>scope (Scope) \u2013 Implicit scope (default: \"run\").</p> </li> <li> <p>extra_scope_labels (dict, optional) \u2013 Additional scope labels to merge.</p> </li> </ul> <p>Returns list[Artifact] \u2013 Search results.</p> <p>Example (scoped search) <pre><code>best_imgs = await context.artifacts().search(kind=\"image\", scope=\"graph\")\n</code></pre></p>"},{"location":"reference/context-artifacts/#artifactsbest","title":"artifacts.best","text":"<p><pre><code>best(*, kind: str, metric: str, mode: Literal[\"max\",\"min\"], scope: Scope = \"run\", filters: dict | None = None) -&gt; Artifact | None\n</code></pre> Return the best artifact by a metric, with optional filters and implicit scope.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Artifact kind.</p> </li> <li> <p>metric (str) \u2013 Metric key.</p> </li> <li> <p>mode ({\"max\",\"min\"}) \u2013 Ranking direction.</p> </li> <li> <p>scope (Scope) \u2013 Implicit scope (default: \"run\").</p> </li> <li> <p>filters (dict, optional) \u2013 Additional label filters.</p> </li> </ul> <p>Returns Artifact | None \u2013 Best match or <code>None</code> if not found.</p>"},{"location":"reference/context-artifacts/#artifactspin","title":"artifacts.pin","text":"<p><pre><code>pin(artifact_id: str, pinned: bool = True) -&gt; None\n</code></pre> Pin or unpin an artifact in the index.</p> <p>Parameters</p> <ul> <li> <p>artifact_id (str) \u2013 ID of the artifact to (un)pin.</p> </li> <li> <p>pinned (bool) \u2013 <code>True</code> to pin; <code>False</code> to unpin.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-artifacts/#scoping-details","title":"Scoping details","text":"<p>The facade enriches queries with labels depending on the <code>scope</code> argument:</p> <ul> <li> <p>node \u2192 <code>{ graph_id, node_id }</code> </p> </li> <li> <p>graph \u2192 <code>{ graph_id }</code> </p> </li> <li> <p>project \u2192 <code>{ project_id }</code> (if tracked)  </p> </li> <li> <p>run \u2192 uses <code>list_for_run(run_id)</code> </p> </li> <li> <p>all \u2192 passes through to index with no implicit labels</p> </li> </ul>"},{"location":"reference/context-artifacts/#practical-examples","title":"Practical examples","text":"<p>1) Direct save <pre><code>uri = \"/tmp/plot.png\"\n# ... generate image to uri ...\nart = await context.artifacts().save(uri, kind=\"image\", labels={\"task\":\"eval\"}, metrics={\"psnr\": 31.2})\n</code></pre></p> <p>2) Stage \u2192 write \u2192 ingest <pre><code>staged = await context.artifacts().stage(\".csv\")\nwith open(staged, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"x,y\\n1,2\\n3,4\\n\")\nart = await context.artifacts().ingest(staged_path=staged, kind=\"table\", labels={\"split\":\"val\"})\n</code></pre></p> <p>3) Search best <pre><code>winner = await context.artifacts().best(kind=\"model\", metric=\"val_acc\", mode=\"max\", scope=\"run\")\nif winner:\n    await context.channel().send_text(f\"Best model: {winner.uri} acc={winner.metrics['val_acc']:.3f}\")\n</code></pre></p> <p>4) Multi\u2011file directory <pre><code>dir_path = await context.artifacts().stage_dir(\"_report\")\n# ... write several files to dir_path ...\nart = await context.artifacts().ingest_dir(dir_path, kind=\"report\", labels={\"format\":\"html\"})\n</code></pre></p> <p>5) Pin <pre><code>await context.artifacts().pin(artifact_id=art.id, pinned=True)\n</code></pre></p>"},{"location":"reference/context-channel/","title":"AetherGraph \u2014 <code>context.channel()</code> Reference","text":"<p>This page documents the ChannelSession methods returned by <code>context.channel()</code> in a concise, PyTorch\u2011style format: signature, brief description, parameters, and returns.</p>"},{"location":"reference/context-channel/#overview-choosing-a-channel","title":"Overview \u2014 Choosing a channel","text":"<p>Use <code>context.channel(&lt;key&gt;)</code> to bind a ChannelSession to a specific destination for all subsequent calls from that session. You can also override per call with the <code>channel=</code> keyword.</p> <p>Common forms - <code>slack:#research</code> \u2014 a Slack channel by name</p> <ul> <li> <p><code>slack:@alice</code> \u2014 a Slack DM</p> </li> <li> <p><code>telegram:@mychannel</code> \u2014 a Telegram channel</p> </li> <li> <p><code>console:stdin</code> \u2014 console fallback (default if nothing is configured)</p> </li> </ul> <p>Resolution order (what channel is used?) 1. Per\u2011call override: <code>await context.channel().send_text(\"hi\", channel=\"slack:#alerts\")</code></p> <ol> <li> <p>Bound session key: <code>ch = context.channel(\"slack:#research\"); await ch.send_text(\"hi\")</code></p> </li> <li> <p>Bus default: whatever <code>services.channels.get_default_channel_key()</code> returns</p> </li> <li> <p>Fallback: <code>console:stdin</code></p> </li> </ol> <p>Examples <pre><code># Bind a session to #research for many messages\nch = context.channel(\"slack:#research\")\nawait ch.send_text(\"Starting the run\u2026\")\nawait ch.send_text(\"Progress will be posted here.\")\n\n# One\u2011off override to a different channel\nawait context.channel().send_text(\"Heads\u2011up in #alerts\", channel=\"slack:#alerts\")\n\n# Stream to #research explicitly\nasync with context.channel().stream(channel=\"slack:#research\") as s:\n    await s.delta(\"Parsing\u2026 \")\n    await s.delta(\"OK\")\n    await s.end(\"Done\")\n\n# Progress bar to the default (no key passed)\nasync with context.channel().progress(title=\"Crunching\", total=100) as bar:\n    await bar.update(current=30, eta_seconds=90)\n    await bar.end(subtitle=\"All set!\")\n</code></pre></p>"},{"location":"reference/context-channel/#channelsend_text","title":"channel.send_text","text":"<p><pre><code>send_text(text, *, meta: dict | None = None, channel: str | None = None)\n</code></pre> Send a plain text message to a channel.</p> <p>Parameters</p> <ul> <li> <p>text (str) \u2013 Message body to send.</p> </li> <li> <p>meta (dict, optional) \u2013 Arbitrary metadata for adapters/analytics.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override (e.g., <code>\"slack:#research\"</code>).</p> </li> </ul> <p>Returns </p> <p><code>None</code></p>"},{"location":"reference/context-channel/#channelsend_image","title":"channel.send_image","text":"<p><pre><code>send_image(url: str | None = None, *, alt: str = \"image\", title: str | None = None, channel: str | None = None)\n</code></pre> Post an image by URL with <code>alt</code>/<code>title</code> text.</p> <p>Parameters</p> <ul> <li> <p>url (str, optional) \u2013 Image URL. Use <code>send_file</code> for file bytes.</p> </li> <li> <p>alt (str) \u2013 Alt text (default: <code>\"image\"</code>).</p> </li> <li> <p>title (str, optional) \u2013 Optional title/caption.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p><code>None</code></p>"},{"location":"reference/context-channel/#channelsend_file","title":"channel.send_file","text":"<p><pre><code>send_file(url: str | None = None, *, file_bytes: bytes | None = None, filename: str = \"file.bin\", title: str | None = None, channel: str | None = None)\n</code></pre> Upload or link a file to the channel. Provide either <code>url</code> or <code>file_bytes</code>.</p> <p>Parameters</p> <ul> <li> <p>url (str, optional) \u2013 Remote file URL to attach.</p> </li> <li> <p>file_bytes (bytes, optional) \u2013 Raw bytes to upload.</p> </li> <li> <p>filename (str) \u2013 Display filename (default: <code>\"file.bin\"</code>).</p> </li> <li> <p>title (str, optional) \u2013 Optional caption/label.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p><code>None</code></p>"},{"location":"reference/context-channel/#channelsend_buttons","title":"channel.send_buttons","text":"<p><pre><code>send_buttons(text: str, buttons: list[Button], *, meta: dict | None = None, channel: str | None = None)\n</code></pre> Send a short message with interactive buttons (links or postbacks depending on adapter).</p> <p>Parameters</p> <ul> <li> <p>text (str) \u2013 Leading text.</p> </li> <li> <p>buttons (list[Button]) \u2013 Button list; at minimum a <code>label</code> per button.</p> </li> <li> <p>meta (dict, optional) \u2013 Arbitrary metadata.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-channel/#channelask_text","title":"channel.ask_text","text":"<p><pre><code>ask_text(prompt: str, *, timeout_s: int = 3600, silent: bool = False, channel: str | None = None)\n</code></pre> Ask the user for free\u2011text using cooperative wait/continuations.</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Prompt text shown to the user. (Ignored if <code>silent=True</code>.)</p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>silent (bool) \u2013 If <code>True</code>, binds to current thread/channel without posting a prompt.</p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>str \u2013 The user\u2019s text (empty string if none).</p>"},{"location":"reference/context-channel/#channelwait_text","title":"channel.wait_text","text":"<p><pre><code>wait_text(*, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Wait for the next text reply in the current thread/channel without sending a prompt.</p> <p>Parameters</p> <ul> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>str \u2013 The user\u2019s text.</p>"},{"location":"reference/context-channel/#channelask_approval","title":"channel.ask_approval","text":"<p><pre><code>ask_approval(prompt: str, options: Iterable[str] = (\"Approve\", \"Reject\"), *, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Ask the user to approve or pick an option.</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Title or question.</p> </li> <li> <p>options (Iterable[str]) \u2013 Button labels (default: <code>(\"Approve\",\"Reject\")</code>).</p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>dict \u2013 <code>{ \"approved\": bool, \"choice\": str }</code>.</p>"},{"location":"reference/context-channel/#channelget_latest_uploads","title":"channel.get_latest_uploads","text":"<p><pre><code>get_latest_uploads(*, clear: bool = True)\n</code></pre> Fetch latest uploaded files for this channel (Ephemeral KV required).</p> <p>Parameters</p> <ul> <li>clear (bool) \u2013 If <code>True</code>, consume and clear the inbox (default: <code>True</code>).</li> </ul> <p>Returns </p> <p>list[FileRef] \u2013 Recent file references.</p> <p>Raises </p> <p><code>RuntimeError</code> \u2013 if KV is not available.</p>"},{"location":"reference/context-channel/#channelask_files","title":"channel.ask_files","text":"<p><pre><code>ask_files(*, prompt: str, accept: list[str] | None = None, multiple: bool = True, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Ask the user to upload file(s) with optional text input.</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Prompt text.</p> </li> <li> <p>accept (list[str], optional) \u2013 MIME types or extensions (adapter\u2011hint only).</p> </li> <li> <p>multiple (bool) \u2013 Allow selecting multiple files (default: <code>True</code>). </p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>dict \u2013 <code>{ \"text\": str, \"files\": list[FileRef] }</code>.</p>"},{"location":"reference/context-channel/#channelask_text_or_files","title":"channel.ask_text_or_files","text":"<p><pre><code>ask_text_or_files(*, prompt: str, timeout_s: int = 3600, channel: str | None = None)\n</code></pre> Let the user respond with either text or file(s).</p> <p>Parameters</p> <ul> <li> <p>prompt (str) \u2013 Prompt text.</p> </li> <li> <p>timeout_s (int) \u2013 Deadline in seconds (default: <code>3600</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011call channel override.</p> </li> </ul> <p>Returns </p> <p>dict \u2013 <code>{ \"text\": str, \"files\": list[FileRef] }</code>.</p>"},{"location":"reference/context-channel/#channelstream","title":"channel.stream","text":"<p><pre><code>stream(channel: str | None = None)  # async context manager\n</code></pre> Create a stream for incremental message updates (token/delta style). Within the context, use <code>s.delta()</code> to append text and <code>s.end()</code> to finalize.</p> <p>Parameters</p> <ul> <li>channel (str, optional) \u2013 Per\u2011stream channel override.</li> </ul> <p>Yields StreamSender \u2013 with methods:</p> <ul> <li> <p><code>start()</code> \u2013 explicitly start the stream (optional; auto on first delta).</p> </li> <li> <p><code>delta(text_piece: str)</code> \u2013 append a delta (adapter receives upsert with full text).</p> </li> <li> <p><code>end(full_text: str | None = None)</code> \u2013 finalize; optionally set final text.</p> </li> </ul> <p>Example <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"stream_demo\")\nasync def stream_demo(*, context):\n    async with context.channel().stream() as s:\n        for chunk in [\"Hello\", \" \", \"world\", \"\u2026\"]:\n            await s.delta(chunk)\n        await s.end(\"Hello world!\")\n</code></pre></p>"},{"location":"reference/context-channel/#channelprogress","title":"channel.progress","text":"<p><pre><code>progress(*, title: str = \"Working...\", total: int | None = None, key_suffix: str = \"progress\", channel: str | None = None)  # async context manager\n</code></pre> Create a progress reporter (start/update/end) bound to the current run/node.</p> <p>Parameters</p> <ul> <li> <p>title (str) \u2013 Progress title (default: <code>\"Working...\"</code>).</p> </li> <li> <p>total (int, optional) \u2013 If set, progress is shown as <code>current/total</code>; allows <code>percent</code> updates.</p> </li> <li> <p>key_suffix (str) \u2013 Included in the internal upsert key (default: <code>\"progress\"</code>). </p> </li> <li> <p>channel (str, optional) \u2013 Per\u2011progress channel override.</p> </li> </ul> <p>Yields ProgressSender \u2013 with methods:</p> <ul> <li> <p><code>start(subtitle: str | None = None)</code> \u2013 start (auto on first update).</p> </li> <li> <p><code>update(current: int | None = None, inc: int | None = None, subtitle: str | None = None, percent: float | None = None, eta_seconds: float | None = None)</code></p> </li> <li> <p><code>end(subtitle: str | None = \"Done.\", success: bool = True)</code></p> </li> </ul> <p>Example <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"progress_demo\")\nasync def progress_demo(*, context):\n    async with context.channel().progress(title=\"Crunching\", total=5) as bar:\n        for i in range(5):\n            await bar.update(current=i+1, eta_seconds=(4-i)*0.5, subtitle=f\"step {i+1}/5\")\n        await bar.end(subtitle=\"All set!\")\n</code></pre></p>"},{"location":"reference/context-channel/#channel-resolution-notes","title":"Channel resolution notes","text":"<ul> <li> <p>Per\u2011call <code>channel=</code> overrides everything.</p> </li> <li> <p>Otherwise, the session\u2019s bound key (from <code>context.channel(bound_key)</code>) is used.</p> </li> <li> <p>Else, the bus default via <code>services.channels.get_default_channel_key()</code>.</p> </li> <li> <p>Else, fallback <code>\"console:stdin\"</code>.</p> </li> </ul>"},{"location":"reference/context-channel/#guarantees","title":"Guarantees","text":"<ul> <li> <p>Streams/progress use idempotent upsert keys derived from <code>(run_id, node_id, suffix)</code>.</p> </li> <li> <p>Ask methods bind correlators at both message and thread level to capture replies.</p> </li> </ul>"},{"location":"reference/context-kv/","title":"AetherGraph \u2014 <code>context.kv()</code> Reference","text":"<p>This page documents the Key\u2013Value API available via <code>context.kv()</code> in a concise format. The KV store is process\u2011local and transient \u2014 ideal for coordination, small caches, inboxes, and short\u2011lived lists. Not intended for large blobs or durability.</p>"},{"location":"reference/context-kv/#overview","title":"Overview","text":"<ul> <li>Keys are simple strings; values can be any JSON\u2011serializable Python object (adapters may allow arbitrary picklables, but keep it small).</li> <li>Most methods support TTL (time\u2011to\u2011live in seconds). Expired entries are pruned lazily or via <code>purge_expired()</code>.</li> <li>For namespacing, prefer prefixes like <code>\"run:&lt;id&gt;:...\"</code>, <code>\"inbox:&lt;channel&gt;\"</code>, etc.</li> </ul>"},{"location":"reference/context-kv/#kvget","title":"kv.get","text":"<p><pre><code>get(key: str, default: Any = None) -&gt; Any\n</code></pre> Fetch a value by key; returns <code>default</code> if missing or expired.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 Lookup key.</p> </li> <li> <p>default (Any, optional) \u2013 Value to return when absent/expired (default <code>None</code>).</p> </li> </ul> <p>Returns Any \u2013 Stored value or <code>default</code>.</p>"},{"location":"reference/context-kv/#kvset","title":"kv.set","text":"<p><pre><code>set(key: str, value: Any, *, ttl_s: int | None = None) -&gt; None\n</code></pre> Set a key to a value with optional TTL.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 Key to write.</p> </li> <li> <p>value (Any) \u2013 Value to store.</p> </li> <li> <p>ttl_s (int, optional) \u2013 Expiration in seconds.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvdelete","title":"kv.delete","text":"<p><pre><code>delete(key: str) -&gt; None\n</code></pre> Remove a key if present.</p> <p>Parameters</p> <ul> <li>key (str) \u2013 Key to delete.</li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvlist_append_unique","title":"kv.list_append_unique","text":"<p><pre><code>list_append_unique(key: str, items: list[dict], *, id_key: str = \"id\", ttl_s: int | None = None) -&gt; list[dict]\n</code></pre> Append unique dict items to a list value under <code>key</code>. Uniqueness is determined by <code>item[id_key]</code>.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 List container key.</p> </li> <li> <p>items (list[dict]) \u2013 Items to append.</p> </li> <li> <p>id_key (str) \u2013 Field name used for uniqueness (default: <code>\"id\"</code>).</p> </li> <li> <p>ttl_s (int, optional) \u2013 Reset TTL for the list.</p> </li> </ul> <p>Returns list[dict] \u2013 Updated list.</p>"},{"location":"reference/context-kv/#kvlist_pop_all","title":"kv.list_pop_all","text":"<p><pre><code>list_pop_all(key: str) -&gt; list\n</code></pre> Pop and return the entire list stored at <code>key</code>. Empties the container.</p> <p>Parameters</p> <ul> <li>key (str) \u2013 List container key.</li> </ul> <p>Returns list \u2013 Previous list content (empty list if none or not a list).</p>"},{"location":"reference/context-kv/#kvmget","title":"kv.mget","text":"<p><pre><code>mget(keys: list[str]) -&gt; list[Any]\n</code></pre> Batch get multiple keys.</p> <p>Parameters</p> <ul> <li>keys (list[str]) \u2013 Keys to read.</li> </ul> <p>Returns list[Any] \u2013 Values in the same order as <code>keys</code>.</p>"},{"location":"reference/context-kv/#kvmset","title":"kv.mset","text":"<p><pre><code>mset(kv: dict[str, Any], *, ttl_s: int | None = None) -&gt; None\n</code></pre> Batch set multiple keys with an optional shared TTL.</p> <p>Parameters</p> <ul> <li> <p>kv (dict[str, Any]) \u2013 Key\u2013value pairs.</p> </li> <li> <p>ttl_s (int, optional) \u2013 TTL to apply to all entries.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvexpire","title":"kv.expire","text":"<p><pre><code>expire(key: str, ttl_s: int) -&gt; None\n</code></pre> Update/assign a TTL for an existing key.</p> <p>Parameters</p> <ul> <li> <p>key (str) \u2013 Key to expire.</p> </li> <li> <p>ttl_s (int) \u2013 Time\u2011to\u2011live in seconds from now.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-kv/#kvpurge_expired","title":"kv.purge_expired","text":"<p><pre><code>purge_expired(limit: int = 1000) -&gt; int\n</code></pre> Remove up to <code>limit</code> expired keys.</p> <p>Parameters</p> <ul> <li>limit (int) \u2013 Maximum removals per call (default: 1000).</li> </ul> <p>Returns int \u2013 Number of keys purged.</p>"},{"location":"reference/context-kv/#practical-examples","title":"Practical examples","text":"<p>1) Channel inbox (files/messages) <pre><code># Adapter pushes uploads into an inbox list\nawait context.kv().list_append_unique(\n    key=f\"inbox:{channel_key}\",\n    items=[{\"id\": file_id, \"filename\": name, \"url\": url}],\n    ttl_s=3600,\n)\n\n# Agent consumes the inbox later\nfiles = await context.kv().list_pop_all(f\"inbox:{channel_key}\")\nif files:\n    await context.channel().send_text(f\"Received {len(files)} file(s)\")\n</code></pre></p> <p>2) Short\u2011lived cache with TTL <pre><code>k = f\"run:{context.run_id}:spec\"\nspec = await context.kv().get(k)\nif spec is None:\n    spec = await expensive_fetch()\n    await context.kv().set(k, spec, ttl_s=300)  # cache for 5 minutes\n</code></pre></p> <p>3) Batch write/read <pre><code>await context.kv().mset({\n    f\"run:{context.run_id}:step\": 42,\n    f\"run:{context.run_id}:eta\": 120,\n}, ttl_s=600)\n\nvals = await context.kv().mget([\n    f\"run:{context.run_id}:step\",\n    f\"run:{context.run_id}:eta\",\n])\nstep, eta = vals\n</code></pre></p> <p>4) Update TTL <pre><code>await context.kv().expire(f\"run:{context.run_id}:spec\", ttl_s=900)\n</code></pre></p>"},{"location":"reference/context-kv/#notes-behaviors","title":"Notes &amp; behaviors","text":"<ul> <li> <p>Transient: data is in\u2011process only; it disappears on restart.</p> </li> <li> <p>Small values: do not store large binaries; use <code>context.artifacts()</code> for blobs.</p> </li> <li> <p>Concurrency: the implementation uses an internal lock; operations are atomic per call.</p> </li> <li> <p>TTL semantics: reads lazily drop expired entries; use <code>purge_expired()</code> to actively clean.</p> </li> </ul>"},{"location":"reference/context-llm/","title":"AetherGraph \u2014 <code>context.llm()</code> Reference","text":"<p>This page documents the LLM client retrieved via <code>context.llm(profile=\"default\")</code>, in a concise format. The client implements two core calls:</p> <ul> <li><code>chat(messages, **kwargs) -&gt; (text: str, usage: dict)</code></li> <li><code>embed(texts, **kwargs) -&gt; list[list[float]]</code></li> </ul> <p>Profiles are managed by an <code>LLMService</code> that holds one or more configured clients (\"default\", \"azure\", \"local\", etc.).</p>"},{"location":"reference/context-llm/#quick-start","title":"Quick start","text":"<pre><code># 1) Use the default LLM profile\nllm = context.llm()                # == context.llm(\"default\")\ntext, usage = await llm.chat([\n    {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n    {\"role\":\"user\",   \"content\":\"Summarize AetherGraph in one sentence.\"},\n])\n\n# 2) Switch/set a key at runtime (in\u2011memory only)\ncontext.llm_set_key(provider=\"openai\", api_key=\"sk-...\", profile=\"default\")\n\n# 3) Use a named profile (must exist in LLMService)\nllm = context.llm(\"azure\")\n</code></pre>"},{"location":"reference/context-llm/#supported-providers-via-genericllmclient","title":"Supported providers (via GenericLLMClient)","text":"<p><code>{\"openai\",\"azure\",\"anthropic\",\"google\",\"openrouter\",\"lmstudio\",\"ollama\"}</code></p> <p>Credentials and endpoints are read from environment by default, but can be provided at construction time. Runtime key overrides are allowed via <code>context.llm_set_key(...)</code>.</p> <p>Common env vars:</p> <ul> <li><code>OPENAI_API_KEY</code>, <code>OPENAI_BASE_URL</code></li> <li><code>AZURE_OPENAI_KEY</code>, <code>AZURE_OPENAI_ENDPOINT</code>, <code>AZURE_OPENAI_DEPLOYMENT</code></li> <li><code>ANTHROPIC_API_KEY</code></li> <li><code>GOOGLE_API_KEY</code></li> <li><code>OPENROUTER_API_KEY</code></li> <li><code>LMSTUDIO_BASE_URL</code> (default <code>http://localhost:1234/v1</code>)</li> <li><code>OLLAMA_BASE_URL</code>   (default <code>http://localhost:11434/v1</code>)</li> </ul>"},{"location":"reference/context-llm/#llmchat","title":"llm.chat","text":"<p><pre><code>chat(messages: list[dict], **kw) -&gt; tuple[str, dict]\n</code></pre> Send a chat completion request and return <code>(text, usage)</code>.</p> <p>Parameters</p> <ul> <li> <p>messages (list[dict]) \u2013 OpenAI\u2011style conversation turns, e.g. <code>{\"role\":\"system\"|\"user\"|\"assistant\", \"content\": str}</code>.</p> </li> <li> <p>kw \u2013 Common knobs (provider\u2011dependent):  </p> </li> <li> <p>model (str, optional) \u2013 Model name (defaults to client\u2019s configured model).  </p> </li> <li> <p>temperature (float, optional) \u2013 Sampling temperature (default <code>0.5</code>).  </p> </li> <li> <p>top_p (float, optional) \u2013 Nucleus sampling (default <code>1.0</code>).  </p> </li> <li> <p>max_tokens (int, optional) \u2013 Max tokens (Anthropic/Azure/Google paths).  </p> </li> </ul> <p>Returns tuple[str, dict] \u2013 Generated <code>text</code> and a <code>usage</code> dict (token counts where supported).</p> <p>Example <pre><code>sys = {\"role\":\"system\",\"content\":\"Be concise.\"}\nusr = {\"role\":\"user\",\"content\":\"What is AetherGraph?\"}\ntext, usage = await context.llm().chat([sys, usr], temperature=0.2)\nawait context.channel().send_text(text)\n</code></pre></p>"},{"location":"reference/context-llm/#llmembed","title":"llm.embed","text":"<p><pre><code>embed(texts: list[str], **kw) -&gt; list[list[float]]\n</code></pre> Return embeddings for a list of strings.</p> <p>Parameters</p> <ul> <li> <p>texts (list[str]) \u2013 Text strings to embed.</p> </li> <li> <p>kw \u2013 Common knobs (provider\u2011dependent):  </p> </li> <li> <p>model (str, optional) \u2013 Embedding model name (default <code>text-embedding-3-small</code> for OpenAI\u2011like providers).</p> </li> </ul> <p>Returns list[list[float]] \u2013 Embedding vectors.</p> <p>Example <pre><code>vecs = await context.llm().embed([\"lens design\", \"holography basics\"])  # [[...], [...]]\n</code></pre></p>"},{"location":"reference/context-llm/#profiles-and-keys","title":"Profiles and keys","text":""},{"location":"reference/context-llm/#contextllmprofile-str-default-llmclient","title":"<code>context.llm(profile: str = \"default\") -&gt; LLMClient</code>","text":"<p>Retrieve the configured LLM client for a named profile. Raises if <code>LLMService</code> is not bound or profile missing.</p>"},{"location":"reference/context-llm/#contextllm_set_keyprovider-str-api_key-str-profile-str-default-none","title":"<code>context.llm_set_key(provider: str, api_key: str, profile: str = \"default\") -&gt; None</code>","text":"<p>Override an API key in memory for the given profile (good for demos/notebooks). Does not persist.</p> <p>Example <pre><code># Switch the default profile to use a local LM Studio server at runtime\ncontext.llm_set_key(provider=\"lmstudio\", api_key=\"sk-ignore\", profile=\"default\")\ntext, _ = await context.llm().chat([\n    {\"role\":\"user\",\"content\":\"Say hi from LM Studio\"}\n])\n</code></pre></p> <p>For long\u2011lived storage, use your project\u2019s Secrets provider and <code>LLMService.persist_key(secret_name, api_key)</code> if available.</p>"},{"location":"reference/context-llm/#providerspecific-notes","title":"Provider\u2011specific notes","text":"<ul> <li> <p>OpenAI / OpenRouter / LM Studio / Ollama \u2013 uses OpenAI\u2011style <code>/chat/completions</code> and <code>/embeddings</code> routes. <code>usage</code> is included where supported.</p> </li> <li> <p>Azure OpenAI \u2013 requires <code>AZURE_OPENAI_ENDPOINT</code> and <code>AZURE_OPENAI_DEPLOYMENT</code>; uses Azure routes.</p> </li> <li> <p>Anthropic (Claude) \u2013 converts OpenAI\u2011style messages to Anthropic\u2019s message format; returns concatenated text blocks.</p> </li> <li> <p>Google (Gemini) \u2013 uses <code>:generateContent</code> and <code>:embedContent</code>; <code>usage</code> shape differs and may be empty.</p> </li> <li> <p>Embeddings \u2013 not supported for Anthropic in this client.</p> </li> </ul>"},{"location":"reference/context-llm/#error-handling-retries","title":"Error handling &amp; retries","text":"<p>The client wraps calls with exponential backoff (<code>_Retry</code>) for transient HTTP errors (<code>ReadTimeout</code>, <code>ConnectError</code>, <code>HTTPStatusError</code>). You may still want to catch and surface provider\u2011specific errors around quota/keys.</p> <p>Example <pre><code>try:\n    text, usage = await context.llm().chat([{ \"role\":\"user\", \"content\":\"ping\" }])\nexcept Exception as e:\n    await context.channel().send_text(f\"LLM error: {e}\")\n</code></pre></p>"},{"location":"reference/context-llm/#patterns-with-context","title":"Patterns with Context","text":"<p>Router\u2011then\u2011Act <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"router_then_act\")\nasync def router_then_act(*, context):\n    sys = {\"role\":\"system\",\"content\":\"Route to 'summarize' or 'plot'\"}\n    usr = {\"role\":\"user\",\"content\":\"Summarize this research log.\"}\n    decision, _ = await context.llm().chat([sys, usr], temperature=0.0)\n    if \"summarize\" in decision.lower():\n        # Call downstream tool and write a result\n        await context.memory().write_result(\n            topic=\"router\",\n            outputs=[{\"name\":\"route\",\"kind\":\"text\",\"value\":\"summarize\"}],\n        )\n        await context.channel().send_text(\"Routing \u2192 summarize\")\n</code></pre></p> <p>RAG: retrieve \u2192 answer <pre><code>@graph_fn(name=\"rag_answer\")\nasync def rag_answer(*, context, q: str):\n    hits = await context.memory().rag_search(corpus_id=\"notes\", query=q, k=6)\n    prompt = [{\"role\":\"system\",\"content\":\"Answer using the provided notes.\"},\n              {\"role\":\"user\",\"content\":\"\\n\\n\".join(h.get(\"text\",\"\") for h in hits) + \"\\n\\nQ: \" + q}]\n    text, usage = await context.llm().chat(prompt, temperature=0.2, model=\"gpt-4o-mini\")\n    return {\"answer\": text, \"tokens\": usage.get(\"total_tokens\")}\n</code></pre></p>"},{"location":"reference/context-llm/#summary","title":"Summary","text":"<ul> <li>Use <code>context.llm()</code> to get a ready\u2011to\u2011use client for the current profile.</li> <li><code>chat()</code> returns <code>(text, usage)</code>; <code>embed()</code> returns vectors.</li> <li>Switch keys ad\u2011hoc with <code>context.llm_set_key(...)</code>; persist via your Secrets provider when available.</li> </ul>"},{"location":"reference/context-logger/","title":"AetherGraph \u2014 <code>context.logger()</code> Quick Reference","text":"<p><code>context.logger()</code> returns a pre\u2011scoped Python <code>logging.Logger</code> bound to the current run/graph/node via the project\u2019s <code>StdLoggerService</code>.</p> <ul> <li>Namespace: <code>node.&lt;node_id&gt;</code></li> <li>Extra context on every record: <code>{run_id, graph_id, node_id}</code></li> <li>Outputs: console (text) + rotating file (<code>$LOG_DIR/aethergraph.log</code>), optional JSON, optional async QueueHandler</li> </ul>"},{"location":"reference/context-logger/#basics","title":"Basics","text":"<pre><code>log = context.logger()\nlog.info(\"starting step\")\nlog.debug(\"inputs\", extra={\"shape\": [n, d]})\nlog.warning(\"retrying\", extra={\"attempt\": i})\ntry:\n    ...\nexcept Exception:\n    log.exception(\"failed tool call\")  # includes traceback\n</code></pre> <p>Returns <code>logging.Logger</code> \u2014 fully configured for the current node/run.</p>"},{"location":"reference/context-logger/#formatting-levels-service-defaults","title":"Formatting &amp; levels (service defaults)","text":"<p>Configured by <code>StdLoggerService.build(cfg)</code> and <code>LoggingConfig</code>:</p> <ul> <li>Global level: <code>cfg.level</code> (e.g., <code>INFO</code>) with optional per\u2011namespace overrides</li> <li>Console formatter: <code>cfg.console_pattern</code></li> <li>File formatter: text (<code>cfg.file_pattern</code>) or JSON (<code>cfg.use_json = True</code>)</li> <li>File rotation: <code>cfg.max_bytes</code>, <code>cfg.backup_count</code></li> <li>Non\u2011blocking file I/O: <code>cfg.enable_queue = True</code> (QueueHandler + Listener)</li> </ul> <p>You can rebuild the service on server start to apply new settings.</p>"},{"location":"reference/context-logger/#structured-fields","title":"Structured fields","text":"<p>Every call accepts <code>extra={...}</code> for structured, searchable fields. The service injects <code>{run_id, graph_id, node_id}</code> automatically. <pre><code>log.info(\"optimizer step\", extra={\"lr\": 3e-4, \"batch\": 64, \"phase\": \"warmup\"})\n</code></pre></p>"},{"location":"reference/context-logger/#good-practices","title":"Good practices","text":"<ul> <li>Use <code>debug</code> for noisy internals; rely on <code>INFO</code> for milestone breadcrumbs.</li> <li>Prefer <code>extra={...}</code> over string concatenation for metrics/values.</li> <li>Use <code>exception()</code> within <code>except</code> blocks to capture tracebacks.</li> <li>Log artifact URIs (<code>extra={\"artifact\": uri}</code>) instead of large payloads.</li> </ul>"},{"location":"reference/context-logger/#oneliner-pattern-in-tools","title":"One\u2011liner pattern in tools","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"demo\")\nasync def demo(*, context):\n    log = context.logger()\n    log.info(\"hello\", extra={\"stage\": \"start\"})\n    # ... work ...\n    log.info(\"done\", extra={\"duration_ms\": 123})\n</code></pre>"},{"location":"reference/context-logger/#summary","title":"Summary","text":"<ul> <li>Call <code>context.logger()</code> inside graph/tools for a scoped logger.</li> <li>Structured fields available via <code>extra={...}</code>; run/graph/node auto\u2011injected.</li> </ul>"},{"location":"reference/context-mcp/","title":"AetherGraph \u2014 <code>context.mcp()</code> Reference","text":"<p>This page documents the Model Context Protocol (MCP) client you obtain with <code>context.mcp(name)</code>. Use it to call tools, list resources, or read resources exposed by a remote/local MCP server over stdio, WebSocket, or HTTP.</p> <p>Import surface (for examples below): <pre><code>from aethergraph.services.mcp import (\n    MCPService,\n    StdioMCPClient,\n    WsMCPClient,\n    HttpMCPClient,\n)\n</code></pre></p>"},{"location":"reference/context-mcp/#concepts","title":"Concepts","text":"<ul> <li>MCPService: registry of named MCP clients (e.g., <code>\"local\"</code>, <code>\"ws\"</code>, <code>\"http\"</code>), handles lazy open/close and convenience calls.</li> <li>MCPClientProtocol: transport\u2011specific client implementing <code>open()</code>, <code>close()</code>, <code>call(tool, params)</code>, <code>list_tools()</code>, <code>list_resources()</code>, <code>read_resource(uri)</code>.</li> <li>Tools: remote RPCs exposed by the MCP server (e.g., <code>readFile</code>, <code>search</code>, <code>stat</code>).</li> <li>Resources: server\u2011advertised URIs you can <code>read_resource()</code> (e.g., <code>file://\u2026</code>, <code>repo://\u2026</code>).</li> </ul> <p><code>context.mcp(name)</code> returns the client registered under <code>name</code> via your process\u2011global <code>MCPService</code>.</p>"},{"location":"reference/context-mcp/#mcpservice-registry","title":"MCPService (registry)","text":""},{"location":"reference/context-mcp/#register","title":"register","text":"<p><pre><code>register(name: str, client: MCPClientProtocol) -&gt; None\n</code></pre> Register a client under a name.</p>"},{"location":"reference/context-mcp/#remove","title":"remove","text":"<p><pre><code>remove(name: str) -&gt; None\n</code></pre> Unregister a client.</p>"},{"location":"reference/context-mcp/#has-names-get","title":"has / names / get","text":"<p><pre><code>has(name: str) -&gt; bool\nnames() -&gt; list[str]\nget(name: str = \"default\") -&gt; MCPClientProtocol\n</code></pre> Query and retrieve clients by name.</p>"},{"location":"reference/context-mcp/#open-close","title":"open / close","text":"<p><pre><code>open(name: str) -&gt; None\nclose(name: str) -&gt; None\nopen_all() -&gt; None\nclose_all() -&gt; None\n</code></pre> Manage client lifecycles. <code>call()/list_*()</code> implicitly <code>open()</code> on first use.</p>"},{"location":"reference/context-mcp/#call-helpers","title":"call helpers","text":"<p><pre><code>call(name: str, tool: str, params: dict | None = None) -&gt; dict\nlist_tools(name: str) -&gt; list[MCPTool]\nlist_resources(name: str) -&gt; list[MCPResource]\nread_resource(name: str, uri: str) -&gt; dict\n</code></pre> Thin wrappers to keep call sites small; auto\u2011open if needed.</p>"},{"location":"reference/context-mcp/#optional-secretsruntime-headers","title":"optional secrets/runtime headers","text":"<p><pre><code>set_header(name: str, key: str, value: str) -&gt; None\npersist_secret(secret_name: str, value: str) -&gt; None\n</code></pre> <code>set_header()</code> is handy for WS/HTTP auth tokens at runtime. <code>persist_secret()</code> stores a credential via your Secrets provider (if writable).</p>"},{"location":"reference/context-mcp/#transport-clients","title":"Transport clients","text":""},{"location":"reference/context-mcp/#stdiomcpclient","title":"StdioMCPClient","text":"<p><pre><code>StdioMCPClient(cmd: list[str], env: dict[str,str] | None = None, timeout: float = 60.0)\n</code></pre> Spawn a subprocess and speak JSON\u2011RPC over stdio.</p>"},{"location":"reference/context-mcp/#wsmcpclient","title":"WsMCPClient","text":"<p><pre><code>WsMCPClient(url: str, *, headers: dict[str,str] | None = None, timeout: float = 60.0, ping_interval: float = 20.0, ping_timeout: float = 10.0)\n</code></pre> Connect to an MCP server over WebSocket.</p>"},{"location":"reference/context-mcp/#httpmcpclient","title":"HttpMCPClient","text":"<p><pre><code>HttpMCPClient(base_url: str, *, headers: dict[str,str] | None = None, timeout: float = 60.0)\n</code></pre> Call an MCP server over HTTP (JSON).</p>"},{"location":"reference/context-mcp/#contextmcpname","title":"context.mcp(name)","text":"<p><pre><code>context.mcp(name: str) -&gt; MCPClientProtocol\n</code></pre> Return the named client. Typically you register names like <code>\"local\"</code>, <code>\"ws\"</code>, <code>\"http\"</code> during app startup, then retrieve them inside tools/agents.</p> <p>Example <pre><code>client = context.mcp(\"ws\")\nout = await client.call(\"search\", {\"q\": \"holography\", \"k\": 5})\n</code></pre></p>"},{"location":"reference/context-mcp/#calling-tools","title":"Calling tools","text":"<p><pre><code>client.call(tool: str, params: dict | None = None) -&gt; dict\n</code></pre> Invoke a remote tool by name with JSON\u2011serializable params.</p> <p>Parameters - tool (str) \u2013 Tool name (server\u2011defined). - params (dict, optional) \u2013 Arguments for the tool.</p> <p>Returns dict \u2013 Tool result payload (shape defined by the server).</p> <p>Example <pre><code># Filesystem\u2011like server\nres = await context.mcp(\"local\").call(\"readFile\", {\"path\": \"/data/notes.txt\"})\ntext = res.get(\"text\") or res.get(\"content\") or \"\"\nawait context.channel().send_text(f\"len={len(text)}\")\n</code></pre></p>"},{"location":"reference/context-mcp/#listing-tools-resources","title":"Listing tools &amp; resources","text":"<p><pre><code>client.list_tools() -&gt; list[MCPTool]\nclient.list_resources() -&gt; list[MCPResource]\nclient.read_resource(uri: str) -&gt; dict\n</code></pre> Enumerate server capabilities and read advertised resources.</p> <p>Example <pre><code># Tool discovery\nfor t in await context.mcp(\"http\").list_tools():\n    await context.channel().send_text(f\"tool: {t.name} \u2014 {t.description}\")\n\n# Resource fetch\nfor r in await context.mcp(\"ws\").list_resources():\n    if r.uri.startswith(\"file://\"):\n        blob = await context.mcp(\"ws\").read_resource(r.uri)\n        await context.channel().send_text(f\"read {r.uri} \u2192 {len(blob.get('text',''))} chars\")\n</code></pre></p>"},{"location":"reference/context-mcp/#endtoend-setup-startup","title":"End\u2011to\u2011end setup (startup)","text":"<pre><code>from aethergraph.services.mcp import MCPService, StdioMCPClient, WsMCPClient, HttpMCPClient\nfrom aethergraph.v3.core.runtime.runtime_services import set_mcp_service\nimport os, sys\n\nDEMO_HTTP_TOKEN = os.environ.setdefault(\"DEMO_HTTP_TOKEN\", \"demo_token_123\")\n\nmcp = MCPService()\nmcp.register(\"local\", StdioMCPClient(cmd=[sys.executable, \"-m\", \"aethergraph.plugins.mcp.fs_server\"]))\nmcp.register(\"ws\", WsMCPClient(url=\"ws://localhost:8765\", headers={\"Authorization\": \"Bearer demo_token_123\"}))\nmcp.register(\"http\", HttpMCPClient(\"http://127.0.0.1:8769\", headers={\"Authorization\": f\"Bearer {DEMO_HTTP_TOKEN}\"}))\n\nset_mcp_service(mcp)  # make available to NodeContext\n</code></pre>"},{"location":"reference/context-mcp/#using-inside-a-graph-function","title":"Using inside a graph function","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"mcp_search_demo\", inputs=[\"q\"], outputs=[\"text\"], version=\"0.1.0\")\nasync def mcp_search_demo(q: str, *, context):\n    out = await context.mcp(\"ws\").call(\"search\", {\"q\": q, \"k\": 5})\n    text = out.get(\"text\") or out.get(\"content\") or \"\"\n    await context.channel().send_text(text[:200] + (\"\u2026\" if len(text) &gt; 200 else \"\"))\n    return {\"text\": text}\n</code></pre>"},{"location":"reference/context-mcp/#choosing-a-transport","title":"Choosing a transport","text":"<ul> <li>stdio: best when you ship or control the server process (local tools, file system, Git, CLI wrappers). Minimal latency, simple auth via env.</li> <li>WebSocket: interactive servers that push events, need long\u2011lived sessions, or custom headers/tokens.</li> <li>HTTP: stateless request/response, easy to deploy behind gateways; good fit for cloud MCP services.</li> </ul> <p>Tip: You can register multiple transports to the same logical backend under different names (<code>\"fs-local\"</code>, <code>\"fs-ws\"</code>) and switch per call.</p>"},{"location":"reference/context-mcp/#auth-headers","title":"Auth &amp; headers","text":"<ul> <li>Pass headers at client construction (<code>headers={\"Authorization\": \"Bearer \u2026\"}</code>).</li> <li>Update at runtime via <code>MCPService.set_header(name, key, value)</code> for WS/HTTP clients.</li> <li>Persist tokens via <code>MCPService.persist_secret(...)</code> when your Secrets provider supports writes.</li> </ul>"},{"location":"reference/context-mcp/#error-handling","title":"Error handling","text":"<p>Wrap calls to surface clear messages back to the user. <pre><code>try:\n    res = await context.mcp(\"http\").call(\"search\", {\"q\": \"mtf\"})\nexcept KeyError:\n    await context.channel().send_text(\"Unknown MCP profile. Did you register it?\")\nexcept Exception as e:\n    await context.channel().send_text(f\"MCP error: {e}\")\n</code></pre></p>"},{"location":"reference/context-mcp/#summary","title":"Summary","text":"<ul> <li>Register your clients at startup with <code>MCPService.register()</code> and wire the service into runtime so <code>context.mcp(name)</code> can retrieve them.</li> <li>Use <code>.call()</code> for tools, <code>.list_tools()/.list_resources()</code> for discovery, and <code>.read_resource()</code> to fetch URIs.</li> <li>Choose stdio/WS/HTTP based on deployment and interaction needs; manage auth via headers/</li> </ul>"},{"location":"reference/context-memory/","title":"AetherGraph \u2014 <code>context.memory()</code> Reference","text":"<p>This page documents the MemoryFacade returned by <code>context.memory()</code> in a concise format: signature, brief description, parameters, returns, and practical examples. The facade coordinates three core components \u2014 HotLog (recent, transient), Persistence (durable JSONL/appends), and Indices (fast derived views) \u2014 with optional ArtifactStore, RAG, and LLM services.</p>"},{"location":"reference/context-memory/#overview","title":"Overview","text":"<p><code>context.memory()</code> is bound to your current runtime scope (<code>session_id</code>, <code>run_id</code>, <code>graph_id</code>, <code>node_id</code>, <code>agent_id</code>). Typical operations:</p> <ol> <li> <p>Record events (raw or typed results)</p> </li> <li> <p>Query recent/last/by\u2011kind outputs via indices/hotlog</p> </li> <li> <p>Distill (rolling summaries, episode summaries)</p> </li> <li> <p>RAG (optional): upsert, search, answer using a configured RAG + LLM</p> </li> </ol>"},{"location":"reference/context-memory/#memoryrecord_raw","title":"memory.record_raw","text":"<p><pre><code>record_raw(*, base: dict, text: str | None = None, metrics: dict | None = None, sources: list[str] | None = None) -&gt; Event\n</code></pre> Append a normalized event to HotLog (fast) and Persistence (durable). Computes a stable <code>event_id</code> and a lightweight <code>signal</code> if absent.</p> <p>Parameters</p> <ul> <li> <p>base (dict) \u2013 Canonical fields describing the event (e.g., <code>kind</code>, <code>stage</code>, <code>severity</code>, <code>tool</code>, <code>tags</code>, <code>entities</code>, <code>inputs</code>, <code>outputs</code>, \u2026). Missing scope keys are filled from the bound context.</p> </li> <li> <p>text (str, optional) \u2013 Human\u2011readable message/body.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics (latency, token counts, costs, etc.).</p> </li> <li> <p>sources (list[str], optional) \u2013 Event IDs this event summarizes/derives from.</p> </li> </ul> <p>Returns Event \u2013 The appended event.</p> <p>Notes Does not update <code>indices</code> automatically. Use <code>write_result()</code> when you want indices updated for typed outputs.</p>"},{"location":"reference/context-memory/#memoryrecord","title":"memory.record","text":"<p><pre><code>record(kind, data, tags=None, entities=None, severity=2, stage=None, inputs_ref=None, outputs_ref=None, metrics=None, sources=None, signal=None) -&gt; Event\n</code></pre> Convenience wrapper around <code>record_raw()</code> for common fields; stringifies <code>data</code> if needed.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Event kind (e.g., <code>\"user_msg\"</code>, <code>\"tool_call\"</code>).</p> </li> <li> <p>data (Any) \u2013 JSON\u2011serializable payload; will be stringified for <code>text</code>.</p> </li> <li> <p>tags (list[str], optional) \u2013 Tag list.</p> </li> <li> <p>entities (list[str], optional) \u2013 Entity IDs.</p> </li> <li> <p>severity (int) \u2013 1\u20135 scale (default 2).</p> </li> <li> <p>stage (str, optional) \u2013 Phase label (e.g., <code>\"observe\"</code>, <code>\"act\"</code>).</p> </li> <li> <p>inputs_ref (list[dict], optional) \u2013 Typed input references (Value[]).</p> </li> <li> <p>outputs_ref (list[dict], optional) \u2013 Typed output references (Value[]).</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics.</p> </li> <li> <p>sources (list[str], optional) \u2013 Upstream event IDs.</p> </li> <li> <p>signal (float, optional) \u2013 0.0\u20131.0; if omitted, computed heuristically.</p> </li> </ul> <p>Returns Event \u2013 The appended event.</p>"},{"location":"reference/context-memory/#memorywrite_result","title":"memory.write_result","text":"<p><pre><code>write_result(*, topic: str, inputs: list[dict] | None = None, outputs: list[dict] | None = None, tags: list[str] | None = None, metrics: dict | None = None, message: str | None = None, severity: int = 3) -&gt; Event\n</code></pre> Record a typed result (tool/agent/flow) and update indices for quick retrieval.</p> <p>Parameters</p> <ul> <li> <p>topic (str) \u2013 Tool/agent/flow identifier (used by <code>indices.last_outputs_by_topic</code>).</p> </li> <li> <p>inputs (list[dict], optional) \u2013 Typed inputs (Value[]).</p> </li> <li> <p>outputs (list[dict], optional) \u2013 Typed outputs (Value[]). Indices derive from these.</p> </li> <li> <p>tags (list[str], optional) \u2013 Tag list.</p> </li> <li> <p>metrics (dict, optional) \u2013 Numeric metrics.</p> </li> <li> <p>message (str, optional) \u2013 Human\u2011readable summary.</p> </li> <li> <p>severity (int) \u2013 Default 3.</p> </li> </ul> <p>Returns Event \u2013 The normalized <code>tool_result</code> event.</p> <p>Effect Auto\u2011appends to HotLog &amp; Persistence and calls <code>indices.update(session_id, evt)</code>.</p>"},{"location":"reference/context-memory/#memoryrecent","title":"memory.recent","text":"<p><pre><code>recent(*, kinds: list[str] | None = None, limit: int = 50) -&gt; list[Event]\n</code></pre> Return recent events from HotLog (most recent last), optionally filtering by <code>kinds</code>.</p> <p>Parameters</p> <ul> <li> <p>kinds (list[str], optional) \u2013 Filter kinds.</p> </li> <li> <p>limit (int) \u2013 Max events (default 50).</p> </li> </ul> <p>Returns list[Event] \u2013 Recent events.</p>"},{"location":"reference/context-memory/#memorylast_by_name","title":"memory.last_by_name","text":"<p><pre><code>last_by_name(name: str)\n</code></pre> Return the last output value by <code>name</code> from Indices (fast path).</p> <p>Parameters</p> <ul> <li>name (str) \u2013 Output name.</li> </ul> <p>Returns Any \u2013 The stored value for that name (adapter\u2011dependent) or <code>None</code>.</p>"},{"location":"reference/context-memory/#memorylatest_refs_by_kind","title":"memory.latest_refs_by_kind","text":"<p><pre><code>latest_refs_by_kind(kind: str, *, limit: int = 50)\n</code></pre> Return latest ref outputs by <code>ref.kind</code> (fast path, KV\u2011backed) from Indices.</p> <p>Parameters</p> <ul> <li> <p>kind (str) \u2013 Reference kind.</p> </li> <li> <p>limit (int) \u2013 Max items (default 50).</p> </li> </ul> <p>Returns list[Any] \u2013 Recent references.</p>"},{"location":"reference/context-memory/#memorylast_outputs_by_topic","title":"memory.last_outputs_by_topic","text":"<p><pre><code>last_outputs_by_topic(topic: str)\n</code></pre> Return the last output map for a given topic (tool/flow/agent) from Indices.</p> <p>Parameters</p> <ul> <li>topic (str) \u2013 Topic identifier.</li> </ul> <p>Returns dict | None \u2013 Latest outputs or <code>None</code> if absent.</p>"},{"location":"reference/context-memory/#memorydistill_rolling_chat","title":"memory.distill_rolling_chat","text":"<p><pre><code>distill_rolling_chat(*, max_turns: int = 20, min_signal: float | None = None) -&gt; dict\n</code></pre> Build a rolling chat summary from recent user/assistant turns (reads HotLog; typically writes a JSON summary via Persistence).</p> <p>Parameters</p> <ul> <li> <p>max_turns (int) \u2013 Window of turns to include (default 20).</p> </li> <li> <p>min_signal (float, optional) \u2013 Signal threshold; uses facade default if omitted.</p> </li> </ul> <p>Returns dict \u2013 Descriptor (e.g., <code>{ \"uri\": ..., \"sources\": [...] }</code>).</p>"},{"location":"reference/context-memory/#memorydistill_episode","title":"memory.distill_episode","text":"<p><pre><code>distill_episode(*, tool: str, run_id: str, include_metrics: bool = True) -&gt; dict\n</code></pre> Summarize a tool/agent episode (all events for a given <code>run_id</code> + <code>tool</code>). Reads HotLog/Persistence; writes back a summary JSON (and optionally CAS bundle).</p> <p>Parameters</p> <ul> <li> <p>tool (str) \u2013 Tool/agent identifier.</p> </li> <li> <p>run_id (str) \u2013 Run to summarize.</p> </li> <li> <p>include_metrics (bool) \u2013 Include metrics in the summary (default True).</p> </li> </ul> <p>Returns dict \u2013 Descriptor (e.g., <code>{ \"uri\": ..., \"sources\": [...], \"metrics\": {...} }</code>).</p>"},{"location":"reference/context-memory/#rag-helpers-optional","title":"RAG helpers (optional)","text":""},{"location":"reference/context-memory/#memoryrag_upsert","title":"memory.rag_upsert","text":"<p><pre><code>rag_upsert(*, corpus_id: str, docs: Sequence[dict], topic: str | None = None) -&gt; dict\n</code></pre> Upsert documents into a RAG corpus via the configured RAG facade.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>docs (Sequence[dict]) \u2013 Documents/chunks with text and metadata.</p> </li> <li> <p>topic (str, optional) \u2013 Optional topic name to attribute the upsert.</p> </li> </ul> <p>Returns dict \u2013 Upsert stats (shape adapter\u2011specific).</p> <p>Raises <code>RuntimeError</code> \u2013 if RAG facade is not configured.</p>"},{"location":"reference/context-memory/#memoryrag_search","title":"memory.rag_search","text":"<p><pre><code>rag_search(*, corpus_id: str, query: str, k: int = 8) -&gt; list[dict]\n</code></pre> Retrieve best\u2011matching chunks for a query.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>query (str) \u2013 Natural language query.</p> </li> <li> <p>k (int) \u2013 Max results (default 8), reranked.</p> </li> </ul> <p>Returns list[dict] \u2013 Ranked hits.</p> <p>Raises <code>RuntimeError</code> \u2013 if RAG facade is not configured.</p>"},{"location":"reference/context-memory/#memoryrag_answer","title":"memory.rag_answer","text":"<p><pre><code>rag_answer(*, corpus_id: str, question: str, style: str = \"concise\", k: int = 6, llm_profile: str = \"default\") -&gt; dict\n</code></pre> Answer a question using RAG + LLM (both must be configured).</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>question (str) \u2013 User question.</p> </li> <li> <p>style (str) \u2013 Answering style (e.g., <code>\"concise\"</code>).</p> </li> <li> <p>k (int) \u2013 Max retrieved chunks (default 6).</p> </li> <li> <p>llm_profile (str) \u2013 Profile name to select an LLM client.</p> </li> </ul> <p>Returns dict \u2013 Answer payload (adapter\u2011specific).</p> <p>Raises <code>RuntimeError</code> \u2013 if RAG or LLM is not configured.</p>"},{"location":"reference/context-memory/#practical-examples","title":"Practical examples","text":"<p>1) Record + recent <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"mem_record_recent\")\nasync def mem_record_recent(*, context):\n    evt = await context.memory().record(\n        kind=\"user_msg\",\n        data={\"text\":\"hello world\",\"lang\":\"en\"},\n        tags=[\"demo\",\"quickstart\"],\n        severity=2,\n    )\n    recent = await context.memory().recent(kinds=[\"user_msg\"], limit=5)\n    await context.channel().send_text(f\"recent user_msg count={len(recent)}\")\n    return {\"event_id\": evt.event_id, \"recent_count\": len(recent)}\n</code></pre></p> <p>2) Write a typed result and fetch last outputs <pre><code>@graph_fn(name=\"mem_write_result\")\nasync def mem_write_result(*, context):\n    await context.memory().write_result(\n        topic=\"eval.step\",\n        outputs=[{\"name\":\"acc\",\"kind\":\"number\",\"value\":0.912}],\n        metrics={\"latency_ms\": 120},\n        message=\"evaluation complete\",\n    )\n    last = await context.memory().last_outputs_by_topic(\"eval.step\")\n    await context.channel().send_text(f\"last acc={last['acc']:.3f}\")\n</code></pre></p> <p>3) Rolling chat summary <pre><code>@graph_fn(name=\"mem_rolling\")\nasync def mem_rolling(*, context):\n    summary = await context.memory().distill_rolling_chat(max_turns=16)\n    await context.channel().send_text(f\"rolling summary uri: {summary.get('uri','&lt;none&gt;')}\")\n</code></pre></p> <p>4) Episode summary <pre><code>@graph_fn(name=\"mem_episode\")\nasync def mem_episode(*, context, run_id: str, tool: str):\n    desc = await context.memory().distill_episode(tool=tool, run_id=run_id)\n    await context.channel().send_text(f\"episode summary: {desc.get('uri','&lt;none&gt;')}\")\n</code></pre></p> <p>5) RAG (if configured) <pre><code>@graph_fn(name=\"mem_rag\")\nasync def mem_rag(*, context):\n    # Upsert a few docs\n    await context.memory().rag_upsert(\n        corpus_id=\"notes\",\n        docs=[{\"id\":\"1\",\"text\":\"Optics basics: Snell's law\"}],\n    )\n    # Search\n    hits = await context.memory().rag_search(corpus_id=\"notes\", query=\"Snell\")\n    # Answer\n    ans = await context.memory().rag_answer(corpus_id=\"notes\", question=\"What is Snell's law?\", style=\"concise\")\n    await context.channel().send_text(ans.get(\"answer\",\"&lt;no answer&gt;\"))\n</code></pre></p>"},{"location":"reference/context-memory/#notes-behaviors","title":"Notes &amp; behaviors","text":"<ul> <li> <p>Signal heuristic: if not provided, <code>record(_raw)</code> computes a 0.0\u20131.0 <code>signal</code> from severity + presence/length of text + metrics.</p> </li> <li> <p>Durability: every <code>record_raw</code> &amp; <code>write_result</code> appends to Persistence; <code>recent()</code> reads from HotLog.</p> </li> <li> <p>Indices: <code>write_result()</code> updates fast views used by <code>last_by_name</code>, <code>latest_refs_by_kind</code>, <code>last_outputs_by_topic</code>.</p> </li> <li> <p>Artifacts: distillers may produce CAS artifacts when an <code>ArtifactStore</code> is provided.</p> </li> <li> <p>Performance: methods are async; backends should avoid blocking the event loop (use <code>asyncio.to_thread</code> for heavy IO).</p> </li> </ul>"},{"location":"reference/context-rag/","title":"AetherGraph \u2014 <code>context.rag()</code> Reference","text":"<p>This page documents the RAGFacade returned by <code>context.rag()</code> in a concise format: signature, brief description, parameters, returns, and practical examples.</p> <p>The facade covers: corpus management, document ingestion (upsert), retrieval (search/retrieve), and question answering with optional citation resolution.</p>"},{"location":"reference/context-rag/#overview","title":"Overview","text":"<p><code>context.rag()</code> provides high\u2011level helpers backed by:</p> <ul> <li>an Artifact Store (for persisted doc assets),</li> <li>an Embedding client (e.g., <code>context.llm().embed()</code>),</li> <li>a Vector index backend (add/search),</li> <li>a TextSplitter (chunking before embedding), and</li> <li>an optional LLM client for QA.</li> </ul>"},{"location":"reference/context-rag/#ragadd_corpus","title":"rag.add_corpus","text":"<p><pre><code>add_corpus(corpus_id: str, meta: dict | None = None) -&gt; None\n</code></pre> Create a new corpus directory with metadata if it does not exist.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Unique identifier for the corpus.</p> </li> <li> <p>meta (dict, optional) \u2013 Arbitrary metadata stored alongside the corpus.</p> </li> </ul> <p>Returns <code>None</code></p>"},{"location":"reference/context-rag/#ragupsert_docs","title":"rag.upsert_docs","text":"<p><pre><code>upsert_docs(corpus_id: str, docs: list[dict]) -&gt; dict\n</code></pre> Ingest and index a list of documents (file\u2011based or inline text). Handles artifact persistence, chunking, embedding, and index add.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus identifier.</p> </li> <li> <p>docs (list[dict]) \u2013 Each doc is either:</p> </li> <li> <p>File doc: <code>{ \"path\": \"/path/to/file.pdf\", \"labels\": {...}, \"title\": \"Optional\" }</code></p> </li> <li> <p>Inline text doc: <code>{ \"text\": \"...\", \"title\": \"Optional\", \"labels\": {...} }</code></p> </li> </ul> <p>Returns dict \u2013 Summary like <code>{ \"added\": int, \"chunks\": int, \"index\": \"BackendName\" }</code>.</p> <p>Notes - PDFs and Markdown are parsed with built\u2011in extractors; other files default to text.</p> <ul> <li>Each doc and chunk is assigned a stable SHA\u2011derived ID and recorded in <code>docs.jsonl</code> / <code>chunks.jsonl</code> under the corpus folder.</li> </ul>"},{"location":"reference/context-rag/#ragsearch","title":"rag.search","text":"<p><pre><code>search(corpus_id: str, query: str, k: int = 8, filters: dict | None = None, mode: str = \"hybrid\") -&gt; list[SearchHit]\n</code></pre> Hybrid retrieval: dense vector search with optional lexical fusion, returning the top\u2011k chunks.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>query (str) \u2013 Natural language query.</p> </li> <li> <p>k (int) \u2013 Number of results (default 8).</p> </li> <li> <p>filters (dict, optional) \u2013 Reserved for metadata filtering (adapter\u2011specific).</p> </li> <li> <p>mode ({\"dense\",\"hybrid\"}) \u2013 Retrieval mode (default <code>\"hybrid\"</code>).</p> </li> </ul> <p>Returns list[SearchHit] \u2013 Ranked hits with <code>chunk_id</code>, <code>doc_id</code>, <code>corpus_id</code>, <code>score</code>, <code>text</code>, <code>meta</code>.</p>"},{"location":"reference/context-rag/#ragretrieve","title":"rag.retrieve","text":"<p><pre><code>retrieve(corpus_id: str, query: str, k: int = 6, rerank: bool = True) -&gt; list[SearchHit]\n</code></pre> Convenience wrapper over <code>search(..., mode=\"hybrid\")</code> for top\u2011k retrieval.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>query (str) \u2013 Natural language query.</p> </li> <li> <p>k (int) \u2013 Number of results (default 6).</p> </li> <li> <p>rerank (bool) \u2013 Currently ignored (hybrid already fuses scores).</p> </li> </ul> <p>Returns list[SearchHit] \u2013 Ranked hits.</p>"},{"location":"reference/context-rag/#raganswer","title":"rag.answer","text":"<p><pre><code>answer(corpus_id: str, question: str, *, llm: GenericLLMClient | None = None, style: str = \"concise\", with_citations: bool = True, k: int = 6) -&gt; dict\n</code></pre> Answer a question using retrieved context and an LLM.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>question (str) \u2013 End\u2011user question.</p> </li> <li> <p>llm (GenericLLMClient, optional) \u2013 LLM to use; defaults to the facade\u2019s configured client.</p> </li> <li> <p>style ({\"concise\",\"detailed\"}) \u2013 Answer verbosity/style.</p> </li> <li> <p>with_citations (bool) \u2013 Whether to include resolved citations.</p> </li> <li> <p>k (int) \u2013 Retrieval depth (default 6).</p> </li> </ul> <p>Returns dict \u2013 <code>{ \"answer\": str, \"citations\": [...], \"usage\": {...}, \"resolved_citations\": [...]? }</code>.</p> <p>Behavior - Builds a context block from top\u2011k chunks (numbered <code>[1]</code>, <code>[2]</code>, ...).</p> <ul> <li>Prompts the LLM to answer only from the provided context and cite chunk numbers.</li> </ul>"},{"location":"reference/context-rag/#ragresolve_citations","title":"rag.resolve_citations","text":"<p><pre><code>resolve_citations(corpus_id: str, citations: list[dict]) -&gt; list[dict]\n</code></pre> Resolve citation metadata for display/download.</p> <p>Parameters</p> <ul> <li> <p>corpus_id (str) \u2013 Target corpus.</p> </li> <li> <p>citations (list[dict]) \u2013 Items like <code>{ \"chunk_id\", \"doc_id\", \"rank\" }</code>.</p> </li> </ul> <p>Returns list[dict] \u2013 Sorted by <code>rank</code>, each <code>{ rank, doc_id, title, uri, chunk_id, snippet }</code>.</p>"},{"location":"reference/context-rag/#practical-examples","title":"Practical examples","text":"<p>1) Create a corpus and ingest docs <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"rag_ingest\")\nasync def rag_ingest(*, context):\n    await context.rag().add_corpus(\"notes\")\n    stats = await context.rag().upsert_docs(\n        corpus_id=\"notes\",\n        docs=[\n            {\"text\": \"Optics basics: Snell's law relates angles of incidence and refraction.\" , \"title\": \"optics\"},\n            {\"path\": \"/data/papers/holography.md\", \"labels\": {\"topic\": \"holography\"}},\n        ],\n    )\n    await context.channel().send_text(f\"RAG upsert: {stats}\")\n</code></pre></p> <p>2) Search and preview hits <pre><code>@graph_fn(name=\"rag_search_preview\")\nasync def rag_search_preview(*, context, q: str):\n    hits = await context.rag().search(corpus_id=\"notes\", query=q, k=5)\n    for i, h in enumerate(hits, 1):\n        await context.channel().send_text(f\"[{i}] score={h.score:.3f}  doc={h.doc_id}\\n{h.text[:200]}\")\n</code></pre></p> <p>3) Answer with citations <pre><code>@graph_fn(name=\"rag_answer_with_citations\")\nasync def rag_answer_with_citations(*, context, q: str):\n    out = await context.rag().answer(corpus_id=\"notes\", question=q, style=\"concise\", k=6)\n    ans = out.get(\"answer\", \"\")\n    cites = out.get(\"resolved_citations\", [])\n    await context.channel().send_text(ans)\n    for c in cites[:3]:\n        await context.channel().send_text(f\"[#{c['rank']}] {c['title']} \u2014 {c['snippet']}\")\n</code></pre></p>"},{"location":"reference/context-rag/#notes-behaviors","title":"Notes &amp; behaviors","text":"<ul> <li> <p>Chunking &amp; Embedding: Documents are split via <code>TextSplitter</code> then embedded in batch; the index stores <code>(chunk_id, vector, meta)</code>.</p> </li> <li> <p>Artifacts: File docs and inline text are persisted to the Artifact Store; returned URIs appear in doc metadata and resolved citations.</p> </li> <li> <p>IDs: <code>doc_id</code> and <code>chunk_id</code> are stable SHA\u2011derived IDs; re\u2011ingesting the same content usually yields the same IDs (subject to meta changes).</p> </li> <li> <p>Filters: <code>filters</code> is reserved for future adapter support (label\u2011based narrowing).</p> </li> <li> <p>LLM &amp; Usage: <code>answer()</code> returns provider usage where available; some providers may omit it.</p> </li> </ul>"},{"location":"reference/decorators/","title":"Decorator API \u2014 <code>@graph_fn</code>, <code>@graphify</code>, <code>@tool</code>","text":"<p>A single reference page for the three core decorators you\u2019ll use to build with AetherGraph.</p>"},{"location":"reference/decorators/#quick-chooser","title":"Quick chooser","text":"Use this when\u2026 Pick Why You want the quickest way to make a Python function runnable as a graph entrypoint and get a <code>context</code> for services <code>@graph_fn</code> Small, ergonomic, ideal for tutorials, notebooks, single\u2011entry tools/agents You need to expose reusable steps with typed I/O that can run standalone or as graph nodes <code>@tool</code> Dual\u2011mode decorator; gives you fine control of inputs/outputs; portable and composable Your function body is mostly tool wiring (fan\u2011in/fan\u2011out) and you want a static graph spec from Python syntax <code>@graphify</code> Author graphs declaratively; returns a <code>TaskGraph</code> factory; great for orchestration patterns"},{"location":"reference/decorators/#graph_fn","title":"<code>@graph_fn</code>","text":"<p>Wrap a normal async function into a runnable graph with optional <code>context</code> injection.</p>"},{"location":"reference/decorators/#signature","title":"Signature","text":"<pre><code>@graph_fn(name: str, *, inputs: list[str] | None = None, outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\nasync def my_fn(..., *, context: NodeContext): ...\n</code></pre>"},{"location":"reference/decorators/#description","title":"Description","text":"<ul> <li>Builds a fresh <code>TaskGraph</code> under the hood and executes it immediately.</li> <li>If your function signature includes <code>context: NodeContext</code>, AetherGraph injects a <code>NodeContext</code> so you can call <code>context.channel()</code>, <code>context.memory()</code>, <code>context.artifacts()</code>, <code>context.llm()</code>, etc.</li> <li>Ideal for single\u2011file demos, CLI/notebook usage, and simple agents.</li> </ul>"},{"location":"reference/decorators/#parameters","title":"Parameters","text":"<ul> <li>name (str, required) \u2014 Graph ID and human\u2011readable name.</li> <li>inputs (list[str], optional) \u2014 Declared input keys. Purely declarative; your function still gets normal Python args.</li> <li>outputs (list[str], optional) \u2014 Declared output keys. If you return a single literal, declare exactly one.</li> <li>version (str, optional) \u2014 Semantic version for registry.</li> <li>agent (str, optional) \u2014 If provided, registers this graph function as an agent under the given name.</li> </ul>"},{"location":"reference/decorators/#returns","title":"Returns","text":"<ul> <li>The decorator returns a <code>GraphFunction</code> object. Calling/awaiting it executes the graph and returns a <code>dict</code> of outputs keyed by <code>outputs</code>.</li> </ul>"},{"location":"reference/decorators/#minimal-example","title":"Minimal example","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"hello.world\", inputs=[\"name\"], outputs=[\"greeting\"], version=\"0.1.0\")\nasync def hello_world(name: str, *, context: NodeContext):\n    await context.channel().send_text(f\"\ud83d\udc4b Hello {name}\")\n    return {\"greeting\": f\"Hello, {name}!\"}\n\n# Run (async)\nres = await hello_world(name=\"Aether\")\nprint(res[\"greeting\"])  # \u2192 \"Hello, Aether!\"\n</code></pre>"},{"location":"reference/decorators/#tips","title":"Tips","text":"<ul> <li>Return a <code>dict</code> where keys match <code>outputs</code>. If you return a single literal, declare one output.</li> <li>You can call <code>@tool</code> functions inside a <code>@graph_fn</code> (they\u2019ll run immediately, not build nodes). Use this for small, fast helper steps.</li> <li>For complex orchestration (fan\u2011in/fan\u2011out), prefer <code>@graphify</code> so <code>@tool</code> calls become nodes.</li> </ul>"},{"location":"reference/decorators/#tool","title":"<code>@tool</code>","text":"<p>Dual\u2011mode decorator for reusable steps with explicit inputs/outputs.</p>"},{"location":"reference/decorators/#signature_1","title":"Signature","text":"<pre><code>@tool(outputs: list[str], *, inputs: list[str] | None = None, name: str | None = None, version: str = \"0.1.0\")\ndef/async def my_tool(...): ...\n</code></pre>"},{"location":"reference/decorators/#description_1","title":"Description","text":"<ul> <li>Immediate mode (no builder/interpreter active): calling the function executes it now and returns a <code>dict</code> of outputs.</li> <li>Graph mode (inside a <code>graph(...)</code> / <code>@graphify</code> body or during <code>@graph_fn</code> build): calling the proxy adds a node to the current graph and returns a <code>NodeHandle</code> with typed outputs.</li> <li>Registers the underlying implementation in the runtime registry for portability.</li> </ul>"},{"location":"reference/decorators/#parameters_1","title":"Parameters","text":"<ul> <li>outputs (list[str], required) \u2014 Names of output values (e.g., <code>[\"result\"]</code>, <code>[\"image\", \"stats\"]</code>).</li> <li>inputs (list[str], optional) \u2014 Input names (auto\u2011inferred from signature if omitted).</li> <li>name (str, optional) \u2014 Registry/display name; defaults to function name.</li> <li>version (str, optional) \u2014 Semantic version for registry.</li> </ul>"},{"location":"reference/decorators/#returns_1","title":"Returns","text":"<ul> <li>In immediate mode: <code>dict</code> of outputs.</li> <li>In graph mode: <code>NodeHandle</code> with <code>.out_key</code> attributes (e.g., <code>node.result</code>).</li> </ul>"},{"location":"reference/decorators/#example-reusable-step","title":"Example \u2014 reusable step","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"sum\", \"count\"])\ndef aggregate(xs: list[int]):\n    return {\"sum\": sum(xs), \"count\": len(xs)}\n\n# Immediate mode\nprint(aggregate([1,2,3]))  # {\"sum\": 6, \"count\": 3}\n</code></pre>"},{"location":"reference/decorators/#example-using-tool-inside-graph_fn-immediate-execution","title":"Example \u2014 using <code>@tool</code> inside <code>@graph_fn</code> (immediate execution)","text":"<pre><code>from aethergraph import graph_fn, tool, NodeContext\n\n@tool(outputs=[\"sum\"])  \ndef add(x: int, y: int):\n    return {\"sum\": x + y}\n\n@graph_fn(name=\"calc.pipeline\", inputs=[\"a\",\"b\"], outputs=[\"total\"])\nasync def calc(a: int, b: int, *, context: NodeContext):\n    out = add(a, b)                 # immediate mode here\n    await context.channel().send_text(f\"sum = {out['sum']}\")\n    return {\"total\": out[\"sum\"]}\n</code></pre>"},{"location":"reference/decorators/#tips_1","title":"Tips","text":"<ul> <li>Use <code>@tool</code> to make steps portable and inspectable (typed I/O makes graphs predictable).</li> <li>In <code>@graph_fn</code> the <code>@tool</code> call executes immediately; in <code>@graphify</code> the same call becomes a graph node.</li> <li>Control\u2011flow knobs like <code>_after</code>, <code>_id</code>, <code>_alias</code> apply only in graph\u2011building contexts (e.g., <code>@graphify</code>), not in <code>@graph_fn</code> bodies.</li> </ul>"},{"location":"reference/decorators/#graphify","title":"<code>@graphify</code>","text":"<p>Author a static TaskGraph by writing normal Python that calls <code>@tool</code>s. The function body executes during build to register nodes and edges; returned node handles/literals define graph outputs.</p>"},{"location":"reference/decorators/#signature_2","title":"Signature","text":"<pre><code>@graphify(*, name: str = \"default_graph\", inputs: Iterable[str] | dict = (), outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\ndef my_graph(...):\n    ...  # body calls @tool proxies (graph mode)\n    return {...}  # NodeHandle(s) and/or literal refs\n</code></pre>"},{"location":"reference/decorators/#description_2","title":"Description","text":"<ul> <li>The decorated function becomes a factory: calling <code>my_graph.build()</code> returns a <code>TaskGraph</code> spec.</li> <li>When the body runs under the builder, calls to <code>@tool</code> proxies add nodes to the graph and return <code>NodeHandle</code>s.</li> <li>Perfect for fan\u2011out (parallel branches) and fan\u2011in (join/aggregate) patterns.</li> </ul>"},{"location":"reference/decorators/#parameters_2","title":"Parameters","text":"<ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (iterable[str] or dict) \u2014 Required/optional input names. If dict, keys are optional names with defaults in the body.</li> <li>outputs (list[str], optional) \u2014 Names of exposed boundary outputs. If body returns a single literal, declare exactly one.</li> <li>version (str) \u2014 Semantic version.</li> <li>agent (str, optional) \u2014 Register this graph as an agent (factory registered).</li> </ul>"},{"location":"reference/decorators/#returns_2","title":"Returns","text":"<ul> <li> <p>The decorator returns a builder function with:</p> </li> <li> <p><code>.build() -&gt; TaskGraph</code></p> </li> <li><code>.spec() -&gt; TaskGraphSpec</code></li> <li><code>.io() -&gt; IO signature</code></li> </ul>"},{"location":"reference/decorators/#example-fanout-fanin","title":"Example \u2014 fan\u2011out + fan\u2011in","text":"<pre><code>from aethergraph import graphify, tool\n\n@tool(outputs=[\"y\"])\ndef f(x: int):\n    return {\"y\": x * x}\n\n@tool(outputs=[\"z\"])\ndef g(x: int):\n    return {\"z\": x + 1}\n\n@tool(outputs=[\"sum\"])  \ndef add(a: int, b: int):\n    return {\"sum\": a + b}\n\n@graphify(name=\"fan_in_out\", inputs=[\"x\"], outputs=[\"total\"]) \ndef pipe(x):\n    a = f(x=x)          # node A (graph mode)  \u2510\n    b = g(x=x)          # node B (graph mode)  \u2518  \u2190 fan\u2011out\n    c = add(a=a.y, b=b.z)   # node C depends on A,B \u2190 fan\u2011in\n    return {\"total\": c.sum}\n\nG = pipe.build()\n</code></pre>"},{"location":"reference/decorators/#example-ordering-with-_after-and-aliasing","title":"Example \u2014 ordering with <code>_after</code> and aliasing","text":"<pre><code>@tool(outputs=[\"out\"]) \ndef step(name: str):\n    return {\"out\": name}\n\n@graphify(name=\"ordered\", inputs=[]) \ndef ordered():\n    a = step(name=\"A\", _alias=\"first\")\n    b = step(name=\"B\", _after=a)\n    c = step(name=\"C\", _after=[a, b], _id=\"third\")\n    return {\"final\": c.out}\n\nG = ordered.build()\n</code></pre>"},{"location":"reference/decorators/#using-tool-inside-graph_fn-vs-graphify","title":"Using <code>@tool</code> inside <code>@graph_fn</code> vs <code>@graphify</code>","text":"<ul> <li>Inside <code>@graph_fn</code>: <code>@tool</code> calls execute immediately (no <code>_after</code>/alias). Great for quick helpers.</li> <li>Inside <code>@graphify</code>: <code>@tool</code> calls define nodes (support <code>_after</code>, <code>_alias</code>, <code>_id</code>, <code>_labels</code>). Ideal for orchestration.</li> </ul>"},{"location":"reference/decorators/#interop-best-practices","title":"Interop &amp; best practices","text":"<ol> <li>Start simple with <code>@graph_fn</code> \u2014 it\u2019s the easiest way to get <code>context</code> and ship a working demo.</li> <li>Extract reusable steps with <code>@tool</code> \u2014 typed I/O makes debugging, tracing, and promotion to graphs trivial.</li> <li> <p>Promote to <code>@graphify</code> when you need:</p> </li> <li> <p>Parallel branches (fan\u2011out), joins (fan\u2011in)</p> </li> <li>Explicit ordering with <code>_after</code></li> <li>Reuse via <code>NodeHandle</code> composition and aliasing</li> <li> <p>Context access:</p> </li> <li> <p><code>@graph_fn</code> gives you <code>context: NodeContext</code> directly.</p> </li> <li>In <code>@graphify</code>, nodes don\u2019t get <code>context</code>; tools run with context at execution time when the graph is interpreted. Use <code>@tool</code> implementations to call <code>context.*</code>.</li> <li>Outputs discipline \u2014 keep outputs small and typed (e.g., <code>{ \"image\": ref, \"metrics\": {\u2026} }</code>).</li> <li>Registry \u2014 all three decorators register artifacts (graph fn as runnable, tool impls, graph factories) so you can call by name later.</li> </ol>"},{"location":"reference/decorators/#see-also","title":"See also","text":"<ul> <li>Quick Start: install, start server, first <code>@graph_fn</code>.</li> <li>**Contex</li> </ul>"},{"location":"reference/rest-api/","title":"REST API","text":"<ul> <li><code>GET /health</code> \u2192 200 OK</li> <li><code>POST /execute</code> \u2192 Execute a graph function</li> <li><code>GET/PUT /artifacts/*</code> \u2192 Retrieve/store artifacts</li> </ul> <p>(Add OpenAPI/Redoc when ready.)</p>"},{"location":"reference/tools-facade/","title":"Tools Facade","text":""},{"location":"reference/tools-facade/#registerfunc-namenone-inputsnone-outputsnone-str","title":"register(func, *, name=None, inputs=None, outputs=None) \u2192 str","text":"<p>Registers a tool and returns its name/id.</p>"},{"location":"reference/tools-facade/#callname-args-dict-dict","title":"call(name, args: dict) \u2192 dict","text":"<p>Invokes a tool by name with validated args.</p>"}]}