{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AetherGraph","text":"<p>AetherGraph is a Python\u2011first agentic framework designed to supercharge your R&amp;D workflows. Effortlessly build, orchestrate, and trace research pipelines with a powerful combination of graph-based function composition, context-aware services, and safe tool integrations.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Graph Functions: Use Python functions as first-class graph nodes with the intuitive <code>@graph_fn</code> decorator.</li> <li>Integrated Context Services: Out-of-the-box support for LLMs, memory, channels, artifacts, and key-value stores\u2014no bolt\u2011ons required, but fully extensible.</li> <li>Traceable Artifacts &amp; Memory: Capture every step and result for reproducible, auditable research.</li> <li>Safe Tooling: Invoke external capabilities securely and reliably.</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to dive in? Follow our Quickstart guide and build your first agentic workflow in minutes.</p> <p>AetherGraph empowers researchers, engineers, and teams to create robust, transparent, and scalable R&amp;D solutions\u2014all in pure Python.</p>"},{"location":"concept/","title":"AetherGraph \u2014 Architecture Overview (1\u2011page)","text":"<p>Goal: Give newcomers a single \"big picture\" of how AetherGraph fits together, then provide a tiny legend so they know what to look up next.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          AetherGraph Runtime                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                    \u2502\n\u2502  Python Code (your repo)                                           \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500          \u2502\n\u2502  @graph_fn nodes           Tools (@tool)            Services       \u2502\n\u2502  (code-native agents)      (reusable ops,           (external ctx) \u2502\n\u2502                            checkpointable)                          \u2502\n\u2502       \u2502                           \u2502                    \u2502            \u2502\n\u2502       \u25bc                           \u25bc                    \u25bc            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  Node Exec   \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  Tool Exec  \u2502      \u2502  Service API \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502        \u2502                                                         \u2502    \u2502\n\u2502        \u25bc                                                         \u2502    \u2502\n\u2502                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502                       \u2502        NodeContext       \u2502  (per node call)  \u2502\n\u2502                       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                   \u2502\n\u2502                       \u2502 channel()   \u2192  chat/CLI/GUI (send/ask)       \u2502\n\u2502                       \u2502 memory()    \u2192  record/recent/query           \u2502\n\u2502                       \u2502 artifacts() \u2192  write/read refs (provenance)  \u2502\n\u2502                       \u2502 kv()        \u2192  small fast key\u2013value          \u2502\n\u2502                       \u2502 logger()    \u2192  structured logs               \u2502\n\u2502                       \u2502 services()  \u2192  external ctx (domain APIs)    \u2502\n\u2502                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502                                \u2502                                     \u2502\n\u2502                                \u25bc                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502               Sidecar / Adapters (inline server)               \u2502 \u2502\n\u2502  \u2502  - Console/CLI channel                                         \u2502 \u2502\n\u2502  \u2502  - Slack / PyQt / HTTP webhooks                                \u2502 \u2502\n\u2502  \u2502  - File/artifact endpoints (optional, later hosted)            \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concept/#legend-skim-first","title":"Legend (skim-first)","text":"<ul> <li><code>@graph_fn</code> (code\u2011native agents): Turn a plain async Python function into a node with a <code>NodeContext</code> injected.</li> <li>Tools (<code>@tool</code>): Small, explicit, reusable operations. Great for checkpoints, retries, and sharing across graphs.</li> <li>NodeContext: Where your node talks to the world: <code>channel()</code>, <code>memory()</code>, <code>artifacts()</code>, <code>kv()</code>, <code>logger()</code>, <code>services()</code>.</li> <li>Channel: Unifies human I/O (console/Slack/PyQt). Use <code>send_text</code>, <code>ask_text</code>, and progress APIs.</li> <li>Memory &amp; Artifacts: Event\u2011first memory with provenance; artifacts store files/results with stable refs.</li> <li>External Context (Services): Register domain services (e.g., job runner, materials DB) so nodes call them like built\u2011ins.</li> <li>Sidecar: Inline server that powers channels/adapters locally; later you can host these endpoints.</li> </ul> <p>Next: See Memory Internals below, then the Submit \u2192 Poll \u2192 Notify tutorial.</p>"},{"location":"concept/#memory-internals-diagram","title":"Memory Internals (diagram)","text":"<p>Goal: Show how event logging, persistence, indices, and optional RAG hang together.</p> <pre><code>              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502          memory().record(...)           \u2502\n              \u2502   kind \u2022 data \u2022 tags \u2022 entities \u2022 ...   \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                   (in\u2011process event stream / bus)\n                                  \u2502\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u25bc                                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  JSONL Persistence Log    \u2502  append\u2011only     \u2502   KV / Indices            \u2502\n\u2502  (provenance timeline)    \u2502  (durable)       \u2502   (fast lookup/filter)    \u2502\n\u2502  e.g., runs/YYYY/MM/*.jsonl\u2502                 \u2502   tags, kinds, entity ids \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                                 \u2502\n          \u2502                                                 \u2502\n          \u25bc                                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Derived Views / Cursors   \u2502  recent(...)     \u2502   Optional RAG Binding    \u2502\n\u2502 e.g., last_by_name,       \u2502  query(...)      \u2502   (vector index)          \u2502\n\u2502 latest_refs_by_kind       \u2502                  \u2502   embed(data/artifacts)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                                 \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba  NodeContext.memory().query(...) \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Record: Write small structured events with <code>kind</code>, <code>data</code>, <code>tags</code>, <code>entities</code>, <code>metrics</code>.</li> <li>Persist: Append to a JSONL log for durability &amp; replay; perfect for provenance.</li> <li>Index: Maintain fast KV/indices for quick filters (by kind/tags/entity/time).</li> <li>RAG (optional): Bind a vector index to selectively embed text/artifact content for semantic search.</li> <li>Query: Use <code>recent</code>, <code>query</code>, and helpers like <code>latest_refs_by_kind</code> to drive summaries &amp; reports.</li> </ul> <p>When to use RAG? When you want semantic retrieval over larger text blocks or artifact\u2011derived content; otherwise rely on indices and tags for speed/clarity.</p>"},{"location":"concept/#tutorial-submit-poll-notify-single-file-runnable","title":"Tutorial \u2014 Submit \u2192 Poll \u2192 Notify (single file, runnable)","text":"<p>Goal: Minimal end\u2011to\u2011end job orchestration with human notification and provenance. Keep it console\u2011only, no external infra.</p> <pre><code># examples/tutorial_submit_poll_notify.py\nfrom __future__ import annotations\nimport asyncio, random, time\nfrom typing import Dict, Optional\n\n# AetherGraph imports (adjust paths/names to your package layout)\nfrom aethergraph import graph_fn, NodeContext\nfrom aethergraph.server import start\nfrom aethergraph.v3.core.runtime.runtime_services import register_context_service\n\n# --- 1) Start the inline sidecar so channel/memory/artifacts work locally ---\nstart()  # prints a local URL; not required to save here\n\n# --- 2) A tiny external service: fake job runner (auto\u2011bound to NodeContext) ---\nclass FakeJobRunner:\n    \"\"\"Pretend to submit a remote job and poll until it finishes.\n    In a real impl, call your cloud API here.\n    \"\"\"\n    def __init__(self):\n        self._jobs: Dict[str, Dict[str, Optional[str]]] = {}\n\n    async def submit(self, spec: Dict) -&gt; str:\n        job_id = f\"job_{int(time.time()*1000)}_{random.randint(100,999)}\"\n        # status can be: queued \u2192 running \u2192 (succeeded | failed)\n        self._jobs[job_id] = {\"status\": \"queued\", \"result\": None}\n        # Background simulation\n        asyncio.create_task(self._simulate(job_id, spec))\n        return job_id\n\n    async def poll(self, job_id: str) -&gt; Dict[str, Optional[str]]:\n        return self._jobs[job_id]\n\n    async def _simulate(self, job_id: str, spec: Dict):\n        # Fake lifecycle with sleeps\n        await asyncio.sleep(0.5)\n        self._jobs[job_id][\"status\"] = \"running\"\n        await asyncio.sleep(1.2)\n        if random.random() &lt; 0.85:\n            self._jobs[job_id][\"status\"] = \"succeeded\"\n            self._jobs[job_id][\"result\"] = f\"Result for {spec.get('name','demo')}\"\n        else:\n            self._jobs[job_id][\"status\"] = \"failed\"\n            self._jobs[job_id][\"result\"] = None\n\n# Register the service under the name \"jobs\" and auto\u2011bind it to context as context.jobs()\nregister_context_service(\"jobs\", FakeJobRunner())\n\n# --- 3) The graph node: submit \u2192 poll \u2192 notify, with artifacts &amp; memory ---\n@graph_fn(name=\"submit_poll_notify\")\nasync def submit_poll_notify(spec: Dict, *, context: NodeContext) -&gt; Dict:\n    ch = context.channel()\n    mem = context.memory()\n    arts = context.artifacts()\n    jobs = context.jobs()  # auto\u2011bound external service\n\n    await ch.send_text(\"Submitting your job\u2026\")\n    job_id = await jobs.submit(spec)\n    await mem.record(kind=\"job_submitted\", data={\"job_id\": job_id, \"spec\": spec}, tags=[\"demo\"])\n\n    # Persist the spec as an artifact\n    spec_ref = await arts.write_text(f\"spec_{job_id}.json\", content=str(spec))\n\n    # Poll until terminal\n    while True:\n        info = await jobs.poll(job_id)\n        status = info.get(\"status\")\n        await ch.send_text(f\"Status: {status}\")\n        if status in {\"succeeded\", \"failed\"}:\n            break\n        await asyncio.sleep(0.6)\n\n    if status == \"succeeded\":\n        result_text = info.get(\"result\") or \"&lt;no result&gt;\"\n        res_ref = await arts.write_text(f\"result_{job_id}.txt\", content=result_text)\n        await mem.record(kind=\"job_succeeded\", data={\"job_id\": job_id, \"result_ref\": res_ref})\n        await ch.send_text(f\"\u2705 Job {job_id} finished. Saved result \u2192 {res_ref}\")\n        return {\"job_id\": job_id, \"status\": status, \"spec_ref\": spec_ref, \"result_ref\": res_ref}\n    else:\n        await mem.record(kind=\"job_failed\", data={\"job_id\": job_id})\n        ans = await ch.ask_text(f\"\u274c Job {job_id} failed. Retry? (yes/no)\")\n        if str(ans).strip().lower().startswith(\"y\"):\n            return await submit_poll_notify(spec=spec, context=context)\n        await ch.send_text(\"Not retrying; stopping here.\")\n        return {\"job_id\": job_id, \"status\": status, \"spec_ref\": spec_ref}\n\n# --- 4) Tiny runner for local testing ---\nif __name__ == \"__main__\":\n    async def main():\n        out = await submit_poll_notify(spec={\"name\": \"toy-sim\", \"steps\": 3})\n        print(\"FINAL OUTPUT:\\n\", out)\n    asyncio.run(main())\n</code></pre>"},{"location":"concept/#what-this-tutorial-demonstrates","title":"What this tutorial demonstrates","text":"<ul> <li>Channel I/O: human\u2011visible status + retry prompt.</li> <li>External Service: a domain API (<code>jobs</code>) registered once, used like a built\u2011in via <code>context.jobs()</code>.</li> <li>Memory: durable events (<code>job_submitted</code>, <code>job_succeeded</code>, <code>job_failed</code>).</li> <li>Artifacts &amp; provenance: spec/result written with stable refs; returned in the node output.</li> <li>Low friction: single file; console channel only; no extra infra.</li> </ul> <p>Next:</p> <ul> <li>Swap <code>FakeJobRunner</code> for your cloud client.</li> <li>Replace <code>ask_text</code> with an approval UI (Slack/PyQt) once you enable those adapters.</li> <li>Emit metrics in <code>mem.record(..., metrics={...})</code> and add a summary node to close the loop.</li> </ul>"},{"location":"external-context-services/","title":"External Context Services (Revised)","text":"<p>Make reusable, lifecycle\u2011aware helpers available as <code>context.&lt;name&gt;</code> inside any <code>@graph_fn</code>.</p> <p>This page explains what an external context service is, why you might use one, how it looks at a high level, and the APIs you\u2019ll use to define and register services. It also clarifies lifecycle behavior today vs. after you add a server/sidecar, and shows how services can access the active <code>NodeContext</code>.</p>"},{"location":"external-context-services/#1-what-is-an-external-context-service","title":"1) What is an external context service?","text":"<p>An external context service is a Python object managed by AetherGraph\u2019s runtime and exposed to your graph functions through the <code>NodeContext</code>. Once registered, you can access it as <code>context.svc(\"name\")</code> or simply <code>context.&lt;name&gt;</code>.</p> <p>Key ideas:</p> <ul> <li>Dependency injection: Centralize clients, caches, and policies in one place and inject them wherever needed.</li> <li>Lifecycle\u2011ready: Services can implement <code>start()</code> and <code>close()</code> for setup/teardown (e.g., open a pool, kick off a background task). Today these hooks are optional and not auto\u2011invoked unless you wire them (see \u00a74.1).</li> <li>Concurrency controls: Built\u2011in mutex and read/write helpers to safely share state across concurrent nodes.</li> <li>Per\u2011run binding: Each call is bound to a <code>NodeContext</code> so the service can access run_id, logger, artifacts, memory, etc.</li> <li>Uniform surface: The same service works in local scripts today and can be proxied or hosted later without changing call sites.</li> </ul> <p>Use services when logic benefits from a long\u2011lived instance, shared state, or orchestration\u2014not for tiny, pure functions (plain imports are fine there).</p>"},{"location":"external-context-services/#2-highlevel-usage-sketch","title":"2) High\u2011level usage sketch","text":"<p>Below is a conceptual outline (intentionally abstract) of how you would define and call a service.</p>"},{"location":"external-context-services/#define-highlevel","title":"Define (high\u2011level)","text":"<pre><code>class MyService(Service):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self._cache = {}\n\n    async def start(self):\n        # optional: warm up connections, threads, or caches\n        ...\n\n    async def close(self):\n        # optional: flush or close resources\n        ...\n\n    async def do_something(self, key: str) -&gt; str:\n        # example: consult cache, maybe call out to an API, return a value\n        ...\n</code></pre>"},{"location":"external-context-services/#register-at-app-startup","title":"Register (at app startup)","text":"<pre><code>register_context_service(\"myservice\", MyService(config={\"mode\": \"dev\"}))\n</code></pre>"},{"location":"external-context-services/#use-in-a-graph-function","title":"Use in a graph function","text":"<pre><code>@graph_fn(name=\"demo\")\nasync def demo(*, context: NodeContext):\n    value = await context.myservice.do_something(\"foo\")\n    return {\"value\": value}\n</code></pre> <p>That\u2019s it: once registered, your service is reachable from any node via <code>context</code>.</p>"},{"location":"external-context-services/#3-why-use-external-context-benefits-use-cases","title":"3) Why use external context? (Benefits + use cases)","text":""},{"location":"external-context-services/#benefits","title":"Benefits","text":"<ul> <li>Replaceable implementations: Swap local vs. remote, mock vs. real, dev vs. prod\u2014without editing call sites.</li> <li>Centralized auth &amp; config: Put tokens, endpoints, retry/timeout policy, telemetry in one place.</li> <li>Lifecycle &amp; performance: Reuse clients, connection pools, thread pools; warm caches once.</li> <li>Concurrency safety: Use the provided <code>critical()</code> mutex or <code>AsyncRWLock</code> to protect shared state.</li> <li>Per\u2011run awareness: Access <code>self.ctx()</code> to reach logger, artifacts, memory, continuations, etc.</li> <li>Future\u2011proof: The same surface can later be proxied (sidecar/hosted) while keeping your graph code unchanged.</li> </ul>"},{"location":"external-context-services/#itemized-scenarios-no-code","title":"Itemized scenarios (no code)","text":"<ul> <li>Model/Tool Clients: Wrap an LLM, embedding service, vector DB, or a simulation engine with retry, rate limit, and consistent API.</li> <li>Job Orchestration: Submit long\u2011running jobs to a queue/cluster and expose <code>submit/status/wait</code> for nodes.</li> <li>Caching/Indexing: Provide a shared in\u2011memory or on\u2011disk cache with strict read (R/W lock) semantics.</li> <li>Policy Enforcement: Centralize tenant limits, quotas, audit logging, and redaction.</li> <li>Data Access Facades: Read domain data (materials table, experiment registry) with local cache + background refresh.</li> <li>Adapters: Present a unified interface over heterogeneous backends (e.g., multiple vendor APIs behind one broker).</li> </ul>"},{"location":"external-context-services/#4-apis-defining-registering-and-binding-services","title":"4) APIs: defining, registering, and binding services","text":"<p>AetherGraph provides small primitives for service registration and a base class with helpful utilities.</p>"},{"location":"external-context-services/#41-lifecycle-today-vs-serversidecar","title":"4.1 Lifecycle (today vs. server/sidecar)","text":"<ul> <li>Today (no server yet): <code>start()</code>/<code>close()</code> exist but are not auto\u2011invoked. You can omit them or leave them as no\u2011ops.</li> <li>When you add a server/sidecar: wire lifecycle once at boot/shutdown (pseudo\u2011code):</li> </ul> <pre><code># After install_services(...) and registrations\nawait start_all_services()\n# ... run your app/sidecar ...\nawait close_all_services()\n</code></pre> <p>Until those hooks are added, services work fine without lifecycle calls.</p>"},{"location":"external-context-services/#42-registry-functions-runtimelevel","title":"4.2 Registry functions (runtime\u2011level)","text":"<ul> <li><code>install_services(container)</code> \u2013 Set the process\u2011wide service container at startup.</li> <li><code>ensure_services_installed(factory)</code> \u2013 Lazily create/install the container if missing.</li> <li><code>register_context_service(name, instance)</code> \u2013 Add a concrete service instance under <code>name</code>.</li> <li><code>get_context_service(name)</code> \u2013 Retrieve a registered instance.</li> <li><code>list_context_services()</code> \u2013 List the names currently registered.</li> </ul>"},{"location":"external-context-services/#43-base-class-service-aka-basecontextservice","title":"4.3 Base class: <code>Service</code> (aka <code>BaseContextService</code>)","text":"<p>The base class gives you batteries\u2011included ergonomics:</p> <ul> <li> <p>Lifecycle</p> </li> <li> <p><code>async def start(self) -&gt; None</code> \u2013 Optional setup hook.</p> </li> <li> <p><code>async def close(self) -&gt; None</code> \u2013 Optional teardown hook.</p> </li> <li> <p>Binding</p> </li> <li> <p><code>def bind(self, *, context: NodeContext) -&gt; Service</code> \u2013 Called by the runtime so <code>self.ctx()</code> works.</p> </li> <li> <p><code>def ctx(self) -&gt; NodeContext</code> \u2013 Access the current node context (logger, memory, artifacts, etc.).</p> </li> <li> <p>Concurrency</p> </li> <li> <p><code>self._lock</code> \u2013 An async mutex available for your own critical sections.</p> </li> <li><code>def critical()(fn)</code> \u2013 Decorator that serializes an async method (easy mutual exclusion).</li> <li> <p><code>class AsyncRWLock</code> \u2013 Many\u2011readers/one\u2011writer lock for shared tables and caches.</p> </li> <li> <p>Offloading</p> </li> <li> <p><code>async def run_blocking(self, fn, *a, **kw)</code> \u2013 Run CPU or blocking I/O on a worker thread (keeps the event loop responsive).</p> </li> </ul>"},{"location":"external-context-services/#44-accessing-services-from-nodes","title":"4.4 Accessing services from nodes","text":"<ul> <li>Dynamic attribute: <code>context.&lt;name&gt;</code> resolves to the registered service (e.g., <code>context.myservice</code>).</li> <li>Explicit lookup: <code>context.svc(\"name\")</code> (equivalent to the dynamic attribute).</li> </ul>"},{"location":"external-context-services/#45-accessing-nodecontext-from-inside-a-service-essential","title":"4.5 Accessing <code>NodeContext</code> from inside a service (essential)","text":"<p>Services frequently need run\u2011scoped utilities (logger, memory, artifacts, kv, llm, rag, etc.). Enable per\u2011call binding so <code>self.ctx()</code> returns the right <code>NodeContext</code>.</p> <p>Use <code>self.ctx()</code> in the service:</p> <pre><code>class MyService(Service):\n    async def do_work(self, x: int) -&gt; int:\n        ctx = self.ctx()  # NodeContext bound for this call\n        ctx.logger().info(\"working\", extra={\"x\": x})\n        await ctx.memory().record(kind=\"note\", data={\"x\": x})\n        uri = ctx.artifacts().put_text(\"result.txt\", f\"value={x}\")\n        return x + 1\n</code></pre>"},{"location":"external-context-services/#46-event-loop-locking-model","title":"4.6 Event loop &amp; locking model","text":"<ul> <li>External services run on the main event loop used by the executing node.</li> <li>Locks (<code>_lock</code>, <code>AsyncRWLock</code>) coordinate on that loop; use <code>run_blocking()</code> for CPU/IO work.</li> </ul>"},{"location":"external-context-services/#5-summary","title":"5) Summary","text":"<p>External context services provide a clean way to share long\u2011lived capabilities across nodes while keeping graph code small and portable:</p> <ul> <li>Inject reusable helpers via <code>context.&lt;name&gt;</code> (or <code>context.svc(name)</code>).</li> <li>Manage concurrency and performance in one place; offload blocking work with <code>run_blocking()</code>.</li> <li>Abstract environments (mock/local/dev/prod) without touching business logic.</li> <li>Bind to <code>NodeContext</code> automatically so services can use logger, memory, artifacts, kv, llm/rag, etc.</li> <li>Lifecycle now vs later: Today you can skip <code>start()</code>/<code>close()</code>; add startup/shutdown hooks when you introduce a server/sidecar.</li> </ul> <p>Use services for shared state, orchestration, specialized clients, or cross\u2011cutting policies. Use plain imports for tiny, stateless helpers.</p>"},{"location":"graph_fn/","title":"Graph Function <code>graph_fn</code> Quickstart &amp; Reference","text":"<p>Make any Python async function a runnable, inspectable Graph Function with a single decorator. You keep normal Python control\u2011flow; AetherGraph wires in runtime services via <code>context</code> and exposes your outputs as graph boundaries.</p>"},{"location":"graph_fn/#tldr","title":"TL;DR","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}! \ud83d\udc4b\")\n    return {\"greeting\": f\"Hello, {name}\"}\n\n# Run (async)\nres = await hello(name=\"Aether\")          # \u2192 {\"greeting\": \"Hello, Aether\"}\n\n# Or run (sync) for quick scripts\nout = hello.sync(name=\"Aether\")            # same result\n</code></pre>"},{"location":"graph_fn/#what-is-a-graph-function","title":"What is a Graph Function?","text":"<p>A Graph Function is a small wrapper around your Python function that:</p> <ul> <li> <p>builds a fresh internal TaskGraph,</p> </li> <li> <p>injects a <code>NodeContext</code> if your function declares <code>*, context</code>,</p> </li> <li> <p>executes your function (awaiting if needed),</p> </li> <li> <p>normalizes the return value into named outputs, and</p> </li> <li> <p>records graph boundary outputs for downstream composition/inspection.</p> </li> </ul> <p>You do not need to learn a new DSL. Write Python; use <code>context.&lt;service&gt;()</code> when you need IO/state.</p>"},{"location":"graph_fn/#decorator-signature","title":"Decorator signature","text":"<pre><code>@graph_fn(\n    name: str,\n    inputs: list[str] | None = None,\n    outputs: list[str] | None = None,\n    version: str = \"0.1.0\",\n    agent: str | None = None,  # optional: also register as an agent name\n)\n</code></pre> <p>Required</p> <ul> <li>name (str) \u2013 Unique identifier for this graph function.</li> </ul> <p>Optional</p> <ul> <li> <p>inputs (list[str]) \u2013 Declares input names for docs/registry (not enforced at call time).</p> </li> <li> <p>outputs (list[str]) \u2013 Declares output names/order; enables single\u2011literal returns.</p> </li> <li> <p>version (str) \u2013 Semantic version for registry/discovery.</p> </li> <li> <p>agent (str) \u2013 Also register in the <code>agent</code> namespace (advanced).</p> </li> </ul>"},{"location":"graph_fn/#function-shape","title":"Function shape","text":"<p><pre><code>@graph_fn(name=\"example\", inputs=[\"x\"], outputs=[\"y\"])\nasync def example(x: int, *, context):\n    # use services via context: channel/memory/artifacts/kv/llm/rag/mcp/logger\n    await context.channel().send_text(f\"x={x}\")\n    return {\"y\": x + 1}\n</code></pre> - Positional/keyword parameters are your API.</p> <ul> <li>Include <code>*, context</code> to receive the <code>NodeContext</code>. If you don\u2019t declare it, nothing is injected.</li> </ul>"},{"location":"graph_fn/#returning-values-normalization-rules","title":"Returning values (normalization rules)","text":"<p>Your return can be:</p> <p>1) Dict of outputs (recommended) <pre><code>return {\"result\": 42, \"note\": \"ok\"}\n</code></pre></p> <p>2) Single literal \u2014 only if you declared exactly one output <pre><code>@graph_fn(name=\"one\", outputs=[\"y\"])\nasync def one(*, context):\n    return 123  # normalized to {\"y\": 123}\n</code></pre></p> <p>3) NodeHandle / Refs (advanced) If you return node handles or refs created by graph utilities, they\u2019re exposed as boundary outputs automatically. For most users, plain dicts/literals are enough.</p> <p>Validation - If <code>outputs</code> are declared, missing keys raise: <code>ValueError(\"Missing declared outputs: ...\")</code>. - Returning a single literal without exactly one declared output raises an error.</p>"},{"location":"graph_fn/#running","title":"Running","text":"<p><pre><code># Async (preferred in apps/servers)\nres = await my_fn(a=1, b=2)\n\n# Sync helper (scripts/CLI/tests)\nout = my_fn.sync(a=1, b=2)\n</code></pre> Internally this builds a fresh runtime environment, constructs a TaskGraph, executes your function in an interpreter, and returns the normalized outputs.</p>"},{"location":"graph_fn/#accessing-context","title":"Accessing Context","text":"<p>Declare <code>*, context</code> to use built\u2011ins: <pre><code>@graph_fn(name=\"report\", outputs=[\"uri\"])\nasync def report(data: dict, *, context):\n    # Log breadcrumbs\n    log = context.logger(); log.info(\"building report\")\n\n    # Save an artifact\n    art = await context.artifacts().save(path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\":\"A\"})\n\n    # Record a typed result in memory\n    await context.memory().write_result(topic=\"report\", outputs=[{\"name\":\"uri\",\"kind\":\"uri\",\"value\": art.uri}])\n\n    # Notify user\n    await context.channel().send_text(f\"Report ready: {art.uri}\")\n    return {\"uri\": art.uri}\n</code></pre></p>"},{"location":"graph_fn/#concurrency-retry-advanced","title":"Concurrency &amp; retry (advanced)","text":"<p><code>GraphFunction.run()</code> accepts knobs used by the interpreter/runtime: <pre><code>await my_fn.run(\n    env=None,                            # supply a prebuilt RuntimeEnv, or let the runner build one\n    retry=RetryPolicy(),                 # backoff/retries for node execution\n    max_concurrency: int | None = None,  # cap parallelism inside the interpreter\n    **inputs,\n)\n</code></pre> For most users, calling <code>await my_fn(...)</code> / <code>.sync(...)</code> is sufficient; the runner chooses sensible defaults.</p>"},{"location":"graph_fn/#minimal-patterns","title":"Minimal patterns","text":"<p>Hello + context <pre><code>@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}\")\n    return {\"greeting\": f\"Hello, {name}\"}\n</code></pre></p> <p>One output (literal) <pre><code>@graph_fn(name=\"square\", outputs=[\"y\"])\nasync def square(x: int, *, context):\n    return x * x\n</code></pre></p> <p>Multi\u2011output dict <pre><code>@graph_fn(name=\"stats\", outputs=[\"mean\",\"std\"])\nasync def stats(xs: list[float], *, context):\n    import statistics as st\n    return {\"mean\": st.mean(xs), \"std\": st.pstdev(xs)}\n</code></pre></p>"},{"location":"graph_fn/#tips-gotchas","title":"Tips &amp; gotchas","text":"<ul> <li>Always include <code>*, context</code> when you need services (channel/memory/llm/etc.).</li> <li>Declare <code>outputs=[...]</code> if you want to return a single literal; otherwise return a dict.</li> <li>Output validation is strict when <code>outputs</code> are declared\u2014return all of them.</li> <li><code>inputs=[...]</code> is for documentation/registry; your Python signature is the source of truth at call time.</li> <li>You can also register the function as an agent by passing <code>agent=\"name\"</code> (covered later).</li> </ul>"},{"location":"graph_fn/#next-steps","title":"Next steps","text":"<ul> <li><code>graphify</code>: combine multiple functions into a larger graph with explicit edges.</li> <li><code>@tool</code>: publish functions as reusable nodes (IO typed), then orchestrate with <code>graphify</code>.</li> <li>Context services: <code>channel</code>, <code>artifacts</code>, <code>memory</code>, <code>kv</code>, <code>llm</code>, <code>rag</code>, `m</li> </ul>"},{"location":"graphify/","title":"AetherGraph \u2014 <code>@graphify</code> (Builder Decorator)","text":"<p><code>@graphify</code> lets you write a plain Python function whose body builds a <code>TaskGraph</code> using tool calls. Instead of executing immediately, the function becomes a graph factory: call <code>.build()</code> to get a concrete graph, <code>.spec()</code> to inspect, and <code>.io()</code> to see its input/output signature.</p>"},{"location":"graphify/#why-graphify-vs-graph_fn","title":"Why <code>graphify</code> vs <code>graph_fn</code>?","text":"Aspect <code>graph_fn</code> <code>graphify</code> Primary purpose Execute now as a single graph node Build a graph (explicit fan\u2011in/fan\u2011out wiring) Return at call Dict of outputs (or awaitable) A builder you later <code>.build()</code> into a graph Control\u2011flow Pythonic, implicit graph behind the scenes Explicit nodes &amp; edges via tool calls (<code>NodeHandle</code>) Best for Orchestration + <code>context.*</code> services Pipelines, DAGs, reusable subgraphs <p>Use <code>graphify</code> when you want:</p> <ul> <li>Multiple tool calls as separate nodes</li> <li>Explicit dependencies (<code>_after</code>) and fan\u2011in/fan\u2011out</li> <li>To inspect/serialize the graph spec for registry/UI</li> <li>To reuse the same pipeline with different inputs</li> </ul> <p>Use <code>graph_fn</code> when you want:</p> <ul> <li>A simple function that runs immediately and returns values</li> <li>Access to <code>context.channel()/memory()/artifacts()/llm()</code> services</li> <li>Minimal ceremony (one decorator and go)</li> </ul>"},{"location":"graphify/#decorator-signature","title":"Decorator Signature","text":"<pre><code>from aethergraph import graphify\n\n@graphify(*, name=\"default_graph\", inputs=(), outputs=None, version=\"0.1.0\", agent: str | None = None)\ndef build_fn(...):\n    ...  # tool calls returning NodeHandles\n    return {\"y\": handle.y}\n</code></pre> <p>Parameters</p> <ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (Iterable[str] or dict) \u2014 Declare required/optional inputs.  </li> <li>If <code>list/tuple</code>: treated as required input names.  </li> <li>If <code>dict</code>: <code>{required_name: ..., ...}</code> for optional mapping; builder will declare required/optional accordingly.</li> <li>outputs (list[str] | None) \u2014 Names to expose. If you return a single literal, you must declare exactly one.</li> <li>version (str) \u2014 Semantic version for registry/spec metadata.</li> <li>agent (str | None) \u2014 Optionally register the built graph under <code>agent</code> namespace.</li> </ul> <p>Return value</p> <p>The decorated symbol becomes a builder function with helpers:</p> <ul> <li><code>.build() -&gt; TaskGraph</code></li> <li><code>.spec() -&gt; GraphSpec</code></li> <li><code>.io() -&gt; IOSignature</code></li> <li>Attributes: <code>.graph_name</code>, <code>.version</code></li> </ul>"},{"location":"graphify/#writing-a-graphify-body","title":"Writing a <code>@graphify</code> Body","text":"<p>Inside the function:</p> <ol> <li>Use <code>arg(\"name\")</code> to reference declared inputs.</li> <li>Call <code>@tool</code> functions (or <code>call_tool(\"pkg.mod:fn\", ...)</code>) \u2014 each returns a <code>NodeHandle</code> in build mode.</li> <li>Return outputs as:</li> <li>A dict mapping names \u2192 <code>NodeHandle</code> outputs or refs/literals, or</li> <li>A single <code>NodeHandle</code> (its outputs will be exposed), or</li> <li>A single literal only if <code>outputs</code> has length 1.</li> </ol> <pre><code>from aethergraph import graphify, tool\nfrom aethergraph.graph import arg\n\n@tool(outputs=[\"embedded\"])\ndef embed(text: str): ...\n\n@tool(outputs=[\"score\"])\ndef score(vec, query_vec): ...\n\n@graphify(name=\"ranker\", inputs=[\"texts\",\"query\"], outputs=[\"scores\"])\ndef ranker(texts, query):\n    q = embed(text=query)\n    # fan\u2011out: call `embed` for each text\n    vecs = [embed(text=t) for t in texts]  # list[NodeHandle]\n    # fan\u2011in: score each against query vec\n    scs = [score(vec=v.embedded, query_vec=q.embedded) for v in vecs]\n    return {\"scores\": [s.score for s in scs]}\n\nG = ranker.build()\n</code></pre>"},{"location":"graphify/#control-dependencies-without-data-edges","title":"Control Dependencies without Data Edges","text":"<p>Use <code>_after</code> when you must enforce order but don\u2019t pass outputs: <pre><code>@tool(outputs=[\"ok\"])\ndef fetch(): return {\"ok\": True}\n\n@tool(outputs=[\"done\"])\ndef train(): return {\"done\": True}\n\n@graphify(name=\"seq\", inputs=[], outputs=[\"done\"])\ndef seq():\n    a = fetch()\n    b = train(_after=a)   # run b after a\n    return {\"done\": b.done}\n</code></pre></p>"},{"location":"graphify/#registration","title":"Registration","text":"<p>If a registry is active, <code>@graphify</code> registers the built graph under <code>nspace=\"graph\"</code> with <code>name</code>/<code>version</code> so it can be listed or launched elsewhere. You can also register it as an <code>agent</code> via the <code>agent=</code> parameter.</p>"},{"location":"graphify/#example-endtoend-pipeline","title":"Example: End\u2011to\u2011End Pipeline","text":"<pre><code>from aethergraph import tool, graphify\nfrom aethergraph.graph import arg\n\n@tool(outputs=[\"rows\"])\ndef load_csv(path: str): ...\n\n@tool(outputs=[\"clean\"])\ndef clean(rows): ...\n\n@tool(outputs=[\"model\"])\ndef train(data): ...\n\n@tool(outputs=[\"uri\"])\ndef report(model): ...\n\n@graphify(name=\"etl_train_report\", inputs=[\"csv_path\"], outputs=[\"uri\"])\ndef etl_train_report(csv_path):\n    raw  = load_csv(path=arg(\"csv_path\"))\n    tidy = clean(rows=raw.rows)\n    mod  = train(data=tidy.clean)\n    rep  = report(model=mod.model)\n    return {\"uri\": rep.uri}\n\nG = etl_train_report.build()\n</code></pre>"},{"location":"graphify/#using-tool-inside-graph_fn-brief","title":"Using <code>@tool</code> Inside <code>@graph_fn</code> (Brief)","text":"<p>While <code>@graph_fn</code> is for immediate execution, you can drop explicit tool nodes inside a <code>graph_fn</code> when you want finer\u2011grained tracing or parallelism:</p> <pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int): return {\"y\": x*x}\n\n@graph_fn(name=\"mix\")\nasync def mix(x: int, *, context):\n    h = square(x=x)                 # schedules a tool node in the implicit graph\n    await context.channel().send_text(\"running square\u2026\")\n    return {\"y\": h.y}               # exposes tool output as graph_fn output\n</code></pre> <p>Prefer <code>@graphify</code> for full pipeline construction; use <code>@graph_fn</code> when you want to orchestrate services (<code>context.*</code>) and run quickly.</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>A 5\u2011minute on\u2011ramp to AetherGraph: install, start the sidecar server, and run your first <code>@graph_fn</code>.</p>"},{"location":"quickstart/#1-install","title":"1. Install","text":"<pre><code>pip install aethergraph\n# or, from source\n# pip install -e .\n</code></pre> <p>Python: 3.10+</p>"},{"location":"quickstart/#2-start-the-sidecar-server-oneliner","title":"2. Start the sidecar server (one\u2011liner)","text":"<p>AetherGraph ships a lightweight sidecar that wires up core services (logger, artifacts, memory, KV, channels, etc.)</p> <pre><code># quickstart_server.py\nfrom aethergraph import start_server\n\nurl = start_server(port=0)\nprint(\"AetherGraph server:\", url)\n</code></pre> <p>Run it:</p> <pre><code>python quickstart_server.py\n</code></pre> <p>You should see an HTTP URL like <code>http://127.0.0.1:54321</code> printed. (Random free port by default.)</p>"},{"location":"quickstart/#3-your-first-graph-function","title":"3. Your first graph function","text":"<p><code>@graph_fn</code> turns an ordinary async Python function into a runnable graph entrypoint. If you include a <code>context</code> parameter, you get access to built\u2011in services like <code>context.channel()</code> and <code>context.memory()</code>.</p> <pre><code># quickstart_graph_fn.py\nfrom aethergraph import graph_fn, NodeContext\nfrom aethergraph import start_server\n\n# 1) Start the sidecar so services are available\nstart_server()\n\n# 2) Define a small graph function\n@graph_fn(name=\"hello.world\", inputs=[\"name\"], outputs=[\"greeting\"], version=\"0.1.0\")\nasync def hello_world(name: str, *, context: NodeContext):\n    # Use the channel to send a message (console by default)\n    await context.channel().send_text(f\"\ud83d\udc4b Hello, {name}! Running graph\u2026\")\n\n    # Do any Python you want here \u2014 call tools, query memory, etc.\n    greeting = f\"Hello, {name}. Nice to meet you from AetherGraph.\"\n\n    # Return outputs as a dict (keys must match `outputs=[...]`)\n    return {\"greeting\": greeting}\n\n# 3) Run it (async wrapper provided)\nif __name__ == \"__main__\":\n    import asyncio\n    async def main():\n        res = await hello_world(name=\"Researcher\")\n        print(\"Result:\", res)\n    asyncio.run(main())\n</code></pre> <p>Run it:</p> <pre><code>python quickstart_graph_fn.py\n</code></pre> <p>You should see a console message from the channel and printed output like:</p> <pre><code>Result: {\"greeting\": \"Hello, Researcher. Nice to meet you from AetherGraph.\"}\n</code></pre>"},{"location":"quickstart/#4-what-just-happened","title":"4. What just happened?","text":"<ul> <li>Sidecar server booted in the background and installed default services (channels, artifacts, memory, KV, logger).</li> <li><code>@graph_fn</code> built a tiny task graph from your function and executed it.</li> <li><code>context.channel()</code> used the default channel (console) to emit a message.</li> </ul> <p>Tip: You can override the channel at call\u2011site with <code>context.channel(\"slack:#research\")</code>, once you\u2019ve configured a Slack adapter.</p>"},{"location":"quickstart/#5-next-steps","title":"5. Next steps","text":"<ul> <li>Add tools with <code>@tool</code> to wrap reusable steps and surface inputs/outputs.</li> <li>Use <code>@graphify</code> for fan\u2011in / fan\u2011out graph construction when the body is mostly tool calls.</li> <li>Explore artifacts (<code>context.artifacts()</code>), memory (<code>context.memory()</code>), and RAG (<code>context.rag()</code>)</li> <li>Config <code>.env</code> file to integrate external channel and llm features</li> </ul>"},{"location":"server/","title":"AetherGraph \u2014 Server (Sidecar) Overview","text":"<p>The AetherGraph server is a lightweight sidecar that wires up all runtime services (channels, memory, artifacts, KV, LLM, RAG, MCP, logging, etc.) and exposes a small HTTP/WebSocket surface for adapters and tools. You can run AetherGraph without the server, but the sidecar makes it easy to:</p> <ul> <li>Use GUI/chat adapters (Slack/Telegram/Console UI) that push events back to your runs</li> <li>Host continuation callbacks for <code>ask_text()</code> / <code>ask_approval()</code></li> <li>Centralize service wiring (secrets, paths, corpora, registries)</li> <li>Inspect/trace runs, artifacts, and health in one place</li> </ul> <p>Think of it as your local control plane so your graph functions can stay plain Python.</p>"},{"location":"server/#quick-start","title":"Quick Start","text":"<pre><code>from aethergraph.server import start, stop\n\n# 1) Start the sidecar (in a background thread) and get its base URL\nurl = start(host=\"127.0.0.1\", port=0)   # port=0 \u2192 auto-pick a free port\nprint(\"AetherGraph sidecar:\", url)\n\n# 2) Run your graph functions as usual\nfrom aethergraph import graph_fn\n\n@graph_fn(name=\"hello\")\nasync def hello(name: str, *, context):\n    await context.channel().send_text(f\"Hi {name}\")\n    return {\"greeting\": f\"Hello, {name}\"}\n\n# ... elsewhere ...\n# res = await hello(name=\"ZC\")\n\n# 3) (Optional) Stop when done (tests/CLI)\nstop()\n</code></pre>"},{"location":"server/#api-start-start_async-stop","title":"API \u2014 <code>start()</code> / <code>start_async()</code> / <code>stop()</code>","text":""},{"location":"server/#start","title":"start","text":"<p><pre><code>start(*, workspace: str = \"./aeg_workspace\", session_id: str | None = None,\n      host: str = \"127.0.0.1\", port: int = 0, log_level: str = \"warning\") -&gt; str\n</code></pre> Start the sidecar in a background thread. Safe to call at the top of scripts or notebook cells.</p> <p>Parameters</p> <ul> <li> <p>workspace (str) \u2013 Root directory for runtime state (artifacts, logs, corpora, temp files). Auto\u2011created.</p> </li> <li> <p>session_id (str, optional) \u2013 Override the logical session. If <code>None</code>, the runtime will create one.</p> </li> <li> <p>host (str) \u2013 Bind address (defaults to loopback).</p> </li> <li> <p>port (int) \u2013 <code>0</code> picks a free port automatically; otherwise bind an explicit port.</p> </li> <li> <p>log_level (str) \u2013 Uvicorn log level (e.g., <code>\"info\"</code>, <code>\"warning\"</code>).</p> </li> </ul> <p>Returns str \u2013 Base URL, e.g., <code>\"http://127.0.0.1:54321\"</code>.</p>"},{"location":"server/#start_async","title":"start_async","text":"<p><pre><code>start_async(**kwargs) -&gt; str\n</code></pre> Async\u2011friendly wrapper that still runs the server in a thread to avoid clashing with your event loop.</p>"},{"location":"server/#stop","title":"stop","text":"<p><pre><code>stop() -&gt; None\n</code></pre> Signal the background server to shut down and join its thread (useful in tests/CI or ephemeral scripts).</p>"},{"location":"server/#why-a-sidecar","title":"Why a sidecar?","text":"<ul> <li>Continuations: <code>context.channel().ask_*</code> creates a continuation token and waits for a resume callback; the server receives user replies (Slack/Telegram/HTTP) and wakes your run.</li> <li>Adapters: chat/file/progress adapters connect over HTTP/WS to publish events (<code>agent.message</code>, <code>agent.progress.*</code>, uploads) into your run.</li> <li>Central config: one place to load settings, secrets, workspace paths, and register services (LLM, RAG, MCP, artifact store, memory backends).</li> <li>Inspection: optional health and tracing endpoints (depending on your app factory) to debug runs locally.</li> </ul>"},{"location":"server/#what-start-actually-does","title":"What <code>start()</code> actually does","text":"<ol> <li>Loads app settings (<code>load_settings()</code>), installs them as current (<code>set_current_settings(...)</code>).</li> <li>Builds a FastAPI app via <code>create_app(workspace=..., cfg=...)</code> \u2014 this registers services and routes.</li> <li>Picks a free port if <code>port=0</code> and launches Uvicorn in a background thread (non\u2011blocking).</li> <li>Returns the base URL so other components (e.g., WS/HTTP MCP clients) can connect.</li> </ol>"},{"location":"server/#typical-usage-patterns","title":"Typical usage patterns","text":""},{"location":"server/#notebooks-quick-scripts","title":"Notebooks &amp; quick scripts","text":"<pre><code>url = start(port=0)\n# \u2026 run several cells that use context.channel()/continuations\n# restart kernel or call stop() when done\n</code></pre>"},{"location":"server/#longrunning-dev-server","title":"Long\u2011running dev server","text":"<ul> <li>Call <code>start(host=\"0.0.0.0\", port=8787, log_level=\"info\")</code> once at process start.</li> <li>Point Slack/Telegram adapters or local tools at <code>http://localhost:8787</code>.</li> </ul>"},{"location":"server/#testsci","title":"Tests/CI","text":"<pre><code>url = start(port=0)\ntry:\n    # run test suite that uses continuations/artifacts\n    ...\nfinally:\n    stop()\n</code></pre>"},{"location":"server/#interop-with-context-services","title":"Interop with context services","text":"<p>Once the sidecar is up, graph functions can rely on bound services:</p> <ul> <li> <p><code>context.channel()</code> \u2013 routes via the server to your chat adapters</p> </li> <li> <p><code>context.artifacts()</code> \u2013 saves to the workspace CAS under the sidecar</p> </li> <li> <p><code>context.memory()</code> \u2013 hotlog/persistence live alongside the server\u2019s config</p> </li> <li> <p><code>context.rag()</code> \u2013 corpora root under workspace; embedders/indices wired here</p> </li> <li> <p><code>context.mcp(...)</code> \u2013 WS/HTTP MCP clients often target sidecar endpoints</p> </li> </ul>"},{"location":"server/#security-notes","title":"Security notes","text":"<ul> <li>Default bind is <code>127.0.0.1</code> (local only). Use <code>0.0.0.0</code> only in trusted networks.</li> <li>Protect WS/HTTP endpoints behind auth headers/tokens if exposing beyond localhost.</li> <li>Never log plaintext API keys; prefer a Secrets store.</li> </ul>"},{"location":"server/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Port already in use: pass <code>port=0</code> or another free port.</li> <li>Nothing happens after ask_text(): ensure the chat adapter posts replies to the sidecar (correct base URL / token).</li> <li>No LLM/kv/rag configured: your <code>create_app()</code> must wire these services (or the accessors will raise \"\u2026 not available\").</li> <li>Jupyter hangs on restart: call <code>stop()</code> before restarting the kernel, or rely on kernel shutdown to terminate the thread.</li> </ul>"},{"location":"server/#minimal-adapter-sketch-optional","title":"Minimal adapter sketch (optional)","text":"<pre><code># Example: WebSocket adapter connecting to sidecar URL\na_sync_ws_client.connect(f\"{url.replace('http','ws')}/events\", headers={\"Authorization\": \"Bearer demo\"})\n# publish OutEvent / listen for Continuation notifications\n</code></pre>"},{"location":"server/#summary","title":"Summary","text":"<p>Run the sidecar server to centralize runtime services, handle continuations/adapters, and keep your graph functions clean. Use <code>start()</code> to launch in\u2011process, <code>start_async()</code> in async apps, and <code>stop()</code> for tests/CI. Configure paths and services once; build everything else in plain Python.</p>"},{"location":"tools/","title":"AetherGraph \u2014 <code>@tool</code> Decorator (Reference &amp; How\u2011to)","text":"<p><code>@tool</code> turns a plain Python function into a tool node that can be executed immediately or added to a graph during build time. You write ordinary Python, declare outputs, and AetherGraph handles result normalization and graph node creation.</p>"},{"location":"tools/#what-is-a-tool","title":"What is a Tool?","text":"<p>A tool is a reusable, IO\u2011typed operation that can be executed on its own or orchestrated inside a graph. Tools are perfect for things like \u201cload CSV\u201d, \u201ctrain model\u201d, \u201cplot chart\u201d, \u201csend_slack\u201d, etc.</p> <ul> <li>Immediate mode (no graph builder active): calling the tool runs the Python function right away and returns a dict of outputs.</li> <li>Graph mode (inside a <code>with graph(...):</code> block or a <code>@graphify</code> body): calling the tool adds a node to the graph and returns a <code>NodeHandle</code> you can wire to other nodes (fan\u2011in/fan\u2011out).</li> <li>Tools automatically register in the runtime registry (<code>nspace=\"tool\"</code>) when a registry is active.</li> </ul> <p>This page covers the simple function form. (The advanced waitable class form is documented separately.)</p>"},{"location":"tools/#decorator-signature","title":"Decorator Signature","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs: list[str], *, inputs: list[str] | None = None,\n      name: str | None = None, version: str = \"0.1.0\")\ndef your_function(...): ...\n</code></pre> <p>Parameters</p> <ul> <li>outputs (list[str], required) \u2014 Declares the output keys your tool will produce.</li> <li>inputs (list[str], optional) \u2014 Explicit input names. Omit to infer from function signature (excluding <code>*args</code>/<code>**kwargs</code>).  </li> <li>name (str, optional) \u2014 Registry/display name. Defaults to the function\u2019s <code>__name__</code>.  </li> <li>version (str, optional) \u2014 Semantic version recorded in the registry (default: <code>\"0.1.0\"</code>).</li> </ul> <p>Return value (call\u2011site dependent)</p> <ul> <li>Immediate mode: returns a <code>dict</code> of outputs.  </li> <li>Graph mode: returns a <code>NodeHandle</code> (or an awaitable handle under an interpreter) to be wired/exposed by the builder.</li> </ul>"},{"location":"tools/#return-normalization","title":"Return Normalization","text":"<p>The wrapped function can return different shapes; the decorator normalizes into a dict that must include every declared output:</p> <ul> <li><code>None</code> \u2192 <code>{}</code></li> <li><code>dict</code> \u2192 used as\u2011is</li> <li><code>tuple</code> \u2192 <code>{\"out0\": v0, \"out1\": v1, ...}</code></li> <li>single value \u2192 <code>{\"result\": value}</code></li> </ul> <p>If any declared <code>outputs</code> are missing from the normalized dict, a <code>ValueError</code> is raised.</p>"},{"location":"tools/#control-keywords-graph-mode","title":"Control Keywords (graph mode)","text":"<p>When calling a tool while building a graph (e.g., inside a <code>with graph(...):</code> or <code>@graphify</code> body), you may pass these special kwargs to influence scheduling/metadata:</p> <ul> <li><code>_after</code> (NodeHandle | list[NodeHandle | node_id]): explicit dependency edges (fan\u2011in).  </li> <li><code>_name</code> (str): display name for UI/spec.  </li> <li><code>_id</code> (str): hard override of the node ID (must be unique in the graph).  </li> <li><code>_alias</code> (str): optional alias for reverse lookups.  </li> <li><code>_labels</code> (Iterable[str]): lightweight tags for search/grouping.</li> </ul> <p>Example:</p> <pre><code>res = my_tool(a=arg_a, b=arg_b, _after=[prev1, prev2], _name=\"preprocess\", _labels=[\"data\",\"prep\"])\n</code></pre> <p>These control keys are stripped before calling your function and only affect graph construction.</p>"},{"location":"tools/#simple-examples","title":"Simple Examples","text":""},{"location":"tools/#1-immediate-execution-no-graph-builder-active","title":"1) Immediate execution (no graph builder active)","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"sum\", \"mean\"])  # outputs you promise to return\ndef stats(xs: list[float]):\n    s = sum(xs)\n    return {\"sum\": s, \"mean\": s / len(xs)}\n\nout = stats([1,2,3,4])   # \u2192 {\"sum\": 10, \"mean\": 2.5}\n</code></pre>"},{"location":"tools/#2-graph-construction-inside-a-builder","title":"2) Graph construction (inside a builder)","text":"<pre><code>from aethergraph import tool\nfrom aethergraph import graphify\nfrom aethergraph.graph import arg  # or from aethergraph.graph.graph_refs import arg\n\n@tool(outputs=[\"y\"])\ndef add(x: int, z: int): return {\"y\": x + z}\n\n@tool(outputs=[\"z\"])\ndef mul(x: int, k: int): return {\"z\": x * k}\n\n@graphify(name=\"pipeline\", inputs=[\"x\"], outputs=[\"y\"])\ndef pipeline(x):\n    a = mul(x=arg(\"x\"), k=2)          # NodeHandle(\"mul_...\")\n    b = add(x=arg(\"x\"), z=a.z)        # depends on `a` automatically via data edge\n    return {\"y\": b.y}\n\nG = pipeline.build()                    # TaskGraph\nspec = pipeline.spec()                  # graph spec for inspection/registry\nio = pipeline.io()                      # IO signature\n</code></pre>"},{"location":"tools/#3-forcing-an-order-with-_after-no-data-edge","title":"3) Forcing an order with <code>_after</code> (no data edge)","text":"<pre><code>@tool(outputs=[\"ok\"])\ndef init(): return {\"ok\": True}\n\n@tool(outputs=[\"ready\"])\ndef warmup(): return {\"ready\": True}\n\n@graphify(name=\"order_demo\", inputs=[], outputs=[\"ready\"])\ndef order_demo():\n    n1 = init()\n    n2 = warmup(_after=n1)   # enforce sequencing without passing data\n    return {\"ready\": n2.ready}\n</code></pre>"},{"location":"tools/#registration-optional","title":"Registration (Optional)","text":"<p>If a runtime registry is active (via <code>current_registry()</code>), the decorator auto\u2011registers your tool under the <code>tool</code> namespace with its <code>name</code> and <code>version</code> so it can be listed and referenced later.</p> <p>You can also call tools by dotted path via <code>call_tool(\"pkg.module:function\", arg1=..., ...)</code> to avoid importing at build sites, but the recommended ergonomic flow is to <code>import</code> the tool and call it directly.</p>"},{"location":"tools/#best-practices","title":"Best Practices","text":"<ul> <li>Keep tools focused and side\u2011effect aware (e.g., write artifacts via <code>context.artifacts()</code> inside <code>@graph_fn</code> wrappers).</li> <li>Always declare <code>outputs</code> and make your function return those keys.</li> <li>Use <code>_after</code> for control dependencies when no data edge exists.</li> <li>Prefer composing tools via <code>@graphify</code> for explicit fan\u2011in/fan\u2011out graphs.</li> <li>Inside <code>@graph_fn</code>, you can call tools to create explicit nodes, but <code>@graph_fn</code> is for immediate orchestration.</li> </ul>"},{"location":"build-graphs/","title":"Build Graphs in AetherGraph","text":"<p>Welcome! This section is the fastest way to grok how to build and run graphs with Python-first ergonomics.</p> <p>We introduce things in the order you will actually use them:</p> <ol> <li><code>@graph_fn</code> \u2014 the on-ramp. Wrap a regular Python function so it runs as a single graph node, with full <code>context.*</code> access. Great for demos, services, notebooks.</li> <li><code>@tool</code> \u2014 make any function a graph node. Use it inside <code>graph_fn</code> for per-step visibility, metrics, artifacts, and reuse.</li> <li><code>@graphify</code> \u2014 build an explicit DAG for fan-out/fan-in, ordering via <code>_after</code>, subgraphs, and reuse.</li> </ol> <p>Tip: Start with <code>@graph_fn</code> (plus a couple of <code>@tool</code> calls). Move to <code>@graphify</code> when you want explicit topology, parallel map/reduce, barriers, or long-lived pipelines.</p>"},{"location":"build-graphs/#what-is-a-graph-here","title":"What is a \"graph\" here?","text":"<ul> <li>AetherGraph executes TaskGraphs \u2014 directed acyclic graphs of nodes.</li> <li>A node can be:</li> <li>a graph function (<code>@graph_fn</code>) \u2014 runs immediately and can call context services.</li> <li>a tool node (<code>@tool</code>) \u2014 a typed, reusable operation with visible inputs/outputs.</li> <li>The Context (<code>context.*</code>) gives every node uniform access to runtime services:   <code>channel()</code>, <code>artifacts()</code>, <code>memory()</code>, <code>kv()</code>, <code>llm()</code>, <code>rag()</code>, <code>mcp()</code>, <code>logger()</code>.</li> </ul>"},{"location":"build-graphs/#quickstart-30-lines","title":"Quickstart (30 lines)","text":"<pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int):\n    return {\"y\": x * x}\n\n@graph_fn(name=\"demo\", outputs=[\"y\"])\nasync def demo(x: int, *, context):\n    await context.channel().send_text(f\"Computing square of {x}\u2026\")\n    h = square(x=x)              # creates a node you can inspect later\n    await context.channel().send_text(\"Done.\")\n    return {\"y\": h.y}            # expose tool output\n</code></pre> <p>Why this design? - You get instant run semantics (like a normal async function), but steps you mark with <code>@tool</code> become visible graph nodes with metrics/artifacts. - When your flow grows and needs explicit fan-out/fan-in or ordering, switch to <code>@graphify</code>.</p>"},{"location":"build-graphs/#next-steps","title":"Next steps","text":"<ul> <li><code>graph_fn</code> (on-ramp) -&gt; graph_fn.md</li> <li><code>@tool</code> reference -&gt; tool.md</li> <li><code>@graphify</code> (explicit DAG + fan-in/out) -&gt; graphify.md</li> <li>Choosing the right approach -&gt; choosing.md</li> </ul>"},{"location":"build-graphs/choosing/","title":"Choosing: <code>graph_fn</code> vs <code>@graphify</code> vs <code>@tool</code>","text":"<p>Use this one-screen guide to pick the right entry point.</p>"},{"location":"build-graphs/choosing/#start-simple","title":"Start simple","text":"<ul> <li><code>@graph_fn</code> \u2014 quickest way to ship a working function with <code>context.*</code>. Add a couple of <code>@tool</code> calls inside if you want visible/inspectable steps.</li> </ul>"},{"location":"build-graphs/choosing/#scale-up-when-needed","title":"Scale up when needed","text":"<ul> <li><code>@graphify</code> \u2014 when you need explicit DAG control:</li> <li>fan-out / fan-in / map-reduce</li> <li><code>_after</code> (barriers) and <code>_alias</code>/<code>_labels</code> for orchestration and UI</li> <li>subgraph reuse and IO/spec inspection</li> </ul>"},{"location":"build-graphs/choosing/#tool-is-a-building-block","title":"<code>@tool</code> is a building block","text":"<ul> <li>Wrap any function to make it a typed node.</li> <li>Works in both: inside <code>@graph_fn</code> (immediate run, visible steps) and in <code>@graphify</code> (adds nodes to DAG).</li> <li>Control kwargs (<code>_after</code>, <code>_alias</code>, <code>_labels</code>, <code>_id</code>) apply only in graph build contexts.</li> </ul>"},{"location":"build-graphs/choosing/#quick-comparison","title":"Quick comparison","text":"Capability <code>@graph_fn</code> <code>@graphify</code> <code>@tool</code> Immediate \"just run\" Yes Build first Yes (outside graph) Full <code>context.*</code> access Yes (via <code>context</code>) via tools/subgraphs when called under <code>graph_fn</code> Visible per-step nodes via <code>@tool</code> calls native yes Fan-out / fan-in (map/reduce) limited (Python loops) Yes (concise) building block Control edges (<code>_after</code>/barrier) No Yes Yes in graph build Graph spec/IO inspection implicit Yes (<code>.spec()/.io()</code>) n/a Best for demos, services pipelines, orchestration atomic operations <p>Rule of thumb: Start with <code>@graph_fn</code>. When you feel the need for explicit topology or orchestration, switch the same steps into <code>@graphify</code> using the exact same <code>@tool</code>s.</p>"},{"location":"build-graphs/graph_fn/","title":"<code>@graph_fn</code> \u2014 Python-first on-ramp","text":"<p>Wrap a normal (async) Python function so it runs as a single graph node with full access to <code>context.*</code> services. Return values are exposed as graph outputs.</p>"},{"location":"build-graphs/graph_fn/#signature","title":"Signature","text":"<pre><code>@graph_fn(name: str, *, inputs: list[str] | None = None, outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\ndef|async def fn(..., *, context: NodeContext) -&gt; dict | value | NodeHandle\n</code></pre> <ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (list[str], optional) \u2014 Declared input keys (used for IO spec; optional for quickstart).</li> <li>outputs (list[str], optional) \u2014 Declared output keys (enables single-value return).</li> <li>version (str) \u2014 SemVer for registry/lineage.</li> <li>agent (str, optional) \u2014 If set, register this graph function as an agent (advanced).</li> </ul>"},{"location":"build-graphs/graph_fn/#return-normalization","title":"Return normalization","text":"<ul> <li>dict -&gt; keys become outputs; NodeHandles/Refs are exposed.</li> <li>single value -&gt; allowed only if exactly one <code>outputs</code> key is declared (collapsed to that name).</li> <li>NodeHandle -&gt; its outputs are exposed (single output collapses).</li> </ul>"},{"location":"build-graphs/graph_fn/#using-tool-inside-graph_fn","title":"Using <code>@tool</code> inside <code>graph_fn</code>","text":"<p>You can call <code>@tool</code> functions to create visible/inspectable nodes while keeping immediate Python control flow:</p> <pre><code>from aethergraph import graph_fn, tool\n\n@tool(outputs=[\"y\"])\ndef square(x: int): return {\"y\": x*x}\n\n@graph_fn(name=\"demo\", outputs=[\"y\"])\nasync def demo(x: int, *, context):\n    h = square(x=x)          # creates a node\n    await context.channel().send_text(\"computed\")\n    return {\"y\": h.y}\n</code></pre> <p>Important: Control kwargs like <code>_after</code>, <code>_alias</code>, <code>_labels</code> are only honored in graph build contexts (e.g., <code>@graphify</code>). Inside <code>graph_fn</code>, execution order follows normal Python semantics. If you need control edges without passing data, use <code>@graphify</code>.</p>"},{"location":"build-graphs/graph_fn/#when-to-use-graph_fn","title":"When to use <code>@graph_fn</code>","text":"<ul> <li>Quick demos, notebooks, service-style tasks.</li> <li>One to a few steps, mostly sequential.</li> <li>You want full <code>context.*</code> access and instant execution, with optional visibility via <code>@tool</code> calls.</li> </ul> <p>See also: tool.md, graphify.md.</p>"},{"location":"build-graphs/graphify/","title":"<code>@graphify</code> \u2014 Build an explicit DAG (fan-out, fan-in, ordering)","text":"<p>Use <code>@graphify</code> when you need clear topology: map/fan-out, reduce/fan-in, barriers via <code>_after</code>, subgraphs, or reusable pipelines.</p>"},{"location":"build-graphs/graphify/#signature","title":"Signature","text":"<p><pre><code>@graphify(*, name=\"default_graph\", inputs=(), outputs=None, version=\"0.1.0\", agent: str | None = None)\ndef body(...):\n    # Use tool calls to add nodes and return NodeHandles/Refs\n    return {...}\n</code></pre> - The decorated function returns a builder: call <code>.build()</code> to get a <code>TaskGraph</code> instance; <code>.spec()</code> for a serializable spec; <code>.io()</code> for IO signature.</p>"},{"location":"build-graphs/graphify/#control-edges-and-labels-graph-build-only","title":"Control edges and labels (graph build only)","text":"<p><code>@tool</code> control kwargs are honored here: - <code>_after</code>, <code>_alias</code>, <code>_labels</code>, <code>_id</code>, <code>_name</code></p>"},{"location":"build-graphs/graphify/#patterns","title":"Patterns","text":""},{"location":"build-graphs/graphify/#fan-out-map-over-inputs","title":"Fan-out (map over inputs)","text":"<pre><code>from aethergraph import tool, graphify\n\n@tool(outputs=[\"vec\"])\ndef embed(text: str): ...\n\n@graphify(name=\"fanout_demo\", inputs=[\"texts\"], outputs=[\"vecs\"])\ndef fanout_demo(texts):\n    handles = [embed(text=t) for t in texts]          # fan-out\n    return {\"vecs\": [h.vec for h in handles]}         # expose list of outputs\n</code></pre>"},{"location":"build-graphs/graphify/#fan-in-reduce","title":"Fan-in (reduce)","text":"<pre><code>@tool(outputs=[\"score\"])\ndef dot(a, b): ...\n\n@graphify(name=\"fanin_demo\", inputs=[\"query\", \"vecs\"], outputs=[\"scores\"])\ndef fanin_demo(query, vecs):\n    q = embed(text=query)\n    scores = [dot(a=v, b=q.vec) for v in vecs]        # fan-in through q\n    return {\"scores\": [s.score for s in scores]}\n</code></pre>"},{"location":"build-graphs/graphify/#control-edge-without-data","title":"Control edge without data","text":"<pre><code>@tool(outputs=[\"ok\"])   def init(): ...\n@tool(outputs=[\"done\"]) def train(): ...\n\n@graphify(name=\"order\", outputs=[\"done\"])\ndef order():\n    a = init()\n    b = train(_after=a)            # sequence a -&gt; b\n    return {\"done\": b.done}\n</code></pre>"},{"location":"build-graphs/graphify/#subgraph-reuse-optional","title":"Subgraph reuse (optional)","text":"<p>You can register graphs and call them as nodes (advanced). For most cases, compose <code>@tool</code>s directly inside <code>@graphify</code>.</p>"},{"location":"build-graphs/graphify/#when-to-use-graphify","title":"When to use <code>@graphify</code>","text":"<ul> <li>You need parallelism (map) or aggregation (reduce).</li> <li>You need ordering without data flow (<code>_after</code>/barriers).</li> <li>You want a reusable / inspectable DAG (e.g., schedule in a UI).</li> </ul> <p>See also: graph_fn.md, tool.md, choosing.md.</p>"},{"location":"build-graphs/tool/","title":"<code>@tool</code> \u2014 Turn any function into a graph node","text":"<p>Make a plain function a typed, reusable node with explicit inputs/outputs. Works in both <code>@graph_fn</code> (immediate run with visible steps) and <code>@graphify</code> (graph build).</p>"},{"location":"build-graphs/tool/#decorator","title":"Decorator","text":"<pre><code>@tool(outputs: list[str], inputs: list[str] | None = None, *, name: str | None = None, version: str = \"0.1.0\")\ndef fn(...): ...\n</code></pre> <ul> <li>outputs (list[str]) \u2014 Output field names this tool produces.</li> <li>inputs (list[str], optional) \u2014 Input names; inferred from signature if omitted.</li> <li>name (str, optional) \u2014 Registry name (defaults to function name).</li> <li>version (str) \u2014 SemVer for registry/lineage.</li> </ul>"},{"location":"build-graphs/tool/#return-normalization","title":"Return normalization","text":"<ul> <li><code>None</code> -&gt; <code>{}</code></li> <li><code>dict</code> -&gt; as-is</li> <li><code>tuple</code> -&gt; <code>{ \"out0\": v0, \"out1\": v1, ... }</code></li> <li>single value -&gt; <code>{ \"result\": value }</code></li> </ul> <p>Contract check: Declared <code>outputs</code> must be present in the normalized return, otherwise a <code>ValueError</code> is raised.</p>"},{"location":"build-graphs/tool/#two-modes-same-decorator","title":"Two modes (same decorator)","text":"Where called from Behavior Outside any graph Runs immediately and returns a dict. Inside <code>@graph_fn</code> Creates a node handle you can expose. Inside <code>@graphify</code> Adds a node to the DAG (honors control kw). <p>Control kwargs (graph build only): - <code>_after</code> (NodeHandle | list) \u2014 add control-edge dependency. - <code>_alias</code> / <code>_id</code> \u2014 override node id / alias. - <code>_labels</code> (list[str]) \u2014 annotate node for UI/search. - <code>_name</code> \u2014 display name hint.</p>"},{"location":"build-graphs/tool/#minimal-example","title":"Minimal example","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"y\"])\ndef square(x:int) -&gt; dict:\n    return {\"y\": x*x}\n</code></pre> <p>Use in <code>graph_fn</code> or <code>@graphify</code> as shown in their pages.</p>"},{"location":"channel-setup/channel-backend-intro/","title":"Channels &amp; External Interaction","text":"<p>AetherGraph\u2019s channel system is how graphs talk to the outside world:</p> <ul> <li>Send messages, buttons, progress, and files out to users / tools.</li> <li>Receive replies and uploads in and resume waiting nodes.</li> <li>Integrate with Slack, Telegram, web UIs, CLIs, and custom transports.</li> </ul> <p>At a high level, there are five pieces:</p> <ol> <li>ChannelBus \u2013 orchestrates outbound events and binds correlators.</li> <li>ChannelAdapter \u2013 per-platform sender (Slack, Telegram, WS/HTTP, etc.).</li> <li>Continuation Store \u2013 remembers which continuation is waiting on which channel/thread.</li> <li>ChannelIngress \u2013 canonical inbound entry point (external \u2192 AG).</li> <li>HTTP/WS endpoints + ChannelClient \u2013 optional transport for generic web UIs / scripts.</li> </ol> <p>The design is:</p> <p>Adapters handle outbound; Ingress handles inbound; the continuation store ties them together.</p>"},{"location":"channel-setup/channel-backend-intro/#1-outbound-channelbus-channeladapter","title":"1. Outbound: ChannelBus &amp; ChannelAdapter","text":"<p>When a graph calls <code>context.channel().send_text()</code> or <code>context.channel().ask_text()</code>, the flow is:</p> <ol> <li>The ChannelSession builds an <code>OutEvent</code> and hands it to ChannelBus.</li> <li>ChannelBus picks an adapter based on the channel key (e.g. <code>\"slack:team/T:chan/C\"</code>, <code>\"tg:chat/123\"</code>, <code>\"ext:chan/user-123\"</code>).</li> <li>ChannelBus applies capability-aware fallbacks (e.g. buttons \u2192 text if the adapter has no <code>\"buttons\"</code> capability).</li> <li>The adapter sends the shaped event to the external platform.</li> <li>If the adapter returns a Correlator, ChannelBus binds it to the continuation token via the continuation store.</li> </ol>"},{"location":"channel-setup/channel-backend-intro/#11-channelbus-basics","title":"1.1 ChannelBus basics","text":"<pre><code>class ChannelBus:\n    def __init__(\n        self,\n        adapters: dict[str, ChannelAdapter],\n        *,\n        default_channel: str = \"console:stdin\",\n        channel_aliases: dict[str, str] | None = None,\n        logger=None,\n        resume_router=None,\n        store=None,\n    ):\n        ...\n\n    async def publish(self, event: OutEvent) -&gt; dict | None:\n        \"\"\"Send any OutEvent; smart fallbacks; bind correlator if any.\"\"\"\n\n    async def notify(self, continuation) -&gt; dict | None:\n        \"\"\"Ask for input/approval/files from a Continuation.\"\"\"\n\n    async def peek_correlator(self, channel_key: str) -&gt; Correlator | None:\n        ...\n</code></pre> <p>Key ideas:</p> <ul> <li>Channel prefix \u2192 adapter: <code>\"slack:...\"</code> goes to the Slack adapter, <code>\"tg:...\"</code> to Telegram, <code>\"ext:...\"</code> to the generic WS/HTTP adapter, etc.</li> <li>Capability-aware fallbacks: if an adapter doesn\u2019t support buttons or file upload, ChannelBus degrades gracefully to text (numbered options, file links, etc.).</li> <li>Correlator binding: when an adapter returns a <code>Correlator</code>, ChannelBus records a mapping <code>token \u2192 (scheme, channel, thread)</code> in the continuation store.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#12-channeladapter-protocol","title":"1.2 ChannelAdapter protocol","text":"<p>Adapters implement a simple protocol:</p> <pre><code>class ChannelAdapter(Protocol):\n    capabilities: set[str]  # e.g. {\"text\", \"buttons\", \"image\", \"file\", \"stream\"}\n\n    async def send(self, event: OutEvent) -&gt; dict | None:\n        ...\n</code></pre> <ul> <li> <p><code>OutEvent</code> carries everything the adapter might need:</p> </li> <li> <p><code>type</code>: e.g. <code>\"agent.message\"</code>, <code>\"session.need_input\"</code>, <code>\"session.need_approval\"</code>, <code>\"agent.stream.delta\"</code>, <code>\"agent.progress.update\"</code>, <code>\"file.upload\"</code>, etc.</p> </li> <li><code>channel</code>: channel key string (e.g. <code>\"slack:team/T:chan/C:thread/TS\"</code>).</li> <li><code>text</code>, <code>buttons</code>, <code>file</code>, <code>rich</code>, <code>upsert_key</code>, <code>meta</code> (including <code>run_id</code>, <code>node_id</code>, <code>token</code>, <code>resume_key</code>, etc.).</li> <li>The adapter is responsible for mapping <code>OutEvent</code> to the platform-specific API.</li> </ul> <p>You can register custom adapters at container wiring time:</p> <pre><code>queue_adapter = QueueChannelAdapter(container, scheme=\"ext\")\ncontainer.channel_bus.register_adapter(\"ext\", queue_adapter)\ncontainer.channel_bus.set_default_channel_key(\"ext:chan/default\")\n</code></pre> <p>You are not limited to built-in ones; any prefix (e.g. <code>\"mychat\"</code>) can be mapped to your own adapter.</p>"},{"location":"channel-setup/channel-backend-intro/#2-continuations-correlators-and-resumption","title":"2. Continuations, correlators, and resumption","text":"<p>Whenever a node calls <code>ask_text</code>, <code>ask_approval</code>, or similar:</p> <ol> <li> <p>A Continuation object is created and stored:</p> </li> <li> <p><code>run_id</code>, <code>node_id</code>, <code>token</code>, <code>kind</code> (e.g. <code>\"user_input\"</code>, <code>\"approval\"</code>, <code>\"user_files\"</code>).</p> </li> <li><code>channel</code> (where replies must come back).</li> <li><code>prompt</code> and optional payload.</li> <li><code>ChannelBus.notify(continuation)</code> sends a prompt event.</li> <li> <p>If the adapter returns a Correlator, ChannelBus binds:</p> </li> <li> <p>continuation token \u2192 correlator <code>(scheme, channel, thread)</code> in the continuation store.</p> </li> <li>When an inbound message arrives, AG uses this mapping to find and resume the right continuation.</li> </ol> <p>A <code>Correlator</code> typically looks like:</p> <pre><code>Correlator(\n    scheme=\"slack\",\n    channel=\"slack:team/T:chan/C:thread/TS\",\n    thread=\"TS\",          # Slack thread_ts\n    message=\"1700000000.1\" # (optional) message ts\n)\n</code></pre>"},{"location":"channel-setup/channel-backend-intro/#21-matching-inbound-messages-to-a-continuation","title":"2.1 Matching inbound messages to a continuation","text":"<p>On the inbound side, we want to answer:</p> <p>\u201cGiven this (scheme, channel, thread), which continuation is waiting?\u201d</p> <p>The pattern is:</p> <ol> <li> <p>Reconstruct a <code>Correlator</code> key from the inbound event:</p> </li> <li> <p>Slack: <code>scheme=\"slack\"</code>, <code>channel=\"slack:team/T:chan/C[:thread/TS]\"</code>, <code>thread=thread_ts or \"\"</code>.</p> </li> <li>Telegram: <code>scheme=\"tg\"</code>, <code>channel=\"tg:chat/&lt;chat_id&gt;[:topic/&lt;topic_id&gt;]\"</code>, <code>thread=str(topic_id or \"\")</code>.</li> <li> <p>Generic: <code>scheme=\"ext\"</code>, <code>channel=\"ext:chan/&lt;channel_id&gt;\"</code>, <code>thread=provided thread_id or \"\"</code>.</p> </li> <li> <p>Ask the continuation store:</p> </li> </ol> <pre><code>cont = await cont_store.find_by_correlator(corr)\n</code></pre> <ol> <li>If found, call:</li> </ol> <pre><code>await resume_router.resume(run_id=cont.run_id, node_id=cont.node_id, token=cont.token, payload=...)\n</code></pre> <p>For advanced use cases, there is also a manual resume endpoint that bypasses channels entirely and resumes directly with <code>run_id/node_id/token</code>.</p>"},{"location":"channel-setup/channel-backend-intro/#3-slack-telegram-full-featured-adapters","title":"3. Slack &amp; Telegram: full-featured adapters","text":"<p>Slack and Telegram are the most feature-complete channel adapters today. They serve as reference implementations for how to:</p> <ul> <li>Send text, buttons, streamed updates, progress, and file uploads.</li> <li>Verify inbound webhooks.</li> <li>Match inbound messages back to continuations.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#31-slack-http-and-socket-mode","title":"3.1 Slack: HTTP and Socket Mode","text":"<p>Slack integration consists of:</p> <ul> <li> <p>A <code>SlackChannelAdapter</code> implementing <code>ChannelAdapter</code>:</p> </li> <li> <p><code>capabilities = {\"text\", \"buttons\", \"image\", \"file\", \"edit\", \"stream\"}</code>.</p> </li> <li>Sends messages via <code>chat.postMessage</code>, <code>chat.update</code>, <code>files.upload_v2</code>, blocks for buttons, etc.</li> <li>Returns a <code>Correlator</code> so the continuation store can bind channel/thread \u2194 token.</li> <li> <p>HTTP routes:</p> </li> <li> <p><code>POST /slack/events</code> (Events API webhooks).</p> </li> <li><code>POST /slack/interact</code> (interactive buttons).</li> <li>Both verify signatures via Slack\u2019s signing secret.</li> <li> <p>Optional Socket Mode runner:</p> </li> <li> <p>Uses <code>SocketModeClient</code> to receive events and interactive payloads over WebSocket instead of HTTP.</p> </li> <li>Calls the same shared handlers as HTTP.</li> </ul> <p>Slack uses a rich channel key format, e.g.:</p> <pre><code>slack:team/T123:chan/C456[:thread/1700000000.12345]\n</code></pre> <p>The Slack utilities:</p> <ul> <li>Normalize this into a <code>channel_key</code>.</li> <li>Download and save files as artifacts.</li> <li>Append file_refs to a per-channel inbox in <code>kv_hot</code>.</li> <li>Resume continuations with <code>{text, files}</code> payloads when appropriate.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#32-telegram","title":"3.2 Telegram","text":"<p>Telegram has a similar structure:</p> <ul> <li>Webhook route: <code>POST /telegram/webhook</code> with custom header verification.</li> <li> <p>Helpers to:</p> </li> <li> <p>Build channel keys: <code>tg:chat/&lt;chat_id&gt;[:topic/&lt;topic_id&gt;]</code>.</p> </li> <li>Download photos/documents and store them as artifacts.</li> <li>Append file_refs to a per-channel inbox.</li> <li> <p>Matching strategy:</p> </li> <li> <p>Uses correlators with <code>scheme=\"tg\"</code>, <code>channel=\"tg:chat/...\"</code>, <code>thread=str(topic_id or \"\")</code>.</p> </li> <li>For inline buttons (callback queries), it can also use a <code>resume_key</code> and alias \u2192 token mapping for more precise routing.</li> </ul> <p>Slack and Telegram show the full power of the channel system: provider-specific verification, downloads, capability mapping, and rich correlator usage.</p>"},{"location":"channel-setup/channel-backend-intro/#4-generic-channels-channelingress-http-and-ws","title":"4. Generic channels: ChannelIngress, HTTP, and WS","text":"<p>Most users will eventually want a custom UI (React app, internal tool, notebook script) rather than Slack/Telegram. For that, AG provides a generic, provider-agnostic path:</p> <ul> <li><code>ChannelIngress</code> \u2013 handles inbound messages (external \u2192 AG) in a uniform way.</li> <li><code>QueueChannelAdapter</code> \u2013 writes outbound events to <code>kv_hot</code> outboxes.</li> <li>HTTP route <code>POST /channel/incoming</code> \u2013 generic inbound endpoint.</li> <li>WS route <code>/ws/channel</code> \u2013 generic outbound event stream.</li> <li><code>ChannelClient</code> \u2013 a small Python client for talking to the server.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#41-channelingress-the-inbound-core","title":"4.1 ChannelIngress: the inbound core","text":"<p><code>ChannelIngress</code> is the canonical entry point for inbound messages:</p> <pre><code>@dataclass\nclass IncomingMessage:\n    scheme: str             # e.g. \"ext\", \"mychat\", \"tg\", \"slack-http\"\n    channel_id: str         # logical channel / user id\n    thread_id: str | None = None\n\n    text: str | None = None\n    files: list[IncomingFile] | None = None\n    choice: str | None = None\n    meta: dict[str, Any] | None = None\n\n\nclass ChannelIngress:\n    async def handle(self, msg: IncomingMessage) -&gt; bool:\n        \"\"\"Resume the matching continuation if any. Returns True if resumed.\"\"\"\n</code></pre> <p>The default implementation:</p> <ol> <li>Builds a canonical <code>channel_key</code> from <code>(scheme, channel_id)</code> (e.g. <code>\"ext:chan/user-123\"</code>).</li> <li> <p>Finds a continuation by correlator:</p> </li> <li> <p>Prefer <code>(scheme, channel_key, thread_id)</code>.</p> </li> <li>Fallback to <code>(scheme, channel_key, thread=\"\")</code>.</li> <li>Optionally downloads files or uses provided URIs and writes them to the artifact store.</li> <li> <p>If a continuation is found, builds a payload based on its <code>kind</code>:</p> </li> <li> <p><code>\"approval\"</code> \u2192 <code>{\"choice\", \"channel_key\", \"thread_id\", \"meta\"}</code>.</p> </li> <li><code>\"user_files\"</code> / <code>\"user_input_or_files\"</code> \u2192 <code>{\"text\", \"files\", ...}</code>.</li> <li>default \u2192 <code>{\"text\", ...}</code>.</li> <li>Calls <code>resume_router.resume(...)</code> and returns <code>True</code>.</li> </ol> <p>You can sub-class <code>ChannelIngress</code> to support existing formats (e.g. Telegram\u2019s <code>tg:chat/...</code> keys) by overriding <code>_channel_key</code>.</p>"},{"location":"channel-setup/channel-backend-intro/#42-queuechanneladapter-a-generic-outbox","title":"4.2 QueueChannelAdapter: a generic outbox","text":"<p>For custom UIs, AG ships a generic adapter that pushes events into an outbox in <code>kv_hot</code>:</p> <pre><code>class QueueChannelAdapter(ChannelAdapter):\n    capabilities: set[str] = {\"text\", \"buttons\", \"image\", \"file\", \"edit\", \"stream\"}\n\n    async def send(self, event: OutEvent) -&gt; dict | None:\n        ch_key = event.channel       # e.g. \"ext:chan/user-123\"\n        outbox_key = f\"outbox://{ch_key}\"\n\n        payload = {\n            \"type\": event.type,\n            \"channel\": event.channel,\n            \"text\": event.text,\n            \"meta\": event.meta,\n            \"rich\": event.rich,\n            \"upsert_key\": event.upsert_key,\n            \"file\": event.file,\n            \"buttons\": [...],   # flattened label/value/style/url\n            \"ts\": ...,\n        }\n\n        await container.kv_hot.list_append(outbox_key, [payload])\n        return {}\n</code></pre> <p>The purpose of <code>kv_hot</code> here is not correlation, but buffering:</p> <ul> <li>The runtime writes outbound events into outboxes.</li> <li>The WS server (and any debugging tools) read from these outboxes.</li> <li>You can cap/trim these lists or move them to a more scalable queue later.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#43-http-inbound-channelincoming-and-manual-channelresume","title":"4.3 HTTP inbound: <code>/channel/incoming</code> and manual <code>/channel/resume</code>","text":"<p>A generic HTTP route for inbound messages looks like:</p> <pre><code>@router.post(\"/channel/incoming\")\nasync def channel_incoming(body: ChannelIncomingBody, request: Request):\n    container = request.app.state.container\n    ingress: ChannelIngress = container.channel_ingress\n\n    files = [... convert to IncomingFile ...]\n\n    ok = await ingress.handle(\n        IncomingMessage(\n            scheme=body.scheme,\n            channel_id=body.channel_id,\n            thread_id=body.thread_id,\n            text=body.text,\n            files=files or None,\n            choice=body.choice,\n            meta=body.meta,\n        )\n    )\n    return {\"ok\": True, \"resumed\": ok}\n</code></pre> <p>For power users, a manual resume endpoint bypasses channels and correlators entirely:</p> <pre><code>@router.post(\"/channel/resume\")\nasync def channel_resume(body: ChannelManualResumeBody, request: Request):\n    container = request.app.state.container\n    await container.resume_router.resume(\n        run_id=body.run_id,\n        node_id=body.node_id,\n        token=body.token,\n        payload=body.payload or {},\n    )\n    return {\"ok\": True}\n</code></pre>"},{"location":"channel-setup/channel-backend-intro/#44-ws-outbound-wschannel","title":"4.4 WS outbound: <code>/ws/channel</code>","text":"<p>To stream outbound events to a UI over WebSocket, AG exposes a generic endpoint:</p> <pre><code>@router.websocket(\"/ws/channel\")\nasync def ws_channel(ws: WebSocket):\n    await ws.accept()\n\n    hello = await ws.receive_json()\n    scheme = hello.get(\"scheme\") or \"ext\"\n    channel_id = hello[\"channel_id\"]\n\n    container = ws.app.state.container\n    c = container\n\n    ch_key = f\"{scheme}:chan/{channel_id}\"\n    outbox_key = f\"outbox://{ch_key}\"\n\n    last_idx = 0\n    try:\n        while True:\n            await asyncio.sleep(0.25)\n            events = await c.kv_hot.list_get(outbox_key) or []\n            if last_idx &lt; len(events):\n                for ev in events[last_idx:]:\n                    await ws.send_json(ev)\n                last_idx = len(events)\n    except WebSocketDisconnect:\n        return\n</code></pre> <p>This gives you a generic AG channel over WS:</p> <ul> <li>UI connects to <code>/ws/channel</code> and sends a handshake: <code>{ \"scheme\": \"ext\", \"channel_id\": \"user-123\" }</code>.</li> <li>UI calls <code>POST /channel/incoming</code> to send messages back.</li> <li>All heavy lifting (continuation matching, resumption) is done by <code>ChannelIngress</code> + continuation store.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#45-channelclient-talking-to-ag-from-python","title":"4.5 ChannelClient: talking to AG from Python","text":"<p>For scripts, notebooks, or simple tools, AG provides a small <code>ChannelClient</code>:</p> <pre><code>client = ChannelClient(\n    base_url=\"http://localhost:8000\",  # your AG server\n    scheme=\"ext\",\n    channel_id=\"me\",\n)\n\n# send message into AG\nawait client.send_text(\"hello from my script\")\n\n# listen for events from AG\nasync for ev in client.iter_events():\n    print(\"AG event:\", ev[\"type\"], ev.get(\"text\"))\n</code></pre> <p>This is purely a convenience wrapper around:</p> <ul> <li><code>POST /channel/incoming</code> (for inbound \u2192 AG), and</li> <li><code>/ws/channel</code> (for outbound \u2192 client).</li> </ul> <p>You can subclass or wrap it to add headers, auth tokens, retry logic, etc.</p>"},{"location":"channel-setup/channel-backend-intro/#5-auth-security","title":"5. Auth &amp; security","text":"<p>AG does not enforce a global authentication scheme for channels. Responsibilities are split:</p> <ul> <li> <p>Provider-specific webhooks (Slack, Telegram, etc.):</p> </li> <li> <p>The integration modules include helpers like <code>_verify_sig</code> (Slack) and <code>_verify_secret</code> (Telegram).</p> </li> <li>These implement the provider\u2019s required verification (HMAC signatures, secret headers).</li> <li> <p>They are examples of transport-level security, not a global policy.</p> </li> <li> <p>Generic endpoints (<code>/channel/incoming</code>, <code>/ws/channel</code>, <code>/channel/resume</code>):</p> </li> <li> <p>AG treats these as application-level concerns.</p> </li> <li>The framework assumes that if a request reaches these routes, it has already passed whatever authentication / authorization your app requires.</li> <li> <p>You are expected to wrap these routes with your own auth, for example:</p> <pre><code>from fastapi import Depends\nfrom myapp.auth import require_user\n\n@router.post(\"/channel/incoming\")\nasync def channel_incoming(\n    body: ChannelIncomingBody,\n    request: Request,\n    user = Depends(require_user),  # your auth\n):\n    ...\n</code></pre> </li> <li> <p>ChannelClient:</p> </li> <li> <p>Out of the box, it performs no auth.</p> </li> <li> <p>To use it against a secured AG server, you should:</p> <ul> <li>wrap it to add headers (e.g. <code>Authorization: Bearer ...</code> or <code>X-AG-Token</code>), and</li> <li>configure your server-side routes to check those.</li> </ul> </li> </ul>"},{"location":"channel-setup/channel-backend-intro/#51-future-simple-shared-channel-token-idea","title":"5.1 Future: simple shared channel token (idea)","text":"<p>For solo researchers or simple setups, we may add an optional single shared channel token:</p> <ul> <li> <p>Env var: <code>AETHERGRAPH_CHANNEL_TOKEN=\"some-long-random-secret\"</code>.</p> </li> <li> <p>If set, the built-in <code>/channel/incoming</code> and <code>/ws/channel</code> routes would require a header like:</p> </li> </ul> <pre><code>X-AG-Channel-Token: some-long-random-secret\n</code></pre> <ul> <li><code>ChannelClient</code> would grow a <code>token=</code> argument that automatically adds this header.</li> </ul> <p>This would provide a very simple \u201cpersonal secure channel\u201d without forcing a full auth stack, while still leaving real authentication/authorization to the host application for multi-user deployments.</p>"},{"location":"channel-setup/channel-backend-intro/#6-recommended-usage-patterns","title":"6. Recommended usage patterns","text":""},{"location":"channel-setup/channel-backend-intro/#61-simple-chat-ui-one-conversation-per-channel","title":"6.1 Simple chat UI (one conversation per channel)","text":"<p>For a basic custom UI:</p> <ul> <li>Use <code>QueueChannelAdapter</code> with prefix <code>\"ext\"</code>.</li> <li>Use <code>/channel/incoming</code> + <code>/ws/channel</code> + <code>ChannelClient</code>.</li> <li>Treat <code>(scheme, channel_id)</code> as one conversation at a time.</li> <li>Let <code>ChannelIngress</code> handle matching and resumption.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#62-multiple-flows-per-user","title":"6.2 Multiple flows per user","text":"<p>If you need multiple concurrent prompts per user:</p> <ul> <li> <p>Distinguish flows via <code>thread_id</code> or synthetic <code>channel_id</code> values:</p> </li> <li> <p>e.g. <code>channel_id=\"user-123:flow-1\"</code>, <code>thread_id=\"run-abc\"</code>.</p> </li> <li>Ensure adapters return correlators with these details so the continuation store can distinguish them.</li> <li>Optionally expose <code>resume_key</code> / <code>run_id/node_id/token</code> to the UI and use <code>/channel/resume</code> directly.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#63-provider-specific-integrations","title":"6.3 Provider-specific integrations","text":"<p>For Slack/Telegram/other platforms:</p> <ul> <li>Implement a dedicated <code>ChannelAdapter</code> and small provider-specific HTTP routes.</li> <li>Use their verification mechanisms (signing secrets, webhook tokens).</li> <li>Normalize channel keys and threads into a <code>Correlator</code> pattern compatible with your continuation store.</li> </ul>"},{"location":"channel-setup/channel-backend-intro/#7-mental-model","title":"7. Mental model","text":"<p>You can summarize the channel system as:</p> <ul> <li>ChannelBus \u2013 decides what to send where, and binds correlators.</li> <li>ChannelAdapter \u2013 knows how to talk to a specific platform.</li> <li>Continuation store \u2013 remembers who is waiting on which channel/thread.</li> <li>ChannelIngress \u2013 turns inbound messages into resumes.</li> <li>HTTP/WS endpoints + ChannelClient \u2013 optional, generic transport for custom UIs.</li> </ul> <p>Everything else is a specialization of this pattern.</p> <p>This separation keeps AetherGraph\u2019s core runtime clean while still making it easy to:</p> <ul> <li>integrate deeply with rich platforms like Slack/Telegram, and</li> <li>support lightweight custom channels via simple HTTP/WS + a small Python client.</li> </ul>"},{"location":"channel-setup/console-setup/","title":"Console Channel Setup","text":"<p>The console channel is the simplest built\u2011in channel in AetherGraph. It prints messages to your terminal and, when possible, reads replies from standard input.</p> <p>\u2705 No setup required \u2014 enabled by default.</p> <p>\ud83d\udda5\ufe0f Default key: <code>console:stdin</code></p>"},{"location":"channel-setup/console-setup/#usage","title":"Usage","text":"<pre><code># Default: uses console if no other channel is configured\nawait context.channel().send_text(\"Hello from AetherGraph \ud83d\udc4b\")\n\n# Explicit reference\nchan = context.channel(\"console:stdin\")\nawait chan.send_text(\"This goes to the terminal\")\n</code></pre>"},{"location":"channel-setup/console-setup/#capabilities","title":"Capabilities","text":"<ul> <li>Text output (printed to terminal)</li> <li>Input via <code>ask_text</code></li> <li>Buttons/approvals via <code>ask_approval</code> (rendered as numbered options)</li> </ul> <p>Internally:</p> <pre><code>capabilities = {\"text\", \"input\", \"buttons\"}\n</code></pre>"},{"location":"channel-setup/console-setup/#send_text","title":"<code>send_text</code>","text":"<pre><code>await context.channel().send_text(\"Hello \ud83d\udc4b\")\n</code></pre> <p>Prints a line like:</p> <pre><code>[console] agent.message :: Hello \ud83d\udc4b\n</code></pre> <p>Returns immediately (no continuation).</p>"},{"location":"channel-setup/console-setup/#ask_","title":"<code>ask_*</code>","text":"<pre><code>name = await context.channel().ask_text(\"What is your name?\")\nawait context.channel().send_text(f\"Nice to meet you, {name}!\")\n</code></pre> <p>Prompts on the terminal, reads a line from stdin, and resumes inline (no external wait).</p> <p>Notes: In non\u2011interactive environments (CI, no stdin), input may not be available; the runtime persists a continuation for consistency, but does not allow resumption. For normal local terminals, <code>ask_*</code> works inline without extra config.</p>"},{"location":"channel-setup/file-setup/","title":"File Channel Setup","text":"<p>The file channel is a simple, one\u2011way output channel that appends messages from AetherGraph to files on disk.</p> <p>\u2705 No setup required \u2014 writes under your workspace <code>workspace/channel_files</code>.</p> <p>\ud83d\uddc2\ufe0f Key format: <code>file:&lt;relative/path/to/file&gt;</code></p>"},{"location":"channel-setup/file-setup/#when-to-use","title":"When to Use","text":"<ul> <li>Persistent, local run logs (steps, status, results) with custom format</li> <li>Transcripts for papers/debugging</li> <li>Plain\u2011text output you can open with any editor</li> </ul>"},{"location":"channel-setup/file-setup/#where-files-are-written","title":"Where Files Are Written","text":"<p>Files are created under:</p> <pre><code>&lt;workspace&gt;/channel_files\n</code></pre> <ul> <li><code>&lt;workspace&gt;</code> is your AetherGraph data root.</li> <li>The portion after <code>file:</code> becomes a relative path under <code>channel_files</code>.</li> </ul> <p>Example</p> <pre><code>chan = context.channel(\"file:runs/demo_run.log\")\nawait chan.send_text(\"Demo run started\")\n</code></pre> <p>Writes (appends) to:</p> <pre><code>&lt;workspace&gt;/channel_files/runs/demo_run.log\n</code></pre> <p>Parent directories are created automatically.</p>"},{"location":"channel-setup/file-setup/#usage","title":"Usage","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"file_channel_demo\")\nasync def file_channel_demo(*, context: NodeContext):\n    chan = context.channel(\"file:logs/experiment_01.txt\")\n    await chan.send_text(\"Run began\")\n    await chan.send_text(\"Metric: acc=0.93, loss=0.12\")\n    return {\"logged\": True}\n</code></pre>"},{"location":"channel-setup/file-setup/#notes","title":"Notes","text":"<ul> <li>One\u2011way: no <code>ask_*</code> prompts (write\u2011only).</li> <li>Append behavior: messages are appended; rotate/cleanup as needed.</li> <li>Organization tip: include date/run IDs in paths (e.g., <code>file:runs/2025-11-15/expA.txt</code>).</li> </ul>"},{"location":"channel-setup/introduction/","title":"Channels Overview","text":"<p>AetherGraph ships with multiple channels for delivering messages and (optionally) interacting with users or tools. This page gives you a compact map of what exists, when to use each, and what they can do.</p> <p>Scope (OSS build): Channels are designed for personal, local, and small\u2011team use. Treat them as convenient building blocks, not as a hardened, multi\u2011tenant messaging stack. For public or sensitive deployments, keep channels behind trusted networks and review security settings carefully.</p>"},{"location":"channel-setup/introduction/#1-channel-quick-picks-when-to-use-what","title":"1. Channel quick picks (when to use what)","text":"<ul> <li>Console (<code>console:</code>) \u2013 Default. Local dev, quick demos, CLI\u2011style prompts. No setup.</li> <li>Slack (<code>slack:</code>) \u2013 Team chat, rich approvals, durable <code>ask_*</code> resumes. Requires Slack app.</li> <li>Telegram (<code>tg:</code>) \u2013 Mobile\u2011friendly prompts and notifications. Polling (local) or webhook (advanced). Experimental for <code>ask_*</code>.</li> <li>File (<code>file:</code>) \u2013 Write\u2011only logs/transcripts to disk under your workspace. Zero setup.</li> <li>Webhook (<code>webhook:</code>) \u2013 Write\u2011only JSON POST to any incoming webhook (Slack Incoming, Discord, Zapier, etc.). Zero setup in AetherGraph.</li> </ul>"},{"location":"channel-setup/introduction/#2-capabilities-at-a-glance","title":"2. Capabilities at a glance","text":"<p>Legend: \u2705 supported \u2022 \ud83d\udcdd forwarded/logged only \u2022 \u2716\ufe0f not supported</p> Channel Key prefix / example Default Text Input / <code>ask_*</code> Buttons / approval Image File Streaming/Edit Inbound resume Console <code>console:stdin</code> \u2705 \u2705 \u2705 (inline via stdin) \u2705 (numbered) \u2716\ufe0f \u2716\ufe0f \u2716\ufe0f \u2716\ufe0f Slack <code>slack:team/T:chan/C[:thread/TS]</code> \u2716\ufe0f \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Telegram <code>tg:chat/&lt;id&gt;[:topic/&lt;thread_id&gt;]</code> \u2716\ufe0f \u2705 \u2705 (experimental) \u2705 \u2705 \u2705 \u2705 \u2705 (experimental) File <code>file:logs/experiment_01.txt</code> \u2716\ufe0f \u2705 \u2716\ufe0f \ud83d\udcdd \ud83d\udcdd \ud83d\udcdd \ud83d\udcdd N/A Webhook <code>webhook:https://hooks.zapier.com/...</code> \u2716\ufe0f \u2705 \u2716\ufe0f \ud83d\udcdd \ud83d\udcdd \ud83d\udcdd \ud83d\udcdd N/A <p>Notes</p> <ul> <li>Console is simple and local; great for development but not built for durable, cross\u2011process resumes.</li> <li>Slack/Telegram provide two\u2011way chat; Telegram\u2019s interaction support is still experimental.</li> <li>File/Webhook are inform\u2011only: push messages out, no replies.</li> </ul>"},{"location":"channel-setup/introduction/#3-ask_-vs-informonly","title":"3. <code>ask_*</code> vs inform\u2011only","text":"<p>Interactive (<code>ask_*</code>)</p> <ul> <li>Supported: Console, Slack, Telegram.</li> <li>In <code>@graphify</code>, Slack/Telegram create a Continuation and can resume even if your Python process restarts.</li> <li>Console reads inline from stdin; no durable, cross\u2011process resume.</li> </ul> <p>Inform\u2011only</p> <ul> <li>File and Webhook only push out.</li> <li>Use them for notifications, logging, and triggering automations; do not call <code>ask_*</code> on these.</li> </ul>"},{"location":"channel-setup/introduction/#4-concurrency-multichannel-patterns","title":"4. Concurrency &amp; multi\u2011channel patterns","text":"<ul> <li>You can send multiple messages concurrently on a channel.</li> <li>You can use multiple channels in the same graph (e.g., console debug + file log + Slack + webhook).</li> </ul> <p>Example:</p> <pre><code>async def run_with_notifications(context):\n    await context.channel(\"console:stdin\").send_text(\"Run started\")\n    await context.channel(\"file:runs/exp_01.log\").send_text(\"Run started\")\n    await context.channel(\"slack:team/T:chan/C\").send_text(\"Run started (Slack)\")\n    await context.channel(\"webhook:https://hooks.zapier.com/hooks/catch/.../\").send_text(\"Run started (Webhook)\")\n</code></pre>"},{"location":"channel-setup/introduction/#5-security-tips-webhook-chat","title":"5. Security tips (webhook &amp; chat)","text":"<ul> <li>Treat webhook URLs as secrets; rotate if leaked. Prefer HTTPS.</li> <li>For custom receivers, add a shared secret header (e.g., <code>X-AetherGraph-Secret</code>) and verify server\u2011side.</li> <li>Keep public exposure minimal; prefer trusted networks or intermediaries (Zapier/Make, your backend) when adapting payload shapes.</li> <li>Review bot/app permissions on Slack/Telegram\u2014grant only what you need.</li> </ul>"},{"location":"channel-setup/introduction/#6-key-formats-cheat-sheet","title":"6. Key formats (cheat sheet)","text":"<ul> <li>Console: <code>console:stdin</code></li> <li>Slack: <code>slack:team/&lt;TEAM_ID&gt;:chan/&lt;CHANNEL_ID&gt;[:thread/&lt;TS&gt;]</code></li> <li>Telegram: <code>tg:chat/&lt;CHAT_ID&gt;[:topic/&lt;THREAD_ID&gt;]</code></li> <li>File: <code>file:&lt;relative/path/inside/channel_files&gt;</code></li> <li>Webhook: <code>webhook:&lt;FULL_WEBHOOK_URL&gt;</code></li> </ul> <p>Use these keys directly with <code>context.channel(&lt;key&gt;)</code>, or set defaults/aliases as needed.</p>"},{"location":"channel-setup/slack-setup/","title":"Slack Integration Setup (Socket Mode)","text":"<p>This guide shows you how to connect Slack to AetherGraph using Socket Mode \u2014 ideal for local or individual use.</p> <p>\u2705 No public URL or ngrok required.</p> <p>\u2705 Runs securely via WebSocket.</p>"},{"location":"channel-setup/slack-setup/#before-you-start","title":"Before You Start","text":"<ol> <li>Install AetherGraph with Slack extras:</li> </ol> <pre><code>pip install \"aethergraph[slack]\"\n</code></pre> <ol> <li>Make sure you have a <code>.env</code> file in your project root. AetherGraph will read Slack configuration from it.</li> </ol>"},{"location":"channel-setup/slack-setup/#1-create-a-slack-app-with-manifest-json","title":"1. Create a Slack App (with Manifest JSON)","text":"<ol> <li>Go to https://api.slack.com/apps \u2192 click \u201cCreate New App\u201d \u2192 \u201cFrom an app manifest.\u201d</li> <li>Choose your workspace.</li> <li>Paste the following JSON manifest (you can rename the app if you wish):</li> </ol> <pre><code> {\n     \"display_information\": {\n         \"name\": \"AetherGraph\"\n     },\n     \"features\": {\n         \"bot_user\": {\n             \"display_name\": \"AetherGraph\",\n             \"always_online\": true\n         }\n     },\n     \"oauth_config\": {\n         \"scopes\": {\n             \"bot\": [\n                 \"app_mentions:read\",\n                 \"channels:history\",\n                 \"chat:write\",\n                 \"channels:manage\",\n                 \"channels:read\",\n                 \"files:read\",\n                 \"files:write\",\n                 \"groups:read\",\n                 \"groups:history\"\n             ]\n         }\n     },\n     \"settings\": {\n         \"event_subscriptions\": {\n             \"bot_events\": [\n                 \"app_mention\",\n                 \"message.channels\",\n                 \"message.groups\"\n             ]\n         },\n         \"interactivity\": {\n             \"is_enabled\": true\n         },\n         \"org_deploy_enabled\": false,\n         \"socket_mode_enabled\": true,\n         \"token_rotation_enabled\": false\n     }\n }\n</code></pre> <p>Note: For Socket Mode, you do not need to configure an HTTP Request URL for events or interactivity.</p> <ol> <li>Click Create App.</li> <li>Go to OAuth &amp; Permissions \u2192 Install App to Workspace and complete installation.</li> </ol>"},{"location":"channel-setup/slack-setup/#2-enable-socket-mode-and-get-tokens","title":"2. Enable Socket Mode and Get Tokens","text":"<ol> <li>In your app\u2019s left sidebar, go to Socket Mode.</li> <li>Toggle Enable Socket Mode \u2192 ON.</li> <li> <p>Click \u201cGenerate App-Level Token\u201d:</p> </li> <li> <p>Name it something like <code>aethergraph-app-token</code>.</p> </li> <li>Grant it the <code>connections:write</code> scope.</li> <li>Copy the token (starts with <code>xapp-...</code>).</li> <li>Go to OAuth &amp; Permissions, install the app to Slack, and copy the Bot User OAuth Token (starts with <code>xoxb-...</code>).</li> </ol> <p>You now have:</p> <ul> <li>Bot token (<code>xoxb-\u2026</code>)</li> <li>App token (<code>xapp-\u2026</code>)</li> <li>(Optional) Signing secret \u2014 found under Basic Information \u2192 App Credentials \u2192 Signing Secret</li> </ul>"},{"location":"channel-setup/slack-setup/#3-configure-env-for-aethergraph","title":"3. Configure <code>.env</code> for AetherGraph","text":"<p>AetherGraph reads Slack settings from your environment variables.</p> <p>Add the following lines to your <code>.env</code>:</p> <pre><code># Slack (optional)\nAETHERGRAPH_SLACK__ENABLED=true             # must be true to enable\nAETHERGRAPH_SLACK__BOT_TOKEN=xoxb-your-bot-token-here\nAETHERGRAPH_SLACK__APP_TOKEN=xapp-your-app-token-here\nAETHERGRAPH_SLACK__SIGNING_SECRET=your-signing-secret-here\nAETHERGRAPH_SLACK__SOCKET_MODE_ENABLED=true  # usually true for local testing\nAETHERGRAPH_SLACK__WEBHOOK_ENABLED=false     # usually false for local testing\n</code></pre> <p>After saving <code>.env</code>, restart your AetherGraph sidecar so the new settings take effect.</p> <p>With this setup:</p> <ul> <li>AetherGraph connects to Slack via WebSocket (Socket Mode).</li> <li>You don\u2019t need ngrok or a public URL.</li> </ul>"},{"location":"channel-setup/slack-setup/#4-setting-up-slack-channels-and-aliases","title":"4. Setting Up Slack Channels and Aliases","text":"<p>Once Slack is enabled, you can define which channel AetherGraph should talk to.</p> <p>Here\u2019s a typical setup pattern:</p> <pre><code>import os\nfrom aethergraph.channels import set_default_channel, set_channel_alias\n\nSLACK_TEAM_ID = os.getenv(\"SLACK_TEAM_ID\", \"your-slack-team-id\")\nSLACK_CHANNEL_ID = os.getenv(\"SLACK_CHANNEL_ID\", \"your-slack-channel-id\")\nslack_channel_key = f\"slack:team/{SLACK_TEAM_ID}:chan/{SLACK_CHANNEL_ID}\"  # Slack channel key format\n\n# Set as the default channel\nset_default_channel(slack_channel_key)\n\n# Optional: define an alias\nset_channel_alias(\"my_slack\", slack_channel_key)\n</code></pre> <p>Usage examples:</p> <pre><code>chan = context.channel()  # uses the default Slack channel\nawait chan.send_text(\"Hello from AetherGraph \ud83d\udc4b\")\n\nchan2 = context.channel(\"my_slack\")  # use a named alias\nawait chan2.send_text(\"Message to my_slack alias\")\n\n# or directly specify the channel\nawait context.channel().send_text(\"Custom target\", channel=slack_channel_key)\n</code></pre> <p>If nothing is set up, AetherGraph automatically falls back to <code>console:stdin</code>.</p> <p>Finding your Team &amp; Channel IDs</p> <ul> <li>Channel ID: Open Slack in a browser \u2192 navigate to the channel \u2192 copy the <code>C\u2026</code> (public) or <code>G\u2026</code> (private) part from the URL.</li> <li>Team ID: In the same URL, copy the <code>T\u2026</code> segment (your workspace ID).</li> </ul>"},{"location":"channel-setup/slack-setup/#5-quick-test","title":"5. Quick Test","text":"<p>Once everything is configured, test your integration:</p> <p>Invite the bot to your channel</p> <ul> <li> <p>Private channels require the bot to be a member before it can post. Invite it via Add people or mention <code>@YourBot</code> and select Invite to channel.</p> </li> <li> <p>For DMs, post to the DM channel ID (<code>D\u2026</code>), not a user ID.</p> </li> </ul> <p>Run the graph \u2014 if your message appears in Slack, you\u2019re all set!</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"hello_slack\")\nasync def hello_slack(*, context: NodeContext):\n    chan = context.channel()\n    await chan.send_text(\"Hello from AetherGraph \ud83d\udc4b\")\n    return {\"ok\": True}\n</code></pre>"},{"location":"channel-setup/slack-setup/#notes","title":"Notes","text":"<ul> <li>This Socket Mode setup is for local / individual use only.</li> <li>Do not expose your sidecar server directly to the internet.</li> <li>Future versions will include webhook-based production integrations.</li> </ul>"},{"location":"channel-setup/telegram-setup/","title":"Telegram Integration Setup (Local, Experimental)","text":"<p>Connect a Telegram bot to AetherGraph for local / personal use via polling.</p> <p>\u2705 No public URL or webhook required (polling).</p> <p>\u2705 Great for demos and quick experiments.</p> <p>\u26a0\ufe0f Status: Experimental \u2014 if the first message is slow to appear or polling stalls, wait a few more seconds before sending or restart the sidecar and send a fresh message.</p>"},{"location":"channel-setup/telegram-setup/#before-you-start","title":"Before You Start","text":"<ol> <li> <p>You don't need to install additional dependencies for local Telegram setup</p> </li> <li> <p>Ensure you have a <code>.env</code> file in your project root. AetherGraph reads Telegram settings from it.</p> </li> </ol>"},{"location":"channel-setup/telegram-setup/#1-create-a-telegram-bot-botfather","title":"1. Create a Telegram Bot (BotFather)","text":"<ol> <li>In Telegram, start a chat with @BotFather.</li> <li> <p>Send <code>/newbot</code> and follow prompts:</p> </li> <li> <p>Pick a name (e.g., <code>AetherGraph Telegram Bot</code>).</p> </li> <li>Pick a username ending in <code>bot</code> (e.g., <code>aethergraph_dev_bot</code>).</li> <li>Copy the bot token BotFather returns (looks like <code>123456789:ABC...</code>).</li> </ol> <p>That\u2019s all you need for local polling mode.</p>"},{"location":"channel-setup/telegram-setup/#2-configure-env-for-aethergraph","title":"2. Configure <code>.env</code> for AetherGraph","text":"<p>Add the following variables (update the values you received):</p> <pre><code># Telegram (optional)\nAETHERGRAPH_TELEGRAM__ENABLED=true               # must be true to enable\nAETHERGRAPH_TELEGRAM__BOT_TOKEN=123456789:ABC... # from BotFather\n\n# Local/dev polling mode (keep this for local usage)\nAETHERGRAPH_TELEGRAM__POLLING_ENABLED=true\nAETHERGRAPH_TELEGRAM__WEBHOOK_ENABLED=false\n</code></pre> <p>After saving, restart your AetherGraph sidecar so the new settings take effect.</p> <p>If you previously used webhooks with this bot, disable them once so polling receives updates: <pre><code>curl \"https://api.telegram.org/bot&lt;YOUR_BOT_TOKEN&gt;/deleteWebhook\"\n</code></pre></p>"},{"location":"channel-setup/telegram-setup/#3-channel-keys-defaults-and-aliases","title":"3. Channel Keys, Defaults, and Aliases","text":"<p>AetherGraph uses a channel key to address targets. For Telegram, the canonical format is:</p> <pre><code>tg:chat/&lt;CHAT_ID&gt;\n</code></pre> <p>You can wire this up in startup code just like Slack:</p> <pre><code>import os\nfrom aethergraph.channels import set_default_channel, set_channel_alias\n\nTELEGRAM_CHAT_ID = os.getenv(\"TELEGRAM_CHAT_ID\", \"your-telegram-chat-id\")\ntelegram_channel_key = f\"tg:chat/{TELEGRAM_CHAT_ID}\"  # Telegram channel key format\n\n# Set as the default channel for context.channel()\nset_default_channel(telegram_channel_key)\n\n# Optional: create a friendly alias\nset_channel_alias(\"my_tg\", telegram_channel_key)\n</code></pre> <p>Usage patterns:</p> <pre><code>chan = context.channel()              # uses default Telegram chat (if set)\nawait chan.send_text(\"Hello from AetherGraph via Telegram \ud83d\udc4b\")\n\nchan2 = context.channel(\"my_tg\")     # use alias explicitly\nawait chan2.send_text(\"Message via alias\")\n\n# Or target explicitly at call time\nawait context.channel().send_text(\"Custom target\", channel=telegram_channel_key)\n</code></pre> <p>Fallback: If Telegram isn\u2019t configured, <code>context.channel()</code> falls back to <code>console:stdin</code>.</p>"},{"location":"channel-setup/telegram-setup/#4-finding-your-telegram-chat-id","title":"4. Finding Your Telegram Chat ID","text":"<ul> <li> <p>1:1 chats: Start a conversation with your bot (send <code>/start</code>). Then either:</p> <ul> <li>Check recent updates using the Bot API <code>getUpdates</code> (your chat ID appears in the payload), or</li> <li>Forward any message to a utility bot like <code>@userinfobot</code> to read the numeric ID it reports. (Recommended)</li> </ul> </li> <li> <p>Groups / supergroups: Add your bot to the group and send a message in the group. The chat ID is usually a negative number (often begins with <code>-100...</code>). Retrieve it via <code>getUpdates</code>.</p> </li> </ul> <p>See Appendix to learn how to use <code>@userinfobot</code> and <code>getUpdates</code></p>"},{"location":"channel-setup/telegram-setup/#5-add-the-bot-to-a-group-optional","title":"5. Add the Bot to a Group (Optional)","text":"<p>If you want the bot to post in a group:</p> <ol> <li>Add the bot to the group (use the group\u2019s add dialog or mention the bot and choose Add to Group).</li> <li>If your flow requires reading history or reacting to commands, ensure the bot has the needed group permissions.</li> <li>Use the group\u2019s chat ID (negative number) as your target.</li> </ol>"},{"location":"channel-setup/telegram-setup/#6-quick-test","title":"6. Quick Test","text":"<p>Create a tiny graph and send a message:</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"ping_telegram\")\nasync def ping_telegram(*, context: NodeContext):\n    chan = context.channel()  # uses default Telegram chat if configured\n    await chan.send_text(\"Ping from AetherGraph \ud83d\udef0\ufe0f\")\n    return {\"ok\": True}\n</code></pre> <p>Run the graph and confirm the message appears in your Telegram chat.</p>"},{"location":"channel-setup/telegram-setup/#notes-troubleshooting","title":"Notes &amp; Troubleshooting","text":"<ul> <li>First message pickup can be slow on some networks. Send a new message to the bot (e.g., <code>/start</code>) and re-run the test.</li> <li>If polling appears stuck, restart the sidecar.</li> <li>Treat Telegram as best-effort for now; for robust production flows, prefer Slack or your internal web UI until Telegram stabilizes.</li> </ul>"},{"location":"channel-setup/telegram-setup/#appendix-get-your-telegram-chat-id-easiest-userinfobot","title":"Appendix: Get Your Telegram Chat ID (easiest: @userinfobot)","text":"<ul> <li> <p>Private 1:1 (DM):</p> <ul> <li>Open @userinfobot and tap Start.</li> <li>It replies with Your ID:  \u2014 this is your chat ID. <li>Use it as: tg:chat/. <li> <p>Group / Supergroup:</p> <ul> <li>Add @userinfobot to the group.</li> <li>Send any message (e.g., /start or any text).</li> <li>The bot posts the Group ID (a negative number, often -100\u2026).</li> <li>Use it as: tg:chat/. <li> <p>Channel:</p> <ul> <li>Option A (forward): Post in the channel, then forward that post to @userinfobot \u2014 it replies with the Channel ID (negative number).</li> <li>Option B (temporary add): Add @userinfobot to the channel (temporarily, usually as admin), post once, and the bot will report the Channel ID.</li> </ul> </li> <p>You can remove @userinfobot after you\u2019ve captured the ID.</p> <p>Optional alternative: You can also retrieve the ID using Telegram\u2019s Bot API getUpdates and reading chat.id from the JSON. See Telegram official documents on how to use it.</p>"},{"location":"channel-setup/webhook-setup/","title":"Webhook Channel Setup &amp; Usage","text":"<p>The webhook channel lets AetherGraph send JSON payloads via HTTP POST to any service that accepts incoming webhooks (Slack Incoming Webhooks, Discord, Zapier, etc.).</p> <p>\u2705 No installation or configuration in AetherGraph \u2014 just use a webhook URL.</p> <p>\ud83d\udd14 One\u2011way only: webhooks push notifications out; they cannot receive replies or run <code>ask_*</code> prompts.</p>"},{"location":"channel-setup/webhook-setup/#when-to-use","title":"When to Use","text":"<p>Use webhooks for:</p> <ul> <li>Notifications (\"run finished\", progress updates).</li> <li>Logging / audit into external systems.</li> <li>Triggering automations (Zapier, Make) without writing adapters.</li> </ul>"},{"location":"channel-setup/webhook-setup/#key-format","title":"Key Format","text":"<pre><code>webhook:&lt;WEBHOOK_URL&gt;\n</code></pre> <p>Where <code>&lt;WEBHOOK_URL&gt;</code> is the full URL provided by your target service (Slack, Discord, Zapier, etc.).</p>"},{"location":"channel-setup/webhook-setup/#minimal-payload-what-we-send","title":"Minimal Payload (what we send)","text":"<pre><code>{\n  \"type\": \"agent.message\",\n  \"channel\": \"webhook:&lt;WEBHOOK_URL&gt;\",\n  \"text\": \"Run finished \u2705\",\n  \"content\": \"Run finished \u2705\",\n  \"meta\": {},\n  \"timestamp\": \"...\"\n}\n</code></pre> <p>Services that require a custom shape can be adapted via Zapier/Make or by transforming on the receiving side.</p>"},{"location":"channel-setup/webhook-setup/#tested-targets-examples","title":"Tested Targets (Examples)","text":""},{"location":"channel-setup/webhook-setup/#slack-incoming-webhook","title":"Slack \u2013 Incoming Webhook","text":"<ol> <li>Add Incoming Webhooks in Slack \u2192 create a webhook for a channel.</li> <li>Copy the URL like <code>https://hooks.slack.com/services/XXX/YYY/ZZZ</code>.</li> <li>Use it directly as <code>webhook:&lt;URL&gt;</code>.</li> </ol> <pre><code>SLACK_URL = \"https://hooks.slack.com/services/XXX/YYY/ZZZ\"\nchan = context.channel(f\"webhook:{SLACK_URL}\")\nawait chan.send_text(\"AetherGraph run completed \u2705\")\n</code></pre>"},{"location":"channel-setup/webhook-setup/#discord-channel-webhook","title":"Discord \u2013 Channel Webhook","text":"<ol> <li>Server Settings \u2192 Integrations \u2192 Webhooks \u2192 New Webhook \u2192 choose channel.</li> <li>Copy URL like <code>https://discord.com/api/webhooks/123/ABC...</code>.</li> <li>Use as <code>webhook:&lt;URL&gt;</code>.</li> </ol> <pre><code>DISCORD_URL = \"https://discord.com/api/webhooks/123/ABC...\"\nawait context.channel(f\"webhook:{DISCORD_URL}\").send_text(\"Experiment done \ud83c\udf89\")\n</code></pre>"},{"location":"channel-setup/webhook-setup/#zapier-catch-hook","title":"Zapier \u2013 Catch Hook","text":"<ol> <li>Create a Zap \u2192 Trigger: Webhooks by Zapier \u2192 Catch Hook.</li> <li>Copy the Catch Hook URL.</li> <li>Use as <code>webhook:&lt;URL&gt;</code> and map <code>text</code>/<code>content</code> in Zapier.</li> </ol> <pre><code>ZAP_URL = \"https://hooks.zapier.com/hooks/catch/123456/abcdef/\"\nawait context.channel(f\"webhook:{ZAP_URL}\").send_text(\"Model training completed \ud83e\uddea\")\n</code></pre> <p>The same pattern usually works with Microsoft Teams, Google Chat, Mattermost, Rocket.Chat, Zulip, or any endpoint that accepts JSON POSTs.</p>"},{"location":"channel-setup/webhook-setup/#usage-pattern-general","title":"Usage Pattern (General)","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\nWEBHOOK_URL = \"https://example.com/incoming\"\n\n@graph_fn(name=\"webhook_demo\")\nasync def webhook_demo(*, context: NodeContext):\n    chan = context.channel(f\"webhook:{WEBHOOK_URL}\")\n    await chan.send_text(\"Run finished \u2705\")\n    await chan.send_text(\"Metrics: acc=0.93, loss=0.12\")\n    return {\"notified\": True}\n</code></pre>"},{"location":"channel-setup/webhook-setup/#notes-best-practices","title":"Notes &amp; Best Practices","text":"<ul> <li>One\u2011way only: no replies/continuations; combine with other channels for interactions.</li> <li>Resilience: if a webhook returns 4xx/5xx or times out, we log the error; your graph continues by default.</li> <li>Security: treat URLs as secrets; rotate if leaked. Consider using Zapier/Make as a buffer when adapting payload shapes.</li> <li>Multiple endpoints: you can create multiple webhook channels within the same graph run.</li> </ul>"},{"location":"design/server/","title":"AetherGraph Deployment Modes &amp; Data Flow","text":"<p>This doc outlines three primary deployment modes for AetherGraph (AG), how the sidecar/server behaves in each, and how data flows between clients (Slack, browser UI, other services), AG, and storage.</p> <p>We\u2019ll describe:</p> <ul> <li>Mode 1 \u2014 Local Sidecar (Developer / Researcher)</li> <li>Mode 2 \u2014 Single-Tenant App Server (Enterprise Self-Hosted)</li> <li>Mode 3 \u2014 SaaS Control Plane + Worker Pool (Hosted by AIperture)</li> </ul> <p>For each mode, we\u2019ll specify:</p> <ul> <li>Responsibilities of the server</li> <li>Data flow for channel events (e.g., Slack) and UI</li> <li>Where storage lives and how the primitives (BlobStore, DocStore, EventLog, VectorIndex) fit</li> </ul> <p>At the end, we\u2019ll summarize how to evolve from Mode 1 \u2192 Mode 2 \u2192 Mode 3 without changing graph/tool APIs.</p>"},{"location":"design/server/#shared-concepts-across-all-modes","title":"Shared Concepts Across All Modes","text":"<p>Before diving into modes, here are the shared pieces that exist in every deployment:</p> <ul> <li> <p>AG Runtime</p> </li> <li> <p>Executes graphs and tools.</p> </li> <li> <p>Uses <code>ExecutionContext</code> and services (artifacts, memory, state, continuations, LLM, custom services, etc.).</p> </li> <li> <p>Storage primitives</p> </li> <li> <p>BlobStore \u2013 raw bytes (artifacts, bundles, large snapshots).</p> </li> <li>DocStore \u2013 keyed JSON docs (summaries, config, snapshot metadata, continuation payloads, etc.).</li> <li>EventLog \u2013 append-only events (chat turns, tool_results, state events, memory events).</li> <li> <p>VectorIndex \u2013 embeddings search for RAG and semantic retrieval.</p> </li> <li> <p>Domain facades</p> </li> <li> <p><code>ArtifactFacade</code> (ArtifactStore + ArtifactIndex)</p> </li> <li><code>MemoryFacade</code> (hot log + durable log + doc summaries + optional vector)</li> <li><code>GraphStateStore</code> (snapshots + state events)</li> <li> <p><code>ContinuationStore</code> (continuations + token/correlator indices)</p> </li> <li> <p>Channel / Interaction layer</p> </li> <li> <p>Normalises external messages into AG events (e.g. Slack, HTTP chat, WebSocket).</p> </li> <li> <p>Uses continuations + resume router for DualStage tools.</p> </li> <li> <p>HTTP/WS API surface (varies by mode)</p> </li> <li> <p><code>/api/graph/{graph_id}/run</code> \u2013 invoke a graph.</p> </li> <li><code>/api/events</code> \u2013 query events for observability.</li> <li><code>/api/events/stream</code> \u2013 live events via WS or SSE.</li> <li><code>/api/artifacts</code> \u2013 upload/download artifact bytes.</li> <li><code>/api/channel/*</code> \u2013 channel-specific endpoints (Slack, HTTP chat, etc.).</li> </ul>"},{"location":"design/server/#mode-1-local-sidecar-developer-researcher","title":"Mode 1 \u2014 Local Sidecar (Developer / Researcher)","text":"<p>Mental model:</p> <p>\u201cAG runs on my laptop as a sidecar process. It executes graphs and exposes local HTTP/WS APIs for UI &amp; tools. Storage is local.\u201d</p>"},{"location":"design/server/#responsibilities","title":"Responsibilities","text":"<ul> <li>Run the AG runtime and scheduler.</li> <li>Own all storage primitives (BlobStore, DocStore, EventLog, VectorIndex) locally.</li> <li> <p>Expose HTTP/WS endpoints on <code>localhost</code> for:</p> </li> <li> <p>Running graphs (<code>/api/graph/{id}/run</code>).</p> </li> <li>Observability (<code>/api/events</code>, <code>/api/runs</code>, <code>/api/artifacts</code>).</li> <li>Optional local UI (web frontend) and channel adapters.</li> </ul>"},{"location":"design/server/#typical-topology","title":"Typical Topology","text":"<pre><code>+-------------------------------+\n|  User Laptop                  |\n|                               |\n|  +------------------------+   |\n|  | AG Sidecar Process     |   |\n|  |  - Runtime             |   |\n|  |  - Storage primitives  |   |\n|  |  - Channel endpoints   |   |\n|  +-----------+------------+   |\n|              | HTTP/WS        |\n|         +----v-------------+  |\n|         | Local UI / CLI   |  |\n|         |  (React app,     |  |\n|         |   console, etc.) |  |\n|         +------------------+  |\n+-------------------------------+\n</code></pre> <ul> <li>All connections are local (<code>127.0.0.1</code>).</li> <li>Slack (if used) might be wired directly to the laptop via a tunnel (ngrok) or WS hack, but that\u2019s a power-user setup.</li> </ul>"},{"location":"design/server/#data-flow-examples","title":"Data Flow Examples","text":"<ol> <li>Run a graph from the terminal</li> </ol> <pre><code>User \u2192 `python my_graph.py`\n  \u2192 AG Runtime executes graph\n    \u2192 Facades write artifacts/memory/state to local FS/DB\n  \u2192 Terminal prints outputs\n</code></pre> <ol> <li>Inspect runs via local UI</li> </ol> <pre><code>Browser (localhost:3000) \u2192 /api/events?run_id=... (HTTP)\n                           /api/events/stream (WS/SSE)\n  \u2192 Sidecar reads from EventLog / DocStore\n  \u2192 UI renders run timeline &amp; artifacts\n</code></pre>"},{"location":"design/server/#when-to-use","title":"When to Use","text":"<ul> <li>Individual R&amp;D, notebooks, experiments.</li> <li>Local \u201cagent companions\u201d or sidecar tools.</li> <li>No need for remote access by default; safe by binding to <code>localhost</code> only.</li> </ul>"},{"location":"design/server/#mode-2-single-tenant-app-server-enterprise-self-hosted","title":"Mode 2 \u2014 Single-Tenant App Server (Enterprise Self-Hosted)","text":"<p>Mental model:</p> <p>\u201cThe sidecar is our main production AG server. It runs in the company\u2019s cloud or on-prem cluster and exposes AG APIs internally.\u201d</p>"},{"location":"design/server/#responsibilities_1","title":"Responsibilities","text":"<ul> <li> <p>Same as Mode 1, but now:</p> </li> <li> <p>Runs on a server/cluster inside the enterprise network.</p> </li> <li>Might have multiple replicas behind a load balancer.</li> <li> <p>Uses shared storage backends (S3, Postgres, Redis, etc.).</p> </li> <li> <p>Expose stable HTTP/WS APIs for:</p> </li> <li> <p>Internal services (e.g., <code>foo-service</code> calling <code>/api/graph/optimize_lens/run</code>).</p> </li> <li>Internal UIs (AG dashboard, custom apps).</li> <li>Channel integrations (Slack, Teams, internal chat).</li> </ul>"},{"location":"design/server/#topology","title":"Topology","text":"<pre><code>                 Enterprise Network\n+-------------------------------------------------+\n|                                                 |\n|  +----------------------+     +--------------+  |\n|  | Load Balancer        |     | Storage      |  |\n|  | (HTTPS)              |     | (S3, DB, KV) |  |\n|  +----------+-----------+     +------+-------+  |\n|             |                         ^          |\n|       +-----v----------------+        |          |\n|       |  AG App Server       |        |          |\n|       |  (one or many pods)  |        |          |\n|       |  - Runtime           |        |          |\n|       |  - Storage adapters  +--------+          |\n|       |  - Channel endpoints |                   |\n|       +----------+-----------+                   |\n|                  | HTTP/WS                       |\n|   +--------------v-------------+                 |\n|   | Internal UIs / Services    |                 |\n|   | (Dashboards, APIs, etc.)   |                 |\n|   +----------------------------+                 |\n+-------------------------------------------------+\n</code></pre>"},{"location":"design/server/#data-flow-slack-example","title":"Data Flow (Slack example)","text":"<pre><code>Slack \u2192 HTTPS \u2192 AG App Server /api/channel/slack/events\n      \u2192 AG normalizes to ChannelEvent\n      \u2192 AG runtime routes to appropriate graph / continuation\n      \u2192 Graph runs, writes events/artifacts to storage\n      \u2192 AG App Server responds to Slack via Slack Web API\n</code></pre>"},{"location":"design/server/#data-flow-internal-http-client","title":"Data Flow (internal HTTP client)","text":"<pre><code>Internal service \u2192 POST /api/graph/{id}/run {input JSON}\nAG App Server   \u2192 Executes graph\n                \u2192 Writes artifacts / events / memory\n                \u2192 Returns outputs (or run_id for async)\n</code></pre>"},{"location":"design/server/#when-to-use_1","title":"When to Use","text":"<ul> <li>Enterprise wants full control and self-hosts AG.</li> <li>AG integrates with internal systems, SSO, internal Slack.</li> <li>Scaling = add more AG App Server replicas using shared storage.</li> </ul>"},{"location":"design/server/#mode-3-saas-control-plane-worker-pool-hosted-by-aiperture","title":"Mode 3 \u2014 SaaS Control Plane + Worker Pool (Hosted by AIperture)","text":"<p>Mental model:</p> <p>\u201cAG is a cloud platform. A control-plane handles APIs, channels, and UI, while a pool of workers runs graphs. Storage is shared. Slack and UIs talk only to the control-plane.\u201d</p>"},{"location":"design/server/#responsibilities_2","title":"Responsibilities","text":"<p>Control Plane</p> <ul> <li> <p>Owns:</p> </li> <li> <p>Public APIs (<code>/api/graph/run</code>, <code>/api/channel/*</code>, <code>/api/events</code>, <code>/api/artifacts</code>).</p> </li> <li>Auth, multi-tenant routing (which workspace/tenant is this?).</li> <li>Slack/Teams/other channel integrations.</li> <li>Web UI and dashboards.</li> <li>Job dispatch to workers (via queue or internal RPC).</li> </ul> <p>Worker Plane</p> <ul> <li> <p>Multiple AG worker processes/containers:</p> </li> <li> <p>Each runs the AG runtime.</p> </li> <li>Pulls jobs from a queue (or receives them via WS/gRPC).</li> <li>Writes artifacts/memory/state to shared storage.</li> <li>Optionally opens a WS back to control-plane for live events.</li> </ul>"},{"location":"design/server/#topology-diagram","title":"Topology Diagram","text":"<pre><code>                   Internet\n                      |\n          +-----------+------------+\n          |  Control Plane (CP)    |\n          |  - Public APIs         |\n          |  - Channel endpoints   |\n          |  - UI                  |\n          |  - Auth &amp; routing      |\n          +-----+-------------+----+\n                |             |\n      Slack /   |             | HTTP/WS\n      Webhooks  |             v\n                |      +-------------+\n                |      |  Job Queue  |\n                |      +------+------+ \n                |             ^\n                |             |\n                |      (jobs: run graph X)\n                |             |\n+-----------------------------+---------------------------+\n|                   Worker Plane                           |\n|                                                         |\n|   +--------------------+     +--------------------+     |\n|   | AG Worker 1        | ... | AG Worker N        |     |\n|   | - Runtime          |     | - Runtime          |     |\n|   | - Services         |     | - Services         |     |\n|   +---------+----------+     +----------+---------+     |\n|             |                           |               |\n|             +------------+--------------+               |\n|                          |                              |\n|                     Shared Storage                      |\n|               (BlobStore, DocStore, EventLog,          |\n|                VectorIndex: S3/DB/etc.)                |\n+---------------------------------------------------------+\n</code></pre>"},{"location":"design/server/#data-flow-slack-cp-worker-slack","title":"Data Flow (Slack \u2192 CP \u2192 Worker \u2192 Slack)","text":"<ol> <li>Incoming event</li> </ol> <pre><code>Slack \u2192 CP /api/channel/slack/events\n  \u2192 CP authenticates &amp; normalizes event\n  \u2192 CP enqueues a job: {workspace, graph_id, run_id?, input}\n</code></pre> <ol> <li>Execution</li> </ol> <pre><code>Worker \u2192 pulls job from queue\n       \u2192 runs graph via AG runtime\n       \u2192 writes events to EventLog, artifacts to BlobStore, etc.\n       \u2192 optionally sends live events back to CP via WS\n</code></pre> <ol> <li>Outgoing message</li> </ol> <pre><code>CP \u2192 reads events (or receives streamed ones)\n   \u2192 detects outgoing messages for Slack\n   \u2192 calls Slack Web API using stored tokens\nSlack channel shows assistant response\n</code></pre>"},{"location":"design/server/#data-flow-browser-ui","title":"Data Flow (Browser UI)","text":"<pre><code>Browser \u2192 CP /api/graph/run (start run)\nCP      \u2192 enqueues job\nWorker  \u2192 executes, writes events/artifacts\nBrowser \u2192 CP /api/events?run_id=... (poll) or /api/events/stream (WS)\n</code></pre>"},{"location":"design/server/#local-worker-hybrid-optional-future-pattern","title":"Local Worker Hybrid (optional future pattern)","text":"<ul> <li>A user can run an AG Worker locally that connects outbound to the control-plane over WS:</li> </ul> <pre><code>Local Worker \u2192 opens WS to CP: \"I am worker for workspace X\"\nCP           \u2192 sends jobs over WS instead of cloud queue\nWorker       \u2192 runs graphs locally, writes to either:\n              - local storage, and forwards key metadata, or\n              - remote storage via HTTP APIs\n</code></pre> <p>This allows \u201cno exposed local IP\u201d while still leveraging local compute.</p>"},{"location":"design/server/#capability-summary-per-mode","title":"Capability Summary Per Mode","text":"Capability Mode 1: Local Sidecar Mode 2: Single-Tenant Server Mode 3: SaaS CP + Workers Where AG runs Laptop / single machine Single service / several replicas Control plane + many workers Storage backends Local FS / SQLite S3, Postgres, Redis (enterprise infra) Managed S3/DB, multi-tenant Channel entrypoint Localhost / optional tunnel Internal URL (Slack, internal apps) Public URL at your domain Who owns Slack app User (local dev) Enterprise (self-hosted app) You (AIperture-managed Slack app) Graph invocation CLI, localhost HTTP/WS Internal HTTP/WS Public API \u2192 queued \u2192 workers Observability Local UI/CLI via /api/events Internal dashboards via /api/events SaaS UI reading from shared EventLog + DocStore Scaling Single process Scale out app servers Scale control plane + worker pool independently"},{"location":"design/server/#how-to-think-about-evolution","title":"How to Think About Evolution","text":""},{"location":"design/server/#from-mode-1-mode-2","title":"From Mode 1 \u2192 Mode 2","text":"<ul> <li> <p>Take the existing sidecar and:</p> </li> <li> <p>Run it on a server instead of a laptop.</p> </li> <li>Swap storage adapters for cloud ones (S3/DB instead of local FS/SQLite).</li> <li> <p>Add auth, TLS, and a proper domain.</p> </li> <li> <p>Graphs, tools, and services do not need to change \u2014 they still talk to <code>ArtifactFacade</code>, <code>MemoryFacade</code>, etc.</p> </li> </ul>"},{"location":"design/server/#from-mode-2-mode-3","title":"From Mode 2 \u2192 Mode 3","text":"<ul> <li> <p>Split responsibilities into:</p> </li> <li> <p>Control Plane: keep existing HTTP/WS APIs, add multi-tenancy, job queue, auth, and UI.</p> </li> <li> <p>Workers: run a headless version of the AG runtime that:</p> <ul> <li>Listens for jobs (via queue/WS/gRPC).</li> <li>Uses the same storage adapters.</li> </ul> </li> <li> <p>Again, graph/tool APIs stay the same; only the deployment topology changes.</p> </li> </ul>"},{"location":"design/server/#takeaways","title":"Takeaways","text":"<ul> <li> <p>The sidecar you\u2019re building now is already the core of:</p> </li> <li> <p>A local dev tool (Mode 1),</p> </li> <li>A self-hosted app server (Mode 2), and</li> <li> <p>The worker &amp; API pieces of a SaaS platform (Mode 3).</p> </li> <li> <p>By keeping:</p> </li> <li> <p>storage unified via <code>BlobStore / DocStore / EventLog / VectorIndex</code>, and</p> </li> <li>interaction unified via channel + continuation APIs,</li> </ul> <p>you can change where and how many processes run AG without changing how users write graphs and tools.</p>"},{"location":"examples/1-chat-with-memory/","title":"Example: First Steps with Memory, Channel, LLM &amp; Artifacts","text":"<p>Who is this for? Folks who have never used AetherGraph (AG). We\u2019ll build a tiny chat agent that remembers what was said, responds using an LLM, and saves a session summary \u2014 all step\u2011by\u2011step.</p>"},{"location":"examples/1-chat-with-memory/#what-youll-build","title":"What you\u2019ll build","text":"<p>A stateful chat agent that:</p> <ol> <li>Stores each message as a <code>chat_turn</code> event in AG Memory.</li> <li>Reads prior events on startup to preload context.</li> <li>Chats with you via <code>context.channel()</code> (console/Slack/Web supported).</li> <li>Uses <code>context.llm()</code> to reply and later summarize the session.</li> <li>Saves the transcript + summary as an artifact (a file you can inspect).</li> </ol> <p>This mirrors real apps: assistants, experiment logs, ops runbooks, or optimization loops that need persistent context.</p>"},{"location":"examples/1-chat-with-memory/#prerequisites-2-minutes","title":"Prerequisites (2 minutes)","text":"<ul> <li>Python 3.10+</li> <li>Install AG (adjust to your package source):</li> </ul> <p><pre><code>pip install aethergraph\n</code></pre> * LLM key (e.g., OpenAI) in your environment or <code>.env</code>:</p> <p><pre><code>export OPENAI_API_KEY=sk-...\n</code></pre> * (Optional) Slack/Web UI: Not required for this tutorial; we use the console channel. You can switch to Slack/Web later without changing the agent code.</p>"},{"location":"examples/1-chat-with-memory/#glossary-ag-in-60-seconds","title":"Glossary (AG in 60 seconds)","text":"<ul> <li><code>@graph_fn</code>: A Python function that AG can schedule/run (think: a task node with helpful runtime wiring).</li> <li> <p><code>NodeContext</code>: Passed into your <code>@graph_fn</code>; gives you services:</p> </li> <li> <p><code>context.memory()</code> \u2013 record &amp; query events (structured log).</p> </li> <li><code>context.channel()</code> \u2013 input/output with the user (console/Slack/Web).</li> <li><code>context.llm()</code> \u2013 talk to an LLM provider using a unified API.</li> <li><code>context.artifacts()</code> \u2013 save/load files, JSON, blobs.</li> <li>Events (Memory): Append\u2011only records with fields like <code>kind</code>, <code>text</code>, <code>metrics</code>, <code>tags</code>, <code>stage</code>, <code>severity</code>. You choose the schema; AG stores and fetches them for you.</li> </ul> <p>Key idea: keep your logic in plain Python; treat services (memory, channel, llm, artifacts) as pluggable I/O.</p>"},{"location":"examples/1-chat-with-memory/#step-0-minimal-run-harness","title":"Step 0 \u2014 Minimal run harness","text":"<p>We\u2019ll run two functions in one process: one to seed memory, one to chat.</p> <pre><code># run_harness.py\nif __name__ == \"__main__\":\n    import asyncio\n    from aethergraph.runner import run_async\n    from aethergraph import start_server\n\n    # Start the sidecar: enables interactive I/O and resumable waits\n    url = start_server(port=8000, log_level=\"warning\")\n    print(\"Sidecar:\", url)\n\n    async def main():\n        # Same run_id so they share the same memory namespace\n        await run_async(seed_chat_memory_demo, inputs={}, run_id=\"demo_chat_with_memory\")\n        result = await run_async(chat_agent_with_memory, inputs={}, run_id=\"demo_chat_with_memory\")\n        print(\"Result:\", result)\n\n    asyncio.run(main())\n</code></pre> <p>Why a sidecar? For console/Slack/Web prompts (<code>ask_*</code>) AG uses event\u2011driven waits that the sidecar hosts. Pure compute graphs can run without it.</p>"},{"location":"examples/1-chat-with-memory/#step-1-seed-memory-why-how","title":"Step 1 \u2014 Seed memory (why &amp; how)","text":"<p>Why: Many real agents need past context on first run (previous chats, last experiment settings, etc.). We\u2019ll preload two <code>chat_turn</code> events so the agent \u201cremembers\u201d something before you type.</p> <pre><code># seed.py\nfrom aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"seed_chat_memory_demo\")\nasync def seed_chat_memory_demo(*, context: NodeContext):\n    mem = context.memory()\n    logger = context.logger()\n\n    # Each record() call writes an Event. `data` is JSON\u2011encoded into Event.text.\n    await mem.record(\n        kind=\"chat_turn\",\n        data={\"role\": \"user\", \"text\": \"We talked about integrating AetherGraph into my project.\"},\n        tags=[\"chat\", \"user\", \"seed\"], severity=2, stage=\"observe\",\n    )\n    await mem.record(\n        kind=\"chat_turn\",\n        data={\"role\": \"assistant\", \"text\": \"Start with a simple graph_fn and add services later.\"},\n        tags=[\"chat\", \"assistant\", \"seed\"], severity=2, stage=\"act\",\n    )\n\n    logger.info(\"Seeded two chat turns.\")\n    return {\"seeded\": True}\n</code></pre> <p>Design tip: Use <code>kind</code> consistently (<code>\"chat_turn\"</code>) so you can query exactly what you need later. <code>tags</code>, <code>stage</code>, and <code>severity</code> help with reporting and filtering.</p>"},{"location":"examples/1-chat-with-memory/#step-2-load-prior-events-make-memory-useful","title":"Step 2 \u2014 Load prior events (make memory useful)","text":"<p>Goal: On startup, fetch recent <code>chat_turn</code> events, decode them, and prime the chat history.</p> <pre><code># load_history.py (excerpt inside the agent)\nprevious_turns = await mem.recent_data(kinds=[\"chat_turn\"], limit=50)\nconversation = []\nfor d in previous_turns:\n    if isinstance(d, dict) and d.get(\"role\") in (\"user\", \"assistant\") and d.get(\"text\"):\n        conversation.append({\"role\": d[\"role\"], \"text\": d[\"text\"]})\n\nif conversation:\n    await chan.send_text(f\"\ud83e\udde0 I loaded {len(conversation)} prior turns. I\u2019ll use them as context.\")\nelse:\n    await chan.send_text(\"\ud83d\udc4b New session. I\u2019ll remember as we go.\")\n</code></pre> <p>Why not read raw events? <code>recent_data()</code> returns the decoded JSON payloads you originally wrote via <code>data=...</code>. It\u2019s the fastest way to get back to your domain objects.</p>"},{"location":"examples/1-chat-with-memory/#step-3-talk-to-the-user-channel-101","title":"Step 3 \u2014 Talk to the user (Channel 101)","text":"<p>Goal: Use <code>channel.ask_text()</code> to get input and <code>channel.send_text()</code> to reply. This works the same in console, Slack, or a web adapter.</p> <pre><code># channel_loop.py (excerpt inside the agent)\nwhile True:\n    user = await chan.ask_text(\"You:\")\n    if not user:\n        continue\n    if user.strip().lower() in (\"quit\", \"exit\"):\n        await chan.send_text(\"\ud83d\udc4b Ending. Let me summarize...\")\n        break\n\n    # Store the user turn in memory *and* in our local transcript\n    conversation.append({\"role\": \"user\", \"text\": user})\n    await mem.record(kind=\"chat_turn\", data={\"role\": \"user\", \"text\": user},\n                     tags=[\"chat\", \"user\"], severity=2, stage=\"observe\")\n</code></pre> <p>Why Channel? It abstracts the transport. Your agent code stays the same whether you test locally or ship to Slack/Web.</p>"},{"location":"examples/1-chat-with-memory/#step-4-call-the-llm-compact-history","title":"Step 4 \u2014 Call the LLM (compact history)","text":"<p>Goal: Build a small window from the transcript (e.g., last 10 turns) and call <code>llm.chat()</code>.</p> <pre><code># llm_reply.py (excerpt inside the agent)\nhistory_tail = conversation[-10:]\nmessages = ([{\"role\": \"system\", \"content\": \"You are a helpful, concise assistant.\"}] +\n            [{\"role\": t[\"role\"], \"content\": t[\"text\"]} for t in history_tail])\n\nreply, _usage = await llm.chat(messages=messages)\nconversation.append({\"role\": \"assistant\", \"text\": reply})\nawait mem.record(kind=\"chat_turn\", data={\"role\": \"assistant\", \"text\": reply},\n                 tags=[\"chat\", \"assistant\"], severity=2, stage=\"act\")\nawait chan.send_text(reply)\n</code></pre> <p>Why only last 10 turns? Keep prompts cheap. Memory retains all history; your prompt includes a smart slice.</p>"},{"location":"examples/1-chat-with-memory/#step-5-summarize-save-artifacts","title":"Step 5 \u2014 Summarize &amp; Save (Artifacts)","text":"<p>Goal: Generate a session summary, then persist both transcript and summary for later inspection.</p> <pre><code># summarize_and_save.py (excerpt inside the agent)\nhist_text = \"\n\".join(f\"{t['role']}: {t['text']}\" for t in conversation[-20:])\nsummary_text, _ = await llm.chat(messages=[\n    {\"role\": \"system\", \"content\": \"You write clear, concise summaries.\"},\n    {\"role\": \"user\", \"content\": \"Summarize the conversation, focusing on main topics and TODOs.\n\n\" + hist_text},\n])\nawait chan.send_text(\"\ud83d\udccc Session summary:\n\" + summary_text)\n\npayload = {\"conversation\": conversation, \"summary\": summary_text}\ntry:\n    saved = await artifacts.save_json(payload, suggested_uri=\"./chat_session_with_memory.json\")\nexcept Exception:\n    saved = None\n</code></pre> <p>Artifacts act like a project filesystem managed by AG. Save JSON, images, binaries \u2014 and load them from other runs.</p>"},{"location":"examples/1-chat-with-memory/#step-6-put-it-together-the-agent","title":"Step 6 \u2014 Put it together: the agent","text":"<p>Below is the full <code>@graph_fn</code> combining Steps 2\u20135. (Utility imports omitted for brevity.)</p> <pre><code>from typing import Any, Dict, List\nfrom aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"chat_agent_with_memory\")\nasync def chat_agent_with_memory(*, context: NodeContext):\n    logger = context.logger()\n    chan = context.channel()\n    mem = context.memory()\n    artifacts = context.artifacts()\n    llm = context.llm()\n\n    conversation: List[Dict[str, Any]] = []\n\n    # Load prior history\n    try:\n        previous_turns = await mem.recent_data(kinds=[\"chat_turn\"], limit=50)\n    except Exception:\n        previous_turns = []\n    for d in previous_turns:\n        if isinstance(d, dict) and d.get(\"role\") in (\"user\", \"assistant\") and d.get(\"text\"):\n            conversation.append({\"role\": d[\"role\"], \"text\": d[\"text\"]})\n\n    await chan.send_text(\n        f\"\ud83e\udde0 I loaded {len(conversation)} prior turns. Type 'quit' to end.\"\n        if conversation else \"\ud83d\udc4b New session. Type 'quit' to end.\"\n    )\n\n    # Chat loop\n    while True:\n        user = await chan.ask_text(\"You:\")\n        if not user:\n            continue\n        if user.strip().lower() in (\"quit\", \"exit\"):\n            await chan.send_text(\"\ud83d\udc4b Ending. Let me summarize...\")\n            break\n\n        conversation.append({\"role\": \"user\", \"text\": user})\n        await mem.record(kind=\"chat_turn\", data={\"role\": \"user\", \"text\": user},\n                         tags=[\"chat\", \"user\"], severity=2, stage=\"observe\")\n\n        history_tail = conversation[-10:]\n        messages = ([{\"role\": \"system\", \"content\": \"You are a helpful, concise assistant.\"}] +\n                    [{\"role\": t[\"role\"], \"content\": t[\"text\"]} for t in history_tail])\n        reply, _ = await llm.chat(messages=messages)\n\n        conversation.append({\"role\": \"assistant\", \"text\": reply})\n        await mem.record(kind=\"chat_turn\", data={\"role\": \"assistant\", \"text\": reply},\n                         tags=[\"chat\", \"assistant\"], severity=2, stage=\"act\")\n        await chan.send_text(reply)\n\n    # Summarize &amp; save\n    hist_text = \"\n\".join(f\"{t['role']}: {t['text']}\" for t in conversation[-20:])\n    summary_text, _ = await llm.chat(messages=[\n        {\"role\": \"system\", \"content\": \"You write clear, concise summaries.\"},\n        {\"role\": \"user\", \"content\": \"Summarize the conversation with decisions/TODOs.\n\n\" + hist_text},\n    ])\n    await chan.send_text(\"\ud83d\udccc Session summary:\n\" + summary_text)\n\n    try:\n        await artifacts.save_json({\"conversation\": conversation, \"summary\": summary_text},\n                                  suggested_uri=\"./chat_session_with_memory.json\")\n    except Exception:\n        pass\n\n    return {\"turns\": len(conversation), \"summary\": summary_text}\n</code></pre>"},{"location":"examples/1-chat-with-memory/#step-7-run-it","title":"Step 7 \u2014 Run it","text":"<p>From your shell:</p> <pre><code>python run_harness.py\n</code></pre> <p>You\u2019ll see the sidecar URL and then a prompt:</p> <pre><code>You: hello\n... assistant replies ...\nYou: quit\n</code></pre> <p>A <code>chat_session_with_memory.json</code> artifact will be saved. Rerun \u2014 the agent will preload what was said last time.</p>"},{"location":"examples/1-chat-with-memory/#variations-next-steps","title":"Variations &amp; next steps","text":"<ul> <li>Filter by time/kind/tags: <code>recent_data(kinds=[...], limit=..., since=...)</code>.</li> <li>Track metrics: add <code>metrics={...}</code> to <code>record()</code> and chart them later.</li> <li>Multiple channels: same agent works with Slack/Web by switching adapters.</li> <li>Long\u2011term summaries: store occasional <code>kind=\"session_summary\"</code> events.</li> <li>Privacy/retention: implement deletion or redaction policies per <code>tags</code> or <code>stage</code>.</li> </ul>"},{"location":"examples/1-chat-with-memory/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>No replies? Check your LLM key and provider quota.</li> <li>Ask/answer stuck? Ensure the sidecar started (see console log for URL).</li> <li>Artifact not saved? Verify write permissions in the working directory.</li> </ul>"},{"location":"examples/1-chat-with-memory/#summary-full-example-link","title":"Summary &amp; Full Example Link","text":"<p>You built a stateful chat agent without any AG prior knowledge: you logged events, recalled them, chatted via Channel, used LLM to reply/summarize, and persisted results with Artifacts.</p> <p>Full example in repo: TBD (link placeholder)</p>"},{"location":"examples/channel/","title":"Channels \u2013 Practical Usage Cheatsheet","text":"<p>This page is a lightweight guide, not a full API reference. It shows how to use channels in everyday graphs and what the most common operations look like.</p>"},{"location":"examples/channel/#1-getting-a-channel","title":"1. Getting a channel","text":"<p>In any graph, you start from the <code>NodeContext</code>:</p> <pre><code>chan = context.channel()                # default channel (often console or a default chat)\nchan_slack = context.channel(\"slack:team/T:chan/C\")\nchan_file  = context.channel(\"file:runs/demo.log\")\n</code></pre> <p>Use cases:</p> <ul> <li>No argument \u2192 send to whatever default channel you configured.</li> <li>Explicit key \u2192 target a specific channel (Slack, Telegram, file, webhook, etc.).</li> </ul> <p>You then call convenient methods on <code>chan</code>.</p>"},{"location":"examples/channel/#2-sending-messages-send_text","title":"2. Sending messages (<code>send_text</code>)","text":"<pre><code>await chan.send_text(\"Run started \ud83d\ude80\")\n</code></pre> <p>When to use:</p> <ul> <li> <p>Any time you want to tell the user something:</p> </li> <li> <p>status updates (\"loading data\u2026\"),</p> </li> <li>final results (\"accuracy = 0.93\"),</li> <li>errors, tips, or links.</li> </ul> <p>Where it goes depends on the channel:</p> <ul> <li>console \u2192 prints to the terminal.</li> <li>Slack / Telegram \u2192 sends a chat message.</li> <li>file \u2192 appends a line to a log file.</li> <li>webhook \u2192 POSTs JSON to the external URL.</li> </ul> <p>You usually don\u2019t need to worry about the return value; it\u2019s handled internally.</p>"},{"location":"examples/channel/#3-asking-for-input-ask_text","title":"3. Asking for input (<code>ask_text</code>)","text":"<pre><code>name = await context.channel().ask_text(\"What is your name?\")\nawait context.channel().send_text(f\"Nice to meet you, {name}!\")\n</code></pre> <p>When to use:</p> <ul> <li> <p>You need free-form input from a human:</p> </li> <li> <p>names, descriptions, small pieces of text,</p> </li> <li>short commands (\"yes/no\", \"option A\", etc.).</li> </ul> <p>Supported channels:</p> <ul> <li>console \u2192 reads from stdin inline.</li> <li>Slack / Telegram \u2192 sends a prompt and waits for a reply.</li> </ul> <p>Not supported / not meaningful on:</p> <ul> <li>file and webhook (those are inform-only channels).</li> </ul> <p>Behind the scenes, Slack/Telegram use continuations, so the run can be resumed when a reply arrives.</p>"},{"location":"examples/channel/#4-approvals-choices-ask_approval","title":"4. Approvals &amp; choices (<code>ask_approval</code>)","text":"<pre><code>res = await context.channel().ask_approval(\n    \"Deploy the model to production?\",\n    options=[\"Approve\", \"Reject\"],\n)\n\nif res[\"approved\"]:\n    await context.channel().send_text(\"Deployment approved \u2705\")\nelse:\n    await context.channel().send_text(\"Deployment cancelled \u274c\")\n</code></pre> <p>When to use:</p> <ul> <li> <p>You want a simple decision from the user:</p> </li> <li> <p>approve/reject,</p> </li> <li>pick from a short list of options.</li> </ul> <p>Channel behavior:</p> <ul> <li>console \u2192 shows numbered options and waits for a number/label.</li> <li>Slack / Telegram \u2192 renders real buttons; user clicks, continuation resumes.</li> </ul> <p>Again, this is not meant for file/webhook channels.</p>"},{"location":"examples/channel/#5-files-uploads-high-level","title":"5. Files &amp; uploads (high level)","text":"<p>For interactive channels (Slack, Telegram, console) and some tools, you\u2019ll often work with files directly, not just links.</p> <p>Typical high-level patterns:</p>"},{"location":"examples/channel/#51-sending-a-file-send_file","title":"5.1. Sending a file (<code>send_file</code>)","text":"<pre><code># e.g. you just generated a local report\nreport_path = \"./outputs/report.pdf\"\n\nchan = context.channel()  # default chat (Slack/Telegram/console)\nawait chan.send_file(report_path, caption=\"Here is your report \ud83d\udcce\")\n</code></pre> <p>When to use:</p> <ul> <li>You want the user to receive the actual file in their chat or UI.</li> <li>Slack/Telegram will show the file as an attachment; console/file/webhook channels may log or reference it instead, depending on implementation.</li> </ul>"},{"location":"examples/channel/#52-asking-the-user-to-upload-a-file-ask_file","title":"5.2. Asking the user to upload a file (<code>ask_file</code>)","text":"<pre><code>files = await context.channel().ask_file(\"Please upload a CSV file with your data.\")\n\n# `files` is typically a list of file references with\n# fields like name, uri, mimetype, etc.\nfor f in files:\n    await context.channel().send_text(f\"Got file: {f['name']}\")\n</code></pre> <p>When to use:</p> <ul> <li>You need the user to provide input as a file (datasets, configs, documents).</li> <li>Works best on Slack/Telegram or a web UI that supports uploads.</li> </ul>"},{"location":"examples/channel/#53-sending-buttons-for-links","title":"5.3. Sending buttons for links","text":"<p>Buttons are a convenient way to surface links (e.g. to artifacts, dashboards) instead of pasting raw URLs.</p> <p>Conceptually, you can build a message with one or more buttons that open URLs:</p> <pre><code>from aethergraph.contracts.services.channel import Button\n\nreport_url = \"https://example.com/artifacts/runs/123/report.pdf\"\n\nbuttons = {\n    \"open_report\": Button(label=\"Open report\", url=report_url),\n}\n\nchan = context.channel()\nawait chan.send_buttons(\"Your report is ready:\", buttons=buttons)\n</code></pre> <p>On rich channels (Slack/Telegram/web UI) this can render as clickable buttons; on simpler channels, it may fall back to plain text.</p> <p>Use file and button helpers when you want the user to act on artifacts directly (download, open, inspect) rather than just reading text.</p>"},{"location":"examples/channel/#6-streaming-progress-high-level","title":"6. Streaming &amp; progress (high level)","text":"<p>Some adapters (Slack, Telegram, web UI) support streaming and progress updates, so the user sees things evolve in place instead of a single final message.</p> <p>A common pattern is to use a progress helper that periodically updates a status message while your work runs:</p> <pre><code>@graph_fn(name=\"train_with_progress\")\nasync def train_with_progress(*, context: NodeContext):\n    await context.channel().send_text(\"Training started \u23f3\")\n\n    total_steps = 5\n    for step in range(1, total_steps + 1):\n        # Do some work here...\n        await context.channel().send_text(f\"Progress: {step}/{total_steps}\")\n\n    await context.channel().send_text(\"Training finished \u2705\")\n</code></pre> <p>On rich channels (Slack/Telegram/web UI), the framework can render more advanced streaming or progress UIs using specialized helpers; the core idea is the same: send updates frequently so the user sees the task moving forward.</p>"},{"location":"examples/channel/#7-putting-it-together-a-small-example","title":"7. Putting it together \u2013 a small example","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"simple_run\")\nasync def simple_run(*, context: NodeContext):\n    # 1) Notify in the default channel\n    await context.channel().send_text(\"Run started \ud83d\ude80\")\n\n    # 2) Ask the user a question (console/Slack/Telegram)\n    name = await context.channel().ask_text(\"What should we name this experiment?\")\n\n    # 3) Log the name to a file channel\n    await context.channel(\"file:runs/experiment_names.log\").send_text(name)\n\n    # 4) Optionally notify an external system via webhook\n    # await context.channel(\"webhook:https://hooks.zapier.com/hooks/catch/.../\").send_text(\n    #     f\"New experiment: {name}\"\n    # )\n\n    await context.channel().send_text(f\"All set, {name} \u2705\")\n    return {\"experiment_name\": name}\n</code></pre> <p>Use this cheatsheet as a mental model for channels:</p> <ul> <li><code>channel()</code> \u2192 pick where messages go.</li> <li><code>send_text</code> \u2192 tell the user or an external system something.</li> <li><code>ask_*</code> \u2192 only on interactive channels (console/Slack/Telegram), when you need input.</li> <li>Let higher-level tooling take care of files and streaming; the channel adapters handle the transport details.</li> </ul>"},{"location":"examples/example-list/","title":"Examples of Aethergraph Usage","text":"<p>This page is your guided tour to AetherGraph (AG) through runnable, copy\u2011pasteable examples. Start with the Hero Demos to experience a complete flow in minutes (console + LLM only). Then explore two complementary pattern pools:</p> <ul> <li>Method Showcase \u2014 small, focused snippets for individual <code>context.*</code> capabilities and services.</li> <li>Concrete Example Patterns \u2014 larger recipes that combine multiple features for real\u2011world tasks.</li> </ul> <p>This catalog evolves over time. We periodically add new demos, refine existing ones, and may adjust paths as the repo grows. The tables below always reflect the current recommended entry points.</p> <p>All the examples are in a separate example repo. Please clone the repo and set up <code>.env</code> to run the examples.</p>"},{"location":"examples/example-list/#hero-demos","title":"Hero Demos","text":"<p>Short, polished demos that tell a clear story. Each fits in a single file and returns observable outputs (messages, artifacts, or both).</p> # Demo What you\u2019ll see (story) Path 1 Chat Agent with Memory Seeds or loads chat memory \u2192 you chat \u2192 it summarizes the whole session and saves transcript + summary as artifacts. <code>demo_examples/1_chat_with_memory.py</code> 2 Channel Wizard A one\u2011function wizard that collects config using <code>ask_*</code>, shows a recap, lets you Confirm/Restart/Cancel, then saves <code>run_config.json</code>. <code>demo_examples/2_channel_wizard_interactive_workflow.py</code> 3 Optimization Loop A tiny gradient\u2011descent run that logs step metrics to Memory, writes periodic checkpoints as artifacts, and returns final params. <code>demo_examples/3_optimization_loop_with_artifacts.py</code> 4 Extend Prompt Services Agents use <code>context.prompt_store()</code> to fetch prompts and <code>context.llm_observer()</code> to log calls\u2014clean agent code, centralized control. <code>servdemo_examplesices/4_external_service_prompt_store.py</code> 5 Simple Copilot (Tool Router) A console copilot routes each query to a calculator, a summarizer, or a direct answer using a tiny LLM classifier; replies inline. <code>demo_examples/5_simple_copilot_tool_using_router.py</code> 6 Resume a Static Graph (graphify) A static graph with a slow, checkpointed node crashes mid\u2011run; re\u2011running with the same <code>run_id</code> resumes from the checkpoint and completes. <code>demo_examples/6_crash_resume_static_graph.py</code>"},{"location":"examples/example-list/#method-showcase-patterns-by-feature","title":"Method Showcase \u2014 Patterns by Feature","text":"<p>Concise examples organized by capability. Use these as copy\u2011paste starting points.</p>"},{"location":"examples/example-list/#a-interaction-channel","title":"A) Interaction (Channel)","text":"<ul> <li>A.1 <code>send_text</code> \u2014 Minimal \"hello\" send.   Path: <code>method_showcase/1_channels/1_channel_send_text.py</code></li> <li>A.2 <code>ask_text</code> \u2014 Prompt \u2192 await user reply \u2192 echo.   Path: <code>method_showcase/1_channels/2_channel_ask_text.py</code></li> <li>A.3 <code>ask_approval</code> \u2014 Present options, branch on choice.   Path: <code>method_showcase/1_channels/3_channel_ask_approval.py</code></li> <li>A.4 Channel setup \u2014 Configure Slack / Telegram / Webhook adapters and key conventions.   Path: <code>method_showcase/1_channels/4_channel_setup.py</code></li> <li>A.5 Method walk\u2011through \u2014 Linear tour of all channel methods using a small \u201cportfolio\u201d demo.   Path: <code>method_showcase/1_channels/5_channel_method_walkthrough.py</code></li> <li>A.6 File channel \u2014 Ask for files, read them, return attachments/links.   Path: <code>method_showcase/1_channels/6_file_channel_example.py</code></li> </ul>"},{"location":"examples/example-list/#b-memory-artifacts","title":"B) Memory &amp; Artifacts","text":"<ul> <li>B.1 Memory \u2014 record &amp; query \u2014 Append events and fetch recent history.   Path: <code>method_showcase/2_artifacts_memory/1_memory_record.py</code></li> <li>B.2 Memory \u2014 typed results \u2014 Store structured tool results for fast retrieval.   Path: <code>method_showcase/2_artifacts_memory/2_memory_write_result.py</code></li> <li>B.3 Artifacts \u2014 save text/JSON \u2014 Persist text/JSON and auto\u2011index.   Path: <code>method_showcase/2_artifacts_memory/3_artifacts_save_txt_json.py</code></li> <li>B.4 Artifacts \u2014 save &amp; search files \u2014 Save arbitrary files and rank/search across agents.   Path: <code>method_showcase/2_artifacts_memory/4_artifacts_save_search_files.py</code></li> <li>B.5 Memory \u2192 RAG \u2014 Promote memory events into a vector index and answer with citations.   Path: <code>method_showcase/2_artifacts_memory/5_memory_rag.py</code></li> </ul>"},{"location":"examples/example-list/#c-logger-kv","title":"C) Logger &amp; KV","text":"<ul> <li>C.1 Logger \u2014 Structured logs, levels, and tracing.   Path: <code>method_showcase/3_logger_kv/1_logger_usage.py</code></li> <li>C.2 KV \u2014 Store and fetch run\u2011level globals.   Path: <code>method_showcase/3_logger_kv/2_kv_usage.py</code></li> </ul>"},{"location":"examples/example-list/#d-llm","title":"D) LLM","text":"<ul> <li>D.1 Chat \u2014 One\u2011shot and multi\u2011turn <code>context.llm().chat(...)</code>.   Path: <code>method_showcase/4_llm/1_llm_chat.py</code></li> <li>D.2 Multiple profiles \u2014 Configure and use multiple LLM clients.   Path: <code>method_showcase/4_llm/2_setup_multiple_llm_profiles.py</code></li> <li>D.3 Inline profile \u2014 Set keys/models at runtime (no preregistration).   Path: <code>method_showcase/4_llm/3_inline_llm_setup.py</code></li> <li>D.4 Raw API \u2014 Use <code>.raw()</code> to pass advanced payloads directly.   Path: <code>method_showcase/4_llm/4_passing_raw_api.py</code></li> </ul>"},{"location":"examples/example-list/#e-rag","title":"E) RAG","text":"<ul> <li>E.1 Ingest files \u2014 Upsert docs into a vector index.   Path: <code>method_showcase/5_rag/ingest_files.py</code></li> <li>E.2 Inspect corpora \u2014 List/inspect existing corpora.   Path: <code>method_showcase/5_rag/list_inspect_corpora.py</code></li> <li>E.3 Search &amp; answer \u2014 Retrieve \u2192 synthesize with citations.   Path: <code>method_showcase/5_rag/search_retrieve_answer.py</code></li> </ul>"},{"location":"examples/example-list/#f-extending-services","title":"F) Extending Services","text":"<ul> <li>F.1 Materials DB \u2014 Register a materials property service for quick lookups.   Path: <code>method_showcase/6_extending_services/1_material_db.py</code></li> <li>F.2 HuggingFace model \u2014 Expose an external model as a service.   Path: <code>method_showcase/6_extending_services/2_huggingface_model.py</code></li> <li>F.3 Rate limiting \u2014 Token\u2011bucket wrapper with retries/backoff.   Path: <code>method_showcase/6_extending_services/3_rate_limit.py</code></li> <li>F.4 Access NodeContext in a service \u2014 Patterns for using <code>context</code> safely inside services.   Path: <code>method_showcase/6_extending_services/4_access_ctx_in_service.py</code></li> <li>F.5 Critical sections / mutex \u2014 Design a thread\u2011safe service API.   Path: <code>method_showcase/6_extending_services/5_critical_mutex_usage.py</code></li> </ul>"},{"location":"examples/example-list/#g-concurrency","title":"G) Concurrency","text":"<ul> <li>G.1 <code>graphify</code> map\u2011reduce \u2014 Fan\u2011out + fan\u2011in with static graphs.   Path: <code>method_showcase/7_concurrency/graphify_map_reduce.py</code></li> <li>G.2 <code>graph_fn</code> concurrency \u2014 Launch concurrent tasks with a concurrency cap.   Path: <code>method_showcase/7_concurrency/graph_fn_concurrency.py</code></li> </ul>"},{"location":"examples/example-list/#concrete-example-patterns-advanced-recipes","title":"Concrete Example Patterns \u2014 Advanced Recipes","text":"<p>Bigger compositions that mirror real\u2011world tasks.</p>"},{"location":"examples/example-list/#a-state-resumption","title":"A) State &amp; Resumption","text":"<ul> <li>A.1 Crash &amp; Resume (static graph) \u2014 Design a static graph so a long node can checkpoint and resume indefinitely using the same <code>run_id</code>.   Path: <code>pattern_examples/1_state_resumption/1_resume_external_waits.py</code> Run: <code>python 1_resume_external_waits.py run_id</code> </li> <li>A.2 Long Job Monitor \u2014 Submit to <code>job_manager</code>, poll with backoff, surface failures via channel, let the user Retry/Abort.   Path: <code>pattern_examples/1_state_resumption/2_long_job_monitor.py</code></li> </ul>"},{"location":"examples/example-list/#b-agent-patterns","title":"B) Agent Patterns","text":"<ul> <li>B.1 Chain\u2011of\u2011Thought Agent \u2014 Two\u2011stage flow: CoT reasoning trace \u2192 final concise answer (optionally store traces).   Path: <code>pattern_examples/2_agent_patterns/1_chain_of_thought.py</code></li> <li>B.2 ReAct Agent \u2014 Thought \u2192 Action (tool) \u2192 Observation loop until \u201cFinish\u201d, with a compact history state.   Path: <code>pattern_examples/agents/2_simple_react.py</code></li> <li>B.3 RL Policy as <code>graph_fn</code> \u2014 Treat a graph as a policy: observation in, action out; log trajectories via Memory/Artifacts.   Path: <code>pattern_examples/agents/3_reinforcement_learnining_policy.py</code></li> </ul>"},{"location":"examples/example-list/#c-applied-endtoend","title":"C) Applied End\u2011to\u2011End","text":"<ul> <li>C.1 CSV Analyzer (interactive) \u2014 Ask for a CSV, summarize shape (rows/cols), headers, and simple stats; return findings and artifacts.   Path: <code>pattern_examples/3_e2e_patterns/1_csv_analyzer.py</code></li> <li>C.2 Paper \u2192 Implementation Sketch (interactive) \u2014 Ask for a text/PDF, sketch a Python implementation, run sandboxed, return logs/files as artifacts.   Path: <code>pattern_examples/3_e2e_patterns/2_paper_implementation_sketch.py</code></li> <li>C.3 Deep Research Agent \u2014 Use <code>graphify</code> concurrency to parallelize retrieval/summarization and synthesize findings.   Path: <code>pattern_examples/3_e2e_patterns/3_deep_research_agent.py</code></li> </ul>"},{"location":"examples/llm/","title":"<code>context.llm()</code> Mini Tutorial","text":"<p>This is a quick guide to the <code>NodeContext.llm()</code> helper in Aethergraph: how it\u2019s wired, how to use profiles, how to configure providers in <code>.env</code>, and how embeddings + RAG fit in.</p>"},{"location":"examples/llm/#1-what-contextllm-is-and-supported-providers","title":"1. What <code>context.llm()</code> is (and supported providers)","text":"<p><code>context.llm()</code> is a convenience accessor that gives you a ready-to-use LLM client for the current run.</p> <p>Under the hood it returns a <code>GenericLLMClient</code>, which implements:</p> <ul> <li><code>await client.chat(messages, **kw)</code>  \u2013 text (and multimodal) chat</li> <li><code>await client.embed(texts, **kw)</code>    \u2013 embeddings</li> </ul> <p>The client is created from config (<code>LLMSettings</code>) and currently supports these providers:</p> <ul> <li><code>openai</code>    \u2013 OpenAI-compatible API (GPT\u20114o, GPT\u20115, etc.)</li> <li><code>anthropic</code> \u2013 Claude 3.x models via <code>/v1/messages</code></li> <li><code>google</code>    \u2013 Gemini models via <code>generateContent</code> / <code>embedContent</code></li> <li><code>lmstudio</code>  \u2013 LM Studio\u2019s OpenAI-compatible local server</li> <li><code>ollama</code>    \u2013 Ollama\u2019s OpenAI-compatible local server</li> </ul> <p>(Plus <code>azure</code> and <code>openrouter</code> if you configure them.)</p> <p>Important: This layer is meant as a lightweight helper. It is not exhaustively tested across every model and provider variant. If you hit an edge case or a model with a quirky API, you can:</p> <ul> <li>Call the HTTP APIs yourself with plain Python, or</li> <li>Register your own extended service and bypass <code>context.llm()</code> for that use case.</li> </ul>"},{"location":"examples/llm/#2-what-is-a-profile","title":"2. What is a \u201cprofile\u201d?","text":"<p>A profile is a named LLM configuration: provider + model + optional base URL, timeout, and secrets.</p> <p>Structurally (from config):</p> <pre><code>class LLMProfile(BaseModel):\n    provider: Provider = \"openai\"      # e.g. \"openai\", \"anthropic\", \"google\", \"lmstudio\", \"ollama\"\n    model: str = \"gpt-4o-mini\"         # chat / reasoning model\n    embed_model: str | None = None      # optional embedding model\n    base_url: str | None = None\n    timeout: float = 60.0\n    azure_deployment: str | None = None\n    api_key: SecretStr | None = None\n    api_key_ref: str | None = None\n\nclass LLMSettings(BaseModel):\n    enabled: bool = True\n    default: LLMProfile = LLMProfile()\n    profiles: Dict[str, LLMProfile] = Field(default_factory=dict)\n</code></pre> <p>At runtime, these become <code>GenericLLMClient</code> instances keyed by profile name:</p> <ul> <li><code>\"default\"</code> \u2013 always present</li> <li><code>\"gemini\"</code>, <code>\"anthropic\"</code>, <code>\"local\"</code>, etc. \u2013 optional extra profiles</li> </ul>"},{"location":"examples/llm/#3-using-contextllm-inside-a-node","title":"3. Using <code>context.llm()</code> inside a node","text":""},{"location":"examples/llm/#31-basic-usage-default-profile","title":"3.1 Basic usage (default profile)","text":"<pre><code>async def hello_world(*, context: NodeContext, input_text: str):\n    llm = context.llm()  # same as context.llm(\"default\")\n\n    text, usage = await llm.chat([\n        {\"role\": \"system\", \"content\": \"Be brief.\"},\n        {\"role\": \"user\", \"content\": f\"Say hi back to: {input_text}\"},\n    ])\n\n    return {\"reply\": text, \"usage\": usage}\n</code></pre>"},{"location":"examples/llm/#32-using-a-named-profile","title":"3.2 Using a named profile","text":"<pre><code>async def multi_vendor_demo(*, context: NodeContext, query: str):\n    openai_client   = context.llm(\"default\")   # e.g. OpenAI\n    gemini_client   = context.llm(\"gemini\")    # Google Gemini profile\n    anthropic_client = context.llm(\"anthropic\") # Claude profile\n\n    o_text, _ = await openai_client.chat([\n        {\"role\": \"user\", \"content\": f\"OpenAI: {query}\"},\n    ])\n\n    g_text, _ = await gemini_client.chat([\n        {\"role\": \"user\", \"content\": f\"Gemini: {query}\"},\n    ])\n\n    a_text, _ = await anthropic_client.chat([\n        {\"role\": \"user\", \"content\": f\"Claude: {query}\"},\n    ])\n\n    return {\"openai\": o_text, \"gemini\": g_text, \"anthropic\": a_text}\n</code></pre>"},{"location":"examples/llm/#33-embeddings-via-embed","title":"3.3 Embeddings via <code>embed()</code>","text":"<pre><code>async def embed_example(*, context: NodeContext, texts: list[str]):\n    # Use default profile\u2019s embedding model (see .env config section below)\n    client = context.llm()\n\n    vectors = await client.embed(texts)\n    # vectors: List[List[float]]\n\n    return {\"embeddings\": vectors}\n</code></pre>"},{"location":"examples/llm/#34-reasoning-effort-for-gpt5-openai","title":"3.4 Reasoning effort for GPT\u20115 (OpenAI)","text":"<p>For OpenAI GPT\u20115-family models (e.g. <code>gpt-5-nano</code>, <code>gpt-5-mini</code>, etc.), you can pass an optional <code>reasoning_effort</code> kwarg:</p> <pre><code>async def gpt5_reasoning_example(*, context: NodeContext):\n    client = context.llm(\"default\")  # configured with a gpt-5-* model\n\n    text, usage = await client.chat(\n        [\n            {\"role\": \"system\", \"content\": \"Think step-by-step.\"},\n            {\"role\": \"user\", \"content\": \"Explain why 2 + 2 = 4.\"},\n        ],\n        reasoning_effort=\"high\",  # \"low\" | \"medium\" | \"high\" (OpenAI GPT\u20115 only)\n    )\n\n    return {\"answer\": text, \"usage\": usage}\n</code></pre> <p>For non\u2011GPT\u20115 models or non\u2011OpenAI providers, <code>reasoning_effort</code> is ignored.</p>"},{"location":"examples/llm/#4-configuring-profiles-via-env","title":"4. Configuring profiles via <code>.env</code>","text":""},{"location":"examples/llm/#41-default-profile","title":"4.1 Default profile","text":"<p><code>AppSettings</code> uses:</p> <ul> <li><code>env_prefix=\"AETHERGRAPH_\"</code></li> <li><code>env_nested_delimiter=\"__\"</code></li> </ul> <p>So the default LLM profile is configured by env vars like:</p> <pre><code># Turn LLM on globally\nAETHERGRAPH_LLM__ENABLED=true\n\n# Default profile (\"default\")\nAETHERGRAPH_LLM__DEFAULT__PROVIDER=openai\nAETHERGRAPH_LLM__DEFAULT__MODEL=gpt-4o-mini\nAETHERGRAPH_LLM__DEFAULT__EMBED_MODEL=text-embedding-3-small\nAETHERGRAPH_LLM__DEFAULT__BASE_URL=https://api.openai.com/v1\nAETHERGRAPH_LLM__DEFAULT__TIMEOUT=60\nAETHERGRAPH_LLM__DEFAULT__API_KEY=sk-proj-...\n</code></pre>"},{"location":"examples/llm/#42-additional-named-profiles","title":"4.2 Additional named profiles","text":"<p>Profiles live under <code>llm.profiles[\"NAME\"]</code>, which maps to env like:</p> <pre><code># Gemini profile\nAETHERGRAPH_LLM__PROFILES__GEMINI__PROVIDER=google\nAETHERGRAPH_LLM__PROFILES__GEMINI__MODEL=gemini-1.5-pro-latest\nAETHERGRAPH_LLM__PROFILES__GEMINI__EMBED_MODEL=text-embedding-004\nAETHERGRAPH_LLM__PROFILES__GEMINI__TIMEOUT=60\nAETHERGRAPH_LLM__PROFILES__GEMINI__API_KEY=AIzaSy...\n\n# Anthropic profile\nAETHERGRAPH_LLM__PROFILES__ANTHROPIC__PROVIDER=anthropic\nAETHERGRAPH_LLM__PROFILES__ANTHROPIC__MODEL=claude-3-7-sonnet-20250219\nAETHERGRAPH_LLM__PROFILES__ANTHROPIC__TIMEOUT=60\nAETHERGRAPH_LLM__PROFILES__ANTHROPIC__API_KEY=ant-...\n\n# LM Studio local profile\nAETHERGRAPH_LLM__PROFILES__LOCAL__PROVIDER=lmstudio\nAETHERGRAPH_LLM__PROFILES__LOCAL__MODEL=your-lmstudio-model-id\nAETHERGRAPH_LLM__PROFILES__LOCAL__BASE_URL=http://localhost:1234/v1\nAETHERGRAPH_LLM__PROFILES__LOCAL__TIMEOUT=60\n</code></pre> <p>Then you can access them via:</p> <pre><code>context.llm()               # default\ncontext.llm(\"gemini\")      # Gemini\ncontext.llm(\"anthropic\")   # Claude\ncontext.llm(\"local\")       # LM Studio\n</code></pre> <p>Note: <code>embed_model</code> is optional. If omitted, <code>embed()</code> will fall back to <code>EMBED_MODEL</code> env or a sensible default (e.g. <code>text-embedding-3-small</code>).</p>"},{"location":"examples/llm/#5-embeddings-rag-default-behavior","title":"5. Embeddings &amp; RAG default behavior","text":"<p>For RAG and other embedding-heavy workflows, Aethergraph\u2019s helpers (e.g. index / vector store integration) will typically use the default profile\u2019s embedding configuration, i.e.:</p> <ul> <li><code>AETHERGRAPH_LLM__DEFAULT__EMBED_MODEL</code> if set,</li> <li>otherwise <code>EMBED_MODEL</code> env var,</li> <li>otherwise a built\u2011in default like <code>text-embedding-3-small</code>.</li> </ul> <p>So if you want to control which embedding model is used for global RAG, set:</p> <pre><code>AETHERGRAPH_LLM__DEFAULT__EMBED_MODEL=text-embedding-3-small\n</code></pre> <p>or override per call:</p> <pre><code>vectors = await context.llm().embed(texts, model=\"text-embedding-3-large\")\n</code></pre>"},{"location":"examples/llm/#6-adding-overriding-profiles-at-runtime","title":"6. Adding / overriding profiles at runtime","text":"<p>You don\u2019t have to declare everything in <code>.env</code>. You can also create or update profiles in code at runtime.</p>"},{"location":"examples/llm/#61-quick-runtime-profile-with-llm_set_key","title":"6.1 Quick runtime profile with <code>llm_set_key</code>","text":"<p><code>NodeContext.llm_set_key()</code> is a convenience for creating or updating a profile in memory:</p> <pre><code>async def runtime_profile_demo(*, context: NodeContext):\n    # Create/update profile \"runtime-openai\" on the fly\n    context.llm_set_key(\n        provider=\"openai\",\n        model=\"gpt-4o-mini\",               # NEW: model included for convenience\n        api_key=\"sk-proj-...\",            # in-memory only\n        profile=\"runtime-openai\",\n    )\n\n    client = context.llm(\"runtime-openai\")\n\n    text, _ = await client.chat([\n        {\"role\": \"user\", \"content\": \"Hello from runtime profile!\"},\n    ])\n\n    return {\"reply\": text}\n</code></pre> <p>This does not persist anything to disk or secrets store; it\u2019s only for the current process.</p>"},{"location":"examples/llm/#62-fully-configuring-a-profile-via-llm","title":"6.2 Fully configuring a profile via <code>llm()</code>","text":"<p>You can also configure a profile in one shot using <code>context.llm()</code> with overrides:</p> <pre><code>async def llm_inline_config_demo(*, context: NodeContext):\n    client = context.llm(\n        profile=\"lab\",\n        provider=\"google\",\n        model=\"gemini-1.5-pro-latest\",\n        api_key=\"AIzaSy...\",\n        base_url=\"https://generativelanguage.googleapis.com\",\n        timeout=60.0,\n    )\n\n    text, _ = await client.chat([\n        {\"role\": \"user\", \"content\": \"Hi from the lab profile!\"},\n    ])\n\n    return {\"reply\": text}\n</code></pre> <p>If the profile doesn\u2019t exist, it will be created. If it exists, it will be updated in place.</p>"},{"location":"examples/llm/#7-provider-specific-config-notes","title":"7. Provider-specific config notes","text":""},{"location":"examples/llm/#71-openai-provideropenai","title":"7.1 OpenAI (<code>provider=\"openai\"</code>)","text":"<p>Required:</p> <ul> <li><code>AETHERGRAPH_LLM__...__API_KEY</code> \u2013 or <code>OPENAI_API_KEY</code> if using env-based fallback.</li> </ul> <p>Optional / defaults:</p> <ul> <li><code>base_url</code> \u2013 defaults to <code>https://api.openai.com/v1</code>.</li> <li><code>timeout</code> \u2013 defaults to <code>60</code> seconds.</li> <li><code>model</code> \u2013 any chat/vision/reasoning model (e.g. <code>gpt-4o-mini</code>, <code>gpt-4o</code>, <code>gpt-5-nano</code>).</li> <li><code>embed_model</code> \u2013 e.g. <code>text-embedding-3-small</code>.</li> </ul>"},{"location":"examples/llm/#72-anthropic-provideranthropic","title":"7.2 Anthropic (<code>provider=\"anthropic\"</code>)","text":"<p>Required:</p> <ul> <li><code>api_key</code> \u2013 <code>ANTHROPIC_API_KEY</code> or <code>AETHERGRAPH_LLM__...__API_KEY</code>.</li> <li><code>model</code> \u2013 e.g. <code>claude-3-7-sonnet-20250219</code>.</li> </ul> <p>Optional / defaults:</p> <ul> <li><code>base_url</code> \u2013 defaults to <code>https://api.anthropic.com</code>.</li> <li><code>timeout</code> \u2013 defaults to <code>60</code>.</li> </ul> <p>Anthropic does not support embeddings via this client. <code>embed()</code> will raise <code>NotImplementedError</code> for <code>provider=\"anthropic\"</code>.</p>"},{"location":"examples/llm/#73-google-gemini-providergoogle","title":"7.3 Google / Gemini (<code>provider=\"google\"</code>)","text":"<p>Required:</p> <ul> <li><code>api_key</code> \u2013 <code>AETHERGRAPH_LLM__...__API_KEY</code>.</li> <li><code>model</code> \u2013 e.g. <code>gemini-1.5-pro-latest</code> for chat.</li> </ul> <p>Optional / defaults:</p> <ul> <li><code>base_url</code> \u2013 defaults to <code>https://generativelanguage.googleapis.com</code>.</li> <li><code>embed_model</code> \u2013 e.g. <code>text-embedding-004</code>.</li> </ul> <p>Endpoints used:</p> <ul> <li>Chat: <code>POST /v1/models/{model}:generateContent</code>.</li> <li>Embeddings: <code>POST /v1/models/{embed_model}:embedContent</code>.</li> </ul>"},{"location":"examples/llm/#74-lm-studio-providerlmstudio","title":"7.4 LM Studio (<code>provider=\"lmstudio\"</code>)","text":"<p>LM Studio exposes an OpenAI-compatible server.</p> <p>Required:</p> <ul> <li><code>base_url</code> \u2013 usually <code>http://localhost:1234/v1</code> (or whatever the LM Studio UI shows).</li> <li><code>model</code> \u2013 the LM Studio model ID (shown in the UI).</li> </ul> <p>Optional:</p> <ul> <li>No API key is required by default.</li> </ul> <p>Endpoints used:</p> <ul> <li>Chat: <code>POST {base_url}/chat/completions</code>.</li> <li>Embeddings: <code>POST {base_url}/embeddings</code>.</li> </ul>"},{"location":"examples/llm/#75-ollama-providerollama","title":"7.5 Ollama (<code>provider=\"ollama\"</code>)","text":"<p>Ollama also provides an OpenAI-compatible mode.</p> <p>Required/Defaults:</p> <ul> <li><code>base_url</code> \u2013 defaults to <code>http://localhost:11434/v1</code> if not set.</li> <li><code>model</code> \u2013 an Ollama model name (e.g. <code>llama3</code>, <code>mistral</code>, etc. configured in Ollama).</li> </ul> <p>Optional:</p> <ul> <li>Usually no API key.</li> </ul> <p>Endpoints used:</p> <ul> <li>Chat: <code>POST {base_url}/chat/completions</code>.</li> <li>Embeddings: <code>POST {base_url}/embeddings</code>.</li> </ul>"},{"location":"examples/llm/#8-when-not-to-use-contextllm","title":"8. When not to use <code>context.llm()</code>","text":"<p><code>context.llm()</code> is intentionally thin and opinionated. You might want to bypass it when:</p> <ul> <li>You need cutting-edge / experimental API features that aren\u2019t wired yet.</li> <li>You want a very custom request/response shape.</li> <li>You\u2019re targeting a provider that isn\u2019t in the built-in list.</li> </ul> <p>In those cases you can:</p> <ul> <li>Use <code>httpx</code> (or the vendor\u2019s official SDK) directly inside your node, or</li> <li>Wrap your own client as a separate service and inject it into <code>NodeServices</code>.</li> </ul> <p>The built-in <code>llm()</code> helper gives you a fast \u201chappy path\u201d for common providers and models, without preventing you from going lower-level when you need to.</p>"},{"location":"examples/memory-artifact/","title":"Memory &amp; Artifact Mini Examples","text":"<p>These snippets assume you\u2019re inside an async function where you already have:</p> <pre><code>mem: MemoryFacade = context.memory()\narts: ArtifactFacade = context.artifacts()\n</code></pre> <p>They\u2019re meant as supplementary examples, not main docs.</p>"},{"location":"examples/memory-artifact/#1-events-101-what-is-saved-and-how-to-read-it","title":"1. Events 101 \u2013 what is saved and how to read it","text":""},{"location":"examples/memory-artifact/#11-recording-a-simple-event","title":"1.1 Recording a simple event","text":"<pre><code># Record a simple user message as an event.\nevt = await mem.record(\n    kind=\"user_msg\",\n    data={\"role\": \"user\", \"text\": \"How do I use AetherGraph?\"},\n    tags=[\"chat\", \"demo\"],\n    severity=2,\n    stage=\"observe\",\n)\n\nprint(\"Event ID:\", evt.event_id)\nprint(\"Kind:\", evt.kind)\nprint(\"Text payload (JSON string):\", evt.text)\n</code></pre> <p>What happens:</p> <ul> <li><code>record(...)</code> JSON-serializes <code>data</code> and stores it in <code>evt.text</code>.</li> <li>Adds scope fields like <code>session_id</code>, <code>run_id</code>, <code>graph_id</code>, <code>node_id</code>, <code>agent_id</code>.</li> <li>Appends the <code>Event</code> to HotLog (recent buffer) and Persistence (JSONL).</li> </ul> <p>Conceptually, an Event is:</p> <p>\u201cSomething happened (tool call, chat turn, metric, etc.), scoped to this session/run.\u201d</p>"},{"location":"examples/memory-artifact/#12-reading-raw-events-vs-decoded-data","title":"1.2 Reading raw events vs decoded data","text":"<pre><code># Raw events (Event objects)\nfrom aethergraph.contracts.services.memory import Event\n\nevents: list[Event] = await mem.recent(kinds=[\"user_msg\"], limit=10)\nfor e in events:\n    print(\"Raw event kind:\", e.kind, \"text:\", e.text)\n\n# Decoded data (whatever you passed as data=...)\ndata_items = await mem.recent_data(kinds=[\"user_msg\"], limit=10)\nfor d in data_items:\n    print(\"Decoded data:\", d)   # dict: {\"role\": \"...\", \"text\": \"...\"}\n</code></pre> <ul> <li><code>recent(...)</code> \u2192 <code>list[Event]</code> (full event objects).</li> <li><code>recent_data(...)</code> \u2192 <code>list[Any]</code> using the JSON-in-<code>text</code> convention of <code>record()</code>.</li> </ul> <p>Users who just want \u201cthe thing I logged\u201d should use <code>recent_data</code>.</p>"},{"location":"examples/memory-artifact/#2-write_result-indices-structured-toolagent-outputs","title":"2. <code>write_result</code> &amp; indices \u2013 structured tool/agent outputs","text":"<p><code>write_result</code> is a convenience for logging structured outputs from a tool/agent and updating indices so you can ask things like:</p> <ul> <li>\u201cWhat was the last value named <code>result</code>?\u201d</li> <li>\u201cWhat are the latest outputs for <code>tool.calculator</code>?\u201d</li> </ul>"},{"location":"examples/memory-artifact/#21-recording-a-tool-result","title":"2.1 Recording a tool result","text":"<pre><code># Imagine a tiny calculator tool\ninputs = [\n    {\"name\": \"expression\", \"kind\": \"text\", \"value\": \"1 + 2 * 3\"},\n]\noutputs = [\n    {\"name\": \"result\", \"kind\": \"number\", \"value\": 7},\n]\n\nevt = await mem.write_result(\n    topic=\"tool.calculator\",      # identifier for this tool/agent\n    inputs=inputs,\n    outputs=outputs,\n    tags=[\"tool\", \"calculator\"],\n    metrics={\"latency_ms\": 12.3},\n    message=\"Evaluated 1 + 2 * 3\",\n)\n\nprint(\"tool_result event_id:\", evt.event_id)\nprint(\"Kind:\", evt.kind)   # \"tool_result\"\nprint(\"Tool:\", evt.tool)   # \"tool.calculator\"\n</code></pre>"},{"location":"examples/memory-artifact/#22-reading-via-indices","title":"2.2 Reading via indices","text":"<pre><code># 1) Last output value by name (fast)\nlast_result = await mem.get_last_value(\"result\")\nprint(\"get_last_value('result') -&gt;\", last_result)\n# e.g. {\"name\":\"result\",\"kind\":\"number\",\"value\":7}\n\n# 2) Latest reference outputs by kind (e.g. \"number\", \"json\", \"uri\")\nnumber_refs = await mem.get_latest_values_by_kind(\"number\", limit=5)\nprint(\"get_latest_values_by_kind('number'):\", number_refs)\n# e.g. [{\"name\":\"result\",\"kind\":\"number\",\"value\":7}, ...]\n\n# 3) Last outputs for a given topic (tool/agent)\ncalc_outputs = await mem.get_last_outputs_for_topic(\"tool.calculator\")\nprint(\"get_last_outputs_for_topic('tool.calculator'):\", calc_outputs)\n# e.g. {\"result\": 7, \"latency_ms\": 12.3, ...} (depends on index impl)\n</code></pre> <p>Purpose of <code>write_result</code>:</p> <ul> <li>Normalizes tool/agent outputs into a <code>tool_result</code> event.</li> <li>Keeps HotLog + Persistence in sync.</li> <li>Updates indices so other code can quickly answer questions about latest outputs.</li> </ul>"},{"location":"examples/memory-artifact/#3-artifacts-101-save-list-search-best","title":"3. Artifacts 101 \u2013 save, list, search, best","text":"<p>An Artifact is an immutable asset:</p> <ul> <li>Models, reports, checkpoints, metrics files, directories, etc.</li> <li>Stored via an <code>AsyncArtifactStore</code> and indexed via <code>AsyncArtifactIndex</code>.</li> </ul> <p>In agents, you normally access them through ArtifactFacade via <code>context.artifacts()</code>.</p>"},{"location":"examples/memory-artifact/#31-save-small-text-json-artifacts","title":"3.1 Save small text &amp; JSON artifacts","text":"<pre><code># Save a plain-text log\nlog_art = await arts.save_text(\n    \"This is a tiny experiment log.\",\n    suggested_uri=\"./logs/experiment_001.txt\",\n)\nprint(\"Log artifact URI:\", log_art.uri)\n\n# Save structured metrics as JSON\nmetrics_art = await arts.save_json(\n    {\"epoch\": 3, \"train_loss\": 0.42, \"val_loss\": 0.55},\n    suggested_uri=\"./metrics/exp001_epoch3.json\",\n)\nprint(\"Metrics artifact URI:\", metrics_art.uri)\n</code></pre>"},{"location":"examples/memory-artifact/#32-save-a-file-with-kindlabelsmetrics","title":"3.2 Save a file with kind/labels/metrics","text":"<pre><code>checkpoint_path = \"./checkpoints/exp001_step100.pt\"\n\nckpt_art = await arts.save(\n    checkpoint_path,\n    kind=\"model_checkpoint\",\n    labels={\"experiment\": \"exp001\", \"step\": \"100\"},\n    metrics={\"val_loss\": 0.55},\n    suggested_uri=\"./checkpoints/exp001_step100.pt\",\n    pin=True,   # mark as important/keep\n)\n\nprint(\"Checkpoint id:\", ckpt_art.id)\nprint(\"Checkpoint kind:\", ckpt_art.kind)\nprint(\"Checkpoint labels:\", ckpt_art.labels)\nprint(\"Checkpoint metrics:\", ckpt_art.metrics)\n</code></pre>"},{"location":"examples/memory-artifact/#4-listing-searching-artifacts-scope-labels-metrics","title":"4. Listing &amp; searching artifacts (scope, labels, metrics)","text":""},{"location":"examples/memory-artifact/#41-list-all-artifacts-for-this-run","title":"4.1 List all artifacts for this run","text":"<pre><code>arts_in_run = await arts.list(scope=\"run\")\nprint(\"Artifacts in this run:\", [a.uri for a in arts_in_run])\n</code></pre>"},{"location":"examples/memory-artifact/#42-search-by-kind-label","title":"4.2 Search by kind + label","text":"<pre><code>exp_ckpts = await arts.search(\n    kind=\"model_checkpoint\",\n    labels={\"experiment\": \"exp001\"},\n    scope=\"run\",\n)\nprint(\"Exp001 checkpoints:\", [a.uri for a in exp_ckpts])\n</code></pre> <ul> <li><code>kind</code> narrows by artifact type.</li> <li><code>labels</code> filters by label key/value.</li> <li> <p><code>scope</code> controls implicit filters:</p> </li> <li> <p><code>\"run\"</code> = current run only (default).</p> </li> <li><code>\"graph\"</code> / <code>\"node\"</code> = more specific.</li> <li><code>\"project\"</code> / <code>\"all\"</code> = wider.</li> </ul>"},{"location":"examples/memory-artifact/#43-selecting-the-best-artifact-by-metric","title":"4.3 Selecting the \"best\" artifact by metric","text":"<pre><code>best_ckpt = await arts.best(\n    kind=\"model_checkpoint\",\n    metric=\"val_loss\",\n    mode=\"min\",                # minimize validation loss\n    scope=\"run\",\n    filters={\"experiment\": \"exp001\"},\n)\n\nif best_ckpt:\n    print(\"Best checkpoint (by val_loss):\", best_ckpt.uri, best_ckpt.metrics)\nelse:\n    print(\"No checkpoint found.\")\n</code></pre> <p>Here, <code>best(...)</code> asks the index to:</p> <ul> <li>Filter by <code>kind</code> + <code>filters</code> (labels).</li> <li>Select the artifact with min or max on the given <code>metric</code>.</li> </ul>"},{"location":"examples/memory-artifact/#5-loading-artifacts-and-turning-uris-into-paths","title":"5. Loading artifacts and turning URIs into paths","text":""},{"location":"examples/memory-artifact/#51-load-payload-back-from-the-store","title":"5.1 Load payload back from the store","text":"<pre><code># If the artifact payload is JSON\nmetrics = await arts.load_artifact(metrics_art.uri)\nprint(\"Loaded metrics json:\", metrics)\n\n# If it's bytes (e.g., a binary checkpoint)\nckpt_bytes = await arts.load_artifact_bytes(ckpt_art.uri)\nprint(\"Loaded checkpoint size:\", len(ckpt_bytes))\n</code></pre>"},{"location":"examples/memory-artifact/#52-convert-artifact-uri-to-local-filesystem-path","title":"5.2 Convert artifact URI to local filesystem path","text":"<pre><code># Turn an artifact URI into a local file path (for external libs)\nlocal_ckpt_path = arts.to_local_file(ckpt_art)\nprint(\"Local checkpoint path:\", local_ckpt_path)\n\n# Same idea for directories:\n# local_dir = arts.to_local_dir(dir_artifact)\n</code></pre> <p>These helpers are handy when your artifacts are tracked as <code>file://...</code> URIs but some library expects a plain <code>str</code> path.</p>"},{"location":"examples/memory-artifact/#6-combining-memory-artifacts","title":"6. Combining memory + artifacts","text":"<p>Typical pattern:</p> <ol> <li>Agent runs a job.</li> <li>Saves results as artifacts.</li> <li>Logs a <code>tool_result</code> event with artifact URIs in outputs.</li> <li>Later, uses memory indices + artifact search/load to inspect results.</li> </ol> <pre><code># 1) Save metrics as an artifact\nmetrics_art = await arts.save_json(\n    {\"epoch\": 10, \"train_loss\": 0.21, \"val_loss\": 0.24},\n    suggested_uri=\"./metrics/exp002_epoch10.json\",\n)\n\n# 2) Log a structured result referencing the artifact\nawait mem.write_result(\n    topic=\"trainer.exp002\",\n    outputs=[\n        {\"name\": \"final_val_loss\", \"kind\": \"number\", \"value\": 0.24},\n        {\"name\": \"metrics_uri\", \"kind\": \"uri\", \"value\": metrics_art.uri},\n    ],\n    tags=[\"training\", \"exp002\"],\n    message=\"Training finished for exp002\",\n    metrics={\"epoch\": 10},\n)\n\n# 3) Later: quickly get last trainer outputs via indices\ntrainer_outs = await mem.get_last_outputs_for_topic(\"trainer.exp002\")\nprint(\"Trainer last outputs:\", trainer_outs)\n# e.g. {\"final_val_loss\": 0.24, \"metrics_uri\": \"file://.../metrics/exp002_epoch10.json\"}\n\n# 4) Load the metrics artifact via the recorded URI\nloaded_metrics = await arts.load_artifact(trainer_outs[\"metrics_uri\"])\nprint(\"Loaded metrics from artifact:\", loaded_metrics)\n</code></pre> <p>This example shows how memory (events + indices) and artifacts work together:</p> <ul> <li>Memory tells you what happened last and which artifact URIs matter.</li> <li>ArtifactFacade lets you search, rank, and then actually load those files.</li> </ul>"},{"location":"examples/memory-artifact/#7-rag-memory-turning-events-into-searchable-knowledge","title":"7. RAG + Memory \u2013 turning events into searchable knowledge","text":"<p>RAG (Retrieval-Augmented Generation) here is wired through MemoryFacade to let you:</p> <ol> <li>Create or bind to a corpus (a logical collection of documents/chunks).</li> <li>Promote events (e.g., tool results, chat summaries) into that corpus.</li> <li>Search / answer questions over it using an LLM.</li> <li>Optionally snapshot or compact the corpus over time.</li> </ol> <p>These examples assume <code>mem: MemoryFacade</code> is configured with a <code>RAGFacade</code>.</p>"},{"location":"examples/memory-artifact/#71-binding-to-a-corpus-projectsessionrun","title":"7.1 Binding to a corpus (project/session/run)","text":"<pre><code># Common pattern: bind to a project-level corpus.\ncorpus_id = await mem.rag_bind(scope=\"project\")\nprint(\"Using corpus:\", corpus_id)\n\n# Or a session-specific corpus\nsession_corpus = await mem.rag_bind(scope=\"session\")\nprint(\"Session corpus:\", session_corpus)\n\n# Or an explicitly named key (stable across runs if you reuse it)\nteam_corpus = await mem.rag_bind(scope=\"project\", key=\"team-alpha-notes\")\nprint(\"Team corpus:\", team_corpus)\n</code></pre> <ul> <li> <p><code>scope</code> controls how the corpus is keyed:</p> </li> <li> <p><code>\"project\"</code> \u2192 tied to workspace/project.</p> </li> <li><code>\"session\"</code> \u2192 tied to session_id.</li> <li><code>\"run\"</code> \u2192 tied to particular run (more ephemeral).</li> <li>You can override with <code>corpus_id=</code> directly if you already know the ID.</li> </ul>"},{"location":"examples/memory-artifact/#72-promoting-events-into-rag-event-doc","title":"7.2 Promoting events into RAG (event \u2192 doc)","text":"<p>You can convert existing memory events into RAG documents with <code>rag_promote_events()</code>.</p> <pre><code>corpus_id = await mem.rag_bind(scope=\"project\")\n\n# Promote recent high-signal tool_result events into the corpus\nstats = await mem.rag_promote_events(\n    corpus_id=corpus_id,\n    where={\n        \"kinds\": [\"tool_result\"],   # filter by Event.kind\n        \"limit\": 200,\n    },\n    policy={\n        \"min_signal\": 0.3,           # ignore low-signal noise\n        \"chunk\": {\"size\": 800, \"overlap\": 120},  # (if your RAG index supports chunking)\n    },\n)\n\nprint(\"Promoted events stats:\", stats)\n# e.g. {\"added\": 12, \"chunks\": 48, \"index\": \"SomeIndexImpl\"}\n</code></pre> <p>What happens:</p> <ul> <li><code>rag_promote_events</code> pulls events (via <code>recent</code> or your provided <code>events</code> list).</li> <li> <p>For each event, it builds a document:</p> </li> <li> <p><code>text</code> from <code>Event.text</code> (or a JSON of inputs/outputs/metrics).</p> </li> <li><code>title</code> + <code>labels</code> derived from kind/tool/stage/tags.</li> <li>Upserts docs into the RAG index.</li> <li>Logs a <code>tool_result</code> under topic <code>rag.promote.&lt;corpus_id&gt;</code> for traceability.</li> </ul> <p>You can also pass <code>events=</code> yourself if you already filtered them manually.</p>"},{"location":"examples/memory-artifact/#73-direct-upsert-of-custom-docs-bypassing-events","title":"7.3 Direct upsert of custom docs (bypassing events)","text":"<p>If you just have ad-hoc docs (e.g., notes, specs), you can call <code>rag_upsert</code>:</p> <pre><code>corpus_id = await mem.rag_bind(scope=\"project\")\n\ndocs = [\n    {\n        \"text\": \"AetherGraph is a framework for building agentic graphs.\",\n        \"title\": \"AG overview\",\n        \"labels\": {\"topic\": \"overview\", \"source\": \"docs\"},\n    },\n    {\n        \"text\": \"MemoryFacade coordinates HotLog, Persistence, Indices, and optional RAG.\",\n        \"title\": \"MemoryFacade design\",\n        \"labels\": {\"topic\": \"memory\", \"source\": \"notes\"},\n    },\n]\n\nupsert_stats = await mem.rag_upsert(corpus_id=corpus_id, docs=docs)\nprint(\"RAG upsert stats:\", upsert_stats)\n</code></pre> <p>This bypasses events entirely and goes straight to documents.</p>"},{"location":"examples/memory-artifact/#74-searching-the-corpus","title":"7.4 Searching the corpus","text":"<p>Once docs are in the corpus, you can run semantic/hybrid search:</p> <pre><code>corpus_id = await mem.rag_bind(scope=\"project\")\n\nhits = await mem.rag_search(\n    corpus_id=corpus_id,\n    query=\"How does the memory system work?\",\n    k=5,\n    filters={\"topic\": \"memory\"},  # optional label filter\n    mode=\"hybrid\",                 # or \"dense\"\n)\n\nfor h in hits:\n    print(\"Score:\", h[\"score\"])\n    print(\"Title:\", h[\"meta\"].get(\"title\"))\n    print(\"Text snippet:\", h[\"text\"][:120], \"...\")\n    print(\"Labels:\", h[\"meta\"].get(\"labels\"))\n    print(\"---\")\n</code></pre> <p><code>rag_search</code> returns a list of serializable dicts:</p> <ul> <li><code>text</code>  \u2013 chunk text.</li> <li><code>meta</code>  \u2013 metadata (labels, title, etc.).</li> <li><code>score</code> \u2013 similarity/relevance score.</li> </ul>"},{"location":"examples/memory-artifact/#75-rag-answer-retrieval-llm-citations","title":"7.5 RAG answer \u2013 retrieval + LLM + citations","text":"<p>For \u201cask a question over everything in the corpus\u201d you use <code>rag_answer</code>:</p> <pre><code>corpus_id = await mem.rag_bind(scope=\"project\")\n\nanswer = await mem.rag_answer(\n    corpus_id=corpus_id,\n    question=\"Summarize how MemoryFacade and ArtifactFacade work together.\",\n    style=\"concise\",          # or \"detailed\"\n    with_citations=True,\n    k=6,\n)\n\nprint(\"Answer:\\n\", answer.get(\"answer\"))\nprint(\"Citations:\")\nfor c in answer.get(\"resolved_citations\", []):\n    print(\"- From doc:\", c.get(\"doc_id\"), \"score=\", c.get(\"score\"))\n</code></pre> <p><code>rag_answer</code> will:</p> <ol> <li>Run retrieval over the corpus.</li> <li>Call the LLM with retrieved chunks.</li> <li>Return an <code>answer</code> plus <code>resolved_citations</code>.</li> <li>Log a <code>tool_result</code> under topic <code>rag.answer.&lt;corpus_id&gt;</code> with outputs    and usage metrics (via <code>write_result</code>).</li> </ol> <p>These helpers are optional, but they show how the RAG integration fits the same pattern as memory + artifacts:</p> <ul> <li>Memory: events + indices for \u201cwhat happened and when?\u201d.</li> <li>Artifacts: big immutable assets (files, bundles) with labels/metrics.</li> <li>RAG: a semantic index over the content of your events/docs, used by   your agents via standard tools (<code>rag_search</code>, <code>rag_answer</code>, etc.).</li> </ul>"},{"location":"key-concepts/agent-via-graph-fn/","title":"Agents via <code>@graph_fn</code>","text":"<p>This chapter introduces agents in AetherGraph through the <code>@graph_fn</code> decorator. You\u2019ll learn how <code>@tool</code> functions become nodes on the fly, when and why to use async functions, and how to chain or nest them to form structured yet reactive agentic workflows.</p>"},{"location":"key-concepts/agent-via-graph-fn/#1-what-is-a-graph_fn","title":"1. What is a <code>graph_fn</code>?","text":"<p>A <code>graph_fn</code> turns a plain Python function into an agent with access to rich context services\u2014channel, memory, artifacts, logger, and more. It runs in the normal Python runtime by default; no DAG is captured automatically when you invoke it. For most interactive or agentic workflows, this lightweight mode is ideal: you get an ergonomic async function with context utilities for I/O, persistence, and orchestration without committing to graph capture.</p>"},{"location":"key-concepts/agent-via-graph-fn/#function-shape","title":"Function shape","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"example\")\nasync def example(x: int, *, context: NodeContext):\n    # Access runtime services from the context\n    await context.channel().send_text(f\"x={x}\")\n    return {\"y\": x + 1}\n</code></pre> <ul> <li>Define your own API through standard parameters.</li> <li>Include <code>*, context</code> to access the <code>NodeContext</code>; if omitted, nothing is injected.</li> </ul> <p>Minimal example:</p> <pre><code>@graph_fn(name=\"hello_agent\")\nasync def hello_agent(name: str = \"world\", *, context: NodeContext):\n    await context.channel().send_text(f\"\ud83d\udc4b Hello, {name}!\")\n    context.memory().record(kind=\"usr_data\", data={\"name\": name})\n    context.logger().info(\"Greeted user\", extra={\"name\": name})\n    return {\"message\": f\"Hello, {name}\"}\n</code></pre> <p>Key idea: <code>@graph_fn</code> provides a reactive agent interface\u2014async execution with contextual power\u2014while keeping runtime overhead minimal. Nodes are only added when you explicitly use <code>@tool</code> or call other graphs.</p>"},{"location":"key-concepts/agent-via-graph-fn/#2-tools-nodes-on-the-fly","title":"2. Tools: nodes on the fly","text":"<p>The <code>@tool</code> decorator marks a Python function as a tool node. When called inside a <code>graph_fn</code>, the runtime creates a node on the fly and records its inputs/outputs for provenance, inspection, or future resumptions.</p> <p>Rule of thumb: for exploratory, reactive development, call regular Python functions freely. Reach for <code>@tool</code> when you need traceable state, durability, or resume checkpoints.</p>"},{"location":"key-concepts/agent-via-graph-fn/#example-a-simple-sum-tool","title":"Example: a simple sum tool","text":"<pre><code>from typing import List\nfrom aethergraph import tool\n\n@tool(outputs=[\"total\"])\ndef sum_vec(xs: List[float]) -&gt; dict:\n    return {\"total\": float(sum(xs))}\n</code></pre> <p>Use inside a <code>graph_fn</code>:</p> <pre><code>@graph_fn(name=\"tool_demo\")\nasync def tool_demo(values: list[float], *, context: NodeContext):\n    stats = {\"n\": len(values)}             # executed inline\n    out = sum_vec(values)                  # \u2190 captured as a node\n    await context.channel().send_text(f\"n={stats['n']}, sum={out['total']}\")\n    return {\"total\": out[\"total\"]}\n</code></pre> <p>You can mix normal Python code and <code>@tool</code> calls seamlessly. Only <code>@tool</code> calls create nodes.</p> <p>To inspect the implicit graph created during execution, call <code>graph_fn.last_graph</code> \u2014 it returns the captured <code>TaskGraph</code> for visualization or reuse.</p>"},{"location":"key-concepts/agent-via-graph-fn/#3-async-first-chaining-nesting-and-concurrency","title":"3. Async-first: chaining, nesting, and concurrency","text":"<p>AetherGraph adopts async-first design because agents often:</p> <ul> <li>Wait for user input (<code>ask_text</code>, <code>ask_approval</code>)</li> <li>Perform I/O (HTTP, file writes, DB queries)</li> <li>Launch parallel sub-tasks</li> </ul>"},{"location":"key-concepts/agent-via-graph-fn/#chaining-and-nesting-graph_fns","title":"Chaining and nesting <code>graph_fn</code>s","text":"<p>You can call one <code>graph_fn</code> from another. Each call creates a child subgraph node:</p> <pre><code>@graph_fn(name=\"step1\")\nasync def step1(x: int, *, context: NodeContext) -&gt; dict:\n    return {\"y\": x + 1}\n\n@graph_fn(name=\"step2\")\nasync def step2(y: int, *, context: NodeContext) -&gt; dict:\n    return {\"z\": y * 2}\n\n@graph_fn(name=\"pipeline\")\nasync def pipeline(x: int, *, context: NodeContext) -&gt; dict:\n    a = await step1(x)       # \u2192 child node\n    b = await step2(a[\"y\"]) # \u2192 child node\n    return {\"z\": b[\"z\"]}\n</code></pre>"},{"location":"key-concepts/agent-via-graph-fn/#fan-out-concurrency","title":"Fan-out concurrency","text":"<p>Launch multiple subgraphs concurrently with <code>asyncio.gather</code>:</p> <pre><code>import asyncio\n\n@graph_fn(name=\"concurrent_steps\")\nasync def concurrent_steps(a: int, b: int, *, context: NodeContext) -&gt; dict:\n    r1, r2 = await asyncio.gather(step1(a), step2(b))\n    return {\"r1\": r1[\"y\"], \"r2\": r2[\"z\"]}\n</code></pre> <p>This pattern enables natural fan-out/fan-in parallelism within a single reactive agent.</p>"},{"location":"key-concepts/agent-via-graph-fn/#4-running-a-graph_fn","title":"4. Running a <code>graph_fn</code>","text":"<p>You can execute a <code>graph_fn</code> directly from async code or through the provided runners.</p>"},{"location":"key-concepts/agent-via-graph-fn/#option-a-direct-await","title":"Option A \u2013 Direct await","text":"<pre><code># In an async function\nresult = await pipeline(3)\n</code></pre>"},{"location":"key-concepts/agent-via-graph-fn/#option-b-synchronous-helper","title":"Option B \u2013 Synchronous helper","text":"<p><pre><code>from aethergraph.runner import run\nfinal = run(pipeline, inputs={\"x\": 3})\n</code></pre> This is preferred in Jupyter Notebook. </p>"},{"location":"key-concepts/agent-via-graph-fn/#option-c-explicit-async-runner","title":"Option C \u2013 Explicit async runner","text":"<pre><code>from aethergraph.runner import run_async\n# In an async function\nresult = await run_async(pipeline, inputs={\"x\": 3})\n</code></pre> <p>The <code>run_*</code> helpers drive the event loop and normalize execution for both reactive and static graphs.</p>"},{"location":"key-concepts/agent-via-graph-fn/#5-summary","title":"5. Summary","text":"<ul> <li><code>@graph_fn</code> wraps a Python function into an async agent with an injected <code>NodeContext</code> exposing rich runtime services.</li> <li>Execution stays in normal Python until you invoke <code>@tool</code> or another <code>graph_fn</code>\u2014only those create nodes.</li> <li><code>@tool</code> functions let you capture intermediate steps for provenance and durability.</li> <li>Agents are composable: call one <code>graph_fn</code> from another or fan out with <code>asyncio.gather</code>.</li> <li>Use <code>run()</code> or <code>run_async()</code> for simple orchestration; prefer plain calls + context for lightweight workflows.</li> </ul> <p>AetherGraph\u2019s agent model combines Pythonic simplicity with event-driven introspection\u2014reactive first, deterministic when needed.</p>"},{"location":"key-concepts/artifacts-memory/","title":"Artifacts and Memory","text":"<p>This chapter covers two foundational pillars of AetherGraph\u2019s runtime: Artifacts and Memory. Together, they form the provenance backbone \u2014 making every result, file, and intermediate step traceable, reproducible, and retrievable long after execution.</p> <p>Mental model: Artifacts capture what was produced; Memory captures what happened (events, results, metrics) and why (context, summaries, links).</p>"},{"location":"key-concepts/artifacts-memory/#1-why-artifacts-memory-exist","title":"1. Why Artifacts &amp; Memory Exist","text":"<p>Most Python workflows scatter outputs across temp folders and logs with no consistent linkage. AetherGraph fixes this by binding everything to the active run/graph/node and exposing consistent, high\u2011level APIs for saving and recalling state.</p> Concern Manual management With AetherGraph Provenance Files &amp; logs scattered; hard to link Every record stamped with <code>{run_id, graph_id, node_id}</code> + tool metadata Reproducibility Filenames drift; env unknown Content\u2011addressed + typed records \u2192 deterministic recall Discoverability Grep and guess Query by <code>kind</code>, <code>labels</code>, <code>metrics</code>, scope; ask \u201cbest by metric\u201d Durability Ad\u2011hoc paths; stale temp dirs CAS store + index; pins; export/replay Collaboration Tribal conventions Shared schema (URIs/records) + searchable index <p>Takeaway: Use artifacts for durable assets; use memory for structured, queryable history. Both are scoped to your execution so you can reconstruct the story of a run.</p>"},{"location":"key-concepts/artifacts-memory/#2-artifacts-persistent-assets","title":"2. Artifacts \u2014 Persistent Assets","text":"<p>Artifacts are immutable, content\u2011addressed assets (CAS) produced or consumed by agents/tools: files, directories, JSON payloads, or serialized objects.</p>"},{"location":"key-concepts/artifacts-memory/#why-artifacts-vs-manual-files","title":"Why Artifacts (vs. manual files)?","text":"<ul> <li>Content\u2011addressed: the URI reflects the content (CAS) \u2014 no silent overwrites, no need for manual naming.</li> <li>Typed + labeled: add <code>kind</code>, <code>labels</code>, and <code>metrics</code> to organize results.</li> <li>Indexed: query by scope/labels or rank by metric. </li> <li>Provenance\u2011stamped: <code>{run_id, graph_id, node_id, tool_name, tool_version}</code> baked in.</li> <li>Portable: <code>to_local_path(uri)</code> resolves for local or remote stores.</li> </ul>"},{"location":"key-concepts/artifacts-memory/#architecture","title":"Architecture","text":""},{"location":"key-concepts/artifacts-memory/#core-api","title":"Core API","text":"Method Purpose <code>stage()</code> / <code>stage_dir()</code> Reserve a temp path for producing files/dirs safely. <code>save()</code> Save an existing path and index it. Returns an artifact with <code>uri</code>. <code>save_text()</code> Store small text payloads. <code>save_json()</code> Store a JSON payload. <code>writer()</code> Context manager to stream\u2011write binary content; atomically indexes on close. <code>list()</code> / <code>search()</code> / <code>best()</code> Query and rank artifacts by descriptors or metrics. <code>pin()</code> Mark as retained (skip cleanup policies). <code>to_local_path()</code> Resolve a CAS URI to a local filesystem path."},{"location":"key-concepts/artifacts-memory/#examples","title":"Examples","text":"<p>Save a file</p> <pre><code>@graph_fn(name=\"produce_artifact\", outputs=[\"report_uri\"])\nasync def produce_artifact(*, context):\n    art = await context.artifacts().save(\n        path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\": \"A\"}\n    )\n    return {\"report_uri\": art.uri}\n</code></pre> <p>Search a past artfiact</p> <pre><code>@graph_fn(name=\"search_reports\", outputs=[\"top_uri\"])\nasync def search_reports(*, context):\n    results = await context.artifacts().search(\n        kind=\"report\", labels={\"exp\": \"A\"}\n    )\n    return {\"top_uri\": results[0].uri if results else None}\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#3-memory-structured-event-result-log","title":"3. Memory \u2014 Structured Event &amp; Result Log","text":"<p>Memory is a unified fa\u00e7ade for recording, persisting, and querying events during an agent\u2019s lifetime: raw logs, typed results, metrics, and their relationships with artifacts. </p>"},{"location":"key-concepts/artifacts-memory/#why-memory-design-intent","title":"Why Memory (design intent)","text":"<ul> <li>Contextual recall: agents can react based on recent or historical state.</li> <li>Typed outputs: <code>write_result</code> records semantic outputs with names/kinds/values.</li> <li>RAG\u2011ready: promote events to a vector index for retrieval\u2011augmented answers.</li> <li>Analytics: retrieve last actions for logical connection, track trends, or export for traceability.</li> </ul>"},{"location":"key-concepts/artifacts-memory/#architecture_1","title":"Architecture","text":""},{"location":"key-concepts/artifacts-memory/#core-api_1","title":"Core API","text":"Method Purpose <code>record_raw()</code> Append a low\u2011level event. <code>record()</code> Convenience structured logging. <code>write_result()</code> Log a typed output; updates indices. <code>recent()</code> / <code>recent_data()</code> Fetch most recent events / event data <code>last_by_name()</code> Get the latest output value by name. <code>rag_bind()</code> / <code>rag_promote_events()</code> / <code>rag_answer()</code> RAG lifecycle helpers (requires LLM)."},{"location":"key-concepts/artifacts-memory/#examples_1","title":"Examples","text":"<p>Record an event</p> <pre><code>@graph_fn(name=\"remember_output\", outputs=[\"y\"])\nasync def remember_output(x: int, *, context):\n    y = x + 1\n    await context.memory().record(kind=\"cal.result\", data={\"y\": y})\n    return {\"y\": y}\n</code></pre> <p>Recall + summarization</p> <pre><code>recent = await context.memory().recent(limit=10) # return list of events\n</code></pre> <p>Promote to RAG</p> <pre><code>corpus = await context.memory().rag_bind()\nawait context.memory().rag_promote_events(corpus_id=corpus)\nans = await context.memory().rag_answer(corpus_id=corpus, question=\"What was the best run?\")\n</code></pre>"},{"location":"key-concepts/artifacts-memory/#4-artifacts-memory-better-together","title":"4. Artifacts \u00d7 Memory \u2014 Better Together","text":"<p>Artifacts and Memory reference each other: results and metrics point to artifact URIs; artifact metadata references the node that produced them. This bi\u2011directional linking enables:</p> <ul> <li>Reconstructing the full story of a result (inputs \u2192 tools \u2192 outputs \u2192 files).</li> <li>Ranking/searching results across runs/experiments.</li> <li>Efficient clean\u2011up strategies (e.g., keep pinned/best; GC the rest).</li> </ul>"},{"location":"key-concepts/artifacts-memory/#5-extensibility-external-systems","title":"5. Extensibility &amp; External Systems","text":"<p>AetherGraph\u2019s built\u2011ins for Artifacts and Memory are part of the OSS core runtime and are not swappable in place. That is intentional: we rely on their stable semantics for provenance, lineage, and tooling. If you need custom memory or storage systems (local or cloud), see Extending Context Services for <code>Service</code> APIs.</p>"},{"location":"key-concepts/artifacts-memory/#summary","title":"Summary","text":"<ul> <li>Artifacts make outputs durable, searchable, and reproducible with CAS URIs and rich indexing.</li> <li>Memory records the event stream and typed results for contextual recall, analytics, and RAG.</li> <li>Together they provide end\u2011to\u2011end provenance and effortless \u201ctime travel\u201d across runs.</li> </ul> <p>See also: <code>context.artifacts()</code> \u00b7 <code>context.memory()</code> \u00b7 <code>context.rag()</code> \u00b7 External Context Services</p>"},{"location":"key-concepts/channels-interaction/","title":"Channels and Interaction","text":"<p>A channel is how an agent communicates with the outside world \u2014 Slack, Telegram, Console, Web, or any other adapter. The <code>context.channel()</code> method returns a ChannelSession, a lightweight helper that provides a consistent Python API for sending and receiving messages, buttons, files, streams, and progress updates \u2014 regardless of which adapter you use.</p> <p>Default behavior: If no adapters are configured, AetherGraph automatically uses the console (<code>\"console:stdin\"</code>) as the default channel for input/output. To target Slack, Telegram, or Web, see Channel Setup section; your agent code remains unchanged in all channels.</p> <p>In short: Switch communication targets freely. The agent logic stays identical.</p>"},{"location":"key-concepts/channels-interaction/#1-what-is-a-channel","title":"1. What Is a Channel?","text":"<p>A channel is a routing target for interaction. It allows you to interact with an Agent inside a Python function. </p> <p>You can specify a channel key or alias (e.g., <code>\"slack:#research\"</code>) or rely on the system default. See Channel Setup for non-console key setup. </p>"},{"location":"key-concepts/channels-interaction/#resolution-order","title":"Resolution Order","text":"<ol> <li>Per-call override: <code>await context.channel().send_text(\"hi\", channel=\"slack:#alerts\")</code></li> <li>Bound session key: <code>ch = context.channel(\"slack:#research\"); await ch.send_text(\"hi\")</code></li> <li>Bus default: taken from <code>services.channels.get_default_channel_key()</code></li> <li>Fallback: <code>console:stdin</code></li> </ol>"},{"location":"key-concepts/channels-interaction/#2-quick-start","title":"2. Quick Start","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"channel_demo\")\nasync def channel_demo(*, context):\n    ch = context.channel(\"slack:#research\")\n    await ch.send_text(\"Starting experiment\u2026\")\n    resp = await ch.ask_approval(\"Proceed?\", options=[\"Yes\", \"No\"])\n    if resp[\"approved\"]:\n        await ch.send_text(\"\u2705 Launching run.\")\n</code></pre> <p>Channels handle both output and input asynchronously \u2014 messages, approvals, file uploads, and more \u2014 using cooperative waits under the hood.</p>"},{"location":"key-concepts/channels-interaction/#3-core-methods","title":"3. Core Methods","text":"<p>Availability depends on the adapter\u2019s capabilities (e.g., file uploads are not supported in the console channel).</p> Method Purpose <code>send_text()</code>  / <code>ask_text()</code> Send/ask a plain text message. <code>send_file()</code> / <code>ask_file()</code> Upload/ask a file. <code>ask_approval()</code> Request approval or a choice. <code>send_buttons()</code> Send buttons to UI with links <code>stream()</code> Open a streaming session for incremental updates. <code>progress()</code> Stream a progress bar <code>get_last_uploads()</code> Fetch uploaded files from UI at anytime <p>All <code>ask_*</code> methods use event-driven continuations, ensuring replies are properly correlated to their originating node.</p> <p>For exact usage, refer to <code>context.channel()</code> API. </p>"},{"location":"key-concepts/channels-interaction/#4-concurrency-and-fan-out","title":"4. Concurrency and Fan-Out","text":"<p>You can launch multiple concurrent asks in the same bound channel session and correlate the results:</p> <pre><code>import asyncio\n\n@graph_fn(name=\"concurrent_asks\")\nasync def concurrent_asks(*, context):\n    ch = context.channel(\"slack:#research\")\n\n    async def one(tag):\n        name = await ch.ask_text(f\"[{tag}] What\u2019s your name?\")\n        await ch.send_text(f\"[{tag}] Thanks, {name}!\")\n        return {tag: name}\n\n    a, b = await asyncio.gather(one(\"A\"), one(\"B\"))\n    return {\"names\": a | b}\n</code></pre>"},{"location":"key-concepts/channels-interaction/#5-extensibility","title":"5. Extensibility","text":"<p>The channel interface can be extended to support any platform with a compatible API (HTTP, WebSocket, SDK). In practice, the inbound method for resuming interactions depends heavily on the target platform\u2019s event model.</p> <ul> <li>For notification-only channels, the API is straightforward \u2014 send events, no continuations.</li> <li>For interactive channels (e.g., Slack, Telegram, Web), resumptions rely on correlation IDs and continuation stores.</li> </ul> <p>In the OSS edition, AetherGraph currently includes built-in support for Console, Slack, Telegram, and generic Webhooks. We will release adapter protocal API for extension and support for additional adapters in future releases.</p>"},{"location":"key-concepts/channels-interaction/#summary","title":"Summary","text":"<ul> <li>Channels unify all interaction patterns (text, files, approvals, progress, and streaming) under one async API.</li> <li>Default channel is console; others (Slack, Telegram, Web) are pluggable.</li> <li>All <code>ask_*</code> methods suspend execution via event-driven continuations, resuming seamlessly upon reply.</li> <li>Channels are adapter-agnostic and fully extensible \u2014 swap backends, not code.</li> </ul> <p>Write once, interact anywhere \u2014 your agents stay Pythonic, event\u2011driven, and platform\u2011neutral.</p>"},{"location":"key-concepts/concurrency-orchestration/","title":"Concurrency, Fan\u2011In/Fan\u2011Out &amp; Graph\u2011Level Orchestration","text":"<p>AetherGraph provides Python\u2011first concurrency that works from reactive agents to scheduled DAGs. You can orchestrate parallelism naturally in Python, while the runtime enforces safe scheduling and per\u2011run concurrency caps.</p>"},{"location":"key-concepts/concurrency-orchestration/#1-graph_fn-pythonic-concurrency-for-reactive-agents","title":"1. <code>@graph_fn</code> \u2014 Pythonic Concurrency for Reactive Agents","text":"<p><code>@graph_fn</code> functions execute through normal Python async semantics. Plain Python awaits run directly on the event loop, while any <code>@tool</code> calls inside a <code>@graph_fn</code> become implicit nodes managed by the agent\u2019s internal scheduler.</p> <p>Example: bounded fan\u2011out using a semaphore</p> <pre><code>import asyncio\nfrom aethergraph import graph_fn\n\nsem = asyncio.Semaphore(4)  # cap concurrent jobs (user-managed)\n\nasync def run_capped(fn, **kw):\n    async with sem:\n        return await fn(**kw)\n\n@graph_fn(name=\"batch_agent\")\nasync def batch_agent(items: list[str], *, context):\n    async def one(x):\n        await context.channel().send_text(f\"processing {x}\")\n        return {\"y\": x.upper()}\n\n    # fan\u2011out with manual cap\n    tasks = [run_capped(one, x=v) for v in items]\n    results = await asyncio.gather(*tasks)\n\n    # fan\u2011in\n    return {\"ys\": [r[\"y\"] for r in results]}\n</code></pre> <p>Notes:</p> <ul> <li>Plain Python steps execute immediately \u2014 not capped by the scheduler.</li> <li><code>@tool</code> calls are scheduled and counted toward the agent\u2019s concurrency cap through <code>max_concurrency</code> (default = 4).</li> <li>You can override per\u2011run limits by passing <code>max_concurrency=&lt;int&gt;</code> to <code>run()</code> or <code>run_async()</code> or use <code>graph_fn(.., max_concurrency=&lt;int&gt;)</code>.</li> <li>For nested or composed agents, effective concurrency multiplies; use semaphores or pools to control load.</li> <li>Ideal for reactive, exploratory agents or mixed I/O + compute logic.</li> </ul>"},{"location":"key-concepts/concurrency-orchestration/#2-graphify-schedulercontrolled-static-dags","title":"2. <code>@graphify</code> \u2014 Scheduler\u2011Controlled Static DAGs","text":"<p>In static DAGs built with <code>@graphify</code>, every <code>@tool</code> call becomes a node in a TaskGraph. Concurrency is automatically managed by the runtime scheduler, respecting per\u2011run limits.</p> <p>Minimal fan\u2011in/fan\u2011out example:</p> <pre><code>from aethergraph import graphify, tool\n\n@tool(outputs=[\"result\"])\nasync def pick(items: list[int], index: int):\n    return {\"result\": items[index]}\n\n@tool(outputs=[\"out\"])\nasync def work(x: int):\n    print(f\"Working on {x}...\")\n    return {\"out\": x * 2}\n\n@tool(outputs=[\"sum\"])\nasync def reduce_sum(xs: list[int]):\n    return {\"sum\": sum(xs)}\n\n@graphify(name=\"map_reduce\", inputs=[\"vals\"], outputs=[\"sum\"])\ndef map_reduce(vals):\n    results = [pick(items=vals, index=i) for i in range(len(vals))]  # We need use a tool to extract values as vals is a ref not a list \n    outs = [work(x=v.result) for v in results]       # fan\u2011out\n    total = reduce_sum(xs=[o.out for o in outs])     # fan\u2011in\n    return {\"sum\": total.sum}\n</code></pre> <p>Key points:</p> <ul> <li>The scheduler enforces <code>max_concurrency</code> automatically (default = 4).</li> <li>You can override per\u2011run limits by passing <code>max_concurrency=&lt;int&gt;</code> to <code>run()</code>, or <code>run_async()</code>.</li> <li>Static DAG concurrency is global and consistent across all tool nodes.</li> <li>Each node runs once dependencies resolve; no explicit <code>await</code> is required.</li> </ul>"},{"location":"key-concepts/concurrency-orchestration/#3-graphlevel-orchestration-patterns","title":"3. Graph\u2011Level Orchestration Patterns","text":"<p>All orchestration in AetherGraph is just Python. You can run sequentially or concurrently using standard async primitives.</p>"},{"location":"key-concepts/concurrency-orchestration/#a-sequential-orchestration-plain-python","title":"A) Sequential orchestration (plain Python)","text":"<pre><code>res1 = await graph_fn1(a=1, max_concurrency=N) # graph-level concurrency\nres2 = await graph_fn2(b=2, max_concurrency=N)\n</code></pre>"},{"location":"key-concepts/concurrency-orchestration/#b-concurrent-graph_fn-runs-asyncfriendly","title":"B) Concurrent <code>graph_fn</code> runs (async\u2011friendly)","text":"<pre><code>res1, res2 = await asyncio.gather(\n    graph_fn1(a=1, max_concurrency=N),\n    graph_fn2(b=2, max_concurrency=N),\n)\n</code></pre>"},{"location":"key-concepts/concurrency-orchestration/#c-concurrent-graph-runner-works-for-both-graph_fn-and-graphify","title":"C) Concurrent graph runner (works for both <code>graph_fn</code> and <code>graphify</code>)","text":"<pre><code>from aethergraph.runner import run_async\n\nres1, res2 = await asyncio.gather(\n    run_async(graph1, inputs={\"a\": 1}, max_concurrency=8),\n    run_async(graph2, inputs={\"b\": 2}, max_concurrency=2),\n)\n</code></pre> <p>Default concurrency for each graph is 4, but you can override it per call with <code>max_concurrency</code> in either <code>run()</code> or <code>run_async()</code>. Becareful of global concurrency limit. Use semaphores or pools to control load.  Do not use <code>runner.run()</code> for concurrent graph runs.</p>"},{"location":"key-concepts/concurrency-orchestration/#4-concurrency-comparison","title":"4. Concurrency Comparison","text":"Aspect <code>@graph_fn</code> (Reactive) <code>@graphify</code> (Static) Concurrency Control Automatic via scheduler (<code>max_concurrency</code>) Automatic via scheduler (<code>max_concurrency</code>) Default Limit Default 4 per run, multiply with nested calls Default 4 per run Plain Python Awaitables Run immediately, outside scheduler Not applicable (only tool nodes) Nested Calls Supported Not yet supported Failure Behavior Caught at runtime; user decides Scheduler stops on first error (configurable) Use Case Agents, exploration, hybrid control Pipelines, batch workflows, reproducible DAGs"},{"location":"key-concepts/concurrency-orchestration/#takeaways","title":"Takeaways","text":"<ul> <li>Reactive vs Deterministic: <code>graph_fn</code> for interactive exploration; <code>graphify</code> for reproducible pipelines.</li> <li>Fan\u2011In/Fan\u2011Out: Async patterns in <code>graph_fn</code>; data edges in <code>graphify</code>.</li> <li>Concurrency Control: Default cap = 4; override per run with <code>max_concurrency</code>.</li> <li>Scalability: Local schedulers per agent; a global scheduler orchestrates multiple runs.</li> <li>Everything is Python: The runtime extends standard async execution into persistent, inspectable DAG scheduling.</li> </ul>"},{"location":"key-concepts/context-services/","title":"Context Services Overview","text":"<p>Context is the lightweight runtime handle that every agent and tool receives during execution. It represents the active run, graph, and node scope and exposes AetherGraph\u2019s built-in runtime services\u2014channels, memory, artifacts, logs, and more\u2014through a clean, Pythonic interface.</p> <p>In short: Context is what makes an AetherGraph program \u201calive.\u201d It bridges your pure Python logic with interactive I/O, persistence, orchestration, and AI-powered capabilities\u2014without introducing a new DSL or framework-specific syntax.</p>"},{"location":"key-concepts/context-services/#1-why-context-matters","title":"1. Why Context Matters","text":"<p>AetherGraph\u2019s guiding principle is Python-first orchestration. The context system makes that possible by providing a unified way to connect logic and infrastructure.</p> <p>Core benefits:</p> <ul> <li>Decoupled logic: Agents and tools can call <code>context.&lt;service&gt;()</code> without worrying about back-end details or deployment environment.</li> <li>Automatic provenance: Each call carries its <code>run_id</code>, <code>graph_id</code>, and <code>node_id</code>, ensuring full traceability.</li> <li>Zero-friction orchestration: Handles message passing, persistence, and coordination transparently.</li> <li>Optional intelligence: Attach LLMs, RAG corpora, or MCP servers only when needed\u2014no dependencies until configured.</li> </ul> <p>In practice, <code>NodeContext</code> turns plain async functions into interactive, stateful agents that can communicate, remember, reason, and orchestrate\u2014all from Python.</p>"},{"location":"key-concepts/context-services/#2-quick-start","title":"2. Quick Start","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"hello_context\")\nasync def hello_context(*, context):\n    await context.channel().send_text(\"Hello from AetherGraph!\")\n    await context.memory().record(\n        kind=\"chat_data\",\n        data={\"message\": \"Hello from AetherGraph!\"},  \n    ) # remember key events\n    context.logger().info(\"finished\", extra={\"stage\": \"done\"})\n    return {\"ok\": True}\n</code></pre> <p>Each call operates within a specific node scope. The runtime automatically provides <code>run_id</code>, <code>graph_id</code>, and <code>node_id</code> to maintain context and provenance.</p>"},{"location":"key-concepts/context-services/#3-context-structure","title":"3. Context Structure","text":"<p>Each <code>NodeContext</code> carries stable identifiers and bound service references.</p> <pre><code>@dataclass\nclass NodeContext:\n    run_id: str\n    graph_id: str\n    node_id: str\n    services: NodeServices  # all bound runtime services\n</code></pre> <p>Identifiers</p> <ul> <li>run_id \u2014 unique per execution run.</li> <li>graph_id \u2014 identifies which graph the node belongs to.</li> <li>node_id \u2014 unique ID for the node invocation.</li> </ul>"},{"location":"key-concepts/context-services/#4-context-services","title":"4. Context Services","text":"<p>AetherGraph organizes its context services into core, optional, and utility layers.</p>"},{"location":"key-concepts/context-services/#core-services","title":"Core Services","text":"Method Purpose <code>context.channel()</code> Message and interaction bus \u2014 send/receive text/approval/files, show progress, or streaming events. <code>context.memory()</code> Memory fa\u00e7ade \u2014 record events, write typed results, query history, or manage RAG-ready logs. <code>context.artifacts()</code> Artifact store fa\u00e7ade \u2014 save/retrieve files, track outputs, and query files by labels/metrics artifacts. <code>context.kv()</code> Lightweight key\u2013value store for ephemeral coordination and small caches. <code>context.logger()</code> Structured logger with <code>{run_id, graph_id, node_id}</code> metadata automatically included."},{"location":"key-concepts/context-services/#optional-services-config-dependent","title":"Optional Services (config-dependent)","text":"<p>Optional services require API keys or runtime configuration. They are injected dynamically when available.</p> Method Purpose <code>context.llm()</code> Access an LLM client for chat, embeddings, or raw apis access (OpenAI, Anthropic, Google, or local backends, etc.). <code>context.rag()</code> Retrieval-augmented generation fa\u00e7ade \u2014 build corpora, upsert documents, search, and answer queries. <code>context.mcp()</code> Connect to external MCP tool servers via stdio, WebSocket, or HTTP."},{"location":"key-concepts/context-services/#utility-helpers","title":"Utility Helpers","text":"Method Purpose <code>context.clock()</code> Clock utilities for timestamps, delays, and scheduling. <code>context.continuations()</code> Access continuation store; used internally for dual-stage waits (<code>ask_text</code>, <code>ask_approval</code>). <p>If a service is unavailable, its accessor raises a clear runtime error (e.g., <code>LLMService not available</code>). Configure them globally or per-environment to enable.</p>"},{"location":"key-concepts/context-services/#5-typical-patterns","title":"5. Typical Patterns","text":""},{"location":"key-concepts/context-services/#1-ask-wait-resume","title":"1 Ask \u2192 Wait \u2192 Resume","text":"<pre><code>text = await context.channel().ask_text(\"Provide a dataset path\") \n# wait for external input and resume when done\nawait context.channel().send(f\"you provided the dataset path {text}\")\n</code></pre>"},{"location":"key-concepts/context-services/#2-artifacts-memory","title":"2 Artifacts + Memory","text":"<pre><code># save and return an artifact \nart = await context.artifacts().save(path=\"/tmp/report.pdf\", kind=\"report\", labels={\"exp\": \"A\"}) \n# save and return an event  \nevt = await context.memory().record(kind=\"checkpoint\", data={\"info\": \"experiment A saved\"}) \n\n# In later stage or other agent:\n# list all previous saved art with kind == \"report\"\narts = await context.artifacts().search(kind=\"report\")      \n# list past 100 memory with kinds include \"checkpoint\" \nevts = await context.memory().recent(kinds=[\"checkpoint\"], limit=100)    \n</code></pre>"},{"location":"key-concepts/context-services/#4-rag-llm-answers","title":"4 RAG + LLM Answers","text":"<pre><code># ingest data into vector DB\ncorpus_id = \"notes\"\n_ = await context.rag().upsert_docs(corpus_id, docs)  # your documentations in a list \n\n# later search/answer\nhits = await context.rag().search(corpus_id, query=\"Tool used in experiment #A2?\", k=5)\nans = await context.rag().answer(corpus_id, question=\"What is the best iteration and what is the loss?\", style=\"concise\")\nawait context.channel().send_text(ans[\"answer\"])\n</code></pre>"},{"location":"key-concepts/context-services/#5-external-tools-via-mcp","title":"5 External Tools via MCP","text":"<pre><code>res = await context.mcp(\"ws\").call(\"search\", {\"q\": \"tolerance analysis\", \"k\": 5})\n</code></pre>"},{"location":"key-concepts/context-services/#6-custom-context-services","title":"6. Custom Context Services","text":"<p>The context system is fully extensible. You can define your own service and expose it via <code>context.&lt;name&gt;()</code> using <code>register_context_service()</code>.</p> <p>Use cases:</p> <ul> <li>Add domain-specific APIs (e.g., simulation, materials DB, experiment tracking).</li> <li>Provide custom persistence or distributed coordination layers.</li> <li>Implement bridges between external systems (e.g., job schedulers, cloud storage, or lab devices).</li> </ul> <p>See External Context Services for API details and examples.</p>"},{"location":"key-concepts/context-services/#7-design-philosophy","title":"7. Design Philosophy","text":"<ul> <li>Python-first: use direct calls, not DSL syntax.</li> <li>Minimal surface: each service follows a small, composable API.</li> <li>Composable orchestration: mix local and remote services freely.</li> <li>Swappable backends: replace LLM, KV, or artifact backends without touching agent logic.</li> </ul>"},{"location":"key-concepts/context-services/#see-also","title":"See Also","text":"<ul> <li>[<code>context.channel()</code>] \u2014 cooperative waits, streaming, progress updates</li> <li>[<code>context.memory()</code>] \u2014 event log, typed results, summaries, and RAG helpers</li> <li>[<code>context.artifacts()</code>] \u2014 content-addressable storage and retrieval</li> </ul>"},{"location":"key-concepts/event-driven-waits/","title":"Event\u2011Driven Waits: Cooperative vs Dual\u2011Stage","text":"<p>AetherGraph agents are event\u2011driven: they can pause mid\u2011flow and safely resume when a reply, upload, or callback arrives. There are two complementary wait modes, and you can use them flexibly in both <code>@graph_fn</code> and <code>@graphify</code>\u2011built graphs.</p> <ul> <li>Cooperative waits \u2014 via <code>context.channel().ask_*</code>. Simplest way to prompt + wait in reactive agents.</li> <li>Dual\u2011stage waits \u2014 via <code>@tool</code> nodes that split into Stage A (prompt/setup) and Stage B (resume/produce). Best for static graphs and reliable orchestration.</li> </ul> <p>Flexibility: <code>context.*</code> methods are available inside <code>@tool</code> nodes (therefore inside <code>@graphify</code>). Dual\u2011stage tools can also be <code>await</code>\u2011ed directly inside <code>@graph_fn</code>. In either case, they form a node and persist a continuation.</p>"},{"location":"key-concepts/event-driven-waits/#1-cooperative-waits-channelfirst","title":"1 Cooperative Waits (Channel\u2011first)","text":"<p>What: <code>context.channel().ask_text / ask_approval / ask_files</code> send a prompt and yield until a reply or timeout. The runtime persists a continuation token so the run can resume after restarts.</p> <p>Where: Primarily inside <code>@graph_fn</code>. Can also be called from within a <code>@tool</code> if you want cooperative logic inside a node.</p> <p>Example</p> <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"cooperative_demo\", outputs=[\"msg\"]) \nasync def cooperative_demo(*, context):\n    name = await context.channel().ask_text(\"Your name?\")\n    await context.channel().send_text(f\"Hi, {name}!\")\n    return {\"msg\": f\"greeted:{name}\"}\n</code></pre> <p>Properties</p> <ul> <li>Minimal code, great for exploratory, chat\u2011style agents.</li> <li>Thread/channel\u2011aware correlation.</li> <li>Durable continuations; survives restarts.</li> </ul>"},{"location":"key-concepts/event-driven-waits/#2-dualstage-waits-toolfirst","title":"2 Dual\u2011Stage Waits (Tool\u2011first)","text":"<p>What: A node splits into two stages: A emits the prompt/sets up state, B resumes once the event arrives and produces outputs. Maps cleanly to static DAGs and lets the global scheduler manage resumptions and retries.</p> <p>Use in both places:</p> <ul> <li>In <code>@graphify</code> as standard tool nodes.</li> <li>In <code>@graph_fn</code> with <code>await</code> for immediate use \u2014 they still become nodes under the hood.</li> </ul> <p>Built\u2011in channel tools</p> <pre><code># Use these in either style:\nfrom aethergraph.tools import ask_text, ask_approval, ask_files\n\n# A) Inside a static graph\nfrom aethergraph import graphify\n\n@graphify(name=\"collect_input\", inputs=[], outputs=[\"greeting\"]) \ndef collect_input():\n    name = ask_text(prompt=\"Your name?\")      # node yields \u2192 resumes on reply\n    return {\"greeting\": name.text}\n\n# B) Await directly in a graph_fn\nfrom aethergraph import graph_fn\n\n@graph_fn(name=\"dualstage_in_fn\", outputs=[\"choice\"]) \nasync def dualstage_in_fn(*, context):\n    res = await ask_approval(prompt=\"Proceed?\", options=(\"Yes\",\"No\"))\n    return {\"choice\": res[\"choice\"]}\n</code></pre> <p>Properties</p> <ul> <li>Node\u2011level persistence, retries, and metrics.</li> <li>Works seamlessly with global scheduling (centralized control, resumptions at scale).</li> <li>Great for UI + pipeline hybrids (prompt in Stage A, compute in Stage B).</li> </ul>"},{"location":"key-concepts/event-driven-waits/#3-using-context-inside-graphify","title":"3 Using <code>context.*</code> inside <code>@graphify</code>","text":"<p><code>context</code> methods (channels, memory, artifacts, kv, logger, etc.) are available inside <code>@tool</code> nodes. This means your static graphs can still interact, log, and persist during node execution while retaining DAG inspectability.</p> <pre><code>from aethergraph import tool\n\n@tool(outputs=[\"ok\"]) \nasync def notify_and_tag(*, context):\n    await context.channel().send_text(\"Started node\u2026\")\n    await context.memory().record(kind=\"status\", data={\"stage\":\"start\"})\n    return {\"ok\": True}\n</code></pre>"},{"location":"key-concepts/event-driven-waits/#4-comparison-cooperative-vs-dualstage-vs-manual-checkpoints","title":"4 Comparison: Cooperative vs Dual\u2011Stage vs Manual Checkpoints","text":"Aspect Cooperative (<code>context.channel().ask_*</code>) Dual-Stage (<code>@tool</code> ask_*) Manual checkpoints Authoring style Inline, minimal Explicit node with A/B stages N/A in AG (not built-in) Resumability Hard \u2014 stateless unless save the state to memory manually Native continuations per node (resumeable after restart) Possible but manual/fragile Retry / Idempotency Coarse (re-invoke the whole function) Fine (node-level retry, idempotent resumes) Manual Scale Great for interactive sessions, small graphs Excellent for large runs / thousands of waits Limited by implementation CPU load (waiting) Keeps process / event loop alive; lightweight but not zero Zero CPU \u2014 node is dormant until resumed Depends on checkpointing backend Memory footprint Held in local task heap (light) Released after serialization; only metadata retained Depends on snapshot granularity Disk usage Optional if memory writes used Tiny (~1\u201310 KB per node) \u2014 correlator + inputs serialized Potentially heavy (full state dump) Latency to resume Instant within current process Slightly higher (resume event \u2192 lookup \u2192 dispatch) Potentially high (manual restore) <p>Why Dual\u2011Stage scales</p> <ul> <li>Node\u2011granular control: retries, backoff, and resumption are local to the waiting node.</li> <li>Central orchestration: the global scheduler can queue, shard, or migrate blocked nodes.</li> <li>Observability: each wait is a first\u2011class node with metrics and logs.</li> <li>Determinism: Stage boundaries clarify side\u2011effects and make runs reproducible.</li> </ul> <p>Manual checkpoints (framework\u2011agnostic snapshots) aren\u2019t part of AetherGraph. Dual\u2011stage nodes cover the same reliability space with less boilerplate and better provenance.</p>"},{"location":"key-concepts/event-driven-waits/#5-extending-dualstage-tools","title":"5 Extending Dual\u2011Stage Tools","text":"<p>You can author custom dual\u2011stage nodes with <code>DualStageTool</code> to model your own A/B waits (e.g., submit job \u2192 wait \u2192 collect). Some examples of the usage include </p> <ul> <li>custom channel waits</li> <li>submit/run long simualtion on cloud</li> <li>data/model training pipeline on external systems</li> <li>external API Polling that reports a compleltion asynchronously</li> </ul> <p>A compact public API for this is planned; detailed docs will ship soon.</p>"},{"location":"key-concepts/event-driven-waits/#6-takeaways","title":"6 Takeaways","text":"<ul> <li>All <code>context.channel().ask_*</code> calls are cooperative waits by default.</li> <li>Dual\u2011stage tools work in both <code>@graphify</code> and <code>@graph_fn</code> (awaitable) and always materialize as nodes.</li> <li>For large, reliable systems: prefer dual\u2011stage for node\u2011level retries, metrics, and scheduler control.</li> <li><code>context.*</code> is available inside <code>@tool</code> nodes, so static graphs can still interact, log, and persist cleanly.</li> <li>Manual checkpointing isn\u2019t needed; dual\u2011stage nodes give better reliability with less boilerplate.</li> </ul>"},{"location":"key-concepts/extending-context/","title":"Extending Context Services","text":"<p>AetherGraph lets you extend the runtime by adding your own <code>context.&lt;name&gt;</code> methods. These external context services live alongside built\u2011ins like <code>channel</code>, <code>memory</code>, and <code>artifacts</code>, and provide reusable, lifecycle\u2011aware helpers for clients, caches, orchestration, or domain APIs \u2014 without changing your agent code.</p> <p>Key idea: keep agent logic pure\u2011Python; move integration glue and shared state into services that the runtime injects per node.</p>"},{"location":"key-concepts/extending-context/#1-what-is-an-external-context-service","title":"1. What is an External Context Service?","text":"<p>A context service is a registered Python object bound into every <code>NodeContext</code>. After registration, you can use it anywhere inside a graph or tool:</p> <pre><code>@graph_fn(name=\"demo\")\nasync def demo(*, context):\n    info = await context.trainer().inspect_job(job_id=\"abc123\")\n    return {\"status\": info[\"status\"]}\n</code></pre>"},{"location":"key-concepts/extending-context/#whywhen-to-use","title":"Why/When to Use","text":"<ul> <li>Reusable helpers \u2014 share clients (e.g., HPC, S3, DB, solver), connection pools, or token buckets.</li> <li>Shared state \u2014 memoize expensive lookups; coordinate across nodes within a run.</li> <li>Centralized config \u2014 keep API keys, timeouts, routing, or policies in one place.</li> <li>Per\u2011node awareness \u2014 access to built-in services through <code>self.ctx()</code> for provenance or multi\u2011tenancy.</li> </ul> <p>Use a service for long\u2011lived instances or cross\u2011node coordination. For tiny stateless helpers, plain imports are fine.</p>"},{"location":"key-concepts/extending-context/#2-naming-boundaries-important","title":"2. Naming &amp; Boundaries (Important)","text":"<p>Built\u2011ins (<code>context.artifacts()</code>, <code>context.memory()</code>, etc.) are not swappable in OSS. To extend the system, register new services with new names (e.g., <code>context.trainer()</code>, <code>context.datasets()</code>, <code>context.lineage_store()</code>).</p> <ul> <li>Keep agent code explicit about which storage or API it\u2019s using.</li> <li>If mirroring/exporting, record links (artifact URIs, memory event IDs) inside your external system for provenance.</li> </ul>"},{"location":"key-concepts/extending-context/#3-minimal-service-instancebased","title":"3. Minimal Service (Instance\u2011based)","text":""},{"location":"key-concepts/extending-context/#define-a-service-and-use-selfctx","title":"Define a service and use <code>self.ctx()</code>","text":"<pre><code>from aethergraph import Service\n\nclass Trainer(Service):\n    async def submit(self, spec: dict) -&gt; str:\n        # Submit a training job to your HPC/cluster\n        ... \n        return job_id\n\n    async def inspect_job(self, job_id: str) -&gt; dict:\n        # Inspect the job status\n        ...\n        return {\"job_id\": job_id, \"status\": status}\n</code></pre>"},{"location":"key-concepts/extending-context/#register-at-startup-pass-an-instance","title":"Register at startup (pass an instance)","text":"<pre><code>from aethergraph.runtime import register_context_service\nfrom aethergraph import start_server()\n\n# register after server is started \nstart_server() \nregister_context_service(\"trainer\",  Trainer())\n</code></pre> <p>After this, <code>context.trainer()</code> is available everywhere in the runtime.</p>"},{"location":"key-concepts/extending-context/#4-usage-patterns","title":"4. Usage Patterns","text":""},{"location":"key-concepts/extending-context/#a-submit-training-link-artifacts","title":"A) Submit training &amp; link artifacts","text":"<pre><code>@graph_fn(name=\"train_model\", outputs=[\"job_id\", \"ckpt_uri\"]) \nasync def train_model(spec: dict, *, context):\n    #  Submit to your cluster via the custom service\n    job_id = await context.trainer().submit(spec)\n    return {\"job_id\": job_id, \"ckpt_uri\": ckpt.uri}\n</code></pre>"},{"location":"key-concepts/extending-context/#b-inspect-status-in-another-nodetool","title":"B) Inspect status in another node/tool","text":"<pre><code>@tool(name=\"wait_for_training\", outputs=[\"ready\"]) \nasync def wait_for_training(job_id: str, *, context) -&gt; dict:\n    # Inspect you job through your service\n    info = await context.trainer().inspect_job(job_id)\n    return {\"ready\": info[\"status\"] == \"COMPLETED\"}\n</code></pre>"},{"location":"key-concepts/extending-context/#5-concurrency-lifecycle","title":"5. Concurrency &amp; Lifecycle","text":"<p>If you expect your services are accessed by multiple agents concurrently, consider the designs: </p> <ul> <li>Lifecycle hooks: <code>start()</code> / <code>close()</code> are optional; call them from your app/server bootstrap.</li> <li>Shared access: use <code>self.critical()</code> to protect mutable shared state. Design your own mutex when scaling up. </li> <li>Per\u2011node context: call <code>self.ctx()</code> whenever you need <code>{run_id, graph_id, node_id}</code>.</li> <li>Async native: expose async APIs; if integrating queues, consider <code>asyncio.Queue</code>.</li> </ul>"},{"location":"key-concepts/extending-context/#6-common-service-patterns-examples","title":"6. Common Service Patterns (Examples)","text":"Scenario Suggested accessor What it abstracts Typical operations HPC / Training orchestration <code>context.trainer()</code> Submit/track jobs on Slurm/K8s/Ray <code>submit(spec)</code>, <code>inspect_job(id)</code>, <code>cancel(id)</code> External object storage <code>context.storage()</code> S3/GCS/MinIO buckets &amp; signed URLs <code>put(path)</code>, <code>get(uri)</code>, <code>sign(uri)</code>, <code>list(prefix)</code> Vendor API client <code>context.apiclient()</code> Rate\u2011limited, retried HTTP SDK <code>get/put/post</code>, <code>batch()</code>, <code>retry/backoff</code> In\u2011house AI models <code>context.models()</code> Local inference endpoints <code>embed(texts)</code>, <code>generate(prompt)</code> Materials DB / domain registry <code>context.materials()</code> Domain lookups &amp; cached tables <code>get_index(name)</code>, <code>search(filters)</code> Lineage export <code>context.lineage_store()</code> Mirror core provenance to BI/warehouse <code>export_run(run_id)</code>, <code>push(events)</code> <p>Pick names that are explicit in your org (e.g., <code>context.k8s_jobs()</code>, <code>context.minio()</code>). Avoid names that shadow built\u2011ins.</p>"},{"location":"key-concepts/extending-context/#summary","title":"Summary","text":"<ul> <li>External services add named capabilities to <code>context</code> without changing agent code.</li> <li>Built\u2011ins remain stable; extend via new names (no in\u2011place swaps).</li> <li>Register instances, not factories; services run on the main event loop.</li> <li>Use <code>self.ctx()</code> to fetch per\u2011node provenance on demand; protect shared state with <code>critical()</code> or your own lock design.</li> </ul>"},{"location":"key-concepts/introduction/","title":"AetherGraph \u2014 Key Concepts Overview","text":"<p>AetherGraph rethinks how agentic systems are built: graphs are agents, context is the runtime fabric, and execution is event\u2011driven. This document lays out the philosophy and the essential architecture so you can quickly reason about how it differs from typical frameworks\u2014and how to use it effectively.</p>"},{"location":"key-concepts/introduction/#1-introduction-graphs-context","title":"1. Introduction: Graphs + Context","text":"<p>AetherGraph departs from most agent frameworks in two fundamental ways:</p> <ol> <li>Every agent is a graph. You model behavior as a directed acyclic graph (DAG) with nodes that can expand dynamically (reactive) or be planned statically.</li> <li>Every node runs inside a <code>NodeContext</code>. The context is your per\u2011node control plane that exposes rich, injectable services.</li> </ol> <p>NodeContext services include (built\u2011ins plus anything you add):</p> <ul> <li>Channels (Slack, Console/Web, \u2026) for I/O and interaction</li> <li>Artifacts (blob store) for large files and generated assets</li> <li>Memory (history + summaries + optional RAG)</li> <li>KV &amp; Logger for quick state and observability</li> <li>LLM / MCP / RAG bridges for model calls and tool use</li> <li>Your custom services registered at runtime</li> </ul> <p>The system is pythonic, reactive, and extensible: author graphs directly in Python, then let context services handle communication, persistence, and orchestration details.</p>"},{"location":"key-concepts/introduction/#2-graphs-as-agents","title":"2. Graphs as Agents","text":"<p>AetherGraph unifies two modes under one mental model:</p>"},{"location":"key-concepts/introduction/#reactive-agent-graph_fn","title":"Reactive agent \u2014 <code>@graph_fn</code>","text":"<ul> <li>Executes immediately when called (behaves like an async Python function).</li> <li>Expands into an implicit DAG as tools run.</li> <li>Perfect for interactive, service\u2011rich, or conversational workflows.</li> </ul>"},{"location":"key-concepts/introduction/#static-agent-graphify","title":"Static agent \u2014 <code>@graphify</code>","text":"<ul> <li>First builds a concrete <code>TaskGraph</code>, then executes it via the scheduler.</li> <li>Ideal for stable, deterministic, or large\u2011scale workflows.</li> </ul> <p>One model, two tempos. Iterate fast with <code>@graph_fn</code>; snap to repeatable DAGs with <code>@graphify</code>.</p>"},{"location":"key-concepts/introduction/#3-context-as-the-runtime-fabric","title":"3. Context as the Runtime Fabric","text":"<p>Every node (and each <code>graph_fn</code> invocation) receives a NodeContext. Think of it as a scoped service locator with lifecycle hooks.</p> <ul> <li>Per\u2011node scoping \u2192 independent logging, persistence, messaging.</li> <li>Injectable services \u2192 bind built\u2011ins or your own with simple registration.</li> <li>Sidecar\u2011friendly \u2192 channel adapters and stores can run out\u2011of\u2011process.</li> </ul> <p>Common patterns:</p> <ul> <li><code>context.channel().ask_text()</code> to collect input from a user/UI.</li> <li><code>context.artifacts().save(...)</code> to persist files and structured outputs.</li> <li><code>context.memory().record(...)</code> to capture provenance and summaries.</li> <li><code>context.llm().chat(...)</code> to call your configured model provider.</li> </ul>"},{"location":"key-concepts/introduction/#4-eventdriven-execution-waits","title":"4. Event\u2011Driven Execution &amp; Waits","text":"<p>AetherGraph is event\u2011driven (not polling). Nodes suspend when waiting for input or an external event; the runtime persists and resumes continuations on demand so suspended nodes consume little to no CPU/RAM. Two primary wait styles are exposed:</p> <ul> <li>Cooperative waits \u2014 inline <code>await</code> for short, interactive flows where the coroutine remains in memory for low\u2011latency response.</li> <li>Dual\u2011stage waits \u2014 split a node into setup and resume phases; the continuation is durably stored so the process can free resources until an event triggers resume.</li> </ul> Mode Description CPU/RAM Ideal for Cooperative waits Inline <code>await</code> (e.g., <code>await context.channel().ask_text()</code>); coroutine stays in memory Minimal Small reactive loops, interactive sessions Dual\u2011stage waits Split node into setup/resume; continuation stored in a durable store Zero when paused Large DAGs, long\u2011lived pipelines Manual checkpoints User\u2011managed state snapshot/restore User\u2011managed Legacy integration, custom control <p>Dual\u2011stage waits + event routing enable massive concurrency with tiny footprint.</p>"},{"location":"key-concepts/introduction/#5-execution-scheduling","title":"5. Execution &amp; Scheduling","text":"<p>Each graph runs under a scheduler that selects ready nodes and respects your concurrency caps. Within a graph you get async DAG orchestration by default.</p>"},{"location":"key-concepts/introduction/#crossgraph-orchestration","title":"Cross\u2011graph orchestration","text":"<p>AetherGraph intentionally avoids a heavyweight orchestrator in the OSS core. For concurrent runs across multiple agents/graphs, use idiomatic Python:</p> <pre><code># Concurrent reactive runs\nawait asyncio.gather(\n    graph_fn_agent_a(...),   # a @graph_fn with async __call__\n    graph_fn_agent_b(...),\n)\n\n# Sequential orchestration\nawait graph_fn_agent_a(...)\nawait graph_fn_agent_b(...)\n</code></pre>"},{"location":"key-concepts/introduction/#6-extensibility-everywhere","title":"6. Extensibility Everywhere","text":"<p>Pure Python\u2014no DSLs, no codegen.</p> <ul> <li>Tools: <code>@tool</code> to define reusable, typed primitives.</li> <li>Graphs: <code>@graphify</code> to materialize a DAG for repeatable runs.</li> <li>Services: <code>register_context_service()</code> to inject your own capabilities.</li> <li>Adapters: extend ChannelBus to new transports (Slack, Web, PyQt, \u2026).</li> </ul> <p>Example (sketch)</p> <p>Define a <code>@tool</code>: <pre><code>@tool(name=\"analyze\", outputs=[\"val\"])\ndef analyze(x: int) -&gt; int:\n    return {\"val\": x * 2}\n</code></pre></p> <p>Define a <code>@graph_fn</code> (<code>@tool</code> is optional) <pre><code>@graph_fn(name=\"demo\")\nasync def demo(*, context):\n    y = await analyze(21)\n    await context.channel().send_text(f\"Answer: {y}\")\n</code></pre></p>"},{"location":"key-concepts/introduction/#7-how-aethergraph-differs","title":"7. How AetherGraph Differs","text":"Area AetherGraph Typical frameworks Runtime Unified Python, event\u2011driven Split control planes, mixed models Waits Zero\u2011CPU dual\u2011stage waits Blocking awaits, threads, polling Context Rich per\u2011node runtime fabric Globals, hidden singletons Composition Dynamic (<code>graph_fn</code>) + Static (<code>@graphify</code>) Usually one or the other Scheduling Async DAG scheduling per graph Step\u2011based loops, less reactive Extensibility Decorators + Python APIs Fixed plugin systems Provenance Built\u2011in memory/artifacts External or ad\u2011hoc"},{"location":"key-concepts/introduction/#8-takeaways","title":"8. Takeaways","text":"<p>AetherGraph bridges interactive research and deterministic automation:</p> <ul> <li>React first, then structure for production.</li> <li>Keep state, context, and scheduling where they belong\u2014with the nodes.</li> <li>Use event\u2011driven waits to scale without idle compute.</li> </ul>"},{"location":"key-concepts/other-services-overview/","title":"Other Services Overview","text":"<p>AetherGraph\u2019s context exposes a set of lightweight, composable runtime services that complement Channels, Artifacts, and Memory. Use them when your agent needs coordination, observability, or intelligent reasoning \u2014 while keeping your core logic pure Python.</p> <p>Philosophy: keep code idiomatic; reach for <code>context.&lt;service&gt;()</code> only when you need I/O, coordination, or intelligence.</p>"},{"location":"key-concepts/other-services-overview/#1-kv-ephemeral-coordination","title":"1. KV \u2014 Ephemeral Coordination","text":"<p>A minimal key\u2013value store for transient state, synchronization, and quick signals between nodes/agents.</p> <pre><code>@graph_fn(name=\"kv_demo\", outputs=[\"ok\"])\nasync def kv_demo(*, context):\n    kv = context.kv()\n    await kv.set(\"stage\", \"preflight\", ttl_s=300)\n    stage = await kv.get(\"stage\")  # \u2192 \"preflight\"\n    return {\"ok\": stage == \"preflight\"}\n</code></pre> <p>Why/When: feature flags, locks/counters, short-lived coordination.</p> <p>Default backend: in-memory KV. </p>"},{"location":"key-concepts/other-services-overview/#2-logger-structured-logs-with-provenance","title":"2. Logger \u2014 Structured Logs with Provenance","text":"<p>Structured logging with <code>{run_id, graph_id, node_id}</code> automatically injected.</p> <pre><code>@graph_fn(name=\"log_demo\", outputs=[\"done\"])\nasync def log_demo(*, context):\n    log = context.logger()\n    log.info(\"starting\", extra={\"component\": \"ingest\"})\n    try:\n        ...\n        log.info(\"finished\", extra={\"component\": \"ingest\"})\n        return {\"done\": True}\n    except Exception:\n        log.exception(\"ingest failed\")\n        return {\"done\": False}\n</code></pre> <p>Why/When: lifecycle traces, metrics, error reporting.</p> <p>Default backend: Python <code>logging</code>.</p>"},{"location":"key-concepts/other-services-overview/#3-llm-unified-chat-embeddings-optional","title":"3. LLM \u2014 Unified Chat &amp; Embeddings (optional)","text":"<p>Provider-agnostic interface for chat/completions and embeddings. Requires configuration (API keys, profile).</p> <pre><code>@graph_fn(name=\"llm_demo\", outputs=[\"reply\"])\nasync def llm_demo(prompt: str, *, context):\n    llm = context.llm(profile=\"default\")\n    msg = await llm.chat([{\"role\": \"user\", \"content\": prompt}])\n    return {\"reply\": msg[\"content\"]}\n</code></pre> <p>Why/When: summarization, drafting, tool-use planning, embeddings.</p> <p>Backends: OpenAI, Anthropic, local, etc.</p>"},{"location":"key-concepts/other-services-overview/#4-rag-longterm-semantic-recall-optional","title":"4. RAG \u2014 Long\u2011Term Semantic Recall (optional)","text":"<p>Aethergraph provides basic RAG for fast prototype. Build searchable corpora from events/docs; retrieve or answer with citations. Requires an LLM for answering.</p> <pre><code>@graph_fn(name=\"rag_demo\", outputs=[\"answer\"])\nasync def rag_demo(q: str, *, context):\n    corpus_id: str = \"demo-corpus-inline\",\n    rag = context.rag()\n    docs = [\n        ... # you list of docs (text, markdown, or pdf path)\n    ]\n    result = await rag.upsert_docs(corpus_id, docs) # this will create and ingest into the corpus\n    ans = await rag.answer(corpus_id=corpus, question=q)\n    return {\"answer\": ans[\"answer\"]}\n</code></pre> <p>Why/When: semantic search, project recall, retrieval\u2011augmented QA.</p> <p>Default backend: Sqlite Vector DB, switchable to FAISS. </p>"},{"location":"key-concepts/other-services-overview/#5-mcp-external-tool-bridges-optional","title":"5. MCP \u2014 External Tool Bridges (optional)","text":"<p>Connect to external tool servers over stdio/WebSocket/HTTP via Model Context Protocol (MCP).</p> <pre><code>@graph_fn(name=\"mcp_demo\", outputs=[\"hits\"])\nasync def mcp_demo(*, context):\n    ws = context.mcp(\"ws\")  # adapter name\n    res = await ws.call(\"search\", {\"q\": \"tolerance analysis\", \"k\": 3})\n    return {\"hits\": res.get(\"items\", [])}\n</code></pre> <p>Why/When: integrate non-Python tools or remote services with structured contracts.</p>"},{"location":"key-concepts/other-services-overview/#takeaways","title":"Takeaways","text":"<ul> <li>Access everything through <code>context.&lt;service&gt;()</code> \u2014 no globals or custom wiring.</li> <li>KV and Logger work out of the box; LLM/RAG/MCP are optional and enabled by config.</li> <li>Backends are pluggable; you can move from local to managed services without changing agent code.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/","title":"Server (Sidecar) Overview","text":"<p>The AetherGraph sidecar is a lightweight process that boots your runtime services and exposes a tiny HTTP/WebSocket surface for adapters (Slack, Web, Telegram, \u2026) and continuations (event\u2011driven waits). With the server, you can: </p> <ul> <li>Real interactions: <code>ask_text/approval/files</code> from Slack/Web/Telegram and resume the run</li> <li>Centralized service wiring: artifacts, memory, kv, llm, rag, mcp, logger</li> <li>A shared control plane: health, upload hooks, progress streams, basic inspect</li> </ul> <p>In short: keep your agents plain Python; start the sidecar for I/O, resumability, and shared services. Always start the server before registering services and running agents</p>"},{"location":"key-concepts/server-start-sidecar/#quick-start","title":"Quick Start","text":"<pre><code>from aethergraph import start_server, stop_server\n\nurl = start_server(host=\"127.0.0.1\", port=0)  # FastAPI + Uvicorn in a background thread\nprint(\"sidecar:\", url)\n\n# ... run @graph_fn / @graphify normally ...\n\nstop_server()  # optional (handy in tests/CI)\n</code></pre> <p>Tips</p> <ul> <li>Use <code>port=0</code> to pick a free port automatically. </li> <li>Start it once per process; reuse the base URL across adapters/UI.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/#what-start_server-does","title":"What <code>start_server()</code> Does","text":"<ol> <li>Load config &amp; workspace \u2014 resolve paths, secrets, and profiles; make the workspace if needed.</li> <li>Build &amp; register services \u2014 channels, artifacts (store/index), memory (hotlog/persistence/indices), kv, llm, rag, mcp, logger.</li> <li> <p>Expose endpoints \u2014</p> <ul> <li>Continuations: resume callbacks for <code>ask_text / ask_approval / ask_files</code></li> <li>Adapters: chat/events, uploads, progress streams</li> </ul> </li> <li> <p>Launch Uvicorn \u2014 run the app in a background thread and return the base URL.</p> </li> </ol> <p>It is safe to use <code>start_server()</code> in Jupyter notebook</p>"},{"location":"key-concepts/server-start-sidecar/#minimal-api","title":"Minimal API","text":"<p><code>start_server(host=\"127.0.0.1\", port=8000, ...) -&gt; str</code></p> <p>Starts the sidecar in\u2011process and returns the base URL.</p> <p><code>start-server_async(...) -&gt; str</code></p> <p>Async\u2011friendly variant (still hosts the server in a thread), convenient inside async apps/tests.</p> <p><code>stop_server() -&gt; None</code></p> <p>Stops the background server. Useful for teardown in tests/CI.</p>"},{"location":"key-concepts/server-start-sidecar/#common-issues-fixes","title":"Common Issues &amp; Fixes","text":"<ul> <li>No reply after <code>ask_text()</code> \u2192 The adapter isn\u2019t posting resume events to the sidecar. Verify the sidecar base URL and token in the adapter config.</li> <li>CORS blocked in web UI \u2192 Allow your UI origin in sidecar settings (CORS <code>allow_origins</code>).</li> <li>Port busy \u2192 Use <code>port=0</code> or pick an open port.</li> <li>Service not available (e.g., LLM/RAG) \u2192 Ensure your <code>create_app()</code> wires those services or provide the required credentials.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/#notes-on-architecture","title":"Notes on Architecture","text":"<ul> <li>The sidecar runs its own event loop/thread; your agents/tools run on the main loop. They communicate via the ChannelBus/HTTP hooks.</li> <li>External context services you register as instances run on the main loop, so <code>asyncio</code> locks work as expected.</li> </ul>"},{"location":"key-concepts/server-start-sidecar/#takeaways","title":"Takeaways","text":"<ul> <li>The sidecar is your local control plane: services + continuations + adapters.</li> <li>Start it with <code>start()</code> when you need interactions, persistence, or shared wiring.</li> <li>Your agent code stays plain Python either way; the sidecar simply adds I/O and resumability.</li> </ul>"},{"location":"key-concepts/static-graph-agent/","title":"Static Graphs with <code>@graphify</code>","text":"<p><code>@graphify</code> transforms a plain Python function into a graph builder. Instead of running immediately (like <code>@graph_fn</code>), it constructs a TaskGraph from <code>@tool</code> calls \u2014 a reusable, explicit DAG that you can run later.</p> <p>In short:</p> <ul> <li><code>@graph_fn</code> \u2192 executes now (reactive, dynamic)</li> <li><code>@graphify</code> \u2192 builds first, runs later (deterministic DAG)</li> </ul>"},{"location":"key-concepts/static-graph-agent/#1-what-is-a-static-graph","title":"1. What Is a Static Graph?","text":"<p>A static graph is a declarative DAG of tool nodes and dependencies. Each node is a <code>@tool</code> call; edges represent data flow or forced ordering. You build it once, then you can inspect, persist, visualize, and run it repeatedly.</p> <p>Why static? Repeatability, inspectability, resumability, and clear fan\u2011in/fan\u2011out. Static graphs shine for pipelines and reproducible experiments where determinism and analysis matter.</p>"},{"location":"key-concepts/static-graph-agent/#2-graphify-vs-graph_fn","title":"2. <code>@graphify</code> vs <code>@graph_fn</code>","text":"Aspect <code>@graph_fn</code> (Reactive) <code>@graphify</code> (Static) Execution Runs immediately when called Builds a DAG first; run later Composition Mix plain Python + <code>@tool</code> (implicit nodes) Only <code>@tool</code> nodes are valid steps Context usage Rich <code>context.*</code> available inline Need to wrap <code>context.*</code> in a tool to access it Inspectability Inspect implicit graph via <code>graph_fn.last_graph()</code> Full spec via <code>.io()</code>, <code>.spec()</code>, <code>TaskGraph.pretty()</code> Resumability Graph cannot be resumed (use memory to resume sementically) Graph execution can resume to last node without running from the start Best for Interactive agents, quick iteration Pipelines, reproducible runs, analytics <p>Note: Nested static\u2011graph calls are not supported at the moment (no calling one <code>@graphify</code> from another as a node). Compose via tools or run graphs separately.</p>"},{"location":"key-concepts/static-graph-agent/#3-define-and-build-a-graph","title":"3. Define and Build a Graph","text":"<pre><code>from aethergraph import graphify, tool\n\n@tool(outputs=[\"rows\"])       \ndef load_csv(path: str): ...\n\n@tool(outputs=[\"clean\"])      \ndef clean(rows): ...\n\n@tool(outputs=[\"model\"])      \ndef train(data): ...\n\n@tool(outputs=[\"uri\"])        \ndef report(model): ...\n\n@graphify(name=\"etl_train_report\", inputs=[\"csv_path\"], outputs=[\"uri\"]) \ndef etl_train_report(csv_path):\n    raw  = load_csv(path=csv_path)\n    tidy = clean(rows=raw.rows)\n    mod  = train(data=tidy.clean)\n    rep  = report(model=mod.model)\n    return {\"uri\": rep.uri}\n\nG = etl_train_report.build()     # \u2192 TaskGraph\n</code></pre> <p>All the input and output in the definition of a graph builder are <code>graph_ref</code> and <code>NodeHandler</code>, respectively. Accessing them in the graph builder will not display the actual value (e.g. you cannot access <code>raw[0]</code> or <code>raw.rows[0]</code> and pass it to the next tool). Always use a <code>@tool</code> to pack/uppack variables or integrate multiple small steps in a <code>@tool</code>. </p>"},{"location":"key-concepts/static-graph-agent/#control-ordering-without-data-edges","title":"Control ordering without data edges","text":"<p>Use <code>_after</code> to enforce sequence when there\u2019s no data dependency:</p> <pre><code>@tool(outputs=[\"ok\"])    \ndef fetch(): ...\n\n@tool(outputs=[\"done\"])  \ndef train(): ...\n\n@graphify(name=\"seq\", inputs=[], outputs=[\"done\"]) \ndef seq():\n    a = fetch()\n    b = train(_after=a)        # run `train` after `fetch`\n    return {\"done\": b.done}\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#referencing-tool-outputs-dot-vs-key","title":"Referencing Tool Outputs (dot vs. key)","text":"<p>Each <code>@tool</code> must declare its outputs. AetherGraph wraps the call in a handle whose fields mirror those names, so you can access them either as attributes or dict keys \u2014 both are equivalent.</p> <pre><code>@tool(outputs=[\"rows\"])\ndef load_csv(path: str):\n# must return a dict matching declared outputs\nreturn {\"rows\": parse_csv(path)}\n\n\n@tool(outputs=[\"clean\"])\ndef clean(rows):\nreturn {\"clean\": tidy(rows)}\n\n\n@graphify(name=\"etl\", inputs=[\"csv_path\"], outputs=[\"clean\"])\ndef etl(csv_path):\nraw = load_csv(path=csv_path)\n# Access either way; these are equivalent\ntidy1 = clean(rows=raw.rows) # dot access\ntidy2 = clean(rows=raw[\"rows\"]) # key access\nreturn {\"clean\": tidy1.clean}\n</code></pre> <p>Consistency matters: declared output names (e.g.,<code>outputs=[\"rows\"]</code>) must match the keys you return from the tool (e.g., <code>{\"rows\": ...}</code>). Mismatches raise clear build/runtime errors.</p> <p>Multiple outputs <pre><code>@tool(outputs=[\"mean\", \"std\"])\ndef stats(xs: list[float]):\nreturn {\"mean\": avg(xs), \"std\": stdev(xs)}\n\n\n@graphify(name=\"use_stats\", inputs=[\"xs\"], outputs=[\"m\"])\ndef use_stats(xs):\ns = stats(xs=xs)\nreturn {\"m\": s.mean} # or s[\"mean\"]\n</code></pre></p> <p>Think of tool calls as typed nodes whose declared outputs become fields on the node handle.</p>"},{"location":"key-concepts/static-graph-agent/#4-fanin-fanout-patterns","title":"4. Fan\u2011in / Fan\u2011out Patterns","text":"<pre><code>@tool(outputs=[\"v\"]) \ndef step(x: int): ...\n\n@tool(outputs=[\"z\"]) \ndef join(a, b): ...\n\n@graphify(name=\"fan\", inputs=[\"x1\", \"x2\"], outputs=[\"z\"]) \ndef fan(x1, x2):\n    a = step(x=\"x1\")  # fan\u2011out 1\n    b = step(x=\"x2\")  # fan\u2011out 2\n    j = join(a=a.v, b=b.v)  # fan\u2011in\n    return {\"z\": j.z}\n</code></pre> <p>Tips: you can use for loop to create fan-in and fan-out</p>"},{"location":"key-concepts/static-graph-agent/#5-run-a-built-graph","title":"5. Run a Built Graph","text":"<p>Run the materialized DAG with the runner (sync or async):</p> <pre><code>from aethergraph.runner import run, run_async\n\nresult = run(G, inputs={\"csv_path\": \"data/train.csv\"})\n# \u2192 {\"uri\": \"file://...\"}\n\n# Async form (e.g., inside another async function)\nfinal = await run_async(G, inputs={\"csv_path\": \"data/train.csv\"})\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#6-inspect-and-explore","title":"6. Inspect and Explore","text":"<p><code>@graphify</code> builders expose helpers for IO/signature and full spec:</p> <pre><code>sig  = etl_train_report.io()     # inputs/outputs signature\nspec = etl_train_report.spec()   # GraphSpec (nodes, edges, metadata)\n</code></pre> <p>Runtime helpers on <code>TaskGraph</code>:</p> <pre><code>print(G.pretty())                # human\u2011friendly table\nprint(G.ascii_overview())        # compact overview\n\n# Select / find nodes\nids     = G.list_nodes()                         # visible node_ids\nfirst_c = G.find_by_logic(\"clean\", first=True)  # by tool/logic name\nsome    = G.find_by_label(\"train\")              # by label\nsel     = G.select(\"@my_alias\")                 # mini\u2011DSL (@alias, #label, logic:, name:, id:, /regex/)\n\n# Topology &amp; subgraphs\norder   = G.topological_order()                  # raises if cycles\nup      = G.get_upstream_nodes(first_c)          # dependency closure\nsub     = G.get_subgraph_nodes(first_c)          # downstream closure\n</code></pre> <p>Export / visualize</p> <pre><code>dot = G.to_dot()                 # Graphviz DOT\n# G.visualize()                  # if enabled: render to file/viewer\n</code></pre>"},{"location":"key-concepts/static-graph-agent/#key-points","title":"Key Points","text":"<ul> <li><code>@graphify</code> builds a DAG from <code>@tool</code> calls; you run it later.</li> <li>Use <code>_after</code> to force ordering without data edges.</li> <li>Fan\u2011out/fan\u2011in is natural with multiple <code>@tool</code> calls and a later join.</li> <li>Inspect via <code>.io()</code>, <code>.spec()</code>, <code>TaskGraph.pretty()</code>, <code>ascii_overview()</code>, <code>to_dot()</code>.</li> <li>No nested static graphs currently (don\u2019t call one <code>@graphify</code> from another as a node).</li> <li>For reactive agents, stick with <code>@graph_fn</code>; for pipelines, prefer <code>@graphify</code>.</li> </ul>"},{"location":"llm-setup/llm-setup/","title":"LLMs &amp; RAG \u2013 Central Setup Guide","text":"<p>This page is the one\u2011stop reference for configuring LLM profiles and RAG in AetherGraph. It covers environment\u2011based setup, runtime registration, and per\u2011call overrides.</p>"},{"location":"llm-setup/llm-setup/#1-quick-start-via-env","title":"1. Quick Start via <code>.env</code>","text":""},{"location":"llm-setup/llm-setup/#llm-settings-openai-example","title":"LLM Settings (OpenAI example)","text":"<pre><code># Enable LLM service\nAETHERGRAPH_LLM__ENABLED=true\n\n# Default profile \"default\"\nAETHERGRAPH_LLM__DEFAULT__PROVIDER=openai\nAETHERGRAPH_LLM__DEFAULT__MODEL=gpt-4o-mini\nAETHERGRAPH_LLM__DEFAULT__TIMEOUT=60\nAETHERGRAPH_LLM__DEFAULT__API_KEY=sk-...                # your OpenAI key\nAETHERGRAPH_LLM__DEFAULT__EMBED_MODEL=text-embedding-3-small\n</code></pre> <p>This makes <code>context.llm()</code> (or <code>context.llm(profile=\"default\")</code>) available everywhere.</p>"},{"location":"llm-setup/llm-setup/#extra-profiles-example-google-gemini","title":"Extra profiles (example: Google Gemini)","text":"<p><pre><code># Profile name: GEMINI\nAETHERGRAPH_LLM__PROFILES__GEMINI__PROVIDER=google\nAETHERGRAPH_LLM__PROFILES__GEMINI__MODEL=gemini-2.5-flash-lite\nAETHERGRAPH_LLM__PROFILES__GEMINI__TIMEOUT=60\nAETHERGRAPH_LLM__PROFILES__GEMINI__API_KEY=AIzaSy...     # your Google key\nAETHERGRAPH_LLM__PROFILES__GEMINI__EMBED_MODEL=text-embedding-004\n</code></pre> This makes <code>context.llm(profile=\"gemeni\")</code> available everywhere.</p> <p>You can add as many profiles as you like (Anthropic, LM Studio, Azure OpenAI, etc.).</p>"},{"location":"llm-setup/llm-setup/#2-runtime-registration-no-restart-needed","title":"2. Runtime Registration (no restart needed)","text":"<p>Use these helpers from <code>aethergraph.runtime</code> when you want to configure LLMs at runtime (e.g., notebooks, scripts):</p> <pre><code>from aethergraph.server import start as start_server\nfrom aethergraph.runtime import register_llm_client, set_rag_llm_client\n\n# 1) Start the sidecar so services are wired\nurl = start_server(port=0)\n\n# 2) Register multiple LLM profiles\nopenai_client = register_llm_client(\n    profile=\"my_openai\",\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    api_key=\"sk-...\",\n)\n\nregister_llm_client(\n    profile=\"my_lmstudio\",\n    provider=\"lmstudio\",\n    model=\"qwen/qwen2.5-vl-7b\",\n    base_url=\"http://localhost:1234/v1\",   # LM Studio uses /v1\n)\n\nregister_llm_client(\n    profile=\"my_anthropic\",\n    provider=\"anthropic\",\n    model=\"claude-3\",\n    api_key=\"sk-ant-...\",\n)\n\nregister_llm_client(\n    profile=\"my_gemini\",\n    provider=\"google\",\n    model=\"gemini-2.5-flash-lite\",\n    api_key=\"AIzaSy...\",\n)\n\n# 3) RAG LLM client (defaults to \"default\" profile if not set)\nset_rag_llm_client(client=openai_client)\n# or create by parameters\nset_rag_llm_client(\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    embed_model=\"text-embedding-3-small\",\n    api_key=\"sk-...\",\n)\n</code></pre>"},{"location":"llm-setup/llm-setup/#3-inline-overrides-in-code","title":"3. Inline Overrides in Code","text":"<p>When you need to switch providers/models on the fly, use the inline API on <code>NodeContext</code>:</p> <pre><code># Get existing profile (no overrides)\nllm_client = context.llm(profile=\"default\")\n\n# Or create/update a profile inline\nllm_client = context.llm(\n    profile=\"temp_profile\",\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    api_key=\"sk-...\",\n    timeout=60,\n)\n\n# Quick credential swap\ncontext.llm_set_key(provider=\"openai\", model=\"gpt-4o-mini\", api_key=\"sk-...\", profile=\"default\")\n</code></pre>"},{"location":"llm-setup/llm-setup/#4-rag-setup","title":"4. RAG Setup","text":""},{"location":"llm-setup/llm-setup/#defaults","title":"Defaults","text":"<ul> <li>The RAG service uses the default LLM profile unless you explicitly set a RAG client.</li> <li>Ensure the profile used by RAG has a valid <code>embed_model</code>.</li> </ul>"},{"location":"llm-setup/llm-setup/#env-rag-settings","title":"<code>.env</code> RAG Settings","text":"<pre><code># RAG Settings\nAETHERGRAPH_RAG__BACKEND=faiss   # e.g., faiss or sqlite (faiss may need extra deps)\nAETHERGRAPH_RAG__DIM=1536        # match your embedding dimensionality\n</code></pre> <p>If FAISS isn\u2019t available, the system will auto\u2011fallback to SQLite vector index.</p>"},{"location":"llm-setup/llm-setup/#runtime-rag-configuration","title":"Runtime RAG Configuration","text":"<p>Use these helpers from <code>aethergraph.runtime</code>:</p> <pre><code>from aethergraph.runtime import set_rag_llm_client, set_rag_index_backend\n\n# Choose the LLM for RAG (client or by params)\nrag_client = set_rag_llm_client(client=openai_client)\n# or\nrag_client = set_rag_llm_client(\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    embed_model=\"text-embedding-3-small\",\n    api_key=\"sk-...\",\n)\n\n# Configure the index backend (backend=\"sqlite\" | \"faiss\")\nset_rag_index_backend(backend=\"faiss\", index_path=None, dim=1536)\n</code></pre>"},{"location":"llm-setup/llm-setup/#5-minimal-usage-example","title":"5. Minimal Usage Example","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"llm.multiple.profile.demo\")\nasync def llm_multiple_profile_demo(profile: str, *, context: NodeContext):\n    llm_client = context.llm(profile=profile)\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a concise and helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"In one sentence, what is an attention layer in neural networks?\"},\n    ]\n    text, usage = await llm_client.chat(messages)\n\n    # GPT\u20115 models support optional reasoning controls, e.g.:\n    # text, usage = await llm_client.chat(messages, reasoning_effort=\"low\")\n\n    return {\"profile\": profile, \"answer\": text, \"usage\": usage}\n</code></pre>"},{"location":"llm-setup/llm-setup/#6-tips-notes","title":"6. Tips &amp; Notes","text":"<ul> <li>Security: Keep API keys in <code>.env</code> or secret managers; avoid hard\u2011coding in scripts.</li> <li>Profiles: Name them by purpose (e.g., <code>summarize</code>, <code>vision</code>, <code>rag</code>) to keep code expressive.</li> <li>Embedding dim: Ensure <code>AETHERGRAPH_RAG__DIM</code> matches your selected <code>embed_model</code>.</li> <li>LM Studio / self\u2011hosted: set <code>base_url</code> (and deployment name for Azure) as required by your provider.</li> </ul>"},{"location":"recipes/data-analysis-loop/","title":"Recipe: Iterative Data Analysis","text":"<ul> <li>User asks for analysis \u2192 generate code</li> <li>Run code; store figures &amp; tables in <code>artifacts()</code></li> <li>Record metrics in <code>memory()</code> and summarize at the end</li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"recipes/job-submit-poll/","title":"Recipe: Submit + Poll + Notify","text":"<ul> <li>Submit a long-running job</li> <li>Poll status and send progress via <code>channel()</code></li> <li>On failure, ask user to retry or stop; save logs to <code>artifacts()</code></li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"recipes/memory-rag-mini/","title":"Recipe: Mini Memory\u2011RAG","text":"<ul> <li>Ingest notes as <code>memory().record(kind=\"note\", data=...)</code></li> <li>Simple retrieval via <code>memory().recent()/query()</code> into <code>llm().chat()</code></li> </ul> <p>(Implementation stub \u2014 fill with your runtime specifics.)</p>"},{"location":"reference/config/","title":"Config Keys","text":"<ul> <li><code>AETH_PORT</code> \u2014 server port</li> <li><code>LOG_LEVEL</code> \u2014 logging level (e.g., INFO)</li> <li><code>MODEL_NAME</code> \u2014 default LLM</li> <li><code>SLACK_*</code> \u2014 Slack integration (bot token, signing secret)</li> </ul> <p>(Expand with your actual config schema.)</p>"},{"location":"reference/context-adhoc/","title":"Using <code>context</code> outside a graph (Ad\u2011hoc sessions)","text":"<p>Sometimes you want a fully\u2011featured <code>NodeContext</code> without constructing a graph\u2014e.g., quick scripts, notebooks, data prep, or admin tasks. AetherGraph exposes an async helper you can import from <code>aethergraph.runtime</code>:</p> <ul> <li><code>open_session(...)</code> \u2013 async context manager that yields a temporary, fully wired <code>NodeContext</code> and cleans up afterward.</li> </ul> <p>These sessions are \u201csingle\u2011node\u201d and aren\u2019t scheduled like a graph run, but they provide the same services: <code>context.llm()</code>, <code>context.artifacts()</code>, <code>context.memory()</code>, <code>context.kv()</code>, <code>context.logger()</code>, <code>context.rag</code>, <code>context.mcp()</code>, and any custom services you\u2019ve registered.</p>"},{"location":"reference/context-adhoc/#quick-start","title":"Quick Start","text":"<pre><code>from aethergraph.runtime import open_session\n\n# Recommended: context manager form (handles cleanup)\nasync with open_session(run_id=\"adhoc-demo\", graph_id=\"adhoc\", node_id=\"adhoc\") as context:\n    context.logger().info(\"hello from adhoc\")\n    txt, _ = await context.llm().chat([\n        {\"role\": \"user\", \"content\": \"Give me a haiku about photons.\"}\n    ])\n    print(txt)\n</code></pre>"},{"location":"reference/context-adhoc/#common-parameters","title":"Common parameters","text":"<ul> <li><code>run_id: str | None</code> \u2014 logical session/run identifier (default: auto\u2011generated)</li> <li><code>graph_id: str</code> \u2014 namespace label for this session (default: <code>\"adhoc\"</code>)</li> <li><code>node_id: str</code> \u2014 node label within the session (default: <code>\"adhoc\"</code>)</li> <li><code>**rt_overrides</code> \u2014 advanced runtime knobs you can pass through (e.g., <code>max_concurrency</code>, service injections)</li> </ul> <p>Tip: Start your sidecar server (<code>start_server(...)</code>) first if you plan to use external channels, resumable waits, or any service that relies on the sidecar\u2019s wiring.</p>"},{"location":"reference/context-adhoc/#api-reference","title":"API Reference","text":"open_session(*, run_id: str | None = None, graph_id: str = \"adhoc\", node_id: str = \"adhoc\", **rt_overrides) <p>Description: pen a temporary, fully wired <code>NodeContext</code> for ad\u2011hoc use.</p> <p>Inputs:</p> <ul> <li><code>run_id: str</code></li> <li><code>run_id: str</code></li> <li><code>node_id: str</code></li> <li><code>rt_overrides</code> \u2014 runtime overrides. See graph runner doc for details.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Note: Handles setup and implicit cleanup at exit.</p> build_adhoc_context(*, run_id: str | None = None, graph_id: str = \"adhoc\", node_id: str = \"adhoc\", **rt_overrides) <p>Inputs:</p> <ul> <li><code>run_id: str</code></li> <li><code>run_id: str</code></li> <li><code>node_id: str</code></li> <li><code>rt_overrides</code> \u2014 runtime overrides. See graph runner doc for details.</li> </ul> <p>Returns: </p> <ul> <li><code>NodeContext</code> (caller manages any cleanup).</li> </ul>"},{"location":"reference/context-adhoc/#notes-gotchas","title":"Notes &amp; gotchas","text":"<ul> <li><code>open_session()</code> gives you a real <code>NodeContext</code> bound to a throwaway run/graph/node. Treat it like a normal node.</li> <li>If you rely on external adapters (Slack/Telegram/Web UI) or resumable waits, ensure the sidecar server is running.</li> <li>For long\u2011lived scripting, prefer <code>open_session()</code>; reach for <code>build_adhoc_context()</code> only when you need total control.</li> <li>Services which are not configured (e.g., RAG, MCP) will raise a clear <code>RuntimeError</code>. Wire them via your runtime config or server startup.</li> </ul>"},{"location":"reference/context-artifacts/","title":"<code>context.artifacts()</code> \u2013 ArtifactFacade API Reference","text":"<p>The <code>ArtifactFacade</code> wraps an <code>AsyncArtifactStore</code> (persistence) and an <code>AsyncArtifactIndex</code> (search/metadata) and automatically indexes artifacts you create within a node/run.</p>"},{"location":"reference/context-artifacts/#concepts-defaults","title":"Concepts &amp; Defaults","text":"<ul> <li>Store vs Index: <code>store</code> persists bytes/objects; <code>index</code> stores searchable metadata and supports ranking, pinning, and scoping.</li> <li>Automatic indexing: <code>ingest</code>, <code>save*</code>, <code>writer</code>, and <code>ingest_dir</code> all upsert into the index and record an occurrence.</li> <li>Scoping: Many queries default to the current run; you can widen to <code>\"graph\"</code>, <code>\"node\"</code>, or <code>\"all\"</code>.</li> <li><code>last_artifact</code>: Updated to the most recently created/ingested artifact when applicable; <code>None</code> otherwise (e.g., no write in <code>writer</code>).</li> <li><code>suggested_uri</code>: A hint for stores that support friendly paths; stores may ignore or normalize it.</li> <li><code>pin</code>: Marks an artifact as pinned in the index to prevent GC or to highlight in UIs (store behavior may vary).</li> </ul>"},{"location":"reference/context-artifacts/#artifact-schema-contract","title":"Artifact Schema (contract)","text":"<p>A minimal summary of <code>Artifact</code> fields for reference:</p> <pre><code>Artifact(\n  artifact_id: str,\n  uri: str,\n  kind: str,\n  bytes: int,\n  sha256: str,\n  mime: str | None,\n  run_id: str, graph_id: str, node_id: str,\n  tool_name: str, tool_version: str,\n  created_at: str,\n  labels: dict[str, Any],\n  metrics: dict[str, Any],\n  preview_uri: str | None = None,\n  pinned: bool = False,\n)\n</code></pre>"},{"location":"reference/context-artifacts/#quick-reference","title":"Quick Reference","text":"Method Purpose Returns <code>stage(ext=\"\")</code> Plan a staging file path <code>str</code> path <code>ingest(staged_path, *, kind, labels=None, metrics=None, suggested_uri=None, pin=False)</code> Ingest a staged file and index it <code>Artifact</code> <code>save(path, *, kind, labels=None, metrics=None, suggested_uri=None, pin=False)</code> Save an existing file and index it <code>Artifact</code> <code>save_text(payload, *, suggested_uri=None)</code> Save small text as an artifact <code>Artifact</code> <code>save_json(payload, *, suggested_uri=None)</code> Save JSON-serializable object <code>Artifact</code> <code>writer(*, kind, planned_ext=None, pin=False)</code> Open a write context (yields writer) (context manager) <code>stage_dir(suffix=\"\")</code> Plan a staging directory <code>str</code> path <code>ingest_dir(staged_dir, **kw)</code> Ingest a whole directory <code>Artifact</code> <code>tmp_path(suffix=\"\")</code> Alias to plan a staging file path <code>str</code> path <code>load_bytes(uri)</code> Read raw bytes <code>bytes</code> <code>load_text(uri, *, encoding=\"utf-8\", errors=\"strict\")</code> Read text <code>str</code> <code>load_json(uri, *, encoding=\"utf-8\", errors=\"strict\")</code> Read JSON <code>Any</code> <code>load_artifact(uri)</code> Load Artifact metadata/object <code>Artifact           | Any</code> <code>load_artifact_bytes(uri)</code> Read bytes from artifact URI <code>bytes</code> <code>list(*, scope=\"run\")</code> List artifacts by scope <code>list[Artifact]</code> <code>search(*, kind=None, labels=None, metric=None, mode=None, scope=\"run\", extra_scope_labels=None)</code> Search with filters/metrics <code>list[Artifact]</code> <code>best(*, kind, metric, mode, scope=\"run\", filters=None)</code> Best-scoring artifact <code>Artifact           | None</code> <code>pin(artifact_id, pinned=True)</code> Pin/unpin in the index <code>None</code> <code>to_local_path(uri_or_path, *, must_exist=True)</code> Resolve to local path (file:// only) <code>str</code> <code>to_local_file(uri_or_path, *, must_exist=True)</code> Resolve &amp; assert file <code>str</code> <code>to_local_dir(uri_or_path, *, must_exist=True)</code> Resolve &amp; assert dir <code>str</code>"},{"location":"reference/context-artifacts/#methods","title":"Methods","text":"stage(ext=\"\") -&gt; str <p>Description: Plan a staging file location for temporary writes outside the index. Use with external writers, then call <code>ingest()</code>.</p> <p>Inputs:</p> <ul> <li><code>ext: str</code> \u2013 Optional extension (e.g., <code>.png</code>, <code>.txt</code>).</li> </ul> <p>Returns:</p> <ul> <li><code>str</code> \u2013 Path to a writable staging file.</li> </ul> <p>Notes: Staging does not create or index the artifact; call <code>ingest()</code> afterward.</p> ingest(staged_path, *, kind, labels=None, metrics=None, suggested_uri=None, pin=False) -&gt; Artifact <p>Description: Ingest a previously staged file into the store, then upsert &amp; record occurrence in the index.</p> <p>Inputs:</p> <ul> <li><code>staged_path: str</code></li> <li><code>kind: str</code> \u2013 Domain tag (e.g., <code>\"image\"</code>, <code>\"report\"</code>).</li> <li><code>labels: dict | None</code></li> <li><code>metrics: dict | None</code></li> <li><code>suggested_uri: str | None</code></li> <li><code>pin: bool</code> \u2013 Mark as pinned in the index.</li> </ul> <p>Returns:</p> <ul> <li><code>Artifact</code></li> </ul> <p>Notes: Sets <code>last_artifact</code> to the newly ingested item.</p> save(path, *, kind, labels=None, metrics=None, suggested_uri=None, pin=False) -&gt; Artifact <p>Description: Save an existing file to the store and index it.</p> <p>Inputs:</p> <ul> <li><code>path: str</code></li> <li><code>kind: str</code></li> <li><code>labels: dict | None</code></li> <li><code>metrics: dict | None</code></li> <li><code>suggested_uri: str | None</code></li> <li><code>pin: bool</code></li> </ul> <p>Returns:</p> <ul> <li><code>Artifact</code></li> </ul> <p>Notes: Updates <code>last_artifact</code>.</p> save_text(payload, *, suggested_uri=None) -&gt; Artifact <p>Description: Save a small text blob; store chooses encoding/URI. Indexed automatically.</p> <p>Inputs:</p> <ul> <li><code>payload: str</code></li> <li><code>suggested_uri: str | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>Artifact</code></li> </ul> <p>Notes: Prefer <code>save()</code> for large files; stores may have size limits for text.</p> save_json(payload, *, suggested_uri=None) -&gt; Artifact <p>Description: Save a JSON-serializable object.</p> <p>Inputs:</p> <ul> <li><code>payload: dict</code></li> <li><code>suggested_uri: str | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>Artifact</code></li> </ul> <p>Notes: Useful for configs, specs, and structured reports.</p> writer(*, kind, planned_ext=None, pin=False) -&gt; Async CM (yields writer) <p>Description: Open a writer context provided by the store; write bytes within the block. On exit, the created artifact (if any) is indexed.</p> <p>Inputs:</p> <ul> <li><code>kind: str</code></li> <li><code>planned_ext: str | None</code> \u2013 Hint for file extension.</li> <li><code>pin: bool</code></li> </ul> <p>Returns:</p> <ul> <li>(Context Manager) \u2013 Yields a <code>writer</code> implementing <code>write(...)</code> (store-specific).</li> </ul> <p>Notes:</p> <ul> <li>If the writer actually creates an artifact, it will be available as <code>writer._artifact</code> on exit and set as <code>last_artifact</code>.</li> <li>Use this when you don\u2019t have the data on disk yet and want the store to manage file lifecycle.</li> </ul> <p>Example:</p> <pre><code>async with context.artifacts().writer(kind=\"report\", planned_ext=\".txt\") as w:\n    w.write(b\"hello world\")\n# last_artifact now refers to the saved report\n</code></pre> stage_dir(suffix=\"\") -&gt; str <p>Description: Plan a staging directory path. Use with tools that emit multiple files before ingestion.</p> <p>Inputs:</p> <ul> <li><code>suffix: str</code> \u2013 Optional suffix/name.</li> </ul> <p>Returns:</p> <ul> <li><code>str</code> \u2013 Path to a staging directory.</li> </ul> ingest_dir(staged_dir, **kw) -&gt; Artifact <p>Description: Ingest a directory (e.g., a report folder) into the store and index it.</p> <p>Inputs:</p> <ul> <li><code>staged_dir: str</code></li> <li><code>**kw</code> \u2013 Store/index-specific options (e.g., <code>kind</code>, <code>labels</code>, <code>metrics</code>, <code>pin</code>).</li> </ul> <p>Returns:</p> <ul> <li><code>Artifact</code></li> </ul> <p>Notes: Updates <code>last_artifact</code>.</p> tmp_path(suffix=\"\") -&gt; str <p>Description: Convenience alias to plan a staging file path.</p> <p>Inputs:</p> <ul> <li><code>suffix: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>str</code></li> </ul> load_bytes(uri) -&gt; bytes <p>Description: Load raw bytes from an artifact URI.</p> <p>Inputs:</p> <ul> <li><code>uri: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>bytes</code></li> </ul> load_text(uri, *, encoding=\"utf-8\", errors=\"strict\") -&gt; str <p>Description: Load text from an artifact URI.</p> <p>Inputs:</p> <ul> <li><code>uri: str</code></li> <li><code>encoding: str</code></li> <li><code>errors: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>str</code></li> </ul> load_json(uri, *, encoding=\"utf-8\", errors=\"strict\") -&gt; Any <p>Description: Load JSON from an artifact URI.</p> <p>Inputs:</p> <ul> <li><code>uri: str</code></li> <li><code>encoding: str</code></li> <li><code>errors: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>Any</code></li> </ul> load_artifact(uri) -&gt; Artifact | Any <p>Description: Load an Artifact or store-specific artifact object from URI.</p> <p>Inputs:</p> <ul> <li><code>uri: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>Artifact | Any</code></li> </ul> load_artifact_bytes(uri) -&gt; bytes <p>Description: Load bytes from an artifact URI (explicit artifact pathway).</p> <p>Inputs:</p> <ul> <li><code>uri: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>bytes</code></li> </ul> list(*, scope=\"run\") -&gt; list[Artifact] <p>Description: Quick listing of artifacts scoped to <code>\"run\"</code> by default.</p> <p>Inputs:</p> <ul> <li><code>scope: Literal[\"node\", \"run\", \"graph\", \"all\"]</code></li> </ul> <p>Returns:</p> <ul> <li><code>list[Artifact]</code></li> </ul> <p>Notes:</p> <ul> <li><code>node</code> \u2192 labels <code>(run_id, graph_id, node_id)</code>; <code>graph</code> \u2192 <code>(run_id, graph_id)</code>; <code>run</code> \u2192 by <code>run_id</code>; <code>all</code> \u2192 no implicit filters.</li> </ul> search(*, kind=None, labels=None, metric=None, mode=None, scope=\"run\", extra_scope_labels=None) -&gt; list[Artifact] <p>Description: Search the index with optional kind, labels, and metric ranking.</p> <p>Inputs:</p> <ul> <li><code>kind: str | None</code></li> <li><code>labels: dict[str, Any] | None</code></li> <li><code>metric: str | None</code></li> <li><code>mode: Literal[\"max\", \"min\"] | None</code></li> <li><code>scope: Literal[\"node\", \"run\", \"graph\", \"all\"]</code></li> <li><code>extra_scope_labels: dict[str, Any] | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>list[Artifact]</code></li> </ul> <p>Notes: Applies scope labels automatically when scope is <code>node</code> or <code>graph</code>. <code>extra_scope_labels</code> lets you add more filters.</p> best(*, kind, metric, mode, scope=\"run\", filters=None) -&gt; Artifact | None <p>Description: Return the best-scoring artifact for a metric (e.g., highest accuracy).</p> <p>Inputs:</p> <ul> <li><code>kind: str</code></li> <li><code>metric: str</code></li> <li><code>mode: Literal[\"max\", \"min\"]</code></li> <li><code>scope: Literal[\"node\", \"run\", \"graph\", \"all\"]</code></li> <li><code>filters: dict[str, Any] | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>Artifact | None</code></li> </ul> <p>Notes: Applies scope filters automatically for <code>node</code> or <code>graph</code>.</p> pin(artifact_id, pinned=True) -&gt; None <p>Description: Pin or unpin an artifact in the index.</p> <p>Inputs:</p> <ul> <li><code>artifact_id: str</code></li> <li><code>pinned: bool</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> to_local_path(uri_or_path, *, must_exist=True) -&gt; str <p>Description: Resolve a file:// URI or local path to an absolute native path.</p> <p>Inputs:</p> <ul> <li><code>uri_or_path: str | Path | Artifact</code></li> <li><code>must_exist: bool</code></li> </ul> <p>Returns:</p> <ul> <li><code>str</code> \u2013 Absolute path or input string for non-file schemes.</li> </ul> <p>Notes:</p> <ul> <li>If the input uses a non-file scheme (e.g., <code>s3://</code>, <code>http://</code>), the string is returned unchanged.</li> <li>Raises <code>FileNotFoundError</code> if <code>must_exist=True</code> and path missing.</li> </ul> to_local_file(uri_or_path, *, must_exist=True) -&gt; str <p>Description: Resolve to a file path and assert it is a file.</p> <p>Inputs:</p> <ul> <li><code>uri_or_path: str | Path | Artifact</code></li> <li><code>must_exist: bool</code></li> </ul> <p>Returns:</p> <ul> <li><code>str</code></li> </ul> <p>Notes: Raises <code>IsADirectoryError</code> if path is a directory when <code>must_exist=True</code>.</p> to_local_dir(uri_or_path, *, must_exist=True) -&gt; str <p>Description: Resolve to a directory path and assert it is a directory.</p> <p>Inputs:</p> <ul> <li><code>uri_or_path: str | Path | Artifact</code></li> <li><code>must_exist: bool</code></li> </ul> <p>Returns:</p> <ul> <li><code>str</code></li> </ul> <p>Notes: Raises <code>NotADirectoryError</code> if path is a file when <code>must_exist=True</code>.</p>"},{"location":"reference/context-channel/","title":"<code>context.channel()</code> \u2013 ChannelSession API Reference","text":"<p>A <code>ChannelSession</code> provides message I/O, user prompts, streaming text, and progress updates through the configured channel (console/Slack/\u2026). It also manages continuation tokens to avoid race conditions.</p>"},{"location":"reference/context-channel/#channel-resolution-defaults","title":"Channel Resolution &amp; Defaults","text":"<ul> <li>Channel selection priority: explicit <code>channel</code> arg \u2192 session override (from <code>context.channel(channel_key)</code>) \u2192 bus default \u2192 <code>console:stdin</code>.</li> <li>Events are published after alias \u2192 canonical key resolution.</li> </ul>"},{"location":"reference/context-channel/#quick-reference","title":"Quick Reference","text":"Method Purpose Returns <code>send(event, *, channel=None)</code> Publish a pre-built <code>OutEvent</code> <code>None</code> <code>send_text(text, *, meta=None, channel=None)</code> Send a plain text message <code>None</code> <code>send_rich(text=None, *, rich=None, meta=None, channel=None)</code> Send structured content + optional text <code>None</code> <code>send_image(url=None, *, alt=\"image\", title=None, channel=None)</code> Send an image <code>None</code> <code>send_file(url=None, *, file_bytes=None, filename=\"file.bin\", title=None, channel=None)</code> Upload/send a file <code>None</code> <code>send_buttons(text, buttons, *, meta=None, channel=None)</code> Send message with buttons <code>None</code> <code>ask_text(prompt, *, timeout_s=3600, silent=False, channel=None)</code> Prompt for free\u2011form text <code>str</code> <code>wait_text(*, timeout_s=3600, channel=None)</code> Alias of <code>ask_text(None, silent=True)</code> <code>str</code> <code>ask_approval(prompt, options=(\"Approve\",\"Reject\"), *, timeout_s=3600, channel=None)</code> Choice/confirmation <code>{approved, choice}</code> <code>ask_files(*, prompt, accept=None, multiple=True, timeout_s=3600, channel=None)</code> Prompt for uploads <code>{text, files}</code> <code>ask_text_or_files(*, prompt, timeout_s=3600, channel=None)</code> Text or files <code>{text, files}</code> <code>get_latest_uploads(*, clear=True)</code> Read inbox uploads (Ephemeral KV) <code>list[FileRef]</code> <code>stream(channel=None)</code> Streaming text (ctx manager) <code>StreamSender</code> <code>progress(*, title=\"Working...\", total=None, key_suffix=\"progress\", channel=None)</code> Progress UI (ctx manager) <code>ProgressSender</code>"},{"location":"reference/context-channel/#methods","title":"Methods","text":"send(event, *, channel=None) <p>Description: Publish a pre\u2011built <code>OutEvent</code> to the channel.</p> <p>Inputs:</p> <ul> <li><code>event: OutEvent</code> \u2013 If missing <code>channel</code>, resolver fills it.</li> <li><code>channel: str | None</code> \u2013 Optional override.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code> (async)</li> </ul> <p>Notes: Use when you need full control of the event payload/type.</p> send_text(text, *, meta=None, channel=None) <p>Description: Send a plain text message.</p> <p>Inputs:</p> <ul> <li><code>text: str</code></li> <li><code>meta: dict[str, Any] | None</code></li> <li><code>channel: str | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code> (async)</li> </ul> <p>Emits:</p> <ul> <li><code>agent.message</code> with <code>text</code>, optional <code>meta</code>.</li> </ul> send_rich(text=None, *, rich=None, meta=None, channel=None) <p>Description: Send a message with a rich structured payload (cards/blocks), plus optional text.</p> <p>Inputs:</p> <ul> <li><code>text: str | None</code></li> <li><code>rich: dict[str, Any] | None</code></li> <li><code>meta: dict[str, Any] | None</code></li> <li><code>channel: str | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code> (async)</li> </ul> <p>Emits:</p> <ul> <li><code>agent.message</code> with <code>text</code>, <code>rich</code>, <code>meta</code>.</li> </ul> send_image(url=None, *, alt=\"image\", title=None, channel=None) <p>Description: Send an image by URL.</p> <p>Inputs:</p> <ul> <li><code>url: str | None</code></li> <li><code>alt: str</code></li> <li><code>title: str | None</code></li> <li><code>channel: str | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code> (async)</li> </ul> <p>Emits:</p> <ul> <li><code>agent.message</code> with <code>image={url, alt, title}</code>.</li> </ul> send_file(url=None, *, file_bytes=None, filename=\"file.bin\", title=None, channel=None) <p>Description: Upload/send a file.</p> <p>Inputs:</p> <ul> <li><code>url: str | None</code></li> <li><code>file_bytes: bytes | None</code></li> <li><code>filename: str</code></li> <li><code>title: str | None</code></li> <li><code>channel: str | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code> (async)</li> </ul> <p>Emits:</p> <ul> <li><code>file.upload</code> with <code>file={filename, url?, bytes?}</code>.</li> </ul> <p>Notes: Adapters decide whether to fetch <code>url</code> or accept <code>bytes</code>.</p> send_buttons(text, buttons, *, meta=None, channel=None) <p>Description: Send a message with inline buttons/links.</p> <p>Inputs:</p> <ul> <li><code>text: str</code></li> <li><code>buttons: list[Button]</code></li> <li><code>meta: dict[str, Any] | None</code></li> <li><code>channel: str | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code> (async)</li> </ul> <p>Emits:</p> <ul> <li><code>link.buttons</code> with <code>text</code>, <code>buttons</code>, <code>meta</code>.</li> </ul> ask_text(prompt, *, timeout_s=3600, silent=False, channel=None) -&gt; str <p>Description: Ask the user for a free\u2011form text reply. Race\u2011free via continuations.</p> <p>Inputs:</p> <ul> <li><code>prompt: str | None</code></li> <li><code>timeout_s: int</code></li> <li><code>silent: bool</code></li> <li><code>channel: str | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>str</code> \u2013 User\u2019s response text (empty string if none)</li> </ul> <p>Notes: Creates continuation <code>kind=\"user_input\"</code> and awaits resolution; adapters may inline\u2011resolve.</p> wait_text(*, timeout_s=3600, channel=None) -&gt; str <p>Description: Alias for <code>ask_text(prompt=None, silent=True)</code>.</p> <p>Inputs:</p> <ul> <li><code>timeout_s: int</code></li> <li><code>channel: str | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>str</code></li> </ul> ask_approval(prompt, options=(\"Approve\",\"Reject\"), *, timeout_s=3600, channel=None) -&gt; dict <p>Description: Ask the user to choose from options (approval/confirmation). Normalizes to <code>{approved, choice}</code>.</p> <p>Inputs:</p> <ul> <li><code>prompt: str</code></li> <li><code>options: Iterable[str]</code></li> <li><code>timeout_s: int</code></li> <li><code>channel: str | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>{ \"approved\": bool, \"choice\": Any }</code></li> </ul> <p>Notes: Continuation <code>kind=\"approval\"</code> with payload <code>{prompt:{title, buttons}}</code>. If adapter doesn\u2019t set explicit approval, first option means approved (case\u2011insensitive comparison).</p> ask_files(*, prompt, accept=None, multiple=True, timeout_s=3600, channel=None) -&gt; dict <p>Description: Ask for file upload(s), optionally with text.</p> <p>Inputs:</p> <ul> <li><code>prompt: str</code></li> <li><code>accept: list[str] | None</code> (MIME or extensions, client hints)</li> <li><code>multiple: bool</code></li> <li><code>timeout_s: int</code></li> <li><code>channel: str | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>{ \"text\": str, \"files\": list[FileRef] }</code> (empty <code>files</code> on console\u2011only flows)</li> </ul> <p>Notes: Continuation <code>kind=\"user_files\"</code>.</p> ask_text_or_files(*, prompt, timeout_s=3600, channel=None) -&gt; dict <p>Description: Ask for either a text reply or file upload(s).</p> <p>Inputs:</p> <ul> <li><code>prompt: str</code></li> <li><code>timeout_s: int</code></li> <li><code>channel: str | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>{ \"text\": str, \"files\": list[FileRef] }</code></li> </ul> <p>Notes: Continuation <code>kind=\"user_input_or_files\"</code>.</p> get_latest_uploads(*, clear=True) -&gt; list[FileRef] <p>Description: Retrieve recent uploads captured in the channel inbox (Ephemeral KV), optionally clearing them.</p> <p>Inputs:</p> <ul> <li><code>clear: bool</code> \u2013 If <code>True</code>, pops and clears; otherwise returns without clearing.</li> </ul> <p>Returns:</p> <ul> <li><code>list[FileRef]</code></li> </ul> <p>Notes: Requires <code>services.kv</code>. Raises <code>RuntimeError</code> if unavailable. Inbox key: <code>inbox://&lt;resolved-channel-key&gt;</code>.</p> stream(channel=None) -&gt; Async CM yielding StreamSender <p>Description: Incrementally stream text to a single upserted message; adapters rewrite one message.</p> <p>Usage:</p> <pre><code>async with context.channel().stream() as s:\n    await s.delta(\"Hello \")\n    await s.delta(\"world\")\n    await s.end()  # optional\n</code></pre> <p>StreamSender Methods:</p> <ul> <li><code>start()</code> \u2192 emit <code>agent.stream.start</code> (auto\u2011called by first <code>delta</code>).</li> <li><code>delta(text_piece: str)</code> \u2192 append chunk; emits <code>agent.message.update</code> with full concatenated text (buffered) using upsert key <code>&lt;run_id&gt;:&lt;node_id&gt;:stream</code>.</li> <li><code>end(full_text: str | None = None)</code> \u2192 optional final text, then <code>agent.stream.end</code>.</li> </ul> <p>Notes: Pass <code>channel=...</code> to target a specific channel.</p> progress(*, title=\"Working...\", total=None, key_suffix=\"progress\", channel=None) -&gt; Async CM yielding ProgressSender <p>Description: Report task progress with start/update/end events.</p> <p>Usage:</p> <pre><code>async with context.channel().progress(title=\"Downloading\", total=100) as p:\n    await p.update(current=10)\n    await p.update(inc=15, subtitle=\"phase 1\")\n    await p.end(subtitle=\"Done.\")\n</code></pre> <p>ProgressSender Methods:</p> <ul> <li><code>start(subtitle: str | None = None)</code> \u2192 <code>agent.progress.start</code>.</li> <li> <p><code>update(*, current=None, inc=None, subtitle=None, percent=None, eta_seconds=None)</code> \u2192 <code>agent.progress.update</code>.</p> </li> <li> <p><code>percent</code> sets <code>current</code> when <code>total</code> known; <code>inc</code> increments; <code>current</code> overrides.</p> </li> <li><code>end(*, subtitle: str | None = \"Done.\", success: bool = True)</code> \u2192 <code>agent.progress.end</code>.</li> </ul> <p>Notes: Upserts use key <code>&lt;run_id&gt;:&lt;node_id&gt;:&lt;key_suffix&gt;</code>. Use different <code>key_suffix</code> for multiple bars.</p>"},{"location":"reference/context-channel/#continuations-racefree-waiting-behavioral-notes","title":"Continuations &amp; Race\u2011Free Waiting (behavioral notes)","text":"<ol> <li>Create continuation with <code>kind</code>, <code>payload</code>, <code>deadline</code>.</li> <li>Prepare wait future before notify (prevents wait\u2011before\u2011resume race).</li> <li><code>ChannelBus.notify()</code> may inline\u2011resolve; if so, future is resolved and continuation cleaned up.</li> <li>Bind correlators so webhooks can locate the continuation.</li> <li>Await the prepared future until the router resolves it.</li> </ol>"},{"location":"reference/context-channel/#event-types-emitted","title":"Event Types (emitted)","text":"<ul> <li><code>agent.message</code>, <code>agent.message.update</code></li> <li><code>file.upload</code></li> <li><code>link.buttons</code></li> <li><code>agent.stream.start</code>, <code>agent.stream.end</code></li> <li><code>agent.progress.start</code>, <code>agent.progress.update</code>, <code>agent.progress.end</code></li> </ul>"},{"location":"reference/context-extension/","title":"Custom Context Services \u2013 API Guide (using <code>Service</code> alias)","text":"<p>AetherGraph lets you attach your own runtime helpers onto <code>context.*</code> as first\u2011class services (e.g., <code>context.trainer()</code>, <code>context.materials()</code>).</p> <p>Use <code>Service</code> (alias of the underlying <code>BaseContextService</code>) for all subclasses:</p> <pre><code>from aethergraph import Service  # alias of BaseContextService\n</code></pre> <p>Register your service after the sidecar starts, then access it anywhere via the context: <code>context.&lt;name&gt;()</code>.</p>"},{"location":"reference/context-extension/#quick-reference","title":"Quick Reference","text":"Thing What it does <code>Service</code> Base class to subclass for context\u2011aware services (lifecycle, binding, helpers). <code>register_context_service(name, service)</code> Registers an instance under <code>context.&lt;name&gt;</code> (global registry). <code>context.svc(name)</code> Returns a bound service for <code>name</code>. <code>context.&lt;name&gt;()</code> Ergonomic accessor: returns the bound service when called with no args. <code>Service.bind(context=...)</code> Binds the current <code>NodeContext</code> into the service (used by the runtime). <code>Service.ctx()</code> Retrieve the current bound context from within the service. <code>Service.start()</code> / <code>Service.close()</code> Async lifecycle hooks (optional). <code>Service.critical()</code> Decorator for async critical sections (mutex). <code>Service.run_blocking(fn, *args, **kw)</code> Run a blocking function in a thread pool. <code>_ServiceHandle</code> Transparent wrapper returned by <code>context.&lt;name&gt;</code>; forwards attrs/calls."},{"location":"reference/context-extension/#minimal-flow","title":"Minimal Flow","text":"<pre><code>from aethergraph import start_server, Service\nfrom aethergraph.runtime import register_context_service\n\nclass MyCache(Service):\n    def __init__(self):\n        super().__init__()\n        self._data = {}\n\n    async def start(self):\n        # e.g., open connections\n        return None\n\n    async def close(self):\n        # e.g., close connections\n        return None\n\n    def get(self, k, default=None):\n        return self._data.get(k, default)\n\n    def set(self, k, v):\n        self._data[k] = v\n\n# 1) Start sidecar\nstart_server(port=0)\n\n# 2) Register a **singleton** instance for all contexts\nregister_context_service(\"cache\", MyCache())\n\n# 3) Use inside a graph/tool\nasync def do_work(*, context):\n    cache = context.cache()        # no\u2011arg call \u21d2 bound service instance\n    cache.set(\"foo\", 42)\n    return cache.get(\"foo\")\n</code></pre> <p>Why it works:</p> <ul> <li><code>context.__getattr__(\"cache\")</code> resolves the registered service and binds the active <code>NodeContext</code> via <code>Service.bind(...)</code>.</li> <li>The returned <code>_ServiceHandle</code> forwards attribute access and calls. A no\u2011arg call returns the underlying service instance for convenience.</li> </ul>"},{"location":"reference/context-extension/#methods-helpers","title":"Methods &amp; Helpers","text":"<code>class Service</code> \u2013 subclass this for custom services <p>Lifecycle</p> <ul> <li><code>async def start(self) -&gt; None</code> \u2013 optional startup hook.</li> <li><code>async def close(self) -&gt; None</code> \u2013 optional shutdown hook.</li> </ul> <p>Binding</p> <ul> <li><code>def bind(self, *, context) -&gt; Service</code> \u2013 stores the <code>NodeContext</code> and returns a bound handle.</li> <li><code>def ctx(self)</code> \u2013 access the current bound <code>NodeContext</code>.</li> </ul> <p>Concurrency &amp; Utilities</p> <ul> <li><code>def critical(self)</code> \u2013 decorator for an async mutex around a method.</li> <li><code>async def run_blocking(self, fn, *a, **kw)</code> \u2013 run CPU/IO\u2011bound sync code in a thread.</li> </ul> <code>register_context_service(name, service)</code> <p>Description: Put a singleton instance into the global registry so it becomes available as <code>context.&lt;name&gt;</code>.</p> <p>Inputs:</p> <ul> <li><code>name: str</code> \u2013 the attribute exposed on <code>context</code>.</li> <li><code>service: Service</code> \u2013 an instance (not a class).</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes: Call after <code>start_server(...)</code> to ensure the container/registry exists.</p> <code>context.svc(name) -&gt; Service</code> <p>Description: Generic accessor to retrieve a bound service by name. Raises <code>KeyError</code> if not registered.</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>Service</code></li> </ul> <code>context.&lt;name&gt;()</code> (no\u2011arg call) <p>Description: The ergonomic way to get your bound service instance: calling the attribute with no arguments returns the underlying <code>Service</code>.</p> <p>Returns:</p> <ul> <li><code>Service</code></li> </ul> <p>Notes: If your service itself is callable, <code>context.&amp;lt;name&amp;gt;(\u2026)</code> forwards to <code>service.__call__</code> (normal function call behaviour).</p>"},{"location":"reference/context-extension/#patterns","title":"Patterns","text":""},{"location":"reference/context-extension/#1-contextaware-logging-artifacts","title":"1) Context\u2011Aware Logging &amp; Artifacts","text":"<pre><code>from aethergraph import Service\n\nclass Reporter(Service):\n    def log_metric(self, name: str, value: float):\n        self.ctx().logger().info(\"metric\", extra={\"name\": name, \"value\": value})\n\n    async def save_text(self, text: str) -&gt; str:\n        art = await self.ctx().artifacts().save_text(text)\n        return art.uri\n</code></pre>"},{"location":"reference/context-extension/#2-threadsafe-mutations","title":"2) Thread\u2011safe Mutations","text":"<pre><code>from aethergraph import Service\n\nclass Counter(Service):\n    def __init__(self):\n        super().__init__()\n        self._n = 0\n        self.inc = self.critical()(self.inc)\n\n    def inc(self, k: int = 1) -&gt; int:\n        self._n += k\n        return self._n\n</code></pre>"},{"location":"reference/context-extension/#3-wrapping-external-clients","title":"3) Wrapping External Clients","text":"<pre><code>from aethergraph import Service\nimport httpx\n\nclass Weather(Service):\n    def __init__(self, base: str):\n        super().__init__()\n        self._base = base\n        self._http = httpx.Client(timeout=10)\n\n    async def close(self):\n        await self.run_blocking(self._http.close)\n\n    def get_temp_c(self, city: str) -&gt; float:\n        r = self._http.get(f\"{self._base}/temp\", params={\"city\": city})\n        r.raise_for_status()\n        return float(r.json()[\"c\"])  # noqa\n</code></pre>"},{"location":"reference/context-extension/#4-using-in-graphstools","title":"4) Using in Graphs/Tools","text":"<pre><code>from aethergraph import Service\nfrom aethergraph.runtime import register_context_service\n\nclass Greeter(Service):\n    def greet(self, name: str) -&gt; str:\n        return f\"Hello, {name}! (run={self.ctx().run_id})\"\n\nregister_context_service(\"greeter\", Greeter())\n\n# Inside a node\nasync def hello(*, context):\n    g = context.greeter()               # bound service\n    return {\"msg\": g.greet(\"World\")}\n</code></pre>"},{"location":"reference/context-extension/#faq","title":"FAQ","text":"<p>Q: Where should I create the service instance? Usually at process boot, after <code>start_server(...)</code>. Register exactly once (singleton). If you need per\u2011tenant services, implement your own map inside the service keyed by tenant.</p> <p>Q: How do I access other context services from my service? Use <code>self.ctx().&lt;other_service&gt;()</code>, e.g., <code>self.ctx().artifacts()</code> or <code>self.ctx().logger()</code>.</p> <p>Q: Can a service be async? Yes\u2014methods can be <code>async</code> and you may leverage <code>run_blocking(...)</code> for sync IO.</p> <p>Q: How do I store configuration/keys? Provide them to your service constructor, or pull from env. For dynamic secrets, wire a secrets service and read from it inside your custom service.</p>"},{"location":"reference/context-extension/#gotchas-tips","title":"Gotchas &amp; Tips","text":"<ul> <li>Name collisions: Avoid names of built\u2011ins (<code>channel</code>, <code>memory</code>, <code>artifacts</code>, etc.).</li> <li>No context at import time: <code>Service.ctx()</code> works after binding; do not call it in <code>__init__</code>.</li> <li>Thread safety: Use <code>@Service.critical</code> for shared state.</li> <li>Long\u2011running IO: Prefer <code>await self.run_blocking(...)</code> for blocking clients.</li> <li>Testing: You can construct a <code>Service</code> and call <code>bind(context=FakeContext(...))</code> directly in unit tests.</li> </ul>"},{"location":"reference/context-kv/","title":"<code>context.kv()</code> \u2013 EphemeralKV API Reference","text":"<p><code>EphemeralKV</code> is a process\u2011local, transient key\u2013value store for small JSON\u2011serializable values and short\u2011lived coordination. It is thread\u2011safe (RLock), supports TTLs, and provides a few list helpers. Do not use for large blobs or durable state.</p> <p>The API below is consistent across KV backends. In <code>context</code>, this KV is ephemeral; other implementations may be pluggable later.</p>"},{"location":"reference/context-kv/#concepts-defaults","title":"Concepts &amp; Defaults","text":"<ul> <li>Scope: In\u2011process only; cleared on restart. Not replicated.</li> <li>Thread\u2011safety: Internal reads/writes guarded by <code>RLock</code>.</li> <li>TTL: Expiry is checked lazily on access and via <code>purge_expired()</code>.</li> <li>Prefixing: Instances may prepend a <code>prefix</code> to all keys for namespacing.</li> </ul>"},{"location":"reference/context-kv/#quick-reference","title":"Quick Reference","text":"Method Purpose Returns <code>get(key, default=None)</code> Get value if present and not expired <code>Any</code> <code>set(key, value, *, ttl_s=None)</code> Set value with optional TTL <code>None</code> <code>delete(key)</code> Remove key if present <code>None</code> <code>list_append_unique(key, items, *, id_key=\"id\", ttl_s=None)</code> Append unique dict items to a list <code>list[dict]</code> <code>list_pop_all(key)</code> Pop and return entire list <code>list</code> <code>mget(keys)</code> Batch get <code>list[Any]</code> <code>mset(kv, *, ttl_s=None)</code> Batch set with optional TTL <code>None</code> <code>expire(key, ttl_s)</code> Set/refresh expiry for a key <code>None</code> <code>purge_expired(limit=1000)</code> GC expired entries (best\u2011effort) <code>int</code> count purged"},{"location":"reference/context-kv/#methods","title":"Methods","text":"get(key, default=None) -&gt; Any <p>Description: Return the current value for <code>key</code> unless missing or expired; otherwise return <code>default</code>.</p> <p>Inputs:</p> <ul> <li><code>key: str</code></li> <li><code>default: Any</code></li> </ul> <p>Returns:</p> <ul> <li><code>Any</code></li> </ul> <p>Notes: Expired entries are deleted on read.</p> set(key, value, *, ttl_s=None) -&gt; None <p>Description: Set <code>key</code> to <code>value</code> with optional TTL in seconds.</p> <p>Inputs:</p> <ul> <li><code>key: str</code></li> <li><code>value: Any</code> \u2013 Prefer small JSON\u2011serializable payloads.</li> <li><code>ttl_s: int | None</code> \u2013 Time\u2011to\u2011live (seconds).</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> delete(key) -&gt; None <p>Description: Delete <code>key</code> if present.</p> <p>Inputs:</p> <ul> <li><code>key: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> list_append_unique(key, items, *, id_key=\"id\", ttl_s=None) -&gt; list[dict] <p>Description: Append dict <code>items</code> to the list at <code>key</code>, skipping any whose <code>id_key</code> duplicates an existing item.</p> <p>Inputs:</p> <ul> <li><code>key: str</code></li> <li><code>items: list[dict]</code></li> <li><code>id_key: str</code> \u2013 Field used to determine uniqueness (default <code>\"id\"</code>).</li> <li><code>ttl_s: int | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>list[dict]</code> \u2013 The updated list.</li> </ul> <p>Notes: Non\u2011dict items are ignored.</p> list_pop_all(key) -&gt; list <p>Description: Atomically pop and return the entire list at <code>key</code>. If missing or non\u2011list, return an empty list.</p> <p>Inputs:</p> <ul> <li><code>key: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>list</code></li> </ul> mget(keys) -&gt; list[Any] <p>Description: Batch get for multiple keys.</p> <p>Inputs:</p> <ul> <li><code>keys: list[str]</code></li> </ul> <p>Returns:</p> <ul> <li><code>list[Any]</code> \u2013 Values in the same order as <code>keys</code> (expired/missing \u2192 <code>None</code> or default semantics of <code>get</code>).</li> </ul> mset(kv, *, ttl_s=None) -&gt; None <p>Description: Batch set multiple entries with an optional TTL applied to each.</p> <p>Inputs:</p> <ul> <li><code>kv: dict[str, Any]</code></li> <li><code>ttl_s: int | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> expire(key, ttl_s) -&gt; None <p>Description: Set or refresh the expiry for <code>key</code>.</p> <p>Inputs:</p> <ul> <li><code>key: str</code></li> <li><code>ttl_s: int</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes: No\u2011op if <code>key</code> is missing.</p> purge_expired(limit=1000) -&gt; int <p>Description: Remove up to <code>limit</code> expired entries.</p> <p>Inputs:</p> <ul> <li><code>limit: int</code></li> </ul> <p>Returns:</p> <ul> <li><code>int</code> \u2013 Number of entries purged.</li> </ul> <p>Notes: Expiry is also enforced lazily on <code>get()</code>; this provides proactive cleanup.</p>"},{"location":"reference/context-kv/#usage-notes","title":"Usage Notes","text":"<ul> <li>Not durable: For persistent state, use artifacts, memory persistence, or a durable KV backend when available.</li> <li>Small values only: Store references/IDs, not large byte blobs.</li> <li>Namespacing: Prefer scoping keys (e.g., <code>f\"run:{run_id}:\u2026\"</code>) to avoid collisions across flows.</li> </ul>"},{"location":"reference/context-llm/","title":"<code>context.llm()</code> \u2013 LLM Client &amp; Profiles API Reference","text":"<p><code>context.llm()</code> returns an LLM client (profile\u2011aware) with a consistent API across providers (OpenAI, Azure OpenAI, Anthropic, Google, OpenRouter, LM Studio, Ollama). Use it for chat, embeddings, and raw HTTP calls with built\u2011in retries and sane defaults.</p> <p>See LLM Setup &amp; Providers for configuring providers, base URLs, and API keys.</p>"},{"location":"reference/context-llm/#profiles-configuration","title":"Profiles &amp; Configuration","text":"<ul> <li>Profiles: Named client configs (default: <code>\"default\"</code>).</li> <li>Get existing: <code>client = context.llm()</code> or <code>context.llm(profile=\"myprofile\")</code>.</li> <li>Override/update: Pass any of <code>provider/model/base_url/api_key/azure_deployment/timeout</code> to create or update a profile at runtime.</li> <li>Quick set key: <code>context.llm_set_key(provider, model, api_key, profile=\"default\")</code>.</li> </ul> <p>Supported providers: <code>openai</code>, <code>azure</code>, <code>anthropic</code>, <code>google</code>, <code>openrouter</code>, <code>lmstudio</code>, <code>ollama</code>.</p>"},{"location":"reference/context-llm/#quick-reference","title":"Quick Reference","text":"Method Purpose Inputs Returns <code>chat(messages, *, reasoning_effort=None, max_output_tokens=None, **kw)</code> Multi\u2011turn chat completion <code>messages: list[dict]</code> \u2022 optional <code>temperature</code>, <code>top_p</code>, <code>model</code> override <code>(text: str, usage: dict)</code> <code>embed(texts, **kw)</code> Compute embeddings <code>texts: list[str]</code> \u2022 optional <code>model</code> override <code>list[list[float]]</code> <code>raw(*, method=\"POST\", path=None, url=None, json=None, params=None, headers=None, return_response=False)</code> Low\u2011level HTTP with provider auth/headers See details <code>Any</code> (JSON or <code>httpx.Response</code>) <code>aclose()</code> Close the underlying HTTP client \u2014 <code>None</code> <p>Context helpers</p> <ul> <li><code>context.llm(profile=\"default\", provider=None, model=None, base_url=None, api_key=None, azure_deployment=None, timeout=None)</code> \u2192 <code>LLMClient</code></li> <li><code>context.llm_set_key(provider, model, api_key, profile=\"default\")</code> \u2192 <code>None</code></li> </ul>"},{"location":"reference/context-llm/#messages-format","title":"Messages Format","text":"<p>Use OpenAI\u2011style messages:</p> <pre><code>messages = [\n  {\"role\": \"system\", \"content\": \"You are concise.\"},\n  {\"role\": \"user\", \"content\": \"Summarize this:\"},\n]\n</code></pre> <p>Provider adapters translate to vendor formats as needed (e.g., Anthropic messages blocks, Google content parts).</p>"},{"location":"reference/context-llm/#methods","title":"Methods","text":"chat(messages, *, reasoning_effort=None, max_output_tokens=None, **kw) -&gt; (str, dict) <p>Description: Send a chat request to the active provider/model and return <code>(text, usage)</code>.</p> <p>Inputs:</p> <ul> <li><code>messages: list[dict]</code></li> <li><code>reasoning_effort: str | None</code> \u2013 Provider\u2011specific hint (e.g., OpenAI Responses).</li> <li><code>max_output_tokens: int | None</code></li> <li> <p><code>**kw</code> (optional runtime overrides):</p> </li> <li> <p><code>model: str | None</code></p> </li> <li><code>temperature: float | None</code></li> <li><code>top_p: float | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>(text: str, usage: dict)</code></li> </ul> <p>Notes:</p> <ul> <li>Implements exponential backoff for transient HTTP errors.</li> <li>Normalizes different provider response shapes to a first <code>text</code> string.</li> </ul> embed(texts, **kw) -&gt; list[list[float]] <p>Description: Return embeddings for input strings via the provider\u2011specific embeddings endpoint.</p> <p>Inputs:</p> <ul> <li><code>texts: list[str]</code></li> <li> <p><code>**kw</code> (optional):</p> </li> <li> <p><code>model: str | None</code></p> </li> </ul> <p>Returns:</p> <ul> <li><code>list[list[float]]</code></li> </ul> <p>Notes:</p> <ul> <li>Anthropic does not provide embeddings \u2192 not implemented.</li> <li>Google returns a single vector for the concatenated text; client normalizes to a <code>list[list[float]]</code>.</li> </ul> raw(*, method=\"POST\", path=None, url=None, json=None, params=None, headers=None, return_response=False) -&gt; Any <p>Description: Escape hatch for provider APIs. Uses the client\u2019s base URL, authentication, and retry logic.</p> <p>Inputs:</p> <ul> <li><code>method: str</code> \u2013 HTTP method (default <code>\"POST\"</code>).</li> <li><code>path: str | None</code> \u2013 Joined to the client\u2019s <code>base_url</code> when <code>url</code> is not provided.</li> <li><code>url: str | None</code> \u2013 Full URL (overrides <code>path</code>).</li> <li><code>json: Any | None</code> \u2013 JSON body.</li> <li><code>params: dict[str, Any] | None</code> \u2013 Query params.</li> <li><code>headers: dict[str, str] | None</code> \u2013 Extra headers (merged over defaults).</li> <li><code>return_response: bool</code> \u2013 If <code>True</code>, returns raw <code>httpx.Response</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>Any</code> \u2013 Parsed JSON by default, or <code>httpx.Response</code> when <code>return_response=True</code>.</li> </ul> <p>Notes:</p> <ul> <li>Applies provider\u2011specific auth headers (e.g., <code>Authorization: Bearer ...</code>, <code>x-api-key</code>, <code>api-key</code>).</li> <li>Ensures the underlying HTTP client is bound to the current event loop.</li> </ul> aclose() -&gt; None <p>Description: Close the internal <code>httpx.AsyncClient</code>.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"reference/context-llm/#profiles-via-contextllm","title":"Profiles via <code>context.llm()</code>","text":"context.llm(profile=\"default\", *, provider=None, model=None, base_url=None, api_key=None, azure_deployment=None, timeout=None) -&gt; LLMClient <p>Description: Get an LLM client for <code>profile</code>. If any overrides are provided, configure or update that profile on the fly and return the new client.</p> <p>Inputs:</p> <ul> <li><code>profile: str</code></li> <li><code>provider: str | None</code></li> <li><code>model: str | None</code></li> <li><code>base_url: str | None</code></li> <li><code>api_key: str | None</code></li> <li><code>azure_deployment: str | None</code></li> <li><code>timeout: float | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>LLMClient</code></li> </ul> <p>Notes:</p> <ul> <li>If no overrides are provided, this is a pure getter.</li> <li>Profiles are maintained by the runtime LLM service and are reusable across nodes.</li> </ul> context.llm_set_key(provider, model, api_key, profile=\"default\") -&gt; None <p>Description: Quickly set/override provider credentials for a profile.</p> <p>Inputs:</p> <ul> <li><code>provider: str</code> \u2013 One of the supported providers.</li> <li><code>model: str</code></li> <li><code>api_key: str</code></li> <li><code>profile: str</code> \u2013 Defaults to <code>\"default\"</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes:</p> <ul> <li>See LLM Setup &amp; Providers for guidance on keys, endpoints, and environment variables. (placeholder link)</li> </ul>"},{"location":"reference/context-llm/#provider-behaviors-summary","title":"Provider Behaviors (Summary)","text":"<ul> <li>OpenAI: Uses <code>/v1/responses</code> for chat (supports <code>reasoning_effort</code>). Embeddings via <code>/v1/embeddings</code>.</li> <li>Anthropic: Converts messages to <code>v1/messages</code> format. No embeddings.</li> <li>Google (Gemini): Converts to <code>generateContent</code>/<code>embedContent</code> requests with API key in URL.</li> <li>OpenRouter/LM Studio/Ollama: OpenAI\u2011compatible <code>/chat/completions</code> and <code>/embeddings</code> endpoints.</li> </ul>"},{"location":"reference/context-llm/#examples","title":"Examples","text":"<pre><code># 1) Use default profile\nllm = context.llm()\ntext, usage = await llm.chat([\n    {\"role\": \"system\", \"content\": \"Be terse.\"},\n    {\"role\": \"user\", \"content\": \"One sentence on FFT.\"},\n])\n\n# 2) Create/update a named profile at runtime\nllm_fast = context.llm(\n    profile=\"fast\", provider=\"openai\", model=\"gpt-4o-mini\", timeout=20.0,\n)\n\n# 3) Quick key configure\ncontext.llm_set_key(\"openai\", \"gpt-4o-mini\", \"sk-...\", profile=\"prod\")\n\n# 4) Embeddings\nvecs = await llm.embed([\"optical flow\", \"phase mask\"])\n\n# 5) Raw call (escape hatch)\nresp = await llm.raw(path=\"/models\", method=\"GET\")\n</code></pre> <p>Tip: Prefer <code>context.llm()</code> profiles for reuse across nodes and flows; use per\u2011call overrides for experiments or one\u2011offs.</p>"},{"location":"reference/context-logger/","title":"<code>context.logger()</code> \u2013 Contextual Logger API Reference","text":"<p><code>context.logger()</code> returns a Logger already bound to the current run/node (and graph when available). It exposes the standard Python logging methods and injects structured fields like <code>run_id</code>, <code>node_id</code>, and <code>graph_id</code> into every record.</p> <p>Configuration is set when you start the sidecar:  <pre><code>from aethergraph import start_server; \n\n(log_level=\"warning\", ...)\n</code></pre> The runtime also honors environment/app settings (e.g., JSON vs text, rotating files, per\u2011namespace levels).</p>"},{"location":"reference/context-logger/#whats-included-per-log-record","title":"What\u2019s included per log record","text":"<ul> <li><code>run_id</code>, <code>node_id</code>, <code>graph_id</code> (when available)</li> <li>Logger namespace (e.g., <code>aethergraph.node.&lt;id&gt;</code>)</li> <li>Message, level, timestamp, optional <code>metrics</code>/custom fields via <code>extra={...}</code></li> </ul>"},{"location":"reference/context-logger/#quick-reference","title":"Quick Reference","text":"Method Purpose Inputs Notes <code>debug(msg, *args, **kwargs)</code> Verbose diagnostic <code>msg: str</code>, <code>*args</code>, <code>**kwargs</code> Use for high\u2011volume internals. <code>info(msg, *args, **kwargs)</code> Normal operational logs <code>msg: str</code>, <code>*args</code>, <code>**kwargs</code> Default level often shows this. <code>warning(msg, *args, **kwargs)</code> Non\u2011fatal issues <code>msg: str</code>, <code>*args</code>, <code>**kwargs</code> Alias: <code>warn()</code> in stdlib (deprecated). <code>error(msg, *args, **kwargs)</code> Errors that didn\u2019t crash <code>msg: str</code>, <code>*args</code>, <code>**kwargs</code> Consider <code>exc_info=True</code> for tracebacks. <code>exception(msg, *args, **kwargs)</code> Error with traceback <code>msg: str</code>, <code>*args</code>, <code>**kwargs</code> Equivalent to <code>error(..., exc_info=True)</code>. <code>critical(msg, *args, **kwargs)</code> Severe/fatal conditions <code>msg: str</code>, <code>*args</code>, <code>**kwargs</code> Might trigger alerts depending on sink. <p>Common kwargs</p> <ul> <li><code>extra: dict</code> \u2014 Attach structured fields (merged with contextual fields).</li> <li><code>exc_info: bool | BaseException</code> \u2014 Include traceback.</li> </ul>"},{"location":"reference/context-logger/#methods","title":"Methods","text":"debug(msg, *args, **kwargs) -&gt; None <p>Description: Emit a DEBUG\u2011level message.</p> <p>Inputs:</p> <ul> <li><code>msg: str</code></li> <li><code>*args</code> \u2013 <code>%</code>\u2011style parameters if you use <code>%</code> formatting in <code>msg</code>.</li> <li><code>**kwargs</code> \u2013 e.g., <code>extra</code>, <code>exc_info</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes: Evaluated lazily by the logging framework; cheap when DEBUG is disabled.</p> info(msg, *args, **kwargs) -&gt; None <p>Description: Emit an INFO\u2011level message for normal operations.</p> <p>Inputs:</p> <ul> <li><code>msg: str</code></li> <li><code>*args</code>, <code>**kwargs</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> warning(msg, *args, **kwargs) -&gt; None <p>Description: Emit a WARNING\u2011level message for recoverable anomalies.</p> <p>Inputs:</p> <ul> <li><code>msg: str</code></li> <li><code>*args</code>, <code>**kwargs</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> error(msg, *args, **kwargs) -&gt; None <p>Description: Emit an ERROR\u2011level message when something failed but execution continues.</p> <p>Inputs:</p> <ul> <li><code>msg: str</code></li> <li><code>*args</code>, <code>**kwargs</code> (consider <code>exc_info=True</code>)</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> exception(msg, *args, **kwargs) -&gt; None <p>Description: Shortcut for logging an exception with traceback.</p> <p>Inputs:</p> <ul> <li><code>msg: str</code></li> <li><code>*args</code>, <code>**kwargs</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes: Same as <code>error(msg, exc_info=True)</code>.</p> critical(msg, *args, **kwargs) -&gt; None <p>Description: Emit a CRITICAL\u2011level message for severe failures.</p> <p>Inputs:</p> <ul> <li><code>msg: str</code></li> <li><code>*args</code>, <code>**kwargs</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"reference/context-logger/#configuration-behavior","title":"Configuration &amp; Behavior","text":"<ul> <li>Startup: <code>start_server(log_level=\"warning\", ...)</code> sets console verbosity. File sinks, JSON formatting, and per\u2011namespace levels can be configured via app/env settings (<code>LoggingConfig</code>).</li> <li>Context injection: The logger is an adapter that auto\u2011merges contextual fields with any <code>extra</code> you pass.</li> <li>Formatting: Console is human\u2011readable; file logs may be text or JSON depending on configuration.</li> <li>External noise control: Some noisy third\u2011party namespaces are down\u2011leveled by default.</li> </ul>"},{"location":"reference/context-logger/#examples","title":"Examples","text":"<pre><code># Get a contextual logger inside a node/tool\nlog = context.logger()\n\nlog.info(\"starting phase %s\", \"A\", extra={\"phase\": \"A\", \"metrics\": {\"step\": 1}})\n\ntry:\n    do_risky_thing()\nexcept Exception:\n    log.exception(\"risky thing failed\")\n\nlog.debug(\"intermediate state\", extra={\"cache_hit\": True})\nlog.warning(\"fallback engaged\")\nlog.error(\"partial write; retrying\", extra={\"retry_in\": 2.5})\n</code></pre> <p>Result: Each record includes <code>run_id</code>, <code>node_id</code>, and (when available) <code>graph_id</code>, enabling easy correlation across tools and services.</p>"},{"location":"reference/context-mcp/","title":"<code>context.mcp()</code> \u2013 Model Context Protocol (MCP) API Reference","text":"<p><code>context.mcp()</code> gives you access to the MCP service, which manages named MCP clients (stdio / WebSocket / HTTP), handles lifecycle (open/close), and offers thin call helpers.</p> <p>Register clients after the sidecar starts. Example below uses the helper <code>register_mcp_client()</code>.</p>"},{"location":"reference/context-mcp/#quick-reference","title":"Quick Reference","text":"Method Purpose Returns <code>register(name, client)</code> Add/replace an MCP client <code>None</code> <code>remove(name)</code> Unregister a client <code>None</code> <code>has(name)</code> Check if a client exists <code>bool</code> <code>names()</code> / <code>list_clients()</code> List registered client names <code>list[str]</code> <code>get(name=\"default\")</code> Get a client by name (or error) <code>MCPClient</code> <code>open(name)</code> / <code>close(name)</code> Open/close a single client <code>None</code> <code>open_all()</code> / <code>close_all()</code> Open/close all clients <code>None</code> <code>call(name, tool, params=None)</code> Invoke a tool on a client <code>dict</code> (tool result) <code>list_tools(name)</code> Enumerate tools on a client <code>list[MCPTool]</code> <code>list_resources(name)</code> Enumerate resources <code>list[MCPResource]</code> <code>read_resource(name, uri)</code> Fetch a resource by URI <code>dict</code> <code>set_header(name, key, value)</code> (WS) Set/override a header at runtime <code>None</code> <code>persist_secret(secret_name, value)</code> Persist a secret via secrets provider <code>None</code>"},{"location":"reference/context-mcp/#methods","title":"Methods","text":"register(name, client) -&gt; None <p>Description: Register or replace an MCP client under <code>name</code>.</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> <li><code>client: MCPClientProtocol</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> remove(name) -&gt; None <p>Description: Unregister a client if present.</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> has(name) -&gt; bool <p>Description: Check whether a client is registered.</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>bool</code></li> </ul> names() / list_clients() -&gt; list[str] <p>Description: Return all registered client names.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>list[str]</code></li> </ul> get(name=\"default\") -&gt; MCPClientProtocol <p>Description: Return a client by name or raise <code>KeyError</code> if missing.</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>MCPClientProtocol</code></li> </ul> open(name) -&gt; None <p>Description: Ensure the client connection is open.</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> close(name) -&gt; None <p>Description: Close the client connection; logs a warning on failure.</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> open_all() -&gt; None <p>Description: Open all registered clients.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> close_all() -&gt; None <p>Description: Close all registered clients (best\u2011effort).</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> call(name, tool, params=None) -&gt; dict <p>Description: Lazy\u2011open the client and invoke an MCP tool.</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> <li><code>tool: str</code> \u2013 Tool identifier on the target MCP server.</li> <li><code>params: dict[str, Any] | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>dict</code> \u2013 Tool response.</li> </ul> <p>Notes: Clients also self\u2011reconnect on demand.</p> list_tools(name) -&gt; list[MCPTool] <p>Description: List available tools exposed by the MCP server.</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>list[MCPTool]</code></li> </ul> list_resources(name) -&gt; list[MCPResource] <p>Description: List available resources exposed by the MCP server.</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>list[MCPResource]</code></li> </ul> read_resource(name, uri) -&gt; dict <p>Description: Read a resource by URI from the MCP server.</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> <li><code>uri: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>dict</code></li> </ul> set_header(name, key, value) -&gt; None <p>Description: For WebSocket clients, set/override a header at runtime (useful in notebooks/demos).</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> <li><code>key: str</code></li> <li><code>value: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes: Raises <code>RuntimeError</code> if the client does not support headers.</p> persist_secret(secret_name, value) -&gt; None <p>Description: Persist a secret via the bound secrets provider (if writable).</p> <p>Inputs:</p> <ul> <li><code>secret_name: str</code></li> <li><code>value: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes: Raises <code>RuntimeError</code> when no writable secrets provider is configured.</p>"},{"location":"reference/context-mcp/#registering-clients-after-sidecar-starts","title":"Registering Clients (after sidecar starts)","text":"<p>Start the sidecar, then register one or more MCP clients:</p> <pre><code>from aethergraph import start_server\nfrom aethergraph.runtime import register_mcp_client\nfrom aethergraph.plugins.mcp import StdioMCPClient, WsMCPClient, HttpMCPClient\nimport sys\n\n# 1) Start sidecar (choose your port/logging as you prefer)\nstart_server(port=0)\n\n# 2) Register a local stdio MCP (no auth/network)\nregister_mcp_client(\n    \"local\",\n    client=StdioMCPClient(cmd=[sys.executable, \"-m\", \"aethergraph.plugins.mcp.fs_server\"]),\n)\n\n# 3) (Optional) Register a WebSocket MCP\nregister_mcp_client(\n    \"ws\",\n    client=WsMCPClient(\n        url=\"wss://mcp.example.com/ws\",\n        headers={\"Authorization\": \"Bearer &lt;token&gt;\"},\n        timeout=60.0,\n    ),\n)\n\n# 4) (Optional) Register an HTTP MCP\nregister_mcp_client(\n    \"http\",\n    client=HttpMCPClient(\n        base_url=\"https://mcp.example.com/api\",\n        headers={\"Authorization\": \"Bearer &lt;token&gt;\"},\n        timeout=60.0,\n    ),\n)\n</code></pre> <p>Client types:</p> <ul> <li><code>StdioMCPClient(cmd, env=None, timeout=60.0)</code> \u2013 JSON\u2011RPC over stdio to a subprocess.</li> <li><code>WsMCPClient(url, headers=None, timeout=60.0, ping_interval=20.0, ping_timeout=10.0)</code> \u2013 JSON\u2011RPC over WebSocket.</li> <li><code>HttpMCPClient(base_url, headers=None, timeout=60.0)</code> \u2013 JSON\u2011RPC over HTTP.</li> </ul>"},{"location":"reference/context-mcp/#examples","title":"Examples","text":"<pre><code># Get tool list and call a tool\nmcp = context.mcp()\n\ntools = await mcp.list_tools(\"local\")\nres = await mcp.call(\"local\", tool=\"fs.read_text\", params={\"path\": \"/etc/hosts\"})\n\n# Read a resource listing from WS backend\nawait mcp.open(\"ws\")\nresources = await mcp.list_resources(\"ws\")\n\n# Set a header on the WS client (e.g., late\u2011bound token)\nmcp.set_header(\"ws\", \"Authorization\", \"Bearer NEW_TOKEN\")\n\n# Clean up\nawait mcp.close_all()\n</code></pre> <p>Notes:</p> <ul> <li>Clients lazy\u2011open for operations; you may still call <code>open()</code> explicitly.</li> <li>Errors from the server propagate; inspect tool/resource contracts on the MCP server side for required params and shapes.</li> </ul>"},{"location":"reference/context-memory%20copy/","title":"<code>context.memory()</code> \u2013 MemoryFacade API Reference","text":"<p><code>MemoryFacade</code> coordinates HotLog (fast recent events), Persistence (durable JSONL event log + JSON blobs), and Indices (derived KV views), with optional Artifacts and RAG helpers. All public methods are async.</p>"},{"location":"reference/context-memory%20copy/#concepts-defaults","title":"Concepts &amp; Defaults","text":"<ul> <li> <p>Three core services</p> </li> <li> <p>HotLog: append/recent for fast, transient access (TTL, ring buffer).</p> </li> <li>Persistence: durable append/replay (e.g., FS JSONL, S3, DB).</li> <li>Indices: fast lookups (e.g., last by name/topic, latest refs by kind), updated by <code>write_result()</code>.</li> <li>Scope binding: Instance carries <code>{run_id, graph_id, node_id, agent_id}</code> and stamps them on events.</li> <li>Signal heuristic: If not provided, <code>record_raw()</code> estimates a <code>signal</code> (0.0\u20131.0) from text length, metrics, and severity.</li> <li>RAG integration: Optional <code>RAGFacade</code> for corpora, upserts, search, and QA; gated by configuration.</li> </ul>"},{"location":"reference/context-memory%20copy/#event-schema-contract-excerpt","title":"Event Schema (contract excerpt)","text":"<p>Essential <code>Event</code> fields used by this facade (not exhaustive):</p> <pre><code>Event(\n  event_id: str, ts: str, kind: str, stage: str | None, severity: int,\n  text: str | None, metrics: dict[str, Any] | None, signal: float | None,\n  tool: str | None, tags: list[str], entities: list[str],\n  inputs: list[dict] | None, outputs: list[dict] | None,\n  run_id: str, graph_id: str | None, node_id: str | None, agent_id: str | None,\n)\n</code></pre>"},{"location":"reference/context-memory%20copy/#quick-reference","title":"Quick Reference","text":"Method Purpose Returns <code>record_raw(*, base, text=None, metrics=None, sources=None)</code> Append normalized event to HotLog + Persistence <code>Event</code> <code>record(kind, data, tags=None, entities=None, severity=2, stage=None, inputs_ref=None, outputs_ref=None, metrics=None, sources=None, signal=None)</code> Convenience wrapper that stringifies <code>data</code> <code>Event</code> <code>write_result(*, topic, inputs=None, outputs=None, tags=None, metrics=None, message=None, severity=3)</code> Record a typed tool/agent result and update indices <code>Event</code> <code>recent(*, kinds=None, limit=50)</code> Recent events from HotLog <code>list[Event]</code> <code>recent_data(*, kinds, limit=50)</code> Recent events \u2192 decoded JSON/text list <code>list[Any]</code> <code>rag_upsert(*, corpus_id, docs, topic=None)</code> Upsert docs into RAG <code>dict</code> stats <code>rag_bind(*, corpus_id=None, key=None, create_if_missing=True, labels=None)</code> Get/create corpus id <code>str</code> corpus_id <code>rag_status(*, corpus_id)</code> Corpus stats <code>dict</code> <code>rag_snapshot(*, corpus_id, title, labels=None)</code> Export corpus as artifact bundle and log result <code>dict</code> bundle <code>rag_promote_events(*, corpus_id, events=None, where=None, policy=None)</code> Convert events \u2192 docs \u2192 upsert <code>dict</code> stats <code>rag_search(*, corpus_id, query, k=8, filters=None, mode=\"hybrid\")</code> Hybrid/dense search <code>list[dict]</code> hits <code>rag_answer(*, corpus_id, question, style=\"concise\", with_citations=True, k=6)</code> Answer with citations + log <code>dict</code> answer <code>last_by_name(name)</code> Latest output value by name (fast) <code>dict             | Any   | None</code> <code>last_outputs_by_topic(topic)</code> Latest outputs map for a topic <code>dict[str, Any]   | None</code>"},{"location":"reference/context-memory%20copy/#methods","title":"Methods","text":"record_raw(*, base, text=None, metrics=None, sources=None) -&gt; Event <p>Description: Append a normalized <code>Event</code> to HotLog (fast) and Persistence (durable). Stamps missing scope fields and computes a lightweight <code>signal</code> if absent.</p> <p>Inputs:</p> <ul> <li><code>base: dict[str, Any]</code> \u2013 Must include classification fields like <code>kind</code>, <code>stage</code>, <code>severity</code>, <code>tool</code> (optional), <code>tags</code>, <code>entities</code>, <code>inputs</code>, <code>outputs</code>. Missing scope fields are added.</li> <li><code>text: str | None</code> \u2013 Optional human-readable message.</li> <li><code>metrics: dict[str, Any] | None</code> \u2013 Numeric map (latency, token counts, costs, etc.).</li> <li><code>sources: list[str] | None</code> \u2013 Upstream event_ids this event derives from.</li> </ul> <p>Returns:</p> <ul> <li><code>Event</code></li> </ul> <p>Notes:</p> <ul> <li>Does not update <code>indices</code> automatically. Use <code>write_result()</code> for index updates.</li> </ul> record(kind, data, tags=None, entities=None, severity=2, stage=None, inputs_ref=None, outputs_ref=None, metrics=None, sources=None, signal=None) -&gt; Event <p>Description: Convenience wrapper around <code>record_raw()</code>; stringifies <code>data</code> to <code>text</code> (JSON if possible).</p> <p>Inputs:</p> <ul> <li><code>kind: str</code></li> <li><code>data: Any</code> \u2013 Will be stringified; if non-serializable, a warning is logged (when logger is set).</li> <li><code>tags: list[str] | None</code></li> <li><code>entities: list[str] | None</code></li> <li><code>severity: int</code></li> <li><code>stage: str | None</code></li> <li><code>inputs_ref: list[dict] | None</code></li> <li><code>outputs_ref: list[dict] | None</code></li> <li><code>metrics: dict[str, Any] | None</code></li> <li><code>sources: list[str] | None</code></li> <li><code>signal: float | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>Event</code></li> </ul> write_result(*, topic, inputs=None, outputs=None, tags=None, metrics=None, message=None, severity=3) -&gt; Event <p>Description: Record a typed tool/agent result (<code>kind=\"tool_result\"</code>) and update Indices (latest-by-name, latest refs by kind, last outputs-by-topic).</p> <p>Inputs:</p> <ul> <li><code>topic: str</code> \u2013 Tool/agent/flow identifier (used in indices).</li> <li><code>inputs: list[dict] | None</code> \u2013 List of typed values (<code>Value</code>-like dicts).</li> <li><code>outputs: list[dict] | None</code> \u2013 Primary source for index updates.</li> <li><code>tags: list[str] | None</code></li> <li><code>metrics: dict[str, float] | None</code></li> <li><code>message: str | None</code></li> <li><code>severity: int</code></li> </ul> <p>Returns:</p> <ul> <li><code>Event</code></li> </ul> recent(*, kinds=None, limit=50) -&gt; list[Event] <p>Description: Get recent events from HotLog (most recent last).</p> <p>Inputs:</p> <ul> <li><code>kinds: list[str] | None</code></li> <li><code>limit: int</code></li> </ul> <p>Returns:</p> <ul> <li><code>list[Event]</code></li> </ul> recent_data(*, kinds, limit=50) -&gt; list[Any] <p>Description: Convenience wrapper returning decoded <code>data</code> payloads (prefers JSON decode; falls back to raw text).</p> <p>Inputs:</p> <ul> <li><code>kinds: list[str]</code></li> <li><code>limit: int</code></li> </ul> <p>Returns:</p> <ul> <li><code>list[Any]</code></li> </ul> rag_upsert(*, corpus_id, docs, topic=None) -&gt; dict <p>Description: Upsert documents into a RAG corpus via <code>RAGFacade</code>.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> <li><code>docs: Sequence[dict[str, Any]]</code> \u2013 Each doc should include <code>text</code> and optional <code>title</code>, <code>labels</code>.</li> <li><code>topic: str | None</code> \u2013 Reserved for future logging (not used in current function body).</li> </ul> <p>Returns:</p> <ul> <li><code>dict</code> \u2013 Stats from index (e.g., <code>added</code>, <code>chunks</code>).</li> </ul> <p>Notes: Requires <code>self.rag</code> configured; otherwise raises <code>RuntimeError</code>.</p> rag_bind(*, corpus_id=None, key=None, create_if_missing=True, labels=None) -&gt; str <p>Description: Return a corpus id, optionally creating it. If <code>corpus_id</code> is omitted, a stable id is derived from <code>key</code> (or <code>run_id</code>).</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str | None</code></li> <li><code>key: str | None</code> \u2013 If not provided, <code>run_id</code> is used.</li> <li><code>create_if_missing: bool</code></li> <li><code>labels: dict | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>str</code> \u2013 Corpus id.</li> </ul> <p>Notes: Requires <code>self.rag</code> configured.</p> rag_status(*, corpus_id) -&gt; dict <p>Description: Lightweight corpus stats.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>dict</code></li> </ul> <p>Notes: Requires <code>self.rag</code>.</p> rag_snapshot(*, corpus_id, title, labels=None) -&gt; dict <p>Description: Export corpus to an artifact bundle and log a <code>tool_result</code>.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> <li><code>title: str</code></li> <li><code>labels: dict | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>dict</code> \u2013 Bundle descriptor <code>{ uri, ... }</code>.</li> </ul> <p>Notes: Requires <code>self.rag</code>. Uses <code>write_result()</code> to record bundle URI.</p> rag_promote_events(*, corpus_id, events=None, where=None, policy=None) -&gt; dict <p>Description: Select events (by <code>where</code> or <code>recent</code>) \u2192 convert to docs \u2192 upsert into RAG.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> <li><code>events: list[Event] | None</code></li> <li><code>where: dict | None</code> \u2013 e.g., <code>{ \"kinds\": [\"tool_result\"], \"min_signal\": 0.25, \"limit\": 200 }</code>.</li> <li><code>policy: dict | None</code> \u2013 e.g., <code>{ \"min_signal\": float }</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>dict</code> \u2013 Upsert stats (e.g., <code>added</code>, <code>chunks</code>).</li> </ul> <p>Notes: Requires <code>self.rag</code>. Also logs a <code>tool_result</code> with counts.</p> rag_search(*, corpus_id, query, k=8, filters=None, mode=\"hybrid\") -&gt; list[dict] <p>Description: Search a RAG corpus and return serializable hits.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> <li><code>query: str</code></li> <li><code>k: int</code></li> <li><code>filters: dict | None</code></li> <li><code>mode: Literal[\"hybrid\", \"dense\"]</code></li> </ul> <p>Returns:</p> <ul> <li><code>list[dict]</code> \u2013 Each hit includes <code>chunk_id</code>, <code>doc_id</code>, <code>corpus_id</code>, <code>score</code>, <code>text</code>, <code>meta</code>.</li> </ul> <p>Notes: Requires <code>self.rag</code>.</p> rag_answer(*, corpus_id, question, style=\"concise\", with_citations=True, k=6) -&gt; dict <p>Description: Answer with citations using RAG + optional LLM; logs the answer as a <code>tool_result</code> (with usage metrics when available).</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> <li><code>question: str</code></li> <li><code>style: Literal[\"concise\", \"detailed\"]</code></li> <li><code>with_citations: bool</code></li> <li><code>k: int</code></li> </ul> <p>Returns:</p> <ul> <li><code>dict</code> \u2013 Includes <code>answer</code>, <code>citations</code>/<code>resolved_citations</code>, and <code>usage</code> (if provided by LLM).</li> </ul> <p>Notes: Requires <code>self.rag</code>. Outputs are flattened into <code>write_result()</code> for indexing.</p> last_by_name(name) -&gt; dict | Any | None <p>Description: Return the latest output value by <code>name</code> from Indices (fast path). Useful for grabbing a single named value most recently produced by any tool that wrote it via <code>write_result()</code>.</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>dict | Any | None</code> \u2013 Store-dependent value or <code>None</code> if not found.</li> </ul> last_outputs_by_topic(topic) -&gt; dict[str, Any] | None <p>Description: Return the latest outputs map for a given <code>topic</code> (tool/flow/agent) from Indices.</p> <p>Inputs:</p> <ul> <li><code>topic: str</code> \u2013 The identifier you passed as <code>topic</code> to <code>write_result()</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>dict[str, Any] | None</code> \u2013 Name\u2192value map of last outputs, or <code>None</code> if not found.</li> </ul>"},{"location":"reference/context-memory%20copy/#behavioral-notes","title":"Behavioral Notes","text":"<ul> <li>Indices updates: Only <code>write_result()</code> updates indices by default; this keeps ad-hoc <code>record_*</code> logs cheap.</li> <li>Event ordering: HotLog returns most recent last; consumers may reverse if needed.</li> <li>RAG gating: All <code>rag_*</code> methods raise <code>RuntimeError</code> if <code>self.rag</code> is not configured.</li> <li>Unused parameters: Current <code>record()</code> will log a warning on unserializable <code>data</code> if a logger is provided; otherwise it silently stringifies.</li> </ul>"},{"location":"reference/context-memory/","title":"<code>context.memory()</code> \u2013 MemoryFacade API Reference","text":"<p><code>MemoryFacade</code> coordinates HotLog (fast recent events), Persistence (durable JSONL), and Indices (derived KV views).</p> <p>It is accessed via <code>node_context.memory</code> but aggregates functionality from several internal mixins.</p>"},{"location":"reference/context-memory/#1-core-recording","title":"1. Core Recording","text":"<p>Basic event logging and raw data access.</p> <p>               Bases: <code>ChatMixin</code>, <code>ResultMixin</code>, <code>RetrievalMixin</code>, <code>DistillationMixin</code>, <code>RAGMixin</code></p> <p>MemoryFacade coordinates core memory services for a specific run/session. Functionality is split across mixins in the <code>facade/</code> directory.</p>"},{"location":"reference/context-memory/#aethergraph.services.memory.facade.core.MemoryFacade-functions","title":"Functions","text":""},{"location":"reference/context-memory/#aethergraph.services.memory.facade.core.MemoryFacade.record","title":"<code>record(kind, data, tags=None, severity=2, stage=None, inputs_ref=None, outputs_ref=None, metrics=None, signal=None, text=None)</code>  <code>async</code>","text":"<p>Convenience wrapper around record_raw() with common fields.</p> <ul> <li>kind     : logical kind (e.g. \"user_msg\", \"tool_call\", \"chat_turn\")</li> <li>data     : JSON-serializable content, or string</li> <li>tags     : optional list of labels</li> <li>severity : 1=low, 2=medium, 3=high</li> <li>stage    : optional stage (user/assistant/system/etc.)</li> <li>inputs_ref / outputs_ref : optional Value[] references</li> <li>metrics  : numeric map (latency, tokens, etc.)</li> <li>signal   : optional override for signal strength</li> <li>text     : optional preview text override (if None, derived from data)</li> </ul>"},{"location":"reference/context-memory/#aethergraph.services.memory.facade.core.MemoryFacade.record_raw","title":"<code>record_raw(*, base, text=None, metrics=None)</code>  <code>async</code>","text":""},{"location":"reference/context-rag/","title":"<code>RAGFacade</code> \u2013 Retrieval\u2011Augmented Generation API","text":"<p>Manages corpora, document ingestion (text/files), chunking + embeddings, vector indexing, retrieval, and QA.</p> <p>Backends: defaults to a lightweight SQLite vector index. FAISS is supported locally if installed via pip. See LLM &amp; Index Setup for provider/model/index configuration.</p>"},{"location":"reference/context-rag/#quick-reference","title":"Quick Reference","text":"Method Purpose Returns <code>set_llm_client(client)</code> Swap the LLM used for QA <code>None</code> <code>set_index_backend(index_backend)</code> Swap the vector index backend <code>None</code> <code>add_corpus(corpus_id, meta=None)</code> Create/ensure a corpus <code>None</code> <code>upsert_docs(corpus_id, docs)</code> Ingest docs \u2192 chunk, embed, index <code>dict</code> {added,chunks,index} <code>search(corpus_id, query, k=8, filters=None, mode=\"hybrid\")</code> Retrieve top chunks <code>list[SearchHit]</code> <code>retrieve(corpus_id, query, k=6, rerank=True)</code> Alias to <code>search(..., mode=\"hybrid\")</code> <code>list[SearchHit]</code> <code>answer(corpus_id, question, llm=None, style=\"concise\", with_citations=True, k=6)</code> QA over retrieved context <code>dict</code> {answer,citations,usage,resolved_citations?} <code>resolve_citations(corpus_id, citations)</code> Enrich citation refs with titles/URIs/snippets <code>list[dict]</code> <code>list_corpora()</code> Enumerate corpora in <code>corpus_root</code> <code>list[dict]</code> <code>list_docs(corpus_id, limit=200, after=None)</code> Page through docs <code>list[dict]</code> <code>delete_docs(corpus_id, doc_ids)</code> Remove docs + chunks (+drop from index) <code>dict</code> <code>reembed(corpus_id, doc_ids=None, batch=64)</code> Recompute embeddings &amp; re\u2011add to index <code>dict</code> <code>stats(corpus_id)</code> Simple counts + metadata <code>dict</code>"},{"location":"reference/context-rag/#data-types","title":"Data Types","text":"<p><code>SearchHit</code></p> <ul> <li><code>chunk_id: str</code></li> <li><code>doc_id: str</code></li> <li><code>corpus_id: str</code></li> <li><code>score: float</code></li> <li><code>text: str</code></li> <li><code>meta: dict[str, Any]</code></li> </ul>"},{"location":"reference/context-rag/#methods","title":"Methods","text":"set_llm_client(client) -&gt; None <p>Description: Set/replace the LLM client for QA.</p> <p>Inputs:</p> <ul> <li><code>client: LLMClientProtocol</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes:</p> <ul> <li>Requires <code>client.model</code> and <code>client.embed_model</code> to be set; asserts on missing values.</li> </ul> set_index_backend(index_backend) -&gt; None <p>Description: Swap the underlying vector index backend.</p> <p>Inputs:</p> <ul> <li><code>index_backend: Any</code> \u2013 Must implement <code>add(corpus_id, ids, vectors, metas)</code> and <code>search(corpus_id, qvec, k)</code>; optionally <code>remove</code>/<code>delete</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> add_corpus(corpus_id, meta=None) -&gt; None <p>Description: Create/ensure a corpus directory with <code>corpus.json</code>.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> <li><code>meta: dict[str, Any] | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> upsert_docs(corpus_id, docs) -&gt; dict <p>Description: Ingest documents, chunk, embed, and add vectors to the index.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> <li> <p><code>docs: list[dict]</code> \u2013 Each doc is one of:</p> </li> <li> <p>File doc: <code>{ \"path\": \"/path/to/file.ext\", \"labels\": {...} }</code></p> </li> <li>Inline text: <code>{ \"text\": \"...\", \"title\": \"Doc Title\", \"labels\": {...} }</code></li> </ul> <p>Returns:</p> <ul> <li><code>dict</code> \u2013 <code>{ \"added\": int, \"chunks\": int, \"index\": str }</code></li> </ul> <p>Notes:</p> <ul> <li>Files are persisted via the artifact store; PDFs/Markdown/Plain\u2011text are parsed to text.</li> <li>Requires an embedding client; raises if not configured.</li> </ul> search(corpus_id, query, k=8, filters=None, mode=\"hybrid\") -&gt; list[SearchHit] <p>Description: Dense retrieval (embeds query, searches index) with optional hybrid lexical fusion.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> <li><code>query: str</code></li> <li><code>k: int</code></li> <li><code>filters: dict | None</code> \u2013 Reserved; not applied in current implementation.</li> <li><code>mode: str</code> \u2013 <code>\"dense\"</code> or <code>\"hybrid\"</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>list[SearchHit]</code></li> </ul> <p>Notes:</p> <ul> <li>When <code>mode=\"dense\"</code>, returns top\u2011<code>k</code> dense hits.</li> <li>When <code>mode=\"hybrid\"</code>, fuses dense hits with lexical scoring for re\u2011ranking.</li> </ul> retrieve(corpus_id, query, k=6, rerank=True) -&gt; list[SearchHit] <p>Description: Convenience alias to <code>search(..., mode=\"hybrid\")</code>.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> <li><code>query: str</code></li> <li><code>k: int</code></li> <li><code>rerank: bool</code> \u2013 Currently ignored (hybrid fusion already sorts).</li> </ul> <p>Returns:</p> <ul> <li><code>list[SearchHit]</code></li> </ul> answer(corpus_id, question, llm=None, style=\"concise\", with_citations=True, k=6) -&gt; dict <p>Description: Compose a system+user prompt from retrieved chunks and answer with the LLM.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> <li><code>question: str</code></li> <li><code>llm: LLMClientProtocol | None</code> \u2013 If <code>None</code>, uses the facade\u2019s LLM.</li> <li><code>style: str</code> \u2013 <code>\"concise\"</code> or <code>\"detailed\"</code>.</li> <li><code>with_citations: bool</code></li> <li><code>k: int</code></li> </ul> <p>Returns:</p> <ul> <li> <p><code>dict</code> with keys:</p> </li> <li> <p><code>answer: str</code></p> </li> <li><code>citations: list[dict]</code> \u2013 <code>{chunk_id, doc_id, rank}</code></li> <li><code>usage: dict</code> \u2013 model usage as provided by the LLM</li> <li><code>resolved_citations (optional): list[dict]</code> \u2013 enriched refs (see below)</li> </ul> <p>Notes:</p> <ul> <li>See LLM &amp; Index Setup for configuring provider/model.</li> </ul> resolve_citations(corpus_id, citations) -&gt; list[dict] <p>Description: Map <code>{chunk_id, doc_id, rank}</code> to <code>{rank, doc_id, title, uri, chunk_id, snippet}</code>.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> <li><code>citations: list[dict]</code></li> </ul> <p>Returns:</p> <ul> <li><code>list[dict]</code> sorted by <code>rank</code>.</li> </ul> list_corpora() -&gt; list[dict] <p>Description: Enumerate all corpora under <code>corpus_root</code>.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>list[dict]</code> \u2013 <code>[{corpus_id, meta}, ...]</code></li> </ul> list_docs(corpus_id, limit=200, after=None) -&gt; list[dict] <p>Description: Stream-like pagination over <code>docs.jsonl</code>.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> <li><code>limit: int</code></li> <li><code>after: str | None</code> \u2013 Resume from a specific <code>doc_id</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>list[dict]</code></li> </ul> delete_docs(corpus_id, doc_ids) -&gt; dict <p>Description: Remove docs + their chunks; drop vectors from index if supported.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> <li><code>doc_ids: list[str]</code></li> </ul> <p>Returns:</p> <ul> <li><code>dict</code> \u2013 <code>{removed_docs: int, removed_chunks: int}</code></li> </ul> reembed(corpus_id, doc_ids=None, batch=64) -&gt; dict <p>Description: Re\u2011embed selected (or all) chunks and re\u2011add them to the index.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> <li><code>doc_ids: list[str] | None</code></li> <li><code>batch: int</code></li> </ul> <p>Returns:</p> <ul> <li><code>dict</code> \u2013 <code>{reembedded: int, model: str | None}</code></li> </ul> stats(corpus_id) -&gt; dict <p>Description: Return corpus stats and stored metadata.</p> <p>Inputs:</p> <ul> <li><code>corpus_id: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>dict</code> \u2013 <code>{corpus_id, docs, chunks, meta}</code></li> </ul>"},{"location":"reference/context-rag/#examples","title":"Examples","text":"<pre><code># 1) Create/ensure a corpus\ncid = \"research-papers\"\nawait rag.add_corpus(cid, meta={\"owner\": \"team-ml\"})\n\n# 2) Ingest two docs (one file, one inline)\nstats = await rag.upsert_docs(\n    cid,\n    docs=[\n        {\"path\": \"./notes/attention_is_all_you_need.pdf\", \"labels\": {\"kind\": \"paper\"}},\n        {\"text\": open(\"README.md\", encoding=\"utf-8\").read(), \"title\": \"repo-readme\"},\n    ],\n)\n\n# 3) Search and answer\nhits = await rag.search(cid, query=\"self-attention complexity\", k=5)\nans = await rag.answer(cid, question=\"What is the time complexity of self-attention?\", k=6)\n\n# 4) Inspect/enrich citations\nresolved = rag.resolve_citations(cid, ans[\"citations\"])  # already included if with_citations=True\n\n# 5) Maintenance\nawait rag.reembed(cid, doc_ids=[h.doc_id for h in hits])\nawait rag.delete_docs(cid, doc_ids=[hits[0].doc_id])\ninfo = await rag.stats(cid)\n</code></pre> <p>Notes &amp; Setup:</p> <ul> <li>Embeddings/LLM: configure providers/models via your LLM service. See LLM &amp; Index Setup.</li> <li>Index: defaults to SQLite\u2011based vectors; FAISS is supported if installed.</li> <li>Artifacts: binary sources are stored via the artifact store (CAS) before parsing.</li> </ul>"},{"location":"reference/decorators/","title":"Decorator API \u2014 <code>@graph_fn</code>, <code>@graphify</code>, <code>@tool</code>","text":"<p>A single reference page for the three core decorators you\u2019ll use to build with AetherGraph.</p>"},{"location":"reference/decorators/#quick-chooser","title":"Quick chooser","text":"Use this when\u2026 Pick Why You want the quickest way to make a Python function runnable as a graph entrypoint and get a <code>context</code> for services <code>@graph_fn</code> Small, ergonomic, ideal for tutorials, notebooks, single\u2011entry tools/agents You need to expose reusable steps with typed I/O that can run standalone or as graph nodes <code>@tool</code> Dual\u2011mode decorator; gives you fine control of inputs/outputs; portable and composable Your function body is mostly tool wiring (fan\u2011in/fan\u2011out) and you want a static graph spec from Python syntax <code>@graphify</code> Author graphs declaratively; returns a <code>TaskGraph</code> factory; great for orchestration patterns"},{"location":"reference/decorators/#graph_fn","title":"<code>@graph_fn</code>","text":"<p>Wrap a normal async function into a runnable graph with optional <code>context</code> injection.</p>"},{"location":"reference/decorators/#signature","title":"Signature","text":"<pre><code>@graph_fn(name: str, *, inputs: list[str] | None = None, outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\nasync def my_fn(..., *, context: NodeContext): ...\n</code></pre>"},{"location":"reference/decorators/#description","title":"Description","text":"<ul> <li>Builds a fresh <code>TaskGraph</code> under the hood and executes it immediately.</li> <li>If your function signature includes <code>context: NodeContext</code>, AetherGraph injects a <code>NodeContext</code> so you can call <code>context.channel()</code>, <code>context.memory()</code>, <code>context.artifacts()</code>, <code>context.llm()</code>, etc.</li> <li>Ideal for single\u2011file demos, CLI/notebook usage, and simple agents.</li> </ul>"},{"location":"reference/decorators/#parameters","title":"Parameters","text":"<ul> <li>name (str, required) \u2014 Graph ID and human\u2011readable name.</li> <li>inputs (list[str], optional) \u2014 Declared input keys. Purely declarative; your function still gets normal Python args.</li> <li>outputs (list[str], optional) \u2014 Declared output keys. If you return a single literal, declare exactly one.</li> <li>version (str, optional) \u2014 Semantic version for registry.</li> <li>agent (str, optional) \u2014 If provided, registers this graph function as an agent under the given name.</li> </ul>"},{"location":"reference/decorators/#returns","title":"Returns","text":"<ul> <li>The decorator returns a <code>GraphFunction</code> object. Calling/awaiting it executes the graph and returns a <code>dict</code> of outputs keyed by <code>outputs</code>.</li> </ul>"},{"location":"reference/decorators/#minimal-example","title":"Minimal example","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"hello.world\", inputs=[\"name\"], outputs=[\"greeting\"], version=\"0.1.0\")\nasync def hello_world(name: str, *, context: NodeContext):\n    await context.channel().send_text(f\"\ud83d\udc4b Hello {name}\")\n    return {\"greeting\": f\"Hello, {name}!\"}\n\n# Run (async)\nres = await hello_world(name=\"Aether\")\nprint(res[\"greeting\"])  # \u2192 \"Hello, Aether!\"\n</code></pre>"},{"location":"reference/decorators/#tips","title":"Tips","text":"<ul> <li>Return a <code>dict</code> where keys match <code>outputs</code>. If you return a single literal, declare one output.</li> <li>You can call <code>@tool</code> functions inside a <code>@graph_fn</code> (they\u2019ll run immediately, not build nodes). Use this for small, fast helper steps.</li> <li>For complex orchestration (fan\u2011in/fan\u2011out), prefer <code>@graphify</code> so <code>@tool</code> calls become nodes.</li> </ul>"},{"location":"reference/decorators/#tool","title":"<code>@tool</code>","text":"<p>Dual\u2011mode decorator for reusable steps with explicit inputs/outputs.</p>"},{"location":"reference/decorators/#signature_1","title":"Signature","text":"<pre><code>@tool(outputs: list[str], *, inputs: list[str] | None = None, name: str | None = None, version: str = \"0.1.0\")\ndef/async def my_tool(...): ...\n</code></pre>"},{"location":"reference/decorators/#description_1","title":"Description","text":"<ul> <li>Immediate mode (no builder/interpreter active): calling the function executes it now and returns a <code>dict</code> of outputs.</li> <li>Graph mode (inside a <code>graph(...)</code> / <code>@graphify</code> body or during <code>@graph_fn</code> build): calling the proxy adds a node to the current graph and returns a <code>NodeHandle</code> with typed outputs.</li> <li>Registers the underlying implementation in the runtime registry for portability.</li> </ul>"},{"location":"reference/decorators/#parameters_1","title":"Parameters","text":"<ul> <li>outputs (list[str], required) \u2014 Names of output values (e.g., <code>[\"result\"]</code>, <code>[\"image\", \"stats\"]</code>).</li> <li>inputs (list[str], optional) \u2014 Input names (auto\u2011inferred from signature if omitted).</li> <li>name (str, optional) \u2014 Registry/display name; defaults to function name.</li> <li>version (str, optional) \u2014 Semantic version for registry.</li> </ul>"},{"location":"reference/decorators/#returns_1","title":"Returns","text":"<ul> <li>In immediate mode: <code>dict</code> of outputs.</li> <li>In graph mode: <code>NodeHandle</code> with <code>.out_key</code> attributes (e.g., <code>node.result</code>).</li> </ul>"},{"location":"reference/decorators/#example-reusable-step","title":"Example \u2014 reusable step","text":"<pre><code>from aethergraph import tool\n\n@tool(outputs=[\"sum\", \"count\"])\ndef aggregate(xs: list[int]):\n    return {\"sum\": sum(xs), \"count\": len(xs)}\n\n# Immediate mode\nprint(aggregate([1,2,3]))  # {\"sum\": 6, \"count\": 3}\n</code></pre>"},{"location":"reference/decorators/#example-using-tool-inside-graph_fn-immediate-execution","title":"Example \u2014 using <code>@tool</code> inside <code>@graph_fn</code> (immediate execution)","text":"<pre><code>from aethergraph import graph_fn, tool, NodeContext\n\n@tool(outputs=[\"sum\"])  \ndef add(x: int, y: int):\n    return {\"sum\": x + y}\n\n@graph_fn(name=\"calc.pipeline\", inputs=[\"a\",\"b\"], outputs=[\"total\"])\nasync def calc(a: int, b: int, *, context: NodeContext):\n    out = add(a, b)                 # immediate mode here\n    await context.channel().send_text(f\"sum = {out['sum']}\")\n    return {\"total\": out[\"sum\"]}\n</code></pre>"},{"location":"reference/decorators/#tips_1","title":"Tips","text":"<ul> <li>Use <code>@tool</code> to make steps portable and inspectable (typed I/O makes graphs predictable).</li> <li>In <code>@graph_fn</code> the <code>@tool</code> call executes immediately; in <code>@graphify</code> the same call becomes a graph node.</li> <li>Control\u2011flow knobs like <code>_after</code>, <code>_id</code>, <code>_alias</code> apply only in graph\u2011building contexts (e.g., <code>@graphify</code>), not in <code>@graph_fn</code> bodies.</li> </ul>"},{"location":"reference/decorators/#graphify","title":"<code>@graphify</code>","text":"<p>Author a static TaskGraph by writing normal Python that calls <code>@tool</code>s. The function body executes during build to register nodes and edges; returned node handles/literals define graph outputs.</p>"},{"location":"reference/decorators/#signature_2","title":"Signature","text":"<pre><code>@graphify(*, name: str = \"default_graph\", inputs: Iterable[str] | dict = (), outputs: list[str] | None = None, version: str = \"0.1.0\", agent: str | None = None)\ndef my_graph(...):\n    ...  # body calls @tool proxies (graph mode)\n    return {...}  # NodeHandle(s) and/or literal refs\n</code></pre>"},{"location":"reference/decorators/#description_2","title":"Description","text":"<ul> <li>The decorated function becomes a factory: calling <code>my_graph.build()</code> returns a <code>TaskGraph</code> spec.</li> <li>When the body runs under the builder, calls to <code>@tool</code> proxies add nodes to the graph and return <code>NodeHandle</code>s.</li> <li>Perfect for fan\u2011out (parallel branches) and fan\u2011in (join/aggregate) patterns.</li> </ul>"},{"location":"reference/decorators/#parameters_2","title":"Parameters","text":"<ul> <li>name (str) \u2014 Graph identifier.</li> <li>inputs (iterable[str] or dict) \u2014 Required/optional input names. If dict, keys are optional names with defaults in the body.</li> <li>outputs (list[str], optional) \u2014 Names of exposed boundary outputs. If body returns a single literal, declare exactly one.</li> <li>version (str) \u2014 Semantic version.</li> <li>agent (str, optional) \u2014 Register this graph as an agent (factory registered).</li> </ul>"},{"location":"reference/decorators/#returns_2","title":"Returns","text":"<ul> <li> <p>The decorator returns a builder function with:</p> </li> <li> <p><code>.build() -&gt; TaskGraph</code></p> </li> <li><code>.spec() -&gt; TaskGraphSpec</code></li> <li><code>.io() -&gt; IO signature</code></li> </ul>"},{"location":"reference/decorators/#example-fanout-fanin","title":"Example \u2014 fan\u2011out + fan\u2011in","text":"<pre><code>from aethergraph import graphify, tool\n\n@tool(outputs=[\"y\"])\ndef f(x: int):\n    return {\"y\": x * x}\n\n@tool(outputs=[\"z\"])\ndef g(x: int):\n    return {\"z\": x + 1}\n\n@tool(outputs=[\"sum\"])  \ndef add(a: int, b: int):\n    return {\"sum\": a + b}\n\n@graphify(name=\"fan_in_out\", inputs=[\"x\"], outputs=[\"total\"]) \ndef pipe(x):\n    a = f(x=x)          # node A (graph mode)  \u2510\n    b = g(x=x)          # node B (graph mode)  \u2518  \u2190 fan\u2011out\n    c = add(a=a.y, b=b.z)   # node C depends on A,B \u2190 fan\u2011in\n    return {\"total\": c.sum}\n\nG = pipe.build()\n</code></pre>"},{"location":"reference/decorators/#example-ordering-with-_after-and-aliasing","title":"Example \u2014 ordering with <code>_after</code> and aliasing","text":"<pre><code>@tool(outputs=[\"out\"]) \ndef step(name: str):\n    return {\"out\": name}\n\n@graphify(name=\"ordered\", inputs=[]) \ndef ordered():\n    a = step(name=\"A\", _alias=\"first\")\n    b = step(name=\"B\", _after=a)\n    c = step(name=\"C\", _after=[a, b], _id=\"third\")\n    return {\"final\": c.out}\n\nG = ordered.build()\n</code></pre>"},{"location":"reference/decorators/#using-tool-inside-graph_fn-vs-graphify","title":"Using <code>@tool</code> inside <code>@graph_fn</code> vs <code>@graphify</code>","text":"<ul> <li>Inside <code>@graph_fn</code>: <code>@tool</code> calls execute immediately (no <code>_after</code>/alias). Great for quick helpers.</li> <li>Inside <code>@graphify</code>: <code>@tool</code> calls define nodes (support <code>_after</code>, <code>_alias</code>, <code>_id</code>, <code>_labels</code>). Ideal for orchestration.</li> </ul>"},{"location":"reference/decorators/#interop-best-practices","title":"Interop &amp; best practices","text":"<ol> <li>Start simple with <code>@graph_fn</code> \u2014 it\u2019s the easiest way to get <code>context</code> and ship a working demo.</li> <li>Extract reusable steps with <code>@tool</code> \u2014 typed I/O makes debugging, tracing, and promotion to graphs trivial.</li> <li> <p>Promote to <code>@graphify</code> when you need:</p> </li> <li> <p>Parallel branches (fan\u2011out), joins (fan\u2011in)</p> </li> <li>Explicit ordering with <code>_after</code></li> <li>Reuse via <code>NodeHandle</code> composition and aliasing</li> <li> <p>Context access:</p> </li> <li> <p><code>@graph_fn</code> gives you <code>context: NodeContext</code> directly.</p> </li> <li>In <code>@graphify</code>, nodes don\u2019t get <code>context</code>; tools run with context at execution time when the graph is interpreted. Use <code>@tool</code> implementations to call <code>context.*</code>.</li> <li>Outputs discipline \u2014 keep outputs small and typed (e.g., <code>{ \"image\": ref, \"metrics\": {\u2026} }</code>).</li> <li>Registry \u2014 all three decorators register artifacts (graph fn as runnable, tool impls, graph factories) so you can call by name later.</li> </ol>"},{"location":"reference/decorators/#see-also","title":"See also","text":"<ul> <li>Quick Start: install, start server, first <code>@graph_fn</code>.</li> <li>**Contex</li> </ul>"},{"location":"reference/graph-graph-fn/","title":"<code>@graph_fn</code> \u2013 Graph Function API","text":"<p><code>@graph_fn</code> turns a plain Python function into a GraphFunction object \u2013 a named, versioned graph entry that still feels like a normal Python callable.</p> <ul> <li>You write a regular function (usually <code>async</code>) with whatever parameters you want.</li> <li>If the function\u2019s signature includes a <code>context</code> parameter, a <code>NodeContext</code> is injected automatically.</li> <li>Inside the body, calls to <code>@tool</code> functions are recorded as nodes, while plain Python calls just run inline.</li> </ul> <p>This section focuses on the API surface and contracts, not on execution details or schedulers.</p>"},{"location":"reference/graph-graph-fn/#1-graphfunction-overview","title":"1. GraphFunction Overview","text":"<p>The decorator builds a <code>GraphFunction</code> instance:</p> <pre><code>class GraphFunction:\n    def __init__(\n        self,\n        name: str,\n        fn: Callable,\n        inputs: list[str] | None = None,\n        outputs: list[str] | None = None,\n        version: str = \"0.1.0\",\n    ):\n        self.graph_id = name\n        self.name = name\n        self.fn = fn\n        self.inputs = inputs or []\n        self.outputs = outputs or []\n        self.version = version\n        self.registry_key: str | None = None\n        self.last_graph = None\n        self.last_context = None\n        self.last_memory_snapshot = None\n</code></pre>"},{"location":"reference/graph-graph-fn/#key-attributes","title":"Key attributes","text":"Attribute Type Meaning / contract <code>graph_id</code> <code>str</code> Stable identifier for the graph, usually equal to <code>name</code>. Used for specs, visualization, and provenance. <code>name</code> <code>str</code> Human\u2011readable name of the graph function. Shows up in logs/registry. <code>fn</code> <code>Callable</code> The original Python function body you wrote. Used to build the graph and evaluate return values. <code>inputs</code> <code>list[str]</code> Declared graph input names. Optional; can be empty. <code>outputs</code> <code>list[str]</code> Declared graph output names. Optional; used for output normalization/validation. <code>version</code> <code>str</code> Version tag for this graph. Included in registry entries and provenance. <code>registry_key</code> <code>str \\| None</code> Internal hook for registry bookkeeping. Not part of the public contract. <code>last_graph</code> <code>TaskGraph \\| None</code> Last built graph (for inspection and debugging). May be <code>None</code> if not built yet. <code>last_context</code> <code>Any</code> Reserved for runtime use (not a stable API). <code>last_memory_snapshot</code> <code>Any</code> Reserved for runtime use (not a stable API). <p>Treat <code>graph_id</code>, <code>name</code>, <code>inputs</code>, <code>outputs</code>, <code>version</code>, and <code>last_graph</code> as the main public surface; the other fields are implementation details that can change.</p>"},{"location":"reference/graph-graph-fn/#2-graph_fn-decorator-definition","title":"2. <code>@graph_fn</code> Decorator \u2013 Definition","text":"<pre><code>def graph_fn(\n    name: str,\n    inputs: list[str] | None = None,\n    outputs: list[str] | None = None,\n    version: str = \"0.1.0\",\n    agent: str | None = None,\n) -&gt; Callable[[Callable], GraphFunction]:\n    ...\n</code></pre>"},{"location":"reference/graph-graph-fn/#parameters","title":"Parameters","text":"Parameter Type Required? Description <code>name</code> <code>str</code> Yes Logical ID for the graph. Also used as the <code>graphfn</code> registry name. <code>inputs</code> <code>list[str] \\| None</code> No Optional list of named graph inputs. If omitted, the graph can still accept <code>**inputs</code>, but there is no typed input list in the spec. <code>outputs</code> <code>list[str] \\| None</code> No Optional list of declared graph outputs. Used to normalize and validate the function\u2019s return value. <code>version</code> <code>str</code> No Semantic version for this graph. Defaults to <code>\"0.1.0\"</code>. <code>agent</code> <code>str \\| None</code> No If provided, additionally registers this <code>GraphFunction</code> in the registry under the <code>agent</code> namespace with the given name."},{"location":"reference/graph-graph-fn/#what-the-decorator-returns","title":"What the decorator returns","text":"<p>Using <code>@graph_fn</code> on a function returns a <code>GraphFunction</code> instance, not the raw function:</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"example\", inputs=[\"x\"], outputs=[\"y\"])\nasync def example(x: int, *, context: NodeContext):\n    ...\n\n# `example` is now a GraphFunction\nassert isinstance(example, GraphFunction)\n</code></pre> <p>Internally, the decorator:</p> <ol> <li>Constructs <code>GraphFunction(name, fn, inputs, outputs, version)</code>.</li> <li>Looks up the current registry via <code>current_registry()</code>.</li> <li> <p>If a registry is available, registers the object as:</p> <ul> <li><code>nspace=\"graphfn\"</code>, <code>name=name</code>, <code>version=version</code>, <code>obj=gf</code>.</li> </ul> </li> <li> <p>If <code>agent</code> is provided, also registers:</p> <ul> <li><code>nspace=\"agent\"</code>, <code>name=agent</code>, <code>version=version</code>, <code>obj=gf</code>.</li> </ul> </li> </ol> <p>The <code>agent</code> registration currently serves as metadata for higher\u2011level agent frameworks; it does not change how the graph function runs.</p>"},{"location":"reference/graph-graph-fn/#3-function-shape-context-injection","title":"3. Function Shape &amp; Context Injection","text":"<p>A <code>graph_fn</code> is written like a normal Python function. The only special convention is the optional <code>context</code> parameter:</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"example\")\nasync def example(x: int, *, context: NodeContext):\n    # `context` is injected automatically\n    await context.channel().send_text(f\"x={x}\")\n    return {\"y\": x + 1}\n</code></pre> <p>Rules:</p> <ul> <li>You can define any positional/keyword parameters you want (<code>x</code>, <code>y</code>, <code>options</code>, etc.).</li> <li>If the function signature contains a parameter named <code>context</code>, the runtime injects a <code>NodeContext</code> instance when building/executing the graph.</li> <li>If <code>context</code> is not present in the signature, nothing special is injected.</li> </ul> <p>The injected <code>NodeContext</code> gives you access to runtime services like <code>channel()</code>, <code>memory()</code>, <code>artifacts()</code>, <code>logger()</code>, etc. This is the main way <code>graph_fn</code>s interact with the outside world.</p>"},{"location":"reference/graph-graph-fn/#4-tools-vs-plain-callables-inside-a-graph_fn","title":"4. Tools vs Plain Callables Inside a <code>graph_fn</code>","text":"<p>Inside a graph function body, you can freely mix:</p> <ol> <li>Plain Python code and callables \u2013 executed immediately as normal Python.</li> <li><code>@tool</code> functions \u2013 these create graph nodes and tracked edges when invoked.</li> </ol>"},{"location":"reference/graph-graph-fn/#plain-callables","title":"Plain callables","text":"<p>Regular functions or methods (not decorated with <code>@tool</code>) behave exactly as in standard Python:</p> <pre><code>def local_scale(x: int, factor: int = 2) -&gt; int:\n    return x * factor\n\n@graph_fn(name=\"mixed_body\")\nasync def mixed_body(x: int, *, context: NodeContext):\n    y = local_scale(x)          # plain Python call, no node created\n    context.logger().info(\"scaled\", extra={\"y\": y})\n    return {\"y\": y}\n</code></pre>"},{"location":"reference/graph-graph-fn/#tool-functions-nodes-on-the-fly","title":"<code>@tool</code> functions \u2013 nodes on the fly","text":"<p>When you call a <code>@tool</code> inside a <code>graph_fn</code>, the decorator\u2019s proxy detects that a graph builder is active, execute the <code>@tool</code>, and return the <code>NodeHandler</code>. You can access the output of the tool as regular <code>@tool</code>. </p> <p>Conceptually:</p> <pre><code>from aethergraph import tool\n\n@tool(outputs=[\"total\"])\ndef sum_vec(xs: list[float]) -&gt; dict:\n    return {\"total\": sum(xs)}\n\n@graph_fn(name=\"tool_usage\")\nasync def tool_usage(xs: list[float], *, context: NodeContext):\n    stats = {\"n\": len(xs)}           # plain Python\n    out = sum_vec(xs=xs)             # becomes a node (NodeHandle)\n    await context.channel().send_text(\n        f\"n={stats['n']}, total={out['total']}\"\n    )\n    return {\"total\": out[\"total\"]}\n</code></pre> <p>API contract:</p> <ul> <li>Plain callables inside a <code>graph_fn</code> execute immediately and are not tracked as graph nodes.</li> <li><code>@tool</code> calls inside a <code>graph_fn</code> become tracked nodes with inputs/outputs wired into the <code>TaskGraph</code>.</li> <li>You can still treat the <code>@tool</code> call\u2019s result like a dict in your function body; the NodeHandle provides field\u2011like accessors that match the tool outputs.</li> </ul>"},{"location":"reference/graph-graph-fn/#5-output-contract-high-level","title":"5. Output Contract (High Level)","text":"<p>The <code>outputs</code> list given to <code>@graph_fn</code> defines the expected graph outputs. After your function body returns, the runtime normalizes the return value and enforces this contract.</p> <p>Supported return shapes (high\u2011level):</p> <ul> <li>A <code>dict[str, Any]</code> \u2013 keys may map to literals, <code>NodeHandle</code>s, or internal <code>Ref</code> objects.</li> <li>A single <code>NodeHandle</code> \u2013 its outputs will be exposed as graph outputs.</li> <li>A single literal / <code>Ref</code> \u2013 only valid when exactly one output is declared.</li> </ul> <p>If <code>outputs</code> is not <code>None</code>:</p> <ul> <li>The normalized result is restricted to exactly those keys (in order).</li> <li>Missing keys cause a <code>ValueError</code>.</li> </ul>"},{"location":"reference/graph-graph-fn/#6-registry-and-agent-metadata","title":"6. Registry and <code>agent</code> Metadata","text":"<p>When a registry is active (<code>current_registry() is not None</code>), <code>@graph_fn</code> registers the created <code>GraphFunction</code> automatically:</p> <pre><code>registry.register(\n    nspace=\"graphfn\",\n    name=name,\n    version=version,\n    obj=gf,\n)\n</code></pre> <p>If you pass <code>agent=\"my_agent\"</code>, the same object is additionally registered as:</p> <pre><code>registry.register(\n    nspace=\"agent\",\n    name=agent,\n    version=version,\n    obj=gf,\n)\n</code></pre> <p>Current usage:</p> <ul> <li><code>graphfn</code> entries: used to discover graphs by name/version and to attach specs/visualizations.</li> <li><code>agent</code> entries: reserved for higher\u2011level agent orchestration (e.g., routing, multi\u2011agent systems). At present, it is a metadata hook only; the core <code>GraphFunction</code> API does not change.</li> </ul>"},{"location":"reference/graph-graph-fn/#7-summary","title":"7. Summary","text":"<ul> <li><code>@graph_fn</code> wraps a Python function into a GraphFunction: a named, versioned graph entry with optional inputs/outputs.</li> <li>The function signature is normal Python; an optional <code>context</code> parameter triggers  <code>NodeContext</code> injection. </li> <li> <p>Inside the body:</p> <ul> <li>Plain Python calls execute inline.</li> <li><code>@tool</code> calls become nodes on the fly, tracked in the underlying <code>TaskGraph</code>.</li> </ul> </li> <li> <p>Declared <code>outputs</code> define the expected graph outputs and are enforced by the runtime\u2019s normalization logic.</p> </li> <li> <p>When a registry is active, each <code>graph_fn</code> is registered under <code>graphfn</code>, and optionally under <code>agent</code> when <code>agent=\"...\"</code> is provided.</p> </li> </ul>"},{"location":"reference/graph-graphify/","title":"<code>graphify</code> \u2013 Static Graph Builder Decorator","text":"<p><code>graphify</code> turns a plain Python function into a static graph factory: a callable that, when invoked, builds and returns a <code>TaskGraph</code> using the graph builder context.</p> <p>Key ideas:</p> <ul> <li>The decorated function is evaluated at build time, not at run time.</li> <li>Inputs are injected as graph refs (via <code>arg(\"&lt;name&gt;\")</code>), not real values.</li> <li>Calls to <code>@tool</code> inside the function body become tool nodes in the <code>TaskGraph</code>.</li> <li>The function\u2019s return value defines which refs / literals are exposed as graph outputs.</li> <li>There is no <code>NodeContext</code> injection in <code>graphify</code>; if you need context services, put that logic inside a <code>@tool</code> and call the tool from the graphified function.</li> </ul> <p>This section focuses on the API and contracts of <code>graphify</code>. Details of <code>TaskGraph</code> structure and the runner live on dedicated pages.</p>"},{"location":"reference/graph-graphify/#1-decorator-signature","title":"1. Decorator Signature","text":"<pre><code>def graphify(\n    *,\n    name: str,\n    outputs: list[str],\n    inputs = (),\n    version: str = \"0.1.0\",\n    agent: str | None = None,\n):\n    ...\n</code></pre>"},{"location":"reference/graph-graphify/#parameters","title":"Parameters","text":"Parameter Type Required? Description <code>name</code> <code>str</code> Yes Graph ID / name for the built <code>TaskGraph</code>. Used in specs, logs, and registry. <code>inputs</code> <code>Iterable[str]</code> or <code>dict[str, Any]</code> No (default <code>()</code> ) Graph input declaration. Can be a list/tuple of required input names, or a dict of optional inputs with default values. <code>outputs</code> <code>list[str] \\| None</code> Yes List of declared graph output names. <code>version</code> <code>str</code> No Version tag for this graph. Included in registry entries and provenance. <code>agent</code> <code>str \\| None</code> No If provided, also registers this graph under the <code>agent</code> namespace in the registry. <p>Inputs semantics</p> <ul> <li> <p>If <code>inputs</code> is a sequence (e.g. <code>(\"x\", \"y\")</code>):</p> <ul> <li>All listed names are treated as required inputs.</li> </ul> </li> <li> <p>If <code>inputs</code> is a dict (e.g. <code>{ \"x\": 0.0, \"y\": 1.0 }</code>):</p> <ul> <li>All keys are treated as optional inputs, and the dict values can be used as default metadata.</li> </ul> </li> </ul> <p>The function parameters determine which inputs are injected (see below); <code>inputs</code> defines the graph-level input signature (required/optional) via <code>g.declare_inputs(...)</code>.</p>"},{"location":"reference/graph-graphify/#2-what-graphify-returns","title":"2. What <code>graphify</code> Returns","text":"<p>Applying <code>graphify</code> to a function wraps it into a builder function:</p> <pre><code>@graphify(name=\"my_graph\", inputs=[\"x\"], outputs=[\"y\"])\ndef my_graph(x):\n    ...\n</code></pre> <p>The decorated object (e.g. <code>my_graph</code>) is not the original function anymore. Instead, it is a zero-argument callable that builds a new <code>TaskGraph</code> each time you call it:</p> <pre><code>g = my_graph()     # builds and returns a TaskGraph\n</code></pre> <p>For convenience, the builder also exposes a few attributes:</p> Attribute Type Meaning <code>my_graph()</code> <code>() -&gt; TaskGraph</code> Calling the object builds a fresh <code>TaskGraph</code>. <code>my_graph.build</code> <code>() -&gt; TaskGraph</code> Alias to the same build function. <code>my_graph.graph_name</code> <code>str</code> Graph name used when constructing the <code>TaskGraph</code> (from <code>name</code>). <code>my_graph.version</code> <code>str</code> Version tag passed via <code>version</code>. <code>my_graph.spec()</code> <code>() -&gt; TaskGraphSpec</code> Builds a graph and returns only the spec (<code>g.spec</code>). <code>my_graph.io()</code> <code>() -&gt; dict</code> Builds a graph and returns <code>g.io_signature()</code>. <p>In other words: <code>graphify</code> gives you a graph factory with lightweight helpers to inspect the spec and I/O signature. The original function body is used only to describe how the graph should be built.</p>"},{"location":"reference/graph-graphify/#3-function-shape-input-injection","title":"3. Function Shape &amp; Input Injection","text":"<p>The function you decorate with <code>graphify</code> is written like a regular synchronous function. Always return a dictionary of outputs specified in the decorator. </p> <pre><code>from aethergraph import graphify\n\n@graphify(name=\"sum_graph\", inputs=[\"xs\"], outputs=[\"total\"])\ndef sum_graph(xs):\n    # body uses NodeHandles / refs\n    ...\n    return {\"total\": some_handle}\n</code></pre>"},{"location":"reference/graph-graphify/#parameter-matching","title":"Parameter matching","text":"<p>Contract:</p> <ul> <li>Only parameters whose name appears in <code>inputs</code> are injected.</li> <li>Injected values are <code>arg(\"&lt;name&gt;\")</code> refs, not concrete Python values.</li> <li>Extra parameters in the function signature that are not listed in <code>inputs</code> are not passed by <code>graphify</code>.</li> <li>There is no <code>context</code> parameter here \u2013 if you need <code>NodeContext</code> services (channel, memory, artifacts, logger, etc.), move that logic into a <code>@tool</code> and call the tool from the graphified function.</li> </ul> <p>This means:</p> <ul> <li>Use <code>inputs</code> to define which parameters are part of the graph I/O.</li> <li>Inside the function body, treat injected parameters as graph references \u2013 suitable for passing into <code>@tool</code> calls and other graph APIs.</li> </ul>"},{"location":"reference/graph-graphify/#4-tools-vs-plain-code-inside-a-graphify-body","title":"4. Tools vs Plain Code Inside a <code>graphify</code> Body","text":"<p>When the builder runs your function inside <code>with graph(name=...) as g</code>, the graph builder is active (<code>current_builder()</code> is not <code>None</code>). This affects how <code>@tool</code> calls behave.</p>"},{"location":"reference/graph-graphify/#plain-python-code","title":"Plain Python code","text":"<p>Non-tool code runs normally and is not recorded as a node:</p> <pre><code>def local_scale(xs, factor=2):\n    return [x * factor for x in xs]\n\n@graphify(name=\"scaled_sum\", inputs=[\"xs\"], outputs=[\"total\"])\ndef scaled_sum(xs):\n    ys = local_scale(xs)   # plain Python; not a node\n    ...\n</code></pre>"},{"location":"reference/graph-graphify/#tool-calls-nodes","title":"<code>@tool</code> calls \u2192 nodes","text":"<p>When you call a <code>@tool</code> inside a <code>graphify</code> body, the <code>tool</code> proxy uses <code>call_tool(...)</code>:</p> <ul> <li>It detects the active <code>GraphBuilder</code> via <code>current_builder()</code>.</li> <li>It creates a tool node via <code>builder.add_tool_node(...)</code>.</li> <li>It returns a NodeHandle (static build-mode handle) with <code>node_id</code> and <code>output_keys</code>.</li> </ul> <p>Example shape:</p> <pre><code>from aethergraph import tool\n\n@tool(outputs=[\"total\"])\ndef sum_vec(xs: list[float]) -&gt; dict:\n    return {\"total\": sum(xs)}\n\n@graphify(name=\"sum_graph\", inputs=[\"xs\"], outputs=[\"total\"])\ndef sum_graph(xs):\n    node = sum_vec(xs=xs)     # \u2190 NodeHandle, not a concrete result\n    return {\"total\": node}   # expose this node\u2019s outputs\n</code></pre> <p>Contract:</p> <ul> <li>Plain Python functions: execute at build time, do not affect the graph structure (unless they call graph APIs themselves).</li> <li><code>@tool</code> calls: always become nodes in the resulting <code>TaskGraph</code> when used inside a <code>graphify</code> body.</li> </ul>"},{"location":"reference/graph-graphify/#control-kwargs-for-node-ordering-ids","title":"Control kwargs for node ordering &amp; IDs","text":"<p>Calling tools inside a <code>@graphify</code> also supports a small set of control-plane keyword arguments that do not become data inputs but affect node metadata and ordering. </p> <p>You can pass them when calling a <code>@tool</code> inside a <code>graphify</code> body:</p> <ul> <li><code>_after</code>: one node or a list of nodes/IDs this node should run after.</li> <li><code>_name</code>: human-readable display name for the node (stored in metadata as <code>display_name</code>).</li> <li><code>_alias</code>: short, unique alias for the node; can be used for later lookup via <code>find_by_alias</code>.</li> <li><code>_id</code>: hard override for the underlying <code>node_id</code> (must be unique within the graph).</li> <li><code>_labels</code>: one or more string labels (as a str or list[str]) to tag the node; used for later lookup and grouping via <code>find_by_label</code>.</li> </ul> <p>Example:</p> <pre><code>n1 = sum_vec(xs=xs, _alias=\"sum1\")\nn2 = sum_vec(xs=n1.total, _after=[n1], _name=\"sum_again\", _id=\"sum_again_node\")\n</code></pre> <p>These control fields are optional and are taken out of <code>kwargs</code> before computing tool inputs, so they never appear as data edges.</p>"},{"location":"reference/graph-graphify/#5-registry","title":"5. Registry","text":"<p>At decoration time, <code>graphify</code> checks the current registry:</p> <p>Behavior:</p> <ul> <li> <p>If a registry is active:</p> <ul> <li>A <code>TaskGraph</code> instance is registered under <code>nspace=\"graph\"</code> with the given <code>name</code> and <code>version</code>.</li> <li>If <code>agent</code> is provided, another <code>TaskGraph</code> instance is registered under <code>nspace=\"agent\"</code> with that name.</li> </ul> </li> <li> <p>If no registry is active:</p> <ul> <li>The builder still works; registration is simply skipped.</li> </ul> </li> </ul> <p>Current implementation registers a concrete <code>TaskGraph</code> instance, not the factory itself.</p>"},{"location":"reference/graph-graphify/#6-summary","title":"6. Summary","text":"<ul> <li><code>graphify</code> decorates a synchronous function and turns it into a TaskGraph factory: calling the decorated object builds a fresh <code>TaskGraph</code>.</li> <li><code>inputs</code> defines graph-level input names (required vs optional) and determines which parameters are injected as <code>arg(\"&lt;name&gt;\")</code> refs.</li> <li> <p>Inside the body:</p> <ul> <li>Plain Python code runs at build time and is not recorded as nodes.</li> <li><code>@tool</code> calls become tool nodes via the active <code>GraphBuilder</code> (<code>current_builder()</code>), with optional control-plane kwargs (<code>_after</code>, <code>_name</code>, <code>_alias</code>, <code>_id</code>) for ordering and IDs.</li> </ul> </li> <li> <p>There is no <code>NodeContext</code> in <code>graphify</code>; use a <code>@tool</code> when you need context services and call that tool from the graphified function.</p> </li> <li>When a registry is active, <code>graphify</code> registers a built <code>TaskGraph</code> under <code>graph</code> and optionally under <code>agent</code> when <code>agent=\"...\"</code> is provided.</li> <li>When executed, <code>graphify</code> graphs automatically take advantage of concurrent execution under a configurable concurrency cap for nodes that are ready at the same level.</li> </ul>"},{"location":"reference/graph-task-graph/","title":"<code>TaskGraph</code> \u2013 Runtime Graph Representation","text":"<p><code>TaskGraph</code> is the runtime representation of a graph in AetherGraph. It combines:</p> <ul> <li>A structural spec (<code>TaskGraphSpec</code>) \u2013 nodes, dependencies, metadata.</li> <li>A mutable state (<code>TaskGraphState</code>) \u2013 node statuses, outputs, patches.</li> <li>Ephemeral runtime nodes (<code>TaskNodeRuntime</code>) \u2013 convenience wrappers used by the scheduler and tools.</li> </ul> <p>This page documents the most commonly used APIs on <code>TaskGraph</code>. </p> <p>You typically will not directly use <code>TaskGraph</code> method except for inspection. Using <code>@graph_fn</code> and <code>graphify</code> to create graph is preferred.</p>"},{"location":"reference/graph-task-graph/#1-construction-core-attributes","title":"1. Construction &amp; Core Attributes","text":""},{"location":"reference/graph-task-graph/#classmethods","title":"Classmethods","text":"<pre><code>TaskGraph.new_run(spec: TaskGraphSpec, *, run_id: str | None = None, **kwargs) -&gt; TaskGraph\nTaskGraph.from_spec(spec: TaskGraphSpec, *, state: TaskGraphState | None = None) -&gt; TaskGraph\n</code></pre> <ul> <li><code>new_run(...)</code> \u2013 convenience to create a fresh run with a new <code>run_id</code> and an empty <code>TaskGraphState</code> (all nodes start in <code>PENDING</code> except the inputs node, which is set to <code>DONE</code>).</li> <li><code>from_spec(...)</code> \u2013 construct a <code>TaskGraph</code> from an existing spec and optional state (used for resuming or inspecting previous runs).</li> </ul> <p>Key attributes</p> <ul> <li><code>graph.spec: TaskGraphSpec</code> \u2013 structural definition.</li> <li><code>graph.state: TaskGraphState</code> \u2013 statuses, outputs, patches, bound inputs.</li> <li><code>graph.graph_id: str</code> \u2013 alias for <code>spec.graph_id</code>.</li> <li><code>graph.nodes: list[TaskNodeRuntime]</code> \u2013 list of runtime node wrappers.</li> <li><code>graph._runtime_nodes: dict[str, TaskNodeRuntime]</code> \u2013 internal node table (id \u2192 runtime node).</li> </ul> Typical construction: new run vs resume <pre><code># New run from a spec\nspec = ...  # TaskGraphSpec from graphify / storage\nG = TaskGraph.new_run(spec)\n\n# Resume with existing state\nstate = ...  # TaskGraphState loaded from storage\nG_resumed = TaskGraph.from_spec(spec, state=state)\n</code></pre> <p>You rarely instantiate <code>TaskGraph</code> directly; use <code>new_run</code> or <code>from_spec</code> (or runner helpers) instead.</p>"},{"location":"reference/graph-task-graph/#2-node-access-selection","title":"2. Node Access &amp; Selection","text":""},{"location":"reference/graph-task-graph/#direct-access","title":"Direct access","text":"<pre><code>node(self, node_id: str) -&gt; TaskNodeRuntime\n@property\nnodes(self) -&gt; list[TaskNodeRuntime]\n\nnode_ids(self) -&gt; list[str]\nget_by_id(self, node_id: str) -&gt; str\n</code></pre> <ul> <li><code>node(node_id)</code> \u2013 get the runtime node (raises if not found).</li> <li><code>nodes</code> \u2013 list of all runtime nodes.</li> <li><code>node_ids()</code> \u2013 list of node IDs.</li> <li><code>get_by_id()</code> \u2013 returns the same ID or raises if missing (useful when normalizing selectors).</li> </ul>"},{"location":"reference/graph-task-graph/#indexed-finders","title":"Indexed finders","text":"<pre><code>get_by_alias(alias: str) -&gt; str\nfind_by_label(label: str) -&gt; list[str]\nfind_by_logic(logic_prefix: str, *, first: bool = False) -&gt; list[str] | str | None\nfind_by_display(name_prefix: str, *, first: bool = False) -&gt; list[str] | str | None\n</code></pre> <p>These use metadata created at build time (e.g., via <code>call_tool(..., _alias=..., _labels=[...], _name=...)</code>).</p> <ul> <li><code>get_by_alias(\"sum1\")</code> \u2192 node id for alias <code>sum1</code> or <code>KeyError</code>.</li> <li><code>find_by_label(\"critical\")</code> \u2192 all node ids tagged with that label.</li> <li><code>find_by_logic(\"tool_name\")</code> \u2192 nodes whose logic name equals or starts with <code>tool_name</code>.</li> <li><code>find_by_display(\"My Step\")</code> \u2192 nodes whose display name equals or starts with <code>\"My Step\"</code>.</li> </ul>"},{"location":"reference/graph-task-graph/#unified-selector-dsl","title":"Unified selector DSL","text":"<pre><code>select(selector: str, *, first: bool = False) -&gt; str | list[str] | None\npick_one(selector: str) -&gt; str\npick_all(selector: str) -&gt; list[str]\n</code></pre> <p>Selector mini\u2011DSL:</p> <ul> <li><code>\"@alias\"</code> \u2192 by alias.</li> <li><code>\"#label\"</code> \u2192 by label (may return many).</li> <li><code>\"id:&lt;id&gt;\"</code> \u2192 exact id.</li> <li><code>\"logic:&lt;prefix&gt;\"</code> \u2192 logic name prefix.</li> <li><code>\"name:&lt;prefix&gt;\"</code> \u2192 display name prefix.</li> <li><code>\"/regex/\"</code> \u2192 regex on <code>node_id</code>.</li> <li>anything else \u2192 prefix match on <code>node_id</code>.</li> </ul> Selector examples <pre><code># Single node by alias\ntarget_id = graph.pick_one(\"@sum1\")\n\n# All nodes with a label\ncritical_ids = graph.pick_all(\"#critical\")\n\n# First node whose logic name starts with \"normalize_\"\nnid = graph.select(\"logic:normalize_\", first=True)\n\n# Regex on node id\ndebug_nodes = graph.pick_all(\"/debug_.*/\")\n</code></pre> <p>Use selectors when building debug tooling, partial resets, or visualization filters.</p>"},{"location":"reference/graph-task-graph/#3-readonly-views","title":"3. Read\u2011only Views","text":"<pre><code>view(self) -&gt; GraphView\nlist_nodes(self, exclude_internal: bool = True) -&gt; list[str]\n</code></pre> <ul> <li> <p><code>view()</code> \u2013 returns a <code>GraphView</code> with:</p> </li> <li> <p><code>graph_id</code>,</p> </li> <li><code>nodes</code> (specs),</li> <li><code>node_status</code> (derived map: node id \u2192 <code>NodeStatus</code>),</li> <li><code>metadata</code>.</li> <li><code>list_nodes(exclude_internal=True)</code> \u2013 list node ids, optionally excluding internal nodes (ids starting with <code>_</code>).</li> </ul> Inspecting a graph view <pre><code>v = graph.view()\nprint(v.graph_id)\nprint(v.node_status)  # {\"node_1\": NodeStatus.DONE, ...}\n</code></pre> <p><code>GraphView</code> is a snapshot for inspection / APIs; it does not expose mutation methods.</p>"},{"location":"reference/graph-task-graph/#4-graph-mutation-patches","title":"4. Graph Mutation (Patches)","text":"<p>Dynamic graph edits are represented as patches in <code>TaskGraphState</code>.</p> <pre><code>patch_add_or_replace_node(node_spec: dict[str, Any]) -&gt; None\npatch_remove_node(node_id: str) -&gt; None\npatch_add_dependency(node_id: str, dependency_id: str) -&gt; None\n</code></pre> <ul> <li><code>patch_add_or_replace_node</code> \u2013 add a new node or replace an existing one (payload is a plain dict convertible to <code>TaskNodeSpec</code>).</li> <li><code>patch_remove_node</code> \u2013 remove a node by id.</li> <li><code>patch_add_dependency</code> \u2013 add a new dependency edge.</li> </ul> <p>Each method:</p> <ul> <li>appends a <code>GraphPatch</code> entry to <code>state.patches</code> and increments <code>state.rev</code>,</li> <li>notifies observers via <code>on_patch_applied</code>,</li> <li>rebuilds <code>_runtime_nodes</code> for the effective view.</li> </ul> <p>These APIs are intended for advanced dynamic graph editing and patch flows; many users won\u2019t need them directly.</p>"},{"location":"reference/graph-task-graph/#5-topology-subgraphs","title":"5. Topology &amp; Subgraphs","text":"<pre><code>dependents(node_id: str) -&gt; list[str]\ntopological_order() -&gt; list[str]\nget_subgraph_nodes(start_node_id: str) -&gt; list[str]\nget_upstream_nodes(start_node_id: str) -&gt; list[str]\n</code></pre> <ul> <li><code>dependents(nid)</code> \u2013 all nodes that list <code>nid</code> as a dependency.</li> <li><code>topological_order()</code> \u2013 a topological sort of all nodes (raises on cycles).</li> <li><code>get_subgraph_nodes(start)</code> \u2013 <code>start</code> plus all nodes reachable downstream (dependents).</li> <li><code>get_upstream_nodes(start)</code> \u2013 <code>start</code> plus all nodes it depends on (upstream).</li> </ul> Working with subgraphs <pre><code># All nodes that can be affected if you change `node_a`\nforward = graph.get_subgraph_nodes(\"node_a\")\n\n# All nodes that must run before `node_b`\nupstream = graph.get_upstream_nodes(\"node_b\")\n</code></pre> <p>These helpers are typically used for partial reset, impact analysis, or visualization filters.</p>"},{"location":"reference/graph-task-graph/#6-state-mutation-reset","title":"6. State Mutation &amp; Reset","text":"<pre><code>async def set_node_status(self, node_id: str, status: NodeStatus) -&gt; None\nasync def set_node_outputs(self, node_id: str, outputs: dict[str, Any]) -&gt; None\n\nasync def reset_node(self, node_id: str, *, preserve_outputs: bool = False)\nasync def reset(\n    self,\n    node_ids: list[str] | None = None,\n    *,\n    recursive: bool = True,\n    direction: str = \"forward\",\n    preserve_outputs: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <ul> <li><code>set_node_status</code> \u2013 update a node\u2019s status and notify observers (<code>on_node_status_change</code>).</li> <li><code>set_node_outputs</code> \u2013 update a node\u2019s outputs and notify observers (<code>on_node_output_change</code>).</li> <li><code>reset_node</code> \u2013 reset a single node to <code>PENDING</code>, optionally keeping outputs.</li> <li> <p><code>reset</code> \u2013 reset all or part of the graph:</p> </li> <li> <p><code>node_ids=None</code> \u2192 reset all nodes (except the synthetic inputs node).</p> </li> <li><code>recursive=True, direction=\"forward\"</code> \u2192 also reset all dependents.</li> <li><code>recursive=True, direction=\"backward\"</code> \u2192 reset upstream dependencies.</li> </ul> Partial reset patterns <pre><code># Reset a node and everything that depends on it\nawait graph.reset(node_ids=[\"step_3\"], recursive=True, direction=\"forward\")\n\n# Reset only a single node, keeping its outputs\nawait graph.reset(node_ids=[\"step_3\"], recursive=False, preserve_outputs=True)\n\n# Reset entire graph (except inputs)\nawait graph.reset(node_ids=None)\n</code></pre> <p>These methods are used by runners / UIs to implement retry, rerun from here, and what-if operations.</p>"},{"location":"reference/graph-task-graph/#7-io-definition-binding","title":"7. IO Definition &amp; Binding","text":""},{"location":"reference/graph-task-graph/#io-apis","title":"IO APIs","text":"<pre><code>declare_inputs(\n    *,\n    required: Iterable[str] | None = None,\n    optional: dict[str, Any] | None = None,\n) -&gt; None\n\nexpose(name: str, value: Ref | Any) -&gt; None\nrequire_outputs(*names: str) -&gt; None\n\nio_signature(include_values: bool = False) -&gt; dict[str, Any]\n</code></pre> <ul> <li> <p><code>declare_inputs(...)</code> \u2013 declares graph-level inputs:</p> </li> <li> <p><code>required</code> \u2013 names that must be provided when binding inputs.</p> </li> <li><code>optional</code> \u2013 names with default values (modeled via <code>ParamSpec</code>).</li> <li> <p><code>expose(name, value)</code> \u2013 declare a graph output:</p> </li> <li> <p><code>value</code> can be a Ref (to node outputs or inputs) or a literal.</p> </li> <li><code>require_outputs(...)</code> \u2013 sanity check for required outputs (uses internal <code>_io_outputs</code>).</li> <li> <p><code>io_signature(include_values=False)</code> \u2013 summarized IO description:</p> </li> <li> <p><code>inputs.required</code> / <code>inputs.optional</code> (names and defaults).</p> </li> <li><code>outputs.keys</code> \u2013 names of exposed outputs.</li> <li><code>outputs.bindings</code> \u2013 raw bindings (Refs or literals).</li> <li><code>outputs.values</code> \u2013 optional resolved values (when <code>include_values=True</code>).</li> </ul> <p>Binding of actual input values happens via the runner, which calls the internal <code>_validate_and_bind_inputs(...)</code> helper.</p> Inspect IO signature <pre><code>sig = graph.io_signature()\nprint(sig[\"inputs\"][\"required\"])\nprint(sig[\"outputs\"][\"keys\"])\n\n# After a run, you can inspect resolved output values\nfull = graph.io_signature(include_values=True)\nprint(full[\"outputs\"][\"values\"])\n</code></pre> <p>The IO signature is useful for APIs, UIs, and tooling that needs to describe how to call a graph without inspecting internals.</p>"},{"location":"reference/graph-task-graph/#8-observers-notifications","title":"8. Observers &amp; Notifications","text":"<pre><code>add_observer(observer: Any) -&gt; None\n</code></pre> <p>Observers are objects that can implement any of the following methods:</p> <ul> <li><code>on_node_status_change(runtime_node)</code></li> <li><code>on_node_output_change(runtime_node)</code></li> <li><code>on_inputs_bound(graph)</code></li> <li><code>on_patch_applied(graph, patch)</code></li> </ul> <p>They are invoked whenever the corresponding events occur.</p> Lightweight observer usage <pre><code>class PrintObserver:\n    def on_node_status_change(self, node):\n        print(\"status\", node.node_id, node.state.status)\n\ngraph.add_observer(PrintObserver())\n</code></pre> <p>Observers are the main extension point for logging, metrics, and live UI updates.</p>"},{"location":"reference/graph-task-graph/#9-diff-persistence-helpers","title":"9. Diff &amp; Persistence Helpers","text":""},{"location":"reference/graph-task-graph/#diffing","title":"Diffing","text":"<pre><code>diff(other: TaskGraph) -&gt; dict[str, Any]\n</code></pre> <ul> <li>Compare two graphs with the same <code>graph_id</code>.</li> <li> <p>Returns a dict with:</p> </li> <li> <p><code>\"added\"</code>: list of node ids present only in <code>other</code>.</p> </li> <li><code>\"removed\"</code>: list of node ids present only in <code>self</code>.</li> <li><code>\"modified\"</code>: node ids whose dependencies or metadata differ.</li> </ul> Basic diff usage <pre><code>d = graph_v2.diff(graph_v1)\nprint(\"added\", d[\"added\"])\nprint(\"modified\", d[\"modified\"])\n</code></pre> <p>Useful for visualizing evolution, reviewing patches, or migration tooling.</p>"},{"location":"reference/graph-task-graph/#spec-serialization","title":"Spec serialization","text":"<pre><code>spec_json(self) -&gt; dict[str, Any]\n</code></pre> <ul> <li>Returns a JSON\u2011safe representation of the spec (<code>TaskGraphSpec</code>) using <code>_dataclass_to_plain</code>.</li> <li>Storage/layout is left to callers (file, DB, etc.).</li> </ul>"},{"location":"reference/graph-task-graph/#10-debug-visualization","title":"10. Debug &amp; Visualization","text":""},{"location":"reference/graph-task-graph/#humanreadable-summary","title":"Human\u2011readable summary","text":"<pre><code>pretty(self, *, max_nodes: int = 20, max_width: int = 100) -&gt; str\n__str__(self) -&gt; str\n</code></pre> <ul> <li> <p><code>pretty(...)</code> \u2013 a compact, human\u2011friendly summary including:</p> </li> <li> <p>graph id, node count, observer count;</p> </li> <li>IO signature summary;</li> <li>state summary;</li> <li>a small table of nodes with id, type, status, dependencies count, and logic.</li> <li><code>__str__</code> \u2013 uses <code>pretty(max_nodes=12, max_width=96)</code> for <code>print(graph)</code>.</li> </ul> Quick debug print <pre><code>print(graph)          # uses __str__\nprint(graph.pretty()) # full summary\n</code></pre> <p>This is the fastest way to get an overview of a graph in a REPL or log.</p>"},{"location":"reference/graph-task-graph/#visualization-helpers","title":"Visualization helpers","text":"<p>At the bottom of the module, these are attached as methods:</p> <pre><code>TaskGraph.to_dot = to_dot\nTaskGraph.visualize = visualize\nTaskGraph.ascii_overview = ascii_overview\n</code></pre> <ul> <li><code>graph.to_dot(...)</code> \u2013 export a DOT representation.</li> <li><code>graph.visualize(...)</code> \u2013 high\u2011level helper for rich visualizations (see Visualization docs).</li> <li><code>graph.ascii_overview(...)</code> \u2013 ASCII summary for terminals / logs.</li> </ul> High\u2011level usage (shape only) <pre><code>dot_str = graph.to_dot()\nprint(graph.ascii_overview())\n# graph.visualize(...)  # see visualization docs for options\n</code></pre> <p>Exact options and rendering backends are described on the Visualization page.</p>"},{"location":"reference/graph-task-graph/#11-summary","title":"11. Summary","text":"<ul> <li><code>TaskGraph</code> ties together spec, state, and runtime node table.</li> <li>Use <code>new_run</code> / <code>from_spec</code> to construct graphs; use selectors (<code>pick_one</code>, <code>pick_all</code>) to locate nodes.</li> <li>IO is declared via <code>declare_inputs</code> / <code>expose</code> and inspected via <code>io_signature</code>.</li> <li>Topology helpers (<code>dependents</code>, <code>get_subgraph_nodes</code>, <code>get_upstream_nodes</code>) support partial reset and analysis.</li> <li>State mutation APIs (<code>set_node_status</code>, <code>set_node_outputs</code>, <code>reset</code>) underpin runners and interactive tooling.</li> <li>Patches, diff, observers, and visualization helpers are advanced tools for dynamic graphs, UIs, and diagnostics.</li> </ul>"},{"location":"reference/graph-tool/","title":"<code>@tool</code> \u2013 Dual\u2011mode Tool Decorator","text":"<p>The <code>@tool</code> decorator turns a plain Python callable into an AetherGraph tool:</p> <ul> <li>Immediate mode (no active graph): calling the decorated function executes it directly and returns a dict of outputs (or an awaitable for async tools).</li> <li>Graph mode (inside <code>@graph_fn</code> / <code>@graphify</code>): calling the decorated function builds a node and returns a <code>NodeHandle</code>; nothing is executed yet.</li> <li>Registry integration: when a registry is active, the implementation is automatically registered under the <code>tool</code> namespace.</li> <li>Context injection: if the tool signature includes a <code>context</code> parameter, a <code>NodeContext</code> is injected automatically at run time when the tool runs as a node in a graph.</li> </ul>"},{"location":"reference/graph-tool/#signature","title":"Signature","text":"<pre><code>@tool(\n    outputs: list[str],\n    inputs: list[str] | None = None,\n    *,\n    name: str | None = None,\n    version: str = \"0.1.0\",\n)\n</code></pre>"},{"location":"reference/graph-tool/#required-vs-optional","title":"Required vs optional","text":"Parameter Type Required? Description <code>outputs</code> <code>list[str]</code> Yes Declares the named outputs produced by the tool. Every call must return a dict containing exactly these keys. <code>inputs</code> <code>list[str] \\| None</code> No Optional explicit input names. If <code>None</code>, they are inferred from the implementation\u2019s signature (excluding <code>*args</code>, <code>**kwargs</code>). <code>name</code> <code>str \\| None</code> (keyword\u2011only) No Optional registry / UI name. Defaults to the underlying implementation\u2019s <code>__name__</code>. <code>version</code> <code>str</code> (keyword\u2011only) No Semantic version used for registry and provenance. Defaults to <code>\"0.1.0\"</code>. <p>Notes</p> <ul> <li><code>outputs</code> is always required and defines the contract enforced at runtime.</li> <li><code>inputs</code> is usually optional \u2013 in most cases, you can let AetherGraph infer it from the function\u2019s parameters.</li> <li><code>name</code> and <code>version</code> matter when you later want to look up tools in a registry or inspect runs.</li> </ul>"},{"location":"reference/graph-tool/#behavior-overview","title":"Behavior Overview","text":"<ul> <li> <p>Sync vs async</p> </li> <li> <p>If the implementation is synchronous, the decorated function returns a dict in immediate mode.</p> </li> <li> <p>If the implementation is async, the decorated function returns an awaitable in immediate mode.</p> </li> <li> <p>Graph vs immediate mode</p> </li> <li> <p>When a graph builder is active (<code>current_builder() is not None</code>), calling the tool returns a <code>NodeHandle</code> (graph node), not data.</p> </li> <li> <p>When no builder is active, calling the tool executes the implementation and returns results.</p> </li> <li> <p>Context injection</p> </li> <li> <p>If the tool\u2019s signature includes a parameter named <code>context</code>, it is treated as a reserved injectable rather than a normal data input.</p> </li> <li>You do not list <code>\"context\"</code> in the <code>inputs</code> list; <code>inputs</code> is for data\u2011flow arguments only.</li> <li>When the tool runs as a node in a graph (via <code>@graph_fn</code> or <code>graphify</code>), AetherGraph automatically injects a <code>NodeContext</code> instance for that node. Callers do not pass it manually when running the graph.</li> </ul>"},{"location":"reference/graph-tool/#runtime-contracts","title":"Runtime Contracts","text":"<p>The decorator enforces a strict I/O contract for every tool call:</p> <ul> <li> <p>The implementation must return either:</p> </li> <li> <p>a <code>dict</code> with exactly the declared <code>outputs</code> keys, or</p> </li> <li> <p>a value that <code>_normalize_result</code> can convert into such a dict.</p> </li> <li> <p><code>_check_contract(outputs, out, impl)</code> validates that:</p> </li> <li> <p>all <code>outputs</code> keys are present,</p> </li> <li>no unexpected keys are produced (where applicable).</li> </ul> <p>On violation, the runtime raises a clear error pointing at the original implementation.</p>"},{"location":"reference/graph-tool/#contextaware-tools","title":"Context\u2011aware Tools","text":"<p>To use runtime services inside a tool (channel, memory, artifacts, logger, etc.), add a <code>context</code> parameter:</p> <pre><code>from aethergraph import tool, NodeContext\n\n@tool(outputs=[\"y\"], inputs=[\"x\"])\nasync def double_with_log(x: int, *, context: NodeContext) -&gt; dict:\n    # `context` is injected automatically when this tool runs as a graph node\n    context.logger().info(\"doubling\", extra={\"x\": x})\n    return {\"y\": x * 2}\n</code></pre> <p>Contracts:</p> <ul> <li><code>context</code> is not part of the data\u2011flow; it never becomes an edge in the graph.</li> <li>In graph mode, callers simply write <code>double_with_log(x=42)</code> in a <code>@graph_fn</code> / <code>graphify</code> body; the runner injects <code>NodeContext</code> when executing the node.</li> <li>In immediate mode (outside any graph), you can optionally pass a <code>context</code> argument manually if you want to test the tool with a synthetic context, but typical usage is inside graphs.</li> </ul>"},{"location":"reference/graph-tool/#usage-patterns","title":"Usage Patterns","text":"<p>Below are common ways to define and call tools. These examples focus on shape and contracts; see the main Graph docs for deeper patterns.</p> 1. Simple synchronous tool <pre><code>from aethergraph import tool\n\n@tool(outputs=[\"y\"])\ndef double(x: int) -&gt; dict:\n    return {\"y\": x * 2}\n\n# Immediate mode (no active graph)\nresult = double(x=21)\nassert result == {\"y\": 42}\n</code></pre> <p>Key points</p> <ul> <li><code>outputs=[\"y\"]</code> is required.</li> <li><code>inputs</code> is omitted \u2192 inferred as <code>[\"x\"]</code> from the function signature.</li> <li>In immediate mode, <code>double(...)</code> runs immediately and returns a dict.</li> </ul> 2. Async tool <pre><code>from aethergraph import tool\n\n@tool(outputs=[\"text\"])\nasync def fetch_text(url: str) -&gt; dict:\n    data = await some_async_http_get(url)\n    return {\"text\": data}\n\n# Immediate mode\nresult = await fetch_text(url=\"https://example.com\")\nprint(result[\"text\"])\n</code></pre> <p>Key points</p> <ul> <li>Implementation is async \u2192 in immediate mode, calling <code>fetch_text(...)</code> returns an awaitable.</li> <li><code>inputs</code> again inferred from the signature (<code>[\"url\"]</code>).</li> </ul> 3. Explicit inputs (with non\u2011data params) <pre><code>from aethergraph import tool, NodeContext\n\n@tool(outputs=[\"out\"], inputs=[\"a\", \"b\"])\nasync def add(a: int, b: int = 0, *, scale: int = 1, context: NodeContext | None = None) -&gt; dict:\n    # `a`, `b` are data inputs; `scale` and `context` are not graph edges\n    if context is not None:\n        context.logger().info(\"adding\", extra={\"a\": a, \"b\": b, \"scale\": scale})\n    return {\"out\": (a + b) * scale}\n</code></pre> <p>Key points</p> <ul> <li><code>inputs=[\"a\", \"b\"]</code> means only <code>a</code> and <code>b</code> are considered data\u2011inputs from upstream nodes.</li> <li> <p><code>scale</code> and <code>context</code> are non\u2011data parameters:</p> </li> <li> <p><code>context</code> is a reserved injectable (runtime will supply it in graph mode).</p> </li> <li><code>scale</code> can be passed as a literal or configured inside the graph; it does not appear as an edge unless you model it explicitly.</li> </ul> 4. Graph mode \u2013 building nodes <pre><code>from aethergraph import graph_fn, tool, NodeContext\n\n@tool(outputs=[\"y\"])\ndef double(x: int):\n    return {\"y\": x * 2}\n\n@tool(outputs=[\"y\"], inputs=[\"x\"])\nasync def double_with_log(x: int, *, context: NodeContext) -&gt; dict:\n    context.logger().info(\"doubling\", extra={\"x\": x})\n    return {\"y\": x * 2}\n\n@graph_fn(name=\"pipeline\", outputs=[\"z\"])\nasync def pipeline(*, context: NodeContext):\n    # Inside a graph_fn, calling tools builds nodes\n    n1 = double(x=21)                 # NodeHandle, not the numeric 42\n    n2 = double_with_log(x=n1.y)      # NodeHandle with NodeContext injection at run time\n    return {\"z\": n2.y}\n</code></pre> <p>Key points</p> <ul> <li>Inside <code>@graph_fn</code>, both <code>double(...)</code> and <code>double_with_log(...)</code> do not run immediately.</li> <li>Instead, <code>NodeHandle</code>s are returned and the builder records tool nodes and their dependencies.</li> <li> <p>When the graph is executed:</p> </li> <li> <p>The runtime calls the underlying implementation with concrete values for data inputs.</p> </li> <li>If a <code>context</code> parameter is present, a <code>NodeContext</code> instance is injected automatically for that node.</li> </ul>"},{"location":"reference/graph-tool/#registry-behavior-advanced","title":"Registry Behavior (Advanced)","text":"<p>When a registry is active (<code>current_registry() is not None</code>), the decorator automatically registers the underlying implementation:</p> <pre><code>registry.register(\n    nspace=\"tool\",\n    name=name or impl.__name__,\n    version=version,\n    obj=impl,\n)\n</code></pre> <p>This enables:</p> <ul> <li>Discovery \u2013 listing available tools by <code>(name, version)</code>.</li> <li>Provenance \u2013 runs can record which version of which tool was used.</li> <li>Hot reload / development \u2013 registries can swap implementations without changing graph code.</li> </ul> <p>If no registry is active, the decorator still works normally; registration is simply skipped.</p>"},{"location":"reference/graph-tool/#summary","title":"Summary","text":"<ul> <li>Use <code>@tool(outputs=[...])</code> on any function to make it part of the graph runtime.</li> <li>Required: <code>outputs</code> \u2013 define the contract.</li> <li>Optional: <code>inputs</code>, <code>name</code>, <code>version</code> \u2013 control graph wiring and registry metadata.</li> <li>Immediate calls return concrete data (or awaitables); calls inside graphs create nodes and wire dependencies.</li> <li> <p>To use runtime services inside a tool, add a <code>context</code> parameter:</p> </li> <li> <p>It is automatically injected as <code>NodeContext</code> when the tool runs as a node in a graph.</p> </li> <li>Callers never pass <code>context</code> manually when running <code>@graph_fn</code> or <code>graphify</code> graphs; the runner does it for them.</li> </ul>"},{"location":"reference/rest-api/","title":"REST API","text":"<ul> <li><code>GET /health</code> \u2192 200 OK</li> <li><code>POST /execute</code> \u2192 Execute a graph function</li> <li><code>GET/PUT /artifacts/*</code> \u2192 Retrieve/store artifacts</li> </ul> <p>(Add OpenAPI/Redoc when ready.)</p>"},{"location":"reference/runner-api/","title":"Runner API \u2013 <code>run_async</code> &amp; <code>run</code>","text":"<p>The runner is the unified entry point to execute either:</p> <ul> <li>a <code>GraphFunction</code> (from <code>@graph_fn</code>), or</li> <li>a static <code>TaskGraph</code> (built via <code>graphify</code> / builder / storage).</li> </ul> <p>Under the hood it builds a <code>RuntimeEnv</code>, wires services, and drives a <code>ForwardScheduler</code> with configurable retry and concurrency.</p>"},{"location":"reference/runner-api/#1-function-shapes","title":"1. Function Shapes","text":""},{"location":"reference/runner-api/#run_async","title":"<code>run_async</code>","text":"<pre><code>async def run_async(\n    target,\n    inputs: dict[str, Any] | None = None,\n    **rt_overrides,\n):\n    \"\"\"\n    Generic async runner for TaskGraph or GraphFunction.\n    - GraphFunction \u2192 delegates to gf.run(env=..., **inputs)\n    - TaskGraph/builder \u2192 schedules and resolves graph-level outputs\n    \"\"\"\n</code></pre> <p>Accepted <code>target</code> types</p> <ul> <li><code>GraphFunction</code> instance (<code>@graph_fn</code> result)</li> <li><code>TaskGraph</code> instance </li> <li>Builder with <code>.build()</code> returning a <code>TaskGraph</code></li> <li>Callable returning a <code>TaskGraph</code> when called with no args</li> </ul> <p>Inputs</p> <ul> <li> <p><code>inputs: dict[str, Any] | None</code>   Graph-level inputs (must match the graph\u2019s declared <code>required</code> / <code>optional</code> IO). Defaults to <code>{}</code>.</p> </li> <li> <p><code>**rt_overrides</code> \u2013 runtime overrides (see Runtime overrides).</p> </li> </ul> <p>Returns</p> <ul> <li>For a <code>GraphFunction</code> target: <code>dict</code> matching its declared or inferred <code>outputs</code> list.</li> <li> <p>For a TaskGraph target:</p> <ul> <li>If the graph exposes exactly one output \u2192 that value directly.</li> <li>Otherwise \u2192 <code>dict[name, value]</code> for all exposed outputs. (In <code>@graphify</code> it is the declared <code>outputs</code> list)</li> </ul> </li> <li> <p>May raise <code>GraphHasPendingWaits</code> if the run quiesces with unresolved waits and outputs cannot yet be resolved.</p> </li> </ul>"},{"location":"reference/runner-api/#run-sync-adapter","title":"<code>run</code> (sync adapter)","text":"<pre><code>def run(\n    target,\n    inputs: dict[str, Any] | None = None,\n    **rt_overrides,\n):\n    ...\n</code></pre> <ul> <li>Thin synchronous wrapper around <code>run_async</code>, using a background event loop thread.</li> <li>Same <code>target</code>, <code>inputs</code>, and <code>rt_overrides</code> semantics.</li> <li>Returns the same result shape as <code>run_async(...)</code> but blocks the current thread.</li> </ul> <p>Use <code>run(...)</code> in scripts / notebooks where you don\u2019t want to manage an event loop yourself. Prefer <code>run_async(...)</code> in async applications or services.</p>"},{"location":"reference/runner-api/#2-graphfunction-convenience-await-my_graph_fn","title":"2. GraphFunction Convenience \u2013 <code>await my_graph_fn(...)</code>","text":"<p>For <code>@graph_fn</code> agents, you don\u2019t have to call the runner directly.</p> <p><code>GraphFunction</code> implements <code>__call__</code>:</p> <pre><code>class GraphFunction:\n    ...\n\n    async def __call__(self, **inputs):\n        \"\"\"Async call to run the graph function.\n        Usage:\n           result = await my_graph_fn(input1=value1, input2=value2)\n        \"\"\"\n        from ..runtime.graph_runner import run_async\n        return await run_async(self, inputs)\n</code></pre> <p>So you can simply:</p> <pre><code># my_graph_fn is created via @graph_fn(...)\nresult = await my_graph_fn(x=1, y=2)\n</code></pre> <p>Behind the scenes this:</p> <ul> <li>Builds a <code>RuntimeEnv</code> and <code>ForwardScheduler</code> via <code>run_async</code>.</li> <li>Creates a fresh TaskGraph for this call.</li> <li>Injects <code>NodeContext</code> if the function signature includes <code>context</code>.</li> </ul> <p>Use <code>await my_graph_fn(...)</code> for day-to-day agent usage; reach for <code>run_async(...)</code> when you need fine-grained control over runtime overrides or when running static TaskGraphs.</p>"},{"location":"reference/runner-api/#3-runtime-overrides-rt_overrides","title":"3. Runtime overrides (<code>**rt_overrides</code>)","text":"<p>The <code>run_async</code> / <code>run</code> API accepts extra keyword arguments to customize the runtime. These are passed to <code>_build_env(...)</code>, which:</p> <ol> <li>Creates a default container via <code>build_default_container()</code>.</li> <li>Applies overrides onto the container and env.</li> </ol>"},{"location":"reference/runner-api/#core-overrides","title":"Core overrides","text":"<p>These are the most important knobs:</p> <ul> <li> <p><code>run_id: str</code>   Explicit run identifier (otherwise a random <code>run-&lt;hex&gt;</code> is generated). Used for persistence, resume, and continuations.</p> </li> <li> <p><code>retry: RetryPolicy</code>   Custom retry behavior for node execution. Defaults to a new <code>RetryPolicy()</code> when not provided.</p> </li> <li> <p><code>max_concurrency: int</code>   Upper bound on parallel node execution used by the scheduler. Defaults to <code>getattr(owner, \"max_concurrency\", 4)</code> \u2013 where <code>owner</code> is the <code>GraphFunction</code> or <code>TaskGraph</code>.</p> </li> </ul> <p>In addition, any override where the name matches a container attribute is applied directly:</p> <pre><code>container = _get_container()\nfor k, v in rt_overrides.items():\n    if v is not None and hasattr(container, k):\n        setattr(container, k, v)\n</code></pre> <p>Typical examples include (depending on your container):</p> <ul> <li><code>state_store=...</code> \u2013 custom state store for snapshots / resume.</li> <li><code>artifacts=...</code> \u2013 custom artifact store.</li> <li><code>llm=...</code>, <code>memory=...</code>, <code>rag=...</code>, <code>kv=...</code> \u2013 swapped service instances.</li> <li><code>continuation_store=...</code> \u2013 custom continuation backend.</li> </ul> <p>Rule of thumb: Use explicit overrides (<code>run_id</code>, <code>retry</code>, <code>max_concurrency</code>) most often; use container attribute overrides when you need to plug in custom services for a particular run.</p>"},{"location":"reference/runner-api/#4-scheduler-behavior-execution-model","title":"4. Scheduler Behavior (Execution Model)","text":"<p>Currently, Scheduler API is not exposed. Here we list the function shape for completeness </p> <p><code>run_async</code> builds a <code>ForwardScheduler</code>:</p> <pre><code>sched = ForwardScheduler(\n    graph,\n    env,\n    retry_policy=retry,\n    max_concurrency=max_conc,\n    skip_dep_on_failure=True,\n    stop_on_first_error=True,\n    logger=logger,\n)\n</code></pre> <p>Key parameters</p> <ul> <li><code>max_concurrency: int</code> \u2013 how many ready nodes can run concurrently.</li> <li><code>stop_on_first_error: bool</code> \u2013 whether to stop scheduling when the first node fails (<code>True</code> in the default runner).</li> <li><code>skip_dep_on_failure: bool</code> \u2013 whether to skip downstream nodes whose dependencies failed (<code>True</code> by default).</li> </ul> <p>Execution semantics (high level)</p> <ul> <li>The scheduler maintains a queue of ready nodes (all dependencies completed).</li> <li>It launches nodes in parallel up to <code>max_concurrency</code>.</li> <li>If a node waits (e.g., external continuation), it may park and allow other ready nodes to proceed.</li> <li>At the end of the run, the runner resolves graph-level outputs via <code>io_signature</code> bindings; if some outputs are unresolved due to waits, it raises <code>GraphHasPendingWaits</code>.</li> </ul> <p>More advanced scheduling / resume controls are exposed via the Scheduler and Recovery APIs; this section only covers the knobs surfaced through <code>run_async</code>.</p>"},{"location":"reference/runner-api/#5-run-vs-resume-async","title":"5. Run vs Resume (async)","text":"<p>There is also a helper:</p> <pre><code>async def run_or_resume_async(\n    target,\n    inputs: dict[str, Any],\n    *,\n    run_id: str | None = None,\n    **rt_overrides,\n):\n    \"\"\"\n    If state exists for run_id \u2192 cold resume, else fresh run.\n    Exactly the same signature as run_async plus optional run_id.\n    \"\"\"\n</code></pre> <ul> <li>If a <code>state_store</code> is configured and snapshots exist for the given <code>run_id</code>, the graph is recovered and resumed.</li> <li>If not, it behaves like a fresh <code>run_async(...)</code> call (with the given <code>run_id</code> if provided).</li> </ul> <p>Use this when you:</p> <ul> <li>Know the <code>run_id</code> you want to continue, but</li> <li>Don\u2019t want to manually check whether state exists.</li> </ul>"},{"location":"reference/runner-api/#6-summary","title":"6. Summary","text":"<ul> <li><code>run_async(target, inputs, **rt_overrides)</code> is the core runner for both <code>GraphFunction</code> agents and static <code>TaskGraph</code>s.</li> <li><code>run(...)</code> is a sync adapter around <code>run_async</code> for scripts / notebooks.</li> <li><code>GraphFunction</code> supports direct <code>await my_graph_fn(...)</code>, which simply delegates to <code>run_async(self, inputs)</code>.</li> <li> <p>You can control runtime behavior via:</p> </li> <li> <p><code>run_id</code>, <code>retry</code>, <code>max_concurrency</code>,</p> </li> <li>and container-level overrides (e.g., <code>state_store</code>, <code>artifacts</code>, <code>llm</code>, <code>memory</code>).</li> <li>Execution is handled by a <code>ForwardScheduler</code> that runs ready nodes concurrently up to <code>max_concurrency</code>, stopping on first error and skipping dependents of failed nodes by default.</li> </ul>"},{"location":"reference/runtime-api/","title":"Runtime Services API \u2013 Global Services &amp; Helpers","text":"<p>AetherGraph keeps core runtime services (channels, LLM, RAG, logger, external services, MCP, etc.) in a services container. These helpers give you a consistent way to:</p> <ul> <li>Install or swap the global services container.</li> <li>Access channel/LLM/RAG/logger services outside of a <code>NodeContext</code>.</li> <li>Register extra context services reachable via <code>context.&lt;name&gt;()</code>.</li> <li>Configure RAG backends and MCP clients.</li> </ul> <p>In most applications you don\u2019t call these directly. When you start the sidecar server via <code>start_server()</code> / <code>start(...)</code>, it builds a default container (using <code>build_default_container</code>) and installs it so that <code>current_services()</code> and friends have something to return. The low\u2011level APIs below are mainly for advanced setups, tests, and custom hosts.</p>"},{"location":"reference/runtime-api/#1-core-services-container","title":"1. Core Services Container","text":"<p>These functions manage the process\u2011wide services container. Very likely user does not need to manage these manually.</p> install_services(services) -&gt; None <p>Description: Install a services container globally and in the current <code>ContextVar</code>.</p> <p>Inputs:</p> <ul> <li><code>services: Any</code> \u2013 Typically the result of <code>build_default_container()</code> or your own container.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes:</p> <ul> <li>Call this once at app startup if you\u2019re not using the sidecar server.</li> <li><code>current_services()</code> will fail until either <code>install_services(...)</code> or <code>ensure_services_installed(...)</code> has been used.</li> </ul> ensure_services_installed(factory) -&gt; Any <p>Description: Lazily create and install a services container if none exists, using <code>factory()</code>.</p> <p>Inputs:</p> <ul> <li><code>factory: Callable[[], Any]</code> \u2013 Function that returns a new services container.</li> </ul> <p>Returns:</p> <ul> <li><code>Any</code> \u2013 The active services container.</li> </ul> <p>Notes:</p> <ul> <li>Used internally by the runner to ensure a container exists (<code>build_default_container</code>).</li> <li>If a container is already installed, it is reused and just bound into the current context.</li> </ul> current_services() -&gt; Any <p>Description: Get the currently active services container.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>Any</code> \u2013 The services container instance.</li> </ul> <p>Notes:</p> <ul> <li>Raises <code>RuntimeError</code> if no services have been installed yet.</li> <li>Under normal use, this is wired automatically when the server starts.</li> </ul> use_services(services) -&gt; context manager <p>Description: Temporarily override the services container within a <code>with</code> block.</p> <p>Inputs:</p> <ul> <li><code>services: Any</code> \u2013 Services container to use inside the context.</li> </ul> <p>Returns:</p> <ul> <li>Context manager \u2013 Restores the previous services value on exit.</li> </ul> <p>Notes:</p> <ul> <li>Handy for tests or one\u2011off experiments where you want an isolated container.</li> </ul>"},{"location":"reference/runtime-api/#2-channel-service-helpers","title":"2. Channel Service Helpers","text":"<p>Channel helpers give you direct access to the channel bus (the same system behind <code>context.channel()</code>), and let you configure defaults and aliases.</p> get_channel_service() -&gt; Any <p>Description: Return the channel bus from the current services container.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>ChannelBus</code> (typed as <code>Any</code> here)</li> </ul> set_default_channel(key) -&gt; None <p>Description: Set the default channel key used when no explicit channel is specified.</p> <p>Inputs:</p> <ul> <li><code>key: str</code> \u2013 Channel key (e.g., <code>\"console\"</code>, <code>\"slack:my-bot\"</code>).</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> get_default_channel() -&gt; str <p>Description: Get the current default channel key.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>str</code> \u2013 Default channel key.</li> </ul> set_channel_alias(alias, channel_key) -&gt; None <p>Description: Register a human\u2011friendly alias for a channel key.</p> <p>Inputs:</p> <ul> <li><code>alias: str</code> \u2013 Short name to use in configs / code.</li> <li><code>channel_key: str</code> \u2013 Real channel key (e.g., full Slack route).</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> register_channel_adapter(name, adapter) -&gt; None <p>Description: Register a channel adapter implementation (e.g., Slack, Telegram, custom UI) under a name.</p> <p>Inputs:</p> <ul> <li><code>name: str</code> \u2013 Identifier for the adapter.</li> <li><code>adapter: Any</code> \u2013 Adapter instance implementing the channel interface.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes:</p> <ul> <li>Adapters are usually wired by the server configuration; you rarely need this in everyday graph code.</li> </ul>"},{"location":"reference/runtime-api/#3-llm-rag-service-helpers","title":"3. LLM &amp; RAG Service Helpers","text":"<p>These helpers configure the process\u2011wide LLM client profiles and the RAG backend.</p> get_llm_service() -&gt; Any <p>Description: Return the LLM service from the current container (the same one backing <code>context.llm()</code>).</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>Any</code> \u2013 LLM service.</li> </ul> register_llm_client(profile, provider, model, embed_model=None, base_url=None, api_key=None, timeout=None) -&gt; None <p>Description: Configure or update an LLM profile on the global LLM service.</p> <p>Inputs:</p> <ul> <li><code>profile: str</code> \u2013 Profile name (e.g., <code>\"default\"</code>, <code>\"fast\"</code>).</li> <li><code>provider: str</code> \u2013 Provider ID (<code>\"openai\"</code>, <code>\"anthropic\"</code>, etc.).</li> <li><code>model: str</code> \u2013 Chat/completion model name.</li> <li><code>embed_model: str | None</code> \u2013 Optional embeddings model.</li> <li><code>base_url: str | None</code></li> <li><code>api_key: str | None</code></li> <li><code>timeout: float | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>None</code> (under the hood the configured client is created and stored).</li> </ul> <p>Notes:</p> <ul> <li>Alias: <code>set_llm_client = register_llm_client</code>.</li> <li>Equivalent to calling <code>svc.llm.configure_profile(...)</code> on the container.</li> </ul> set_rag_llm_client(client=None, *, provider=None, model=None, embed_model=None, base_url=None, api_key=None, timeout=None) -&gt; LLMClientProtocol <p>Description: Set the LLM client used by the RAG service.</p> <p>Inputs:</p> <ul> <li><code>client: LLMClientProtocol | None</code> \u2013 Existing client instance. If <code>None</code>, a new <code>GenericLLMClient</code> is created.</li> <li><code>provider: str | None</code></li> <li><code>model: str | None</code></li> <li><code>embed_model: str | None</code></li> <li><code>base_url: str | None</code></li> <li><code>api_key: str | None</code></li> <li><code>timeout: float | None</code></li> </ul> <p>Returns:</p> <ul> <li><code>LLMClientProtocol</code> \u2013 The RAG LLM client actually installed.</li> </ul> <p>Notes:</p> <ul> <li>If <code>client</code> is <code>None</code>, you must provide <code>provider</code>, <code>model</code>, and <code>embed_model</code> to construct a <code>GenericLLMClient</code>.</li> <li>The chosen client is stored via <code>svc.rag.set_llm_client(...)</code>.</li> </ul> set_rag_index_backend(*, backend=None, index_path=None, dim=None) -&gt; Any <p>Description: Configure the vector index backend for the RAG service.</p> <p>Inputs:</p> <ul> <li><code>backend: str | None</code> \u2013 e.g., <code>\"sqlite\"</code> or <code>\"faiss\"</code> (defaults from <code>settings.rag.backend</code>).</li> <li><code>index_path: str | None</code> \u2013 Relative or absolute path (defaults from <code>settings.rag.index_path</code>).</li> <li><code>dim: int | None</code> \u2013 Embedding dimension (defaults from <code>settings.rag.dim</code>).</li> </ul> <p>Returns:</p> <ul> <li><code>Any</code> \u2013 The created vector index instance.</li> </ul> <p>Notes:</p> <ul> <li>Uses <code>create_vector_index(...)</code> under the hood and registers it via <code>svc.rag.set_index_backend(...)</code>.</li> <li>If <code>backend=\"faiss\"</code> but FAISS isn\u2019t available, the factory may log a warning and fall back to SQLite.</li> </ul>"},{"location":"reference/runtime-api/#4-logger-helper","title":"4. Logger Helper","text":"current_logger_factory() -&gt; Any <p>Description: Get the logger factory from the services container.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>Any</code> \u2013 Logger factory; typical usage is <code>current_logger_factory().for_scheduler()</code> or similar.</li> </ul>"},{"location":"reference/runtime-api/#5-external-context-services","title":"5. External Context Services","text":"<p>External context services are extra objects you register on the container and then access via <code>context.&lt;name&gt;()</code> inside nodes and tools.</p> register_context_service(name, service) -&gt; None <p>Description: Register a custom service in <code>svc.ext_services</code>.</p> <p>Inputs:</p> <ul> <li><code>name: str</code> \u2013 Name under which it will be exposed (e.g., <code>\"trainer\"</code>, <code>\"materials\"</code>).</li> <li><code>service: Any</code> \u2013 Service instance.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes:</p> <ul> <li>Once registered, your <code>NodeContext</code> can expose it using a helper like <code>context.trainer()</code> (depending on your <code>NodeContext</code> implementation).</li> </ul> get_ext_context_service(name) -&gt; Any <p>Description: Get a previously registered external context service.</p> <p>Inputs:</p> <ul> <li><code>name: str</code></li> </ul> <p>Returns:</p> <ul> <li><code>Any | None</code> \u2013 The service instance or <code>None</code> if not present.</li> </ul> list_ext_context_services() -&gt; list[str] <p>Description: List all registered external context service names.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>list[str]</code> \u2013 Names registered in <code>svc.ext_services</code>.</li> </ul>"},{"location":"reference/runtime-api/#6-mcp-service-helpers","title":"6. MCP Service Helpers","text":"<p>These functions configure the Model Context Protocol (MCP) integration on the services container.</p> set_mcp_service(mcp_service) -&gt; None <p>Description: Install an MCP service object on the container.</p> <p>Inputs:</p> <ul> <li><code>mcp_service: Any</code> \u2013 Service implementing MCP coordination.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> get_mcp_service() -&gt; Any <p>Description: Get the current MCP service instance.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>Any</code> \u2013 The MCP service instance.</li> </ul> register_mcp_client(name, client) -&gt; None <p>Description: Register an MCP client with the active MCP service.</p> <p>Inputs:</p> <ul> <li><code>name: str</code> \u2013 Logical client name.</li> <li><code>client: Any</code> \u2013 MCP client instance.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Notes:</p> <ul> <li>Raises <code>RuntimeError</code> if no MCP service has been installed (<code>set_mcp_service(...)</code>).</li> </ul> list_mcp_clients() -&gt; list[str] <p>Description: List all registered MCP client names.</p> <p>Inputs:</p> <ul> <li>\u2014</li> </ul> <p>Returns:</p> <ul> <li><code>list[str]</code> \u2013 Client names; returns <code>[]</code> if no MCP service or clients.</li> </ul>"},{"location":"reference/runtime-api/#7-summary","title":"7. Summary","text":"<ul> <li>The services container is the central place where channels, LLM, RAG, logger, external services, and MCP live.</li> <li>The sidecar server normally installs this container for you; otherwise use <code>install_services(...)</code> / <code>ensure_services_installed(...)</code>.</li> <li>Channel, LLM, RAG, and MCP helpers let you configure global behavior without touching individual graphs.</li> <li><code>register_context_service(...)</code> is the main hook for extending <code>NodeContext</code> with custom domain services (e.g., trainers, simulators, material DBs).</li> </ul>"},{"location":"reference/runtime-server/","title":"Sidecar Server \u2013 <code>start_server</code> and <code>stop_server</code>","text":"<p>The sidecar server is a lightweight FastAPI+Uvicorn process that:</p> <ul> <li>Boots the services container (channels, LLM, RAG, artifacts, state store, etc.).</li> <li>Exposes a small HTTP/WebSocket API for adapters, web UI, and continuations.</li> <li>Installs a web channel so you can talk to agents via a browser.</li> </ul> <p>For most applications you:</p> <ol> <li>Call <code>start_server(...)</code> once at startup (or in a notebook cell).</li> <li>Use <code>@graph_fn</code>, <code>run_async</code>, and <code>context.*</code> as usual.</li> <li>Optionally call <code>stop_server()</code> in tests or when shutting down.</li> </ol>"},{"location":"reference/runtime-server/#1-start_server-sync-sidecar-starter","title":"1. <code>start_server</code> \u2013 sync sidecar starter","text":"<pre><code>def start_server(\n    *,\n    workspace: str = \"./aethergraph_data\",\n    host: str = \"127.0.0.1\",\n    port: int = 8000,      # 0 = auto free port\n    log_level: str = \"warning\",\n    unvicorn_log_level: str = \"warning\",\n    return_container: bool = False,\n) -&gt; str | tuple[str, Any]:\n    \"\"\"Start the AetherGraph sidecar server in a background thread.\"\"\"\n</code></pre> <p>Description:</p> <p>Start the sidecar server in a background thread and install runtime services. This is safe to call at the top of any script or notebook cell; if the server is already running, it simply returns the previously computed URL.</p>"},{"location":"reference/runtime-server/#parameters","title":"Parameters","text":"<ul> <li> <p><code>workspace: str = \"./aethergraph_data\"</code></p> <ul> <li> <p>Filesystem root directory for AetherGraph data:</p> <ul> <li>artifact storage</li> <li>RAG indexes</li> <li>logs</li> <li>snapshots/state store</li> <li>any other on-disk runtime data<ul> <li>Use an absolute or project\u2011relative path; it will be created if missing.</li> </ul> </li> </ul> </li> </ul> </li> <li> <p><code>host: str = \"127.0.0.1\"</code></p> <ul> <li>Host interface for the HTTP server (e.g., <code>\"0.0.0.0\"</code> to expose externally).</li> </ul> </li> <li> <p><code>port: int = 8000</code></p> <ul> <li>Desired port.</li> <li>If <code>port == 0</code>, a free port is chosen automatically.</li> </ul> </li> <li> <p><code>log_level: str = \"warning\"</code></p> <ul> <li>Application log level (used by <code>create_app</code>, e.g., <code>\"info\"</code>, <code>\"debug\"</code>, <code>\"warning\"</code>).</li> </ul> </li> <li> <p><code>unvicorn_log_level: str = \"warning\"</code></p> <ul> <li>Log level passed to Uvicorn\u2019s <code>Config</code> (note: parameter name uses the existing spelling in code).</li> </ul> </li> <li> <p><code>return_container: bool = False</code></p> <ul> <li>If <code>False</code> (default): return only the base URL as <code>str</code>.</li> <li>If <code>True</code>: return a tuple <code>(url, container)</code> where <code>container</code> is the installed services container (<code>app.state.container</code>).</li> </ul> </li> </ul>"},{"location":"reference/runtime-server/#returns","title":"Returns","text":"<ul> <li><code>str</code> \u2013 Base URL of the sidecar server (e.g., <code>\"http://127.0.0.1:8001\"</code>).</li> <li>or <code>(str, container)</code> when <code>return_container=True</code>.</li> </ul>"},{"location":"reference/runtime-server/#behavior-notes","title":"Behavior &amp; notes","text":"<ul> <li>Idempotent: If called multiple times in the same process, only the first call actually starts the server; later calls return the same <code>_url</code>.</li> <li>Runs uvicorn in a separate thread (<code>\"aethergraph-sidecar\"</code>) with its own event loop.</li> <li>Installs the services container for the process, so <code>current_services()</code>, <code>context.llm()</code>, <code>context.channel()</code>, etc. all work.</li> </ul>"},{"location":"reference/runtime-server/#2-start_server_async-async-friendly-wrapper","title":"2. <code>start_server_async</code> \u2013 async-friendly wrapper","text":"<pre><code>async def start_server_async(**kw) -&gt; str:\n    \"\"\"Async-friendly wrapper; still uses a thread to avoid clashing with caller loop.\"\"\"\n</code></pre> <p>Description:</p> <p>Async wrapper around <code>start_server(...)</code>. It still starts Uvicorn in a thread, but exposes an <code>async</code> signature so you can <code>await</code> it from async code.</p>"},{"location":"reference/runtime-server/#parameters_1","title":"Parameters","text":"<ul> <li><code>**kw</code> \u2013 Same keyword arguments as <code>start_server(...)</code> (<code>workspace</code>, <code>host</code>, <code>port</code>, etc.).</li> </ul>"},{"location":"reference/runtime-server/#returns_1","title":"Returns","text":"<ul> <li><code>str</code> \u2013 Base URL of the sidecar server.</li> </ul> <p>Notes:</p> <ul> <li>Useful in async applications where you want to start the sidecar without blocking the existing event loop.</li> </ul>"},{"location":"reference/runtime-server/#3-stop_server-optional-shutdown-helper","title":"3. <code>stop_server</code> \u2013 optional shutdown helper","text":"<pre><code>def stop_server() -&gt; None:\n    \"\"\"Optional: stop the background server (useful in tests).\"\"\"\n</code></pre> <p>Description:</p> <p>Request a clean shutdown of the background Uvicorn thread and reset internal globals.</p>"},{"location":"reference/runtime-server/#parameters_2","title":"Parameters","text":"<ul> <li>\u2014</li> </ul>"},{"location":"reference/runtime-server/#returns_2","title":"Returns","text":"<ul> <li><code>None</code></li> </ul>"},{"location":"reference/runtime-server/#behavior-notes_1","title":"Behavior &amp; notes","text":"<ul> <li>If the server was never started, this is a no-op.</li> <li>Sets an internal <code>_shutdown_flag</code>, which the background loop polls; once set, the server\u2019s <code>should_exit</code> flag is toggled and the main serve task finishes.</li> <li>Joins the server thread (up to 5 seconds) and clears <code>_started</code>, <code>_server_thread</code>, <code>_url</code>, and <code>_shutdown_flag</code>.</li> <li> <p>Particularly useful for:</p> </li> <li> <p>Unit tests / integration tests where you need to bring up and tear down the sidecar.</p> </li> <li>Long\u2011running notebooks where you want to restart with a fresh workspace or settings.</li> </ul>"},{"location":"reference/runtime-server/#4-when-to-use-the-sidecar","title":"4. When to use the sidecar","text":"<p>You should start the sidecar server when you need:</p> <ul> <li>External channels beyond the console (Slack, web UI, etc.).</li> <li>Resumable runs and centralized state store / snapshots.</li> <li>A shared services container for multiple processes or adapters.</li> </ul> <p>For purely local, one\u2011off experiments that only use <code>run_async</code> and <code>context</code> from a single process, you can rely on the default container (via <code>ensure_services_installed</code>). But for anything interactive or multi\u2011channel, <code>start_server(...)</code> is the recommended entrypoint.</p>"},{"location":"reference/tool-channel/","title":"Built-in Channel Tools (<code>@tool</code>)","text":"<p>AetherGraph ships a small set of built-in channel tools that wrap the active <code>context.channel()</code> service. They are all defined as <code>@tool(...)</code> so that:</p> <ul> <li>In graph mode (<code>@graph_fn</code>, <code>@graphify</code>), each call becomes a tool node with proper provenance.</li> <li>The <code>ask_*</code> tools support resumable waits (user input, approvals, file uploads).</li> <li>In immediate mode (no active graph), they behave like plain async functions returning dicts.</li> </ul> <p>Important: All <code>ask_*</code> tools are meant to be called from a graph (<code>@graph_fn</code> or <code>@graphify</code>), not from inside another <code>@tool</code> implementation. They rely on a <code>NodeContext</code> and wait/resume semantics that only exist at the graph level.</p> <p>All tools here assume <code>context</code> is injected automatically by the runtime; you typically do not pass <code>context</code> yourself.</p>"},{"location":"reference/tool-channel/#1-ask_text-prompt-wait-for-free-form-reply","title":"1. <code>ask_text</code> \u2013 prompt + wait for free-form reply","text":"<pre><code>@tool(name=\"ask_text\", outputs=[\"text\"])\nasync def ask_text(\n    *,\n    resume=None,\n    context=None,\n    prompt: str | None = None,\n    silent: bool = False,\n    timeout_s: int = 3600,\n    channel: str | None = None,\n):\n    ...\n</code></pre> ask_text(*, resume=None, context=None, prompt=None, silent=False, timeout_s=3600, channel=None) -&gt; {\"text\": str} <p>Description:</p> <p>Send an optional <code>prompt</code> message via the active channel and wait for a text reply. Under the hood this uses a dual-stage tool (<code>ask_text_ds</code>) so the node can enter a WAITING state and be resumed later when the user responds.</p> <p>Inputs (data/control):</p> <ul> <li><code>prompt: str | None</code> \u2013 Text shown to the user. If <code>None</code>, some channels may only show a generic input request.</li> <li><code>silent: bool</code> \u2013 If <code>True</code>, do not send a visible prompt; only wait for incoming text.</li> <li><code>timeout_s: int</code> \u2013 Max seconds to wait before timing out.</li> <li><code>channel: str | None</code> \u2013 Optional channel key or alias (Slack thread, web session, etc.). If <code>None</code>, uses the default channel.</li> <li><code>resume: Any</code> \u2013 Continuation payload used internally on resume. You do not set this manually in normal usage.</li> <li><code>context</code> \u2013 Injected <code>NodeContext</code>, used internally via <code>context.channel()</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>{\"text\": str}</code> \u2013 The captured user reply as plain text.</li> </ul> <p>Notes:</p> <ul> <li>Use this inside <code>@graph_fn</code> / <code>@graphify</code> for resumable user input.</li> <li>Not intended to be called from within another <code>@tool</code> implementation.</li> </ul>"},{"location":"reference/tool-channel/#2-wait_text-wait-for-a-reply-without-sending-a-prompt","title":"2. <code>wait_text</code> \u2013 wait for a reply without sending a prompt","text":"<pre><code>@tool(name=\"wait_text\", outputs=[\"text\"])\nasync def wait_text(\n    *, resume=None, context=None, timeout_s: int = 3600, channel: str | None = None\n):\n    ...\n</code></pre> wait_text(*, resume=None, context=None, timeout_s=3600, channel=None) -&gt; {\"text\": str} <p>Description:</p> <p>Wait for the next incoming text message on a given channel without sending a new prompt. Useful when a prior node already sent a message, and you only need to block until the user responds.</p> <p>Inputs:</p> <ul> <li><code>timeout_s: int</code> \u2013 Max seconds to wait.</li> <li><code>channel: str | None</code> \u2013 Channel key/alias; defaults to the current/default channel.</li> <li><code>resume</code>, <code>context</code> \u2013 Internal, handled by the runtime.</li> </ul> <p>Returns:</p> <ul> <li><code>{\"text\": str}</code> \u2013 The received message.</li> </ul> <p>Notes:</p> <ul> <li>Like <code>ask_text</code>, this is a waitable tool node \u2013 only use at graph level.</li> </ul>"},{"location":"reference/tool-channel/#3-ask_approval-buttons-approval-flow","title":"3. <code>ask_approval</code> \u2013 buttons / approval flow","text":"<pre><code>@tool(name=\"ask_approval\", outputs=[\"approved\", \"choice\"])\nasync def ask_approval(\n    *,\n    resume=None,\n    context=None,\n    prompt: str,\n    options: list[str] | tuple[str, ...] = (\"Approve\", \"Reject\"),\n    timeout_s: int = 3600,\n    channel: str | None = None,\n):\n    ...\n</code></pre> ask_approval(*, prompt, options=(\"Approve\",\"Reject\"), timeout_s=3600, channel=None, ...) -&gt; {\"approved\": bool, \"choice\": str} <p>Description:</p> <p>Send a message with button options (e.g., Approve / Reject) and wait for the user to click one. Ideal for human-in-the-loop approvals in a workflow.</p> <p>Inputs:</p> <ul> <li><code>prompt: str</code> \u2013 Text shown above the buttons.</li> <li><code>options: list[str] | tuple[str, ...]</code> \u2013 Labels for buttons (first is typically the \"approve\" action).</li> <li><code>timeout_s: int</code> \u2013 Max seconds to wait.</li> <li><code>channel: str | None</code> \u2013 Optional channel key/alias.</li> <li><code>resume</code>, <code>context</code> \u2013 Internal; managed by the runtime.</li> </ul> <p>Returns:</p> <ul> <li> <p><code>{\"approved\": bool, \"choice\": str}</code></p> </li> <li> <p><code>approved</code> \u2013 <code>True</code> if the chosen label is considered positive (by current policy; typically the first option), <code>False</code> otherwise.</p> </li> <li><code>choice</code> \u2013 The raw string label clicked by the user.</li> </ul> <p>Notes:</p> <ul> <li>Implemented as a dual-stage waitable tool; use from <code>@graph_fn</code> / <code>@graphify</code>.</li> </ul>"},{"location":"reference/tool-channel/#4-ask_files-prompt-for-uploads","title":"4. <code>ask_files</code> \u2013 prompt for uploads","text":"<pre><code>@tool(name=\"ask_files\", outputs=[\"text\", \"files\"])\nasync def ask_files(\n    *,\n    resume=None,\n    context=None,\n    prompt: str,\n    accept: list[str] | None = None,\n    multiple: bool = True,\n    timeout_s: int = 3600,\n    channel: str | None = None,\n):\n    ...\n</code></pre> ask_files(*, prompt, accept=None, multiple=True, timeout_s=3600, channel=None, ...) -&gt; {\"text\": str, \"files\": list[FileRef]} <p>Description:</p> <p>Ask the user to upload one or more files, optionally constraining allowed types, and wait until they respond.</p> <p>Inputs:</p> <ul> <li><code>prompt: str</code> \u2013 Text requesting the upload.</li> <li><code>accept: list[str] | None</code> \u2013 Optional list of accepted types (MIME types or extensions), depending on channel implementation.</li> <li><code>multiple: bool</code> \u2013 If <code>True</code>, allow multiple files; otherwise require a single upload.</li> <li><code>timeout_s: int</code> \u2013 Max seconds to wait.</li> <li><code>channel: str | None</code> \u2013 Optional channel key/alias.</li> <li><code>resume</code>, <code>context</code> \u2013 Internal.</li> </ul> <p>Returns:</p> <ul> <li> <p><code>{\"text\": str, \"files\": list[FileRef]}</code></p> </li> <li> <p><code>text</code> \u2013 Optional message text the user sent along with the files.</p> </li> <li><code>files</code> \u2013 List of <code>FileRef</code> objects pointing at uploaded files.</li> </ul> <p>Notes:</p> <ul> <li>Files are typically stored via the artifact service behind the scenes; <code>FileRef</code> carries enough info to retrieve them.</li> <li>Use in graph-level code only.</li> </ul>"},{"location":"reference/tool-channel/#5-send_text-fire-and-forget-text-message","title":"5. <code>send_text</code> \u2013 fire-and-forget text message","text":"<pre><code>@tool(name=\"send_text\", outputs=[\"ok\"])\nasync def send_text(\n    *, text: str, meta: dict[str, Any] | None = None, channel: str | None = None, context=None\n):\n    ...\n</code></pre> send_text(*, text, meta=None, channel=None, context=None) -&gt; {\"ok\": bool} <p>Description:</p> <p>Send a text message to the selected channel and return immediately (no wait).</p> <p>Inputs:</p> <ul> <li><code>text: str</code> \u2013 Message body.</li> <li><code>meta: dict[str, Any] | None</code> \u2013 Optional metadata for the channel adapter (thread IDs, tags, etc.).</li> <li><code>channel: str | None</code> \u2013 Target channel key/alias; defaults to the current/default channel.</li> <li><code>context</code> \u2013 Injected <code>NodeContext</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>{\"ok\": True}</code> on success.</li> </ul> <p>Notes:</p> <ul> <li>Non-waiting tool: useful for notifications, logging, or streaming intermediate updates.</li> </ul>"},{"location":"reference/tool-channel/#6-send_image-post-an-image","title":"6. <code>send_image</code> \u2013 post an image","text":"<pre><code>@tool(name=\"send_image\", outputs=[\"ok\"])\nasync def send_image(\n    *,\n    url: str | None = None,\n    alt: str = \"image\",\n    title: str | None = None,\n    channel: str | None = None,\n    context=None,\n):\n    ...\n</code></pre> send_image(*, url=None, alt=\"image\", title=None, channel=None, context=None) -&gt; {\"ok\": bool} <p>Description:</p> <p>Send an image message to the channel, typically by URL.</p> <p>Inputs:</p> <ul> <li><code>url: str | None</code> \u2013 Public or internally resolvable image URL.</li> <li><code>alt: str</code> \u2013 Alt text.</li> <li><code>title: str | None</code> \u2013 Optional title/caption.</li> <li><code>channel: str | None</code> \u2013 Target channel key/alias.</li> <li><code>context</code> \u2013 Injected.</li> </ul> <p>Returns:</p> <ul> <li><code>{\"ok\": True}</code> on success.</li> </ul>"},{"location":"reference/tool-channel/#7-send_file-attach-a-file","title":"7. <code>send_file</code> \u2013 attach a file","text":"<pre><code>@tool(name=\"send_file\", outputs=[\"ok\"])\nasync def send_file(\n    *,\n    url: str | None = None,\n    file_bytes: bytes | None = None,\n    filename: str = \"file.bin\",\n    title: str | None = None,\n    channel: str | None = None,\n    context=None,\n):\n    ...\n</code></pre> send_file(*, url=None, file_bytes=None, filename=\"file.bin\", title=None, channel=None, context=None) -&gt; {\"ok\": bool} <p>Description:</p> <p>Send a file attachment to the channel, either by URL or raw bytes.</p> <p>Inputs:</p> <ul> <li><code>url: str | None</code> \u2013 If provided, the channel may fetch the file from this URL.</li> <li><code>file_bytes: bytes | None</code> \u2013 Raw file contents; used when you already have the bytes in memory.</li> <li><code>filename: str</code> \u2013 Name to show to the user.</li> <li><code>title: str | None</code> \u2013 Optional human-friendly label.</li> <li><code>channel: str | None</code> \u2013 Target channel key/alias.</li> <li><code>context</code> \u2013 Injected.</li> </ul> <p>Returns:</p> <ul> <li><code>{\"ok\": True}</code> on success.</li> </ul> <p>Notes:</p> <ul> <li>Channel adapters decide how to handle <code>url</code> vs <code>file_bytes</code>.</li> </ul>"},{"location":"reference/tool-channel/#8-send_buttons-message-with-interactive-buttons","title":"8. <code>send_buttons</code> \u2013 message with interactive buttons","text":"<pre><code>@tool(name=\"send_buttons\", outputs=[\"ok\"])\nasync def send_buttons(\n    *,\n    text: str,\n    buttons: list[Button],\n    meta: dict[str, Any] | None = None,\n    channel: str | None = None,\n    context=None,\n):\n    ...\n</code></pre> send_buttons(*, text, buttons, meta=None, channel=None, context=None) -&gt; {\"ok\": bool} <p>Description:</p> <p>Send a message with interactive buttons, without waiting for a response in this node. Useful for UI-only affordances when another node will handle the actual click.</p> <p>Inputs:</p> <ul> <li><code>text: str</code> \u2013 Message text.</li> <li><code>buttons: list[Button]</code> \u2013 Channel-specific button descriptors.</li> <li><code>meta: dict[str, Any] | None</code> \u2013 Optional metadata.</li> <li><code>channel: str | None</code> \u2013 Target channel key/alias.</li> <li><code>context</code> \u2013 Injected.</li> </ul> <p>Returns:</p> <ul> <li><code>{\"ok\": True}</code> on success.</li> </ul> <p>Notes:</p> <ul> <li>To wait on a button click, use <code>ask_approval</code> instead.</li> </ul>"},{"location":"reference/tool-channel/#9-get_latest_uploads-retrieve-recent-file-uploads","title":"9. <code>get_latest_uploads</code> \u2013 retrieve recent file uploads","text":"<pre><code>@tool(name=\"get_lastest_uploads\", outputs=[\"files\"])\nasync def get_latest_uploads(*, clear: bool = True, context) -&gt; list[FileRef]:\n    ...\n</code></pre> get_latest_uploads(*, clear=True, context) -&gt; {\"files\": list[FileRef]} <p>Description:</p> <p>Fetch the most recent file uploads associated with the current channel session. This is a convenience around <code>channel.get_latest_uploads</code>.</p> <p>Inputs:</p> <ul> <li><code>clear: bool = True</code> \u2013 If <code>True</code>, clear the internal buffer after reading so subsequent calls only see newer uploads.</li> <li><code>context</code> \u2013 Injected <code>NodeContext</code>; used to access <code>context.channel()</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>{\"files\": list[FileRef]}</code> \u2013 List of file references.</li> </ul> <p>Notes:</p> <ul> <li>Tool is registered under the name <code>\"get_lastest_uploads\"</code> (note the spelling) but the Python symbol is <code>get_latest_uploads</code>.</li> <li>Any channel session that supports uploads will expose the same upload buffer.</li> </ul>"},{"location":"reference/tool-channel/#usage-resumption","title":"Usage &amp; Resumption","text":"<ul> <li>All of these are tools, so in <code>@graph_fn</code> and <code>@graphify</code> they appear as nodes in the <code>TaskGraph</code>.</li> <li><code>ask_text</code>, <code>wait_text</code>, <code>ask_approval</code>, and <code>ask_files</code> are waitable tool nodes \u2014 they can pause a run and be resumed via continuations (Slack/web UI/etc.).</li> <li>Do not call these from inside another <code>@tool</code> implementation; they depend on graph-level scheduling and <code>NodeContext</code>.</li> <li>For simple, non-graph scripts, you can still <code>await</code> them directly as async functions, but you will not get resumability or persisted state unless running under the sidecar / scheduler.</li> </ul>"},{"location":"reference/tools-facade/","title":"Tools Facade","text":""},{"location":"reference/tools-facade/#registerfunc-namenone-inputsnone-outputsnone-str","title":"register(func, *, name=None, inputs=None, outputs=None) \u2192 str","text":"<p>Registers a tool and returns its name/id.</p>"},{"location":"reference/tools-facade/#callname-args-dict-dict","title":"call(name, args: dict) \u2192 dict","text":"<p>Invokes a tool by name with validated args.</p>"},{"location":"tutorials/t0-plan/","title":"Agentic R&amp;D in 60 Seconds \u2013 with AetherGraph","text":"<p>Short, practical demos of agentic R&amp;D workflows in plain Python, using AetherGraph.</p> <ul> <li>Format: ~60s video + LinkedIn post + example link</li> <li>Focus: \u201cWhat can I do THIS WEEK that makes R&amp;D less painful?\u201d</li> <li> <p>Core building blocks:</p> </li> <li> <p>Channels \u2013 Python that talks back</p> </li> <li>Artifacts \u2013 stop losing results</li> <li>Memory \u2013 pipeline as a lab notebook</li> <li>External services \u2013 glue for your stack</li> <li>Bonus: LLMs \u2013 reasoning inside the flow</li> </ul>"},{"location":"tutorials/t0-plan/#series-overview-for-intro-guides-page","title":"Series Overview (for intro / guides page)","text":"<p>Most \u201cagentic frameworks\u201d promise the world. I just want my R&amp;D scripts to talk back, remember things, and organize results \u2014 without leaving Python.</p> <p>\u201cAgentic R&amp;D in 60 Seconds \u2013 with AetherGraph\u201d is a short series of ~1-minute demos where I slowly build up practical \u201cagentic\u201d behaviors on top of normal Python code:</p> <ul> <li>Ep.1 \u2013 Channels: Python that talks back.</li> <li>Ep.2 \u2013 Artifacts: stop losing results in random folders.</li> <li>Ep.3 \u2013 Memory: your pipeline as a lightweight lab notebook.</li> <li>Ep.4 \u2013 External services: glue for your existing stack.</li> <li>Ep.5 (Bonus) \u2013 LLMs: let the model look at your data and suggest next steps.</li> </ul> <p>AetherGraph can be used to build much more complex agentic systems, but this series stays deliberately practical:</p> <p>\u2192 What can I do this week that makes my R&amp;D life less painful?</p>"},{"location":"tutorials/t0-plan/#episode-1-python-that-talks-back-channels","title":"Episode 1 \u2013 Python That Talks Back (Channels)","text":""},{"location":"tutorials/t0-plan/#linkedin-post-draft","title":"LinkedIn Post Draft","text":"<p>Most \u201cagentic frameworks\u201d promise the world. I just want my R&amp;D scripts to talk back, remember things, and organize results \u2014 without leaving Python.</p> <p>Over the next few weeks I\u2019m sharing a short mini-series:</p> <p>\u201cAgentic R&amp;D in 60 Seconds \u2013 with AetherGraph\u201d</p> <p>Each post = a ~1 minute demo + a small code snippet, focused on one capability in AetherGraph:</p> <ul> <li>Channels \u2013 Python that talks back</li> <li>Artifacts \u2013 stop losing results in random folders</li> <li>Memory \u2013 your pipeline as a lightweight lab notebook</li> <li>External services \u2013 glue for your existing stack</li> </ul> <p>AetherGraph can be used to build much fancier agentic systems, but in this series I want to stay very practical:</p> <p>\u2192 \u201cWhat can I do this week that makes my R&amp;D life less painful?\u201d</p> <p>Ep.1 \u2013 Python that talks back (channels)</p> <p>In this first demo, I turn a plain Python script into a tiny \u201cexperiment setup wizard\u201d.</p> <p>The script:</p> <ul> <li>Asks for a project name</li> <li>Asks how many steps to run</li> <li>Lets you enable or skip an \u201cadvanced mode\u201d</li> <li>Then prints the final config and starts the run</li> </ul> <p>All of this is powered by AetherGraph\u2019s channel API, but it still looks and feels like normal async Python.</p> <p>Code sketch:</p> <p>from aethergraph import graph_fn from aethergraph.core.runtime import ExecutionContext</p> <p>@graph_fn(name=\"channel_wizard\") async def channel_wizard(context: ExecutionContext): chan = context.channel()</p> <pre><code>project = await chan.ask_text(\"Project name?\")\nsteps = int(\n    await chan.ask_text(\"Number of steps (e.g. 10)?\")\n)\n\nadvanced_choice = await chan.ask_approval(\n    \"Enable advanced mode?\",\n    options=[\"Yes\", \"No\"],\n)\n\nconfig = {\n    \"project\": project,\n    \"steps\": steps,\n    \"advanced\": (advanced_choice == \"Yes\"),\n}\n\nawait chan.send_text(f\"Running experiment with config: {config}\")\n\n# Replace this with your real workload\n# e.g. run_training(config), submit_job(config), etc.\nawait chan.send_text(\"Experiment started \u2705\")\n</code></pre> <p>Beyond this demo</p> <p>Once you\u2019re comfortable with context.channel(), you can:</p> <ul> <li>Turn any CLI script into a chat-like wizard (console today, Slack / others later).</li> <li>Ask for approvals before expensive steps (e.g. \u201cSubmit this job to the cluster?\u201d).</li> <li>Build interactive workflows where the system asks you for missing pieces instead of failing.</li> </ul> <p>Links you can add:</p> <ul> <li>Examples repo</li> <li>Main repo</li> <li>Intro page</li> </ul>"},{"location":"tutorials/t0-plan/#episode-2-stop-losing-your-results-artifacts","title":"Episode 2 \u2013 Stop Losing Your Results (Artifacts)","text":"<p>Most R&amp;D projects eventually become a graveyard of:</p> <ul> <li>final_results_v7_really_final.json</li> <li>new_final_results_latest.csv</li> </ul> <p>In Ep.2 of \u201cAgentic R&amp;D in 60 Seconds \u2013 with AetherGraph\u201d, I show how to use artifacts so your workflow always knows where its outputs live.</p> <p>Demo idea:</p> <p>A tiny analysis script:</p> <ul> <li>Loads a CSV</li> <li>Computes a small summary (e.g. min / max / mean)</li> <li>Saves it as a versioned artifact tied to an experiment scope</li> <li>Prints the artifact URI so you can retrieve it later</li> </ul> <p>Code sketch:</p> <p>from aethergraph import graph_fn from aethergraph.core.runtime import ExecutionContext</p> <p>import pandas as pd</p> <p>@graph_fn(name=\"summarize_csv\") async def summarize_csv(context: ExecutionContext, path: str, scope: str = \"demo_exp\"): df = pd.read_csv(path)</p> <pre><code>summary = {\n    \"rows\": int(len(df)),\n    \"cols\": int(len(df.columns)),\n    \"col_names\": list(df.columns),\n    \"mean_of_first_col\": float(df[df.columns[0]].mean()),\n}\n\nref = await context.artifacts.save_json(\n    data=summary,\n    scope=scope,\n    tag=\"summary\",\n)\n\nawait context.channel().send_text(\n    f\"Saved summary artifact:\\n{ref.uri}\"\n)\n\nreturn {\"artifact_uri\": ref.uri}\n</code></pre> <p>A second graph to load the last summary:</p> <p>@graph_fn(name=\"load_last_summary\") async def load_last_summary(context: ExecutionContext, scope: str = \"demo_exp\"): last = await context.artifacts.load_last( scope=scope, tag=\"summary\", ) await context.channel().send_text(f\"Last summary:\\n{last}\")</p> <p>Beyond this demo</p> <p>With artifacts you can:</p> <ul> <li>Version reports, plots, and models per experiment or per run.</li> <li>Build \u201clatest report\u201d and \u201clatest metrics\u201d helpers instead of guessing file names.</li> <li>Connect artifacts with memory (e.g. log events that reference artifact URIs).</li> </ul>"},{"location":"tutorials/t0-plan/#episode-3-pipelines-with-a-memory-memory","title":"Episode 3 \u2013 Pipelines With a Memory (Memory)","text":"<p>Most R&amp;D work gets lost in log files and screenshots. The process \u2014 which configs you tried, what worked, what failed \u2014 is rarely captured in a way you can query.</p> <p>In Ep.3 of \u201cAgentic R&amp;D in 60 Seconds \u2013 with AetherGraph\u201d, I treat the pipeline itself as a tiny lab notebook using the memory layer.</p> <p>Demo idea:</p> <p>Each run:</p> <ul> <li>Logs its config and metric as a memory event</li> <li>Tags events by experiment scope</li> <li>Then we ask memory to summarize the last few runs</li> </ul> <p>Code sketch for logging a run:</p> <p>from aethergraph import graph_fn from aethergraph.core.runtime import ExecutionContext</p> <p>@graph_fn(name=\"train_once\") async def train_once( context: ExecutionContext, lr: float, steps: int, scope: str = \"exp1\", ): chan = context.channel() mem = context.memory()</p> <pre><code>metric = 0.75 + (lr * 0.05)  # fake metric\n\nawait chan.send_text(f\"Run finished: lr={lr}, steps={steps}, metric={metric:.3f}\")\n\nawait mem.log_event(\n    scope_id=scope,\n    kind=\"run\",\n    payload={\n        \"lr\": lr,\n        \"steps\": steps,\n        \"metric\": metric,\n    },\n    tags=[\"demo\", \"train_run\"],\n)\n\nreturn {\"metric\": metric}\n</code></pre> <p>Code sketch for summarizing recent runs:</p> <p>@graph_fn(name=\"summarize_recent_runs\") async def summarize_recent_runs(context: ExecutionContext, scope: str = \"exp1\"): mem = context.memory() chan = context.channel()</p> <pre><code>summary = await mem.distill_long_term(\n    scope_id=scope,\n    question=\"Summarize the last 3 runs and what changed between them.\",\n    include_kinds=[\"run\"],\n    max_events=3,\n)\n\nawait chan.send_text(\"Summary of recent runs:\")\nawait chan.send_text(summary[\"text\"])\n</code></pre> <p>Beyond this demo</p> <p>With memory you can:</p> <ul> <li>Keep a queryable history of runs (config + results + comments).</li> <li>Ask the system to highlight trends.</li> <li>Combine with channels: ask for human notes mid-run and store them.</li> </ul>"},{"location":"tutorials/t0-plan/#episode-4-glue-for-your-stack-external-services","title":"Episode 4 \u2013 Glue for Your Stack (External Services)","text":"<p>Real R&amp;D rarely lives in one library. You probably have:</p> <ul> <li>a simulation codebase over here</li> <li>a queue / job runner over there</li> <li>some custom Python scripts everywhere</li> </ul> <p>In Ep.4 of \u201cAgentic R&amp;D in 60 Seconds \u2013 with AetherGraph\u201d, I treat one of those tools as a first-class service inside the execution context.</p> <p>Demo idea:</p> <ul> <li>Define a simple SimService that mocks running a simulation.</li> <li>Register it with the AetherGraph runtime as a context service.</li> <li>Call it from a graph function, then save results as artifacts.</li> </ul> <p>Service sketch:</p> <p>class SimService: async def run(self, config: dict) -&gt; dict: return { \"status\": \"ok\", \"config\": config, \"result\": 42.0, }</p>"},{"location":"tutorials/t0-plan/#register-under-the-name-sim-in-your-container-setup","title":"Register under the name \"sim\" in your container / setup","text":"<p>Graph sketch:</p> <p>from aethergraph import graph_fn from aethergraph.core.runtime import ExecutionContext</p> <p>@graph_fn(name=\"run_sim_experiment\") async def run_sim_experiment( context: ExecutionContext, steps: int = 10, param: float = 0.5, scope: str = \"sim_demo\", ): chan = context.channel() arts = context.artifacts() sim = context.ext_service(\"sim\")  # or context.sim</p> <pre><code>config = {\"steps\": steps, \"param\": param}\nawait chan.send_text(f\"Submitting simulation with config: {config}\")\n\nresult = await sim.run(config)\nawait chan.send_text(f\"Simulation finished with result={result['result']}\")\n\nref = await arts.save_json(\n    data=result,\n    scope=scope,\n    tag=\"sim_result\",\n)\n\nawait chan.send_text(f\"Saved result as artifact: {ref.uri}\")\nreturn {\"artifact_uri\": ref.uri}\n</code></pre> <p>Beyond this demo</p> <p>With external services you can:</p> <ul> <li>Wrap existing simulators, API clients, or job runners as context services.</li> <li>Keep your graph logic clean while delegating heavy lifting to those services.</li> <li>Combine with channels, memory, and artifacts to build a coherent workflow.</li> </ul>"},{"location":"tutorials/t0-plan/#episode-5-bonus-let-the-llm-look-at-your-data-contextllm","title":"Episode 5 (Bonus) \u2013 Let the LLM Look at Your Data (context.llm())","text":"<p>We now have:</p> <ul> <li>Channels \u2013 Python that talks back</li> <li>Artifacts \u2013 structured results you can find again</li> <li>Memory \u2013 a lightweight lab notebook</li> <li>External services \u2013 glue to call your existing stack</li> </ul> <p>In this bonus episode of \u201cAgentic R&amp;D in 60 Seconds \u2013 with AetherGraph\u201d, I plug in the last piece:</p> <p>Let an LLM look at your data inside the flow, instead of manually copying logs into a chat box.</p> <p>Demo idea:</p> <ul> <li>Load the last summary artifact for an experiment.</li> <li>Call context.llm(\"default\").chat(...) to ask for an explanation.</li> <li>Have the pipeline print / send the explanation back via the channel.</li> </ul> <p>Code sketch:</p> <p>from aethergraph import graph_fn from aethergraph.core.runtime import ExecutionContext</p> <p>@graph_fn(name=\"explain_last_summary\") async def explain_last_summary( context: ExecutionContext, scope: str = \"demo_exp\", profile: str = \"default\", ): chan = context.channel() arts = context.artifacts() llm = context.llm(profile)</p> <pre><code>last = await arts.load_last(scope=scope, tag=\"summary\")\nif last is None:\n    await chan.send_text(\"No summary artifact found yet.\")\n    return\n\nuser_content = (\n    \"Here is a JSON summary of a recent experiment:\\n\\n\"\n    f\"{last}\\n\\n\"\n    \"Explain what this tells us, and suggest one reasonable next experiment to run.\"\n)\n\ntext, usage = await llm.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are an R&amp;D lab assistant.\"},\n        {\"role\": \"user\", \"content\": user_content},\n    ],\n)\n\nawait chan.send_text(\"LLM analysis of last summary:\")\nawait chan.send_text(text)\n\nreturn {\"analysis\": text, \"usage\": usage}\n</code></pre> <p>Beyond this demo</p> <p>With context.llm() you can:</p> <ul> <li>Explain artifact contents.</li> <li>Summarize memory events.</li> <li>Propose next steps based on metrics and configs.</li> <li>Implement small policy loops where the LLM suggests new parameters and the graph runs them (with human approvals in the loop).</li> </ul> <p>This is where the earlier pieces come together:</p> <ul> <li>Channels \u2192 interactive conversation.</li> <li>Artifacts \u2192 structured results to feed the model.</li> <li>Memory \u2192 history and context.</li> <li>External services \u2192 real simulators and tools.</li> </ul>"},{"location":"tutorials/t1-build-your-first-graph-fn/","title":"Tutorial 1: Build Your First <code>graph_fn</code>","text":"<p>This tutorial walks you through the core API of AetherGraph and helps you build your first reactive agent using the <code>@graph_fn</code> decorator.</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#overview","title":"\ud83d\ude80 Overview","text":"<p>AetherGraph introduces a Python\u2011native way to create agents that can think, wait, and talk \u2014 all inside a normal Python function. With the <code>@graph_fn</code> decorator, you can:</p> <ul> <li>Add context\u2011aware I/O (<code>context.channel()</code>, <code>context.llm()</code>, <code>context.memory()</code>)</li> <li>Run interactively or headlessly</li> <li>Chain, nest, and resume computations without defining a custom graph DSL</li> </ul> <p>In this tutorial, you will:</p> <ol> <li>Start the AetherGraph server (the sidecar)</li> <li>Define your first <code>graph_fn</code></li> <li>Call an LLM and send messages through the channel</li> <li>Run it synchronously and see the result</li> </ol>"},{"location":"tutorials/t1-build-your-first-graph-fn/#1-boot-the-sidecar","title":"1. Boot the Sidecar","text":"<p>Before you run any agent, you must start the sidecar server, which wires up the runtime services such as channel communication, artifact storage, memory, and resumptions.</p> <pre><code>from aethergraph import start_server\n\nurl = start_server()  # launches a lightweight FastAPI server in the background\nprint(\"AetherGraph sidecar server started at:\", url)\n</code></pre> <p>The sidecar is safe to start anywhere \u2014 even in Jupyter or interactive shells. It sets up a workspace under <code>./aethergraph_data</code> by default. Your data, including artifacts, memory, resumption files, will all be exported to workspace for persist access.</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#2-define-a-minimal-agent","title":"2. Define a Minimal Agent","text":"<p>A <code>graph_fn</code> is a context\u2011injected async function that represents a reactive node or agent.</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"hello_world\")\nasync def hello_world(input_text: str, *, context: NodeContext):\n    context.logger().info(\"hello_world started\")\n\n    # Send a message via the default channel (console)\n    await context.channel().send_text(f\"\ud83d\udc4b Hello! You sent: {input_text}\")\n\n    # Optional: Call an LLM directly from the context\n    llm_text, _usage = await context.llm().chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"Be brief.\"},\n            {\"role\": \"user\", \"content\": f\"Say hi back to: {input_text}\"},\n        ]\n    )\n    await context.channel().send_text(f\"LLM replied: {llm_text}\")\n\n    output = input_text.upper()\n    context.logger().info(\"hello_world finished\")\n    return {\"final_output\": output}\n</code></pre> <p>Return value: Although you can return any data type in a dictionary, it is suggested to return a dictionary of JSON-serializable results (e.g. <code>{\"result\": value}</code>). For large data or binary files, save them via <code>context.artifacts().write(...)</code> and return the artifact path/uri instead for later reuse. </p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#key-concepts","title":"Key Concepts","text":"Concept Description <code>@graph_fn</code> Turns a plain async Python function into a context\u2011aware agent. <code>NodeContext</code> Injected automatically. Gives access to channels, memory, LLMs, and logging. <code>context.channel()</code> Sends and receives messages (console, Slack, web UI, etc.). <code>context.llm()</code> Unified interface to language models via environment configuration. <code>context.logger()</code> Node\u2011aware structured logging. <p>See the full API of <code>graph_fn</code> at Graph Function API</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#3-run-the-agent","title":"3. Run the Agent","text":"<p>You can run your <code>graph_fn</code> directly or use helper runners.</p> <pre><code>from aethergraph import run\n\nif __name__ == \"__main__\":\n    result = run(hello_world, inputs={\"input_text\": \"hello world\"})\n    print(\"Result:\", result)\n</code></pre> <p>Output example:</p> <pre><code>[AetherGraph] \ud83d\udc4b Hello! You sent: hello world\n[AetherGraph] LLM replied: Hi there!\nResult: {'final_output': 'HELLO WORLD'}\n</code></pre> <p>You can also <code>await hello_world(...)</code> in any async context \u2014 all <code>graph_fn</code>s are awaitable by design.</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#4-why-this-is-different","title":"4. Why This is Different","text":"<p>AetherGraph\u2019s <code>@graph_fn</code> model is Pythonic yet agentic. Unlike traditional workflow frameworks that require static DAG definitions, AetherGraph lets you:</p> <ul> <li>Run without pre\u2011declaring a graph \u2013 it dynamically builds one as you go.</li> <li>Access unified runtime services \u2013 channels, memory, artifacts, LLMs, and schedulers are all injected via context.</li> <li>Compose natively \u2013 you can <code>await</code> another <code>graph_fn</code>, mix <code>@tool</code>s, or parallelize with <code>asyncio.gather</code>.</li> <li>Stay resumable \u2013 everything you run is automatically backed by a persistent runtime; you can resume mid\u2011flow later.</li> </ul> <p>These traits make AetherGraph unique among Python agent frameworks \u2014 designed not only for chatbots, but also for scientific, engineering, and simulation workflows.</p>"},{"location":"tutorials/t1-build-your-first-graph-fn/#5-next-steps","title":"5. Next Steps","text":"<p>In the next tutorial, you\u2019ll learn how to turn these reactive functions into static DAGs using <code>graphify()</code>, enabling resumable and inspectable computation graphs.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/","title":"Tutorial 2: Static Graphs with <code>@graphify</code>","text":"<p><code>@graphify</code> turns a plain Python function into a graph builder. Instead of executing immediately (like <code>@graph_fn</code>), it builds a deterministic TaskGraph from <code>@tool</code> calls \u2014 a DAG you can inspect, persist, and run later.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#mental-model","title":"\ud83e\udded Mental Model","text":"<p><code>@graph_fn</code> \u2192 executes now (reactive, context\u2011rich)</p> <p><code>@graphify</code> \u2192 builds first, runs later (deterministic DAG)</p> <ul> <li>Each <code>@tool</code> call becomes a node in the DAG.</li> <li>Edges are formed by data flow and optional ordering via <code>_after=[\u2026]</code>.</li> <li>You get reproducibility, inspectability, and clean fan\u2011in/fan\u2011out.</li> </ul> <p>Note: Access runtime services (<code>channel</code>, <code>llm</code>, <code>memory</code>) through tools in static graphs. If you need direct <code>context.*</code> calls inline, use <code>@graph_fn</code>.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#1-key-rules-short","title":"1. Key Rules (short)","text":"<ul> <li>Only <code>@tool</code> calls are allowed as steps in a <code>@graphify</code> builder. Use plain Python only to wire values or format the graph (no side\u2011effects); such code will not appear as nodes.</li> <li>Build \u2260 Run. Calling a <code>@graphify</code> function returns a TaskGraph. Use a runner to execute it.</li> <li>Async supported. Tools can be sync or async; the runner provides both sync and async entry points.</li> <li>Resumption requires stable IDs. Give important nodes a fixed <code>*_id</code> and reuse the same <code>run_id</code> when resuming.</li> <li>Outputs: Return a dict of JSON\u2011serializable values for resumption. Large/binary data \u2192 save via <code>artifacts()</code> and return a reference. (Full rules live in the API page.)</li> </ul> <p>Related: <code>@graph_fn</code> can also emit an implicit graph when you call <code>@tool</code>s inside it. Use <code>_after</code> to enforce ordering there too, and inspect the last run\u2019s captured graph with <code>graph_fn.last_graph</code>.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#2-shapes-tools-graphify","title":"2. Shapes (tools &amp; graphify)","text":""},{"location":"tutorials/t2-static-graph-with-graphify/#tool-shape-suggested","title":"<code>@tool</code> shape (suggested)","text":"<pre><code>from aethergraph import tool\n\n@tool(name=\"load_csv\", outputs=[\"rows\"])            # names become handle fields. Name is optional running locally\ndef load_csv(path: str) -&gt; dict:                    # return dict matching outputs\n    # ... load and parse ...\n    return {\"rows\": rows}\n</code></pre> <ul> <li><code>@tool</code> can be sync or async, they all run as async internally. </li> <li>Declare <code>outputs=[...]</code>. Returned dict must contain those keys.</li> </ul>"},{"location":"tutorials/t2-static-graph-with-graphify/#graphify-shape-suggested","title":"<code>@graphify</code> shape (suggested)","text":"<pre><code>from aethergraph import graphify\n\n@graphify(name=\"etl\", inputs=[\"csv_path\"], outputs=[\"nrows\"])  # declarative I/O\ndef etl(csv_path: str):\n    raw = load_csv(path=csv_path)         # node\n    # ... add more tool calls ...\n    return {\"nrows\": len(raw.rows)}      # JSON-serializable outputs\n</code></pre> <ul> <li><code>graphify</code> is always a sync function. No <code>await</code> allowed inside the builder.</li> <li>Use <code>_after=...</code> to force ordering when no data edge exists.</li> <li>Calling <code>etl()</code> builds a <code>TaskGraph</code>; it does not run.</li> <li>Run using <code>run(...)</code> / <code>run_async(...)</code> with <code>inputs={...}</code>.</li> </ul>"},{"location":"tutorials/t2-static-graph-with-graphify/#3-minimal-example-build-run","title":"3. Minimal Example \u2014 Build \u2192 Run","text":"<pre><code>from aethergraph import graphify, tool\nfrom aethergraph.runner import run  \n\n@tool(outputs=[\"doubled\"])  \ndef double(x: int) -&gt; dict:\n    return {\"doubled\": x * 2}\n\n@tool(outputs=[\"shifted\"])  \ndef add_ten(x: int) -&gt; dict:\n    return {\"shifted\": x + 10}\n\n@graphify(name=\"tiny_pipeline\", inputs=[\"x\"], outputs=[\"y\"])\ndef tiny_pipeline(x: int):\n    a = double(x=x)                   # node A\n    b = add_ten(x=a.doubled)         # node B depends on A via data edge\n    return {\"y\": b.shifted}\n\n# Build (no execution yet)\nG = tiny_pipeline()                   # \u2192 TaskGraph\n\n# Run (sync helper, useful in Jupyter notebook)\nresult = run(G, inputs={\"x\": 7})\nprint(result)  # {'y': 24}\n</code></pre> <p>Try <code>max_concurrency=1</code> vs <code>&gt;1</code> in the runner if your tools are async and parallelizable.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#4-ordering-without-data-edges-_after","title":"4. Ordering Without Data Edges \u2014 <code>_after</code>","text":"<pre><code>@tool(outputs=[\"ok\"])  \ndef fetch() -&gt; dict: ...\n\n@tool(outputs=[\"done\"])\ndef train() -&gt; dict: ...\n\n@graphify(name=\"seq\", inputs=[], outputs=[\"done\"])\ndef seq():\n    a = fetch()\n    b = train(_after=a)               # force run-after without wiring data\n    return {\"done\": b.done}\n</code></pre> <ul> <li>Use a single node or a list <code>_after=[a, b]</code>.</li> </ul>"},{"location":"tutorials/t2-static-graph-with-graphify/#5-resume-a-run-stable-_id-run_id","title":"5. Resume a Run \u2014 Stable <code>_id</code> + <code>run_id</code>","text":"<p>Resumption lets you continue a partially-completed graph without redoing finished nodes. This is useful for flaky I/O or long pipelines.</p> <ul> <li>Assign deterministic IDs to nodes with <code>_id=\"...\"</code> in your tools.</li> <li>Reuse the same <code>run_id</code> when invoking the runner again.</li> <li>Indefinite waits (e.g., human input) are supported via dedicated wait tools and are covered in the Channels &amp; Wait Models tutorial\u2014this section uses a non\u2011channel example.</li> </ul> <pre><code>from aethergraph import graphify, tool\nfrom aethergraph.core.runtime.graph_runner import run_async\nimport random\n\n@tool(outputs=[\"ok\"])  \ndef prepare() -&gt; dict:\n    # Pretend to set up workspace/artifacts\n    return {\"ok\": True}\n\n@tool(outputs=[\"value\"])  \ndef flaky_compute(x: int) -&gt; dict:\n    # Simulate a transient failure half the time\n    if random.random() &lt; 0.5:\n        raise RuntimeError(\"transient error \u2014 try resuming\")\n    return {\"value\": x * 2}\n\n@tool(outputs=[\"ok\"])  \ndef finalize(v: int) -&gt; dict:\n    # Commit final result (e.g., write an artifact)\n    return {\"ok\": True}\n\n@graphify(name=\"resumable_pipeline\", inputs=[\"x\"], outputs=[\"y\"]) \ndef resumable_pipeline(x: int):\n    s1 = prepare(_id=\"prepare_1\")\n    s2 = flaky_compute(x=x, _after=s1, _id=\"flaky_2\")  # may fail on first run\n    s3 = finalize(v=s2.value, _after=s2, _id=\"finalize_3\")\n    return {\"y\": s2.value}\n\n# First run may fail while computing 'flaky_2'...\n# await run_async(resumable_pipeline(), inputs={\"x\": 21}, run_id=\"run-abc\")\n\n# Re-run with the SAME run_id to resume from the failed node (prepare_1 is skipped):\n# await run_async(resumable_pipeline(), inputs={\"x\": 21}, run_id=\"run-abc\")\n</code></pre> <p>Keep <code>_id</code>s stable to allow the engine to match nodes. If a node fails or is interrupted, resuming with the same <code>run_id</code> will continue from the last successful checkpoint.</p> <p>Use json-serializable output in <code>@tool</code> so that Aethergraph can reload previous outputs; otherwise resumption may fail.</p>"},{"location":"tutorials/t2-static-graph-with-graphify/#6-inspect-beforeafter-running","title":"6. Inspect Before/After Running","text":"<p>Once you have a <code>TaskGraph</code> (e.g., <code>G = tiny_pipeline()</code>), you can:</p> <pre><code>print(G.pretty())           # readable node table\nprint(G.ascii_overview())   # compact topology\nprint(G.topological_order())\n\n# Graph metadata\nsig  = tiny_pipeline.io()   # declared inputs/outputs\nspec = tiny_pipeline.spec() # full GraphSpec (nodes, edges, metadata)\n\n# Export (if enabled)\ndot = G.to_dot()            # Graphviz DOT text\n# G.visualize()             # render to image if your env supports it\n</code></pre>"},{"location":"tutorials/t2-static-graph-with-graphify/#7-practical-tips","title":"7. Practical Tips","text":"<ul> <li>Keep nodes small and typed: expose clear outputs (e.g., <code>outputs=[\"clean\"]</code>).</li> <li>Use JSON\u2011serializable returns; store big/binary as artifacts.</li> <li>Prefer <code>_after</code> for control edges instead of fake data plumb\u2011through.</li> <li>No nested static graphs (don\u2019t call one <code>@graphify</code> from another). Use tools or run graphs separately.</li> <li>Async tools + <code>max_concurrency</code> unlock parallel speedups.</li> </ul>"},{"location":"tutorials/t2-static-graph-with-graphify/#8-summary","title":"8. Summary","text":"<ul> <li><code>@graphify</code> materializes a static DAG from <code>@tool</code> calls.</li> <li>Build with the function call; run with the runner (sync or async).</li> <li>For resumption, use stable <code>_id</code> per node and replay with the same <code>run_id</code>.</li> <li>Inspect graphs via <code>pretty()</code>, <code>ascii_overview()</code>, <code>.io()</code>, <code>.spec()</code>, and <code>to_dot()</code>.</li> </ul> <p>Use <code>@graphify</code> for pipelines and reproducible experiments; stick with <code>@graph_fn</code> for interactive, context\u2011heavy agents.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/","title":"Tutorial 3: Talk to Your Graph \u2014 Channels and Waits","text":"<p>This tutorial explains how your graph talks back \u2014 how agents communicate with the outside world and how different kinds of waits work under the hood. You\u2019ll learn the difference between cooperative waits (for live, stateless agents) and dual-stage waits (for resumable workflows), and how to use each effectively.</p> <p>Goal: Understand how channels unify I/O, and why only <code>@graphify</code> with dual-stage waits can resume safely after a crash.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#1-what-is-a-channel","title":"1. What Is a Channel?","text":"<p>A channel is your agent\u2019s communication route \u2014 Slack, Telegram, Web UI, or Console. It lets your code send messages, request input, and stream updates through a consistent API.</p> <pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"greet\")\nasync def greet(*, context):\n    ch = context.channel(\"slack:#research\")   # pick a target channel\n    await ch.send_text(\"Starting demo\u2026\")\n    name = await ch.ask_text(\"Your name?\")    # cooperative wait\n    await ch.send_text(f\"Nice to meet you, {name}!\")\n    return {\"user\": name}\n</code></pre> <ul> <li>The <code>context.channel()</code> method returns a <code>ChannelSession</code> helper with async methods like <code>send_text</code>, <code>ask_text</code>, <code>ask_approval</code>, <code>ask_files</code>, <code>stream</code>, and <code>progress</code>.</li> <li>If no channel is configured, it falls back to the console (<code>console:stdin</code>).</li> </ul> <p>\ud83d\udca1 Channel setup and adapter configuration (Slack, Telegram, Web) are covered in Channel Setup.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#2-the-two-wait-models","title":"2. The Two Wait Models","text":"<p>AetherGraph supports two wait mechanisms \u2014 cooperative and dual-stage \u2014 both built on the continuation system but with very different lifecycles.</p> <p></p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#cooperative-waits-via-contextchannel-methods","title":"Cooperative waits \u2014 via <code>context.channel()</code> methods","text":"<ul> <li>Implemented by <code>ChannelSession</code> (<code>ask_text</code>, <code>ask_approval</code>, <code>ask_files</code>, etc.).</li> <li>Work inside a running process \u2014 the node suspends, then resumes when the reply arrives.</li> <li>These waits are stateful for inspection, but not resumable; if the process dies, the session is lost.</li> <li>Used mainly in <code>@graph_fn</code> agents, which execute immediately and stay alive.</li> </ul>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#dual-stage-waits-via-built-in-channel-tools","title":"Dual-stage waits \u2014 via built-in channel tools","text":"<ul> <li>Implemented as @tool nodes in <code>aethergraph.tools</code> (<code>ask_text</code>, <code>send_text</code>, etc.).</li> <li>Each wait becomes a graph node stored in the runtime snapshot.</li> <li>Can pause indefinitely and resume after restarts using <code>run_id</code> in <code>@graphify</code>.</li> <li>Used in <code>@graphify</code> graphs, which are strictly persisted and versioned.</li> </ul> <p>\u26a0\ufe0f All built-in dual-stage methods are <code>@tool</code>s \u2014 do not call them inside another tool. They are meant for use in graphify or top-level graph_fn logic, not nested nodes.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#3-lifecycle-and-persistence","title":"3. Lifecycle and Persistence","text":"Concept <code>@graph_fn</code> + Cooperative Waits <code>@graphify</code> + Dual-Stage Waits Execution Runs immediately (reactive) Builds DAG, runs with scheduler State Stateful for in-process waits Snapshot persisted to disk or DB Wait behavior Cooperative (in-process) Dual-stage (resumable) Resume after crash \u274c Lost, consider saving progress in memory and sementic recovery \u2705 Recoverable with <code>run_id</code>  and stable <code>node_id</code>; set up <code>_id</code> when building the graph <p>You can also use the <code>context.channel()</code> method in <code>@graphify</code> for convenience within a <code>@tool</code>, or use dual-stage wait tools in <code>graph_fn</code>. However, these approaches cannot guarantee resumption due to the stateful nature of the method or graph. Caveat for console dual-stage tools: Console input is handled differently, and dual-stage waits do not support resumption for console channels. However, it is rare for a local process using the console to terminate unexpectedly.</p>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#4-cooperative-wait-example","title":"4. Cooperative Wait Example","text":"<pre><code>from aethergraph import graph_fn\n\n@graph_fn(name=\"cooperative_wait\")\nasync def cooperative_wait(*, context):\n    ch = context.channel()\n    await ch.send_text(\"Processing...\")\n    ans = await ch.ask_approval(\"Continue?\", options=[\"Yes\", \"No\"])\n    if ans[\"approved\"]:\n        await ch.send_text(\"\u2705 Proceeding.\")\n    else:\n        await ch.send_text(\"\u274c Stopped.\")\n    return {\"ok\": ans[\"approved\"]}\n</code></pre> <ul> <li>Perfect for short-lived interactive runs.</li> <li>Not resumable if interrupted; all state is lost when the process exits.</li> <li>Consider saving states to memory for sementic recovery for non-critical tasks.</li> </ul>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#5-dual-stage-wait-example-resumable","title":"5. Dual-Stage Wait Example (Resumable)","text":"<pre><code>from aethergraph import graphify\nfrom aethergraph.tools import send_text, ask_text # built-in `@tool`. Do not use them in anohter `@tool`\n\n@graphify(name=\"dual_stage_greet\", inputs=[\"channel\"], outputs=[\"greeting\"])\ndef dual_stage_greet(channel: str):\n    a = send_text(text=\"Hello!\", channel=channel, _id=\"start\")\n    b = ask_text(prompt=\"What's your name?\", channel=channel, _after=a, _id=\"wait_name\")\n    c = send_text(text=f\"Hi {b.text}!\", channel=channel, _after=b, _id=\"reply\")\n    return {\"greeting\": c.text}\n</code></pre> <ul> <li>Each step is a tool node with a unique <code>_id</code>.</li> <li>If the process stops after <code>ask_text</code>, simply rerun with the same <code>run_id</code> to resume.</li> <li>The system restores from the last persisted snapshot.</li> </ul>"},{"location":"tutorials/t3-talk-to-your-graph-channels-waits/#key-takeaways","title":"Key Takeaways","text":"<ul> <li><code>context.channel()</code> methods implement cooperative waits \u2014 great for live agents.</li> <li>Built-in channel tools (<code>ask_text</code>, <code>send_text</code>, etc.) implement dual-stage waits \u2014 required for resumable graphs.</li> <li><code>graph_fn</code> is stateless, inspectable via <code>.last_graph</code> but not recoverable.</li> <li><code>graphify</code> uses snapshots to persist progress and enable recovery with <code>run_id</code>.</li> <li>Dual-stage tools are <code>@tool</code> nodes \u2014 never call them inside another tool.</li> </ul> <p>Channels make your graph talk. Wait models decide how long it remembers the conversation.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/","title":"Tutorial 4: Store &amp; Recall \u2014 Artifacts and Memory","text":"<p>This tutorial focus on the how Aethergraph can memorize what happened before. Learn how to save outputs, log results/events, and recall them later using AetherGraph\u2019s two persistence pillars:</p> <ul> <li>Artifacts \u2014 durable assets (files/dirs/JSON/text) stored by content address (CAS URI) with labels &amp; metrics for ranking and search.</li> <li>Memory \u2014 a structured event &amp; result log with fast \u201cwhat\u2019s the latest?\u201d recall (e.g., <code>last_by_name</code>), plus simple recent\u2011history queries.</li> </ul> <p>We\u2019ll build this up step\u2011by\u2011step with short, copy\u2011ready snippets.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#0-what-youll-use","title":"0. What you\u2019ll use","text":"<pre><code># Access services from your NodeContext\narts = context.artifacts()   # artifact store\nmem  = context.memory()      # event &amp; result log\n</code></pre> <p>Mental model: Artifacts hold large, immutable outputs. Memory records what happened and the small named values you need to recall quickly.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#1-save-something-get-a-uri-open-it-later","title":"1. Save something \u2192 get a URI \u2192 open it later","text":""},{"location":"tutorials/t4-store-recall-artifacts-memory/#a-ingest-an-existing-file","title":"A. Ingest an existing file","text":"<pre><code>art = await arts.save(\n    path=\"/tmp/report.pdf\",\n    kind=\"report\",                 # a short noun; you\u2019ll filter/rank by this\n    labels={\"exp\": \"A\"},          # 1\u20133 filters you actually plan to query\n    # metrics={\"bleu\": 31.2},      # optional if you\u2019ll rank later\n)\nuri = art.uri                       # stable CAS URI\n\n# When you need a real path again:\npath = arts.to_local_path(uri)\n</code></pre> <p>Why CAS? It prevents accidental overwrites and gives you a stable handle you can pass around (in Memory, dashboards, etc.).</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#b-streamwrite-no-temp-file-atomically","title":"B. Stream\u2011write (no temp file), atomically","text":"<pre><code>async with arts.writer(kind=\"plot\", planned_ext=\".png\") as w:\n    w.write(png_bytes)\n# on exit \u2192 the artifact is committed and indexed\n</code></pre> <p>Tip: Prefer <code>writer(...)</code> for programmatically produced bytes.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#2-record-results-youll-want-to-recall-fast","title":"2. Record results you\u2019ll want to recall fast","text":"<p>Use Memory for structured results and lightweight logs.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#a-record-a-typed-result-fast-recall-by-name","title":"A. Record a typed result (fast recall by name)","text":"<pre><code>await mem.write_result(\n    topic=\"train.step\",\n    outputs=[\n        {\"name\": \"val_acc\",  \"kind\": \"number\", \"value\": 0.912},\n        {\"name\": \"ckpt_uri\", \"kind\": \"uri\",    \"value\": uri},\n    ],\n)\n\nlast_acc = await mem.last_by_name(\"val_acc\")\n</code></pre> <p><code>write_result</code> indexes named values so <code>last_by_name(\"val_acc\")</code> is O(1) to fetch the latest.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#b-log-arbitrary-events-structured-but-lightweight","title":"B. Log arbitrary events (structured but lightweight)","text":"<pre><code>await mem.record(\n    kind=\"train_log\",\n    data={\"epoch\": 1, \"loss\": 0.25, \"acc\": 0.91},\n)\n\nrecent = await mem.recent(kinds=[\"train_log\"], limit=3) # recent is an Event\n</code></pre> <p>You will need to load the data from seriazalized <code>recent.text</code> (<code>channel.memory()</code> docs)</p> <p>Need only the decoded payloads?</p> <pre><code>logs = await mem.recent_data(kinds=[\"train_log\"], limit=3) # this returns the `data` saved in memory, not Event\n</code></pre> <p>Use <code>record</code> for progress/trace breadcrumbs. Use <code>write_result</code> for small named values you\u2019ll query later.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#3-search-filter-and-rank-artifacts","title":"3. Search, filter, and rank artifacts","text":""},{"location":"tutorials/t4-store-recall-artifacts-memory/#a-search-by-labels-you-saved-earlier","title":"A. Search by labels you saved earlier","text":"<pre><code>hits = await arts.search(\n    kind=\"report\",\n    labels={\"exp\": \"A\"},    # exact\u2011match filter across indexed labels\n)\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#b-pick-best-so-far-by-a-metric","title":"B. Pick \u201cbest so far\u201d by a metric","text":"<pre><code>best = await arts.best(\n    kind=\"checkpoint\",\n    metric=\"val_acc\",   # must exist in artifact.metrics\n    mode=\"max\",         # or \"min\"\n    scope=\"run\",        # limit to current run | graph | node\n)\nif best:\n    best_path = arts.to_local_path(best.uri)\n</code></pre> <p>Attach <code>metrics={\"val_acc\": ...}</code> when saving to enable ranking later.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#4-endtoend-save-log-and-recall","title":"4. End\u2011to\u2011end: save, log, and recall","text":"<pre><code>@graph_fn(name=\"train_epoch\", outputs=[\"ckpt_uri\"])\nasync def train_epoch(epoch: int, *, context):\n    arts = context.artifacts()\n    mem  = context.memory()\n\n    # 1) Export a checkpoint to a temp path you control\n    tmp_path = \"/tmp/ckpt.bin\"\n\n    # 2) Ingest it as an Artifact\n    ckpt = await arts.save(\n        path=tmp_path,\n        kind=\"checkpoint\",\n        labels={\"epoch\": epoch},\n        # metrics={\"val_acc\": val_acc},\n    )\n\n    # 3) Record the important values for quick recall\n    await mem.write_result(\n        topic=\"train.epoch\",\n        outputs=[\n            {\"name\": \"epoch\",    \"kind\": \"number\", \"value\": epoch},\n            {\"name\": \"ckpt_uri\", \"kind\": \"uri\",    \"value\": ckpt.uri},\n        ],\n    )\n\n    return {\"ckpt_uri\": ckpt.uri}\n</code></pre> <p>Now, any later node can do:</p> <pre><code>latest_uri = await context.memory().last_by_name(\"ckpt_uri\")\npath = context.artifacts().to_local_path(latest_uri)\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#5-practical-recipes","title":"5. Practical recipes","text":""},{"location":"tutorials/t4-store-recall-artifacts-memory/#a-save-small-jsontext-directly","title":"A. Save small JSON/Text directly","text":"<pre><code>cfg_art = await arts.save_json({\"lr\": 1e-3, \"batch\": 64})\nlog_art = await arts.save_text(\"training finished ok\")\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#b-browse-everything-produced-in-this-run","title":"B. Browse everything produced in this run","text":"<pre><code>all_run_outputs = await arts.list(scope=\"run\")\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#c-pin-something-to-keep-forever","title":"C. Pin something to keep forever","text":"<pre><code>await arts.pin(artifact_id=cfg_art.artifact_id)\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#d-keep-memory-lean-but-persistent","title":"D. Keep Memory lean but persistent","text":"<ul> <li>Memory acts like a fixed\u2011length hot queue for fast recall (<code>last_by_name</code>, <code>recent</code>).</li> <li>All events are persisted for later inspection, but only a rolling window stays hot in KV for speed.</li> </ul>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#6-minimal-reference-schemas-helpers","title":"6. Minimal reference (schemas &amp; helpers)","text":"<p>You rarely need all fields. Here are the useful bits to recognize in code and logs.</p> <pre><code>@dataclass\nclass Artifact:\n    artifact_id: str\n    uri: str           # CAS URI\n    kind: str          # short noun (e.g., \"checkpoint\", \"report\")\n    labels: dict[str, Any]\n    metrics: dict[str, Any]\n    preview_uri: str | None = None\n    pinned: bool = False\n</code></pre> <pre><code>@dataclass\nclass Event:\n    event_id: str\n    ts: str\n    kind: str          # e.g., \"tool_result\", \"train_log\"\n    topic: str | None = None\n    inputs: list[Value] | None = None\n    outputs: list[Value] | None = None\n    metrics: dict[str, float] | None = None\n    text: str | None = None   # JSON string or message text\n    version: int = 1\n</code></pre> <p>Helper (already built\u2011in) that returns decoded payloads from <code>recent</code>:</p> <pre><code>async def recent_data(*, kinds: list[str], limit: int = 50) -&gt; list[Any]\n</code></pre>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#7-when-to-use-what","title":"7. When to use what","text":"Need Use Why Keep a file/dir for later <code>artifacts.save(...)</code> / <code>writer(...)</code> Durable, deduped, indexed Store small JSON/Text <code>artifacts.save_json/text</code> Convenience, still indexed Quick recall of named values <code>memory.write_result</code> \u2192 <code>last_by_name</code> O(1) latest lookups Log structured progress/events <code>memory.record</code> \u2192 <code>recent</code>/<code>recent_data</code> Lightweight trace Pick the best checkpoint/report <code>artifacts.best(kind, metric, mode)</code> Built\u2011in ranking List everything from current run <code>artifacts.list(scope=\"run\")</code> One\u2011liner browse"},{"location":"tutorials/t4-store-recall-artifacts-memory/#8-rag-turning-history-into-answers-optional","title":"8. RAG: turning history into answers (optional)","text":"<p>Memory ships with a RAG facade so you can promote events/results into a searchable corpus.</p> <pre><code>@graph_fn(name=\"make_rag_corpus\", outputs=[\"answer\"])\nasync def make_rag_corpus(question: str, *, context):\n    mem = context.memory()\n\n    corpus = await mem.rag_bind()\n    await mem.rag_promote_events(\n        corpus_id=corpus,\n        where={\"kinds\": [\"tool_result\"], \"limit\": 200},\n    )\n    ans = await mem.rag_answer(corpus_id=corpus, question=question)\n    snap = await mem.rag_snapshot(corpus_id=corpus, title=\"Weekly knowledge snapshot\")\n    return {\"answer\": ans.get(\"answer\", \"\"), \"snapshot_uri\": snap.get(\"uri\")}\n</code></pre> <p>Use this if you want citations and cross\u2011run Q&amp;A on top of your logs.</p>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#9-troubleshooting-tips","title":"9. Troubleshooting &amp; tips","text":"<ul> <li>I saved without labels/metrics \u2014 can I still search? Yes. You can list by scope and filter in Python. Add labels/metrics next time for richer queries.</li> <li>URIs vs paths? Always store/share URIs. Resolve to a path only when you need to read the bytes: <code>arts.to_local_path(uri)</code>.</li> <li>Performance: Keep Memory results tiny and focused (a few named values). Put large blobs in Artifacts.</li> <li>Naming: Re\u2011use a small stable set of <code>kind</code> values (e.g., <code>checkpoint</code>, <code>report</code>, <code>plot</code>). It pays off in search and dashboards.</li> </ul>"},{"location":"tutorials/t4-store-recall-artifacts-memory/#summary","title":"Summary","text":"<ul> <li>Artifacts: durable results with CAS URIs, plus labels/metrics for search &amp; ranking.</li> <li>Memory: structured results &amp; events for instant recall (<code>last_by_name</code>, <code>recent</code>).</li> <li>Use them together: save outputs as Artifacts, then record the important URIs and numbers with <code>write_result</code> for fast retrieval.</li> <li>Optional: promote Memory into RAG to get searchable, cited answers across runs.</li> </ul> <p>See also: <code>context.artifacts()</code> \u00b7 <code>context.memory()</code> \u00b7 RAG helpers (<code>rag_bind</code>, <code>rag_promote_events</code>, <code>rag_answer</code>, <code>rag_snapshot</code>)</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/","title":"Tutorial 5: Add Intelligence \u2014 LLM &amp; RAG","text":"<p>This tutorial adds language models and retrieval\u2011augmented generation (RAG) to your agents. You\u2019ll:</p> <ol> <li>set up an LLM profile</li> <li>chat from a graph function</li> <li>build a searchable RAG corpus from your files/memory</li> <li>answer questions grounded by retrieved context (with optional citations)</li> </ol> <p>Works with OpenAI, Anthropic, Google (Gemini), OpenRouter, LM Studio, and Ollama via a unified GenericLLMClient.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#0-mental-model","title":"0. Mental model","text":"<ul> <li>LLM: a provider\u2011agnostic client you access via <code>context.llm(...)</code> for chat and embeddings.</li> <li>RAG: a corpus of documents (from files and/or Memory events) that are chunked, embedded, and retrieved to ground LLM answers.</li> </ul> <pre><code>llm = context.llm(profile=\"default\")   # chat &amp; embed\nrag = context.rag()                     # corpora, upsert, search, answer\n</code></pre>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>API keys for the providers you want (e.g., OpenAI, Anthropic, Gemini, OpenRouter).</li> <li>If using local models: LM Studio or Ollama running locally and a base URL.</li> </ul>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#2-configure-llms-profiles","title":"2. Configure LLMs (Profiles)","text":"<p>You can configure profiles in environment variables (recommended) or at runtime. See docs for complete setup method.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#a-env-profiles-recommended","title":"A) <code>.env</code> profiles (recommended)","text":"<p>Profiles are named by the section after <code>LLM__</code>. Example: a profile called <code>MY_OPENAI</code>:</p> <pre><code>AETHERGRAPH_LLM__MY_OPENAI__PROVIDER=openai\nAETHERGRAPH_LLM__MY_OPENAI__MODEL=gpt-4o-mini\nAETHERGRAPH_LLM__MY_OPENAI__TIMEOUT=60\nAETHERGRAPH_LLM__MY_OPENAI__API_KEY=sk-...\nAETHERGRAPH_LLM__MY_OPENAI__EMBED_MODEL=text-embedding-3-small  # needed for llm().embed() or RAG\n</code></pre> <p>Then in code:</p> <pre><code>llm = context.llm(profile=\"my_openai\")\ntext, usage = await llm.chat([...])\n</code></pre> <p>The default profile comes from your container config. Use profiles when you want to switch providers/models per node or per run.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#b-register-at-runtime-programmatic","title":"B) Register at runtime (programmatic)","text":"<p>Useful for notebooks/demos or dynamically wiring services:</p> <pre><code>from aethergraph.llm import register_llm_client, set_rag_llm_client\n\nclient = register_llm_client(\n    profile=\"runtime_openai\",\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    api_key=\"sk-...\",\n)\n\n# RAG can use a dedicated LLM (for embedding + answering). If not set, it uses the default profile.\nset_rag_llm_client(client=client)\n</code></pre> <p>You can also pass parameters directly to <code>set_rag_llm_client(provider=..., model=..., embed_model=..., api_key=...)</code>.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#c-oneoff-key-injection","title":"C) One\u2011off key injection","text":"<p>If you just need to override a key in memory for a demo:</p> <pre><code>context.llm_set_key(provider=\"openai\", api_key=\"sk-...\")\n</code></pre> <p>Sidecar note: If your run needs channels, resumable waits, or shared services, start the sidecar server before using runtime registration.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#3-chat-embed-from-a-graph-function","title":"3. Chat &amp; Embed from a Graph Function","text":""},{"location":"tutorials/t5-add-intelligence-llm-rag/#chat-provideragnostic","title":"Chat (provider\u2011agnostic)","text":"<pre><code>@graph_fn(name=\"ask_llm\")\nasync def ask_llm(question: str, *, context):\n    llm = context.llm(profile=\"my_openai\")  # or omit profile for default\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are concise and helpful.\"},\n        {\"role\": \"user\",   \"content\": question},\n    ]\n    reply, usage = await llm.chat(messages)\n    return {\"answer\": reply, \"usage\": usage}\n</code></pre>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#embeddings","title":"Embeddings","text":"<pre><code>vectors = await context.llm(profile=\"my_openai\").embed([\n    \"First text chunk\", \"Second text chunk\"\n])\n</code></pre> <p>RAG needs an embed model configured on the chosen profile.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#optional-reasoning-knobs","title":"Optional reasoning knobs","text":"<p>Some models (e.g., GPT\u20115) accept reasoning parameters such as <code>reasoning_effort=\"low|medium|high\"</code> via <code>llm.chat(..., reasoning_effort=...)</code>.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#4-raw-api-escape-hatch","title":"4. Raw API escape hatch","text":"<p>For power users who need endpoints not yet covered by the high\u2011level client (such as low-level inputs, VLM models, custom models):</p> <pre><code>openai = context.llm(profile=\"my_openai\")\npayload = {\n    \"model\": \"gpt-4o-mini\",\n    \"input\": [\n        {\"role\": \"system\", \"content\": \"You are concise.\"},\n        {\"role\": \"user\",   \"content\": \"Explain attention in one sentence.\"}\n    ],\n    \"max_output_tokens\": 128,\n    \"temperature\": 0.3,\n}\nraw = await openai.raw(path=\"/responses\", json=payload)\n</code></pre> <ul> <li><code>raw(path=..., json=...)</code> sends a verbatim request to the provider base URL.</li> <li>You are responsible for parsing the returned JSON shape.</li> </ul> <p>Use this when experimenting with new provider features before first\u2011class support lands in the client.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#5-rag-from-docs-memory-to-grounded-answers","title":"5. RAG: From Docs &amp; Memory to Grounded Answers","text":"<p>Flow: <code>Files/Events \u2192 chunk + embed \u2192 index \u2192 retrieve top\u2011k \u2192 LLM answers with context</code>.</p> <ul> <li>Corpora live behind <code>context.rag()</code>.</li> <li>Ingest files (by path) and inline text, and/or promote Memory events into a corpus.</li> </ul>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#a-backend-storage","title":"A) Backend &amp; storage","text":"<p>Default vector index: SQLite (local, zero\u2011dep) \u2014 great for laptops and small corpora.</p> <p>Switch to FAISS: faster ANN search for larger corpora.</p> <p>Set up RAG backend: </p> <ul> <li>Env:</li> </ul> <pre><code># RAG Settings\nAETHERGRAPH_RAG__BACKEND=faiss        # or sqlite\nAETHERGRAPH_RAG__DIM=1536             # embedding dimension (e.g., OpenAI text-embedding-3-small)\n</code></pre> <ul> <li>Runtime:</li> </ul> <pre><code>from aethergraph.services.rag import set_rag_index_backend\n\nset_rag_index_backend(backend=\"faiss\", dim=1536)\n# If FAISS is not installed, it logs a warning and falls back to SQLite automatically.\n</code></pre> <ul> <li>On\u2011disk layout: each corpus stores <code>corpus.json</code>, <code>docs.jsonl</code>, <code>chunks.jsonl</code>; source files are saved as Artifacts for provenance.</li> </ul>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#b-build-update-a-corpus-from-files-text","title":"B) Build / update a corpus from files &amp; text","text":"<pre><code>await context.rag().upsert_docs(\n    corpus_id=\"my_docs\",\n    docs=[\n        {\"path\": \"data/report.pdf\", \"labels\": {\"type\": \"report\"}},\n        {\"text\": \"Experiment hit 91.2% accuracy on CIFAR-10.\", \"title\": \"exp-log\"},\n    ],\n)\n</code></pre> <ul> <li> <p>Use file docs when you already have a local file: <code>{\"path\": \"/abs/or/relative.ext\", \"labels\": {...}}</code>. Supported \u201csmart-parsed\u201d types are <code>.pdf</code>, <code>.md/markdown</code>, and <code>.txt</code> (others are treated as plain text). The original file is saved as an Artifact for provenance; if your PDF is a scan, run OCR first (we only extract selectable text). </p> </li> <li> <p>Use inline docs when you have content in memory: <code>{\"text\": \"...\", \"title\": \"nice-short-title\", \"labels\": {...}}</code>. Keep titles short and meaningful; add 1\u20133 optional labels you\u2019ll actually filter by (e.g., <code>{\"source\":\"lab\", \"week\":2}</code>).</p> </li> </ul> <p>Behind the scenes: documents are stored as Artifacts, parsed, chunked, embedded, and added to the vector index.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#c-promote-memory-events-into-rag","title":"C) Promote Memory events into RAG","text":"<pre><code>corpus = await context.memory().rag_bind()\nawait context.memory().rag_promote_events(\n    corpus_id=corpus,\n    where={\"kinds\": [\"tool_result\"], \"limit\": 200},\n)\n</code></pre> <p>You can promote any custom <code>kind</code> you recorded for later vector-based search and answer in a same <code>corpus_id</code>.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#d-search-retrieve-answer-with-citations","title":"D) Search, retrieve, answer (with citations)","text":"<pre><code>hits = await context.rag().search(\"my_docs\", \"key findings\", k=8, mode=\"hybrid\")\nans  = await context.rag().answer(\n    corpus_id=\"my_docs\",\n    question=\"Summarize the main findings and list key metrics.\",\n    style=\"concise\",\n    with_citations=True,\n    k=6,\n)\n# ans \u2192 { \"answer\": str, \"citations\": [...], \"resolved_citations\": [...], \"usage\": {...} }\n</code></pre> <p>Use <code>resolved_citations</code> to map snippets back to Artifact URIs for auditability.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#e-choosing-the-llm-for-rag","title":"E) Choosing the LLM for RAG","text":"<p>RAG uses a dedicated RAG LLM client that must have both <code>model</code> and <code>embed_model</code> set.</p> <p>Runtime:</p> <pre><code>from aethergraph.llm import set_rag_llm_client\nset_rag_llm_client(provider=\"openai\", model=\"gpt-4o-mini\", embed_model=\"text-embedding-3-small\", api_key=\"sk-\u2026\")\n</code></pre> <p>If you don\u2019t set one, it falls back to the default LLM profile (ensure that profile also has an <code>embed_model</code>).</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#f-corpus-management-ops","title":"F) Corpus management (ops)","text":"<p>For maintenance and ops you can:</p> <ul> <li>List corpora / docs to inspect what\u2019s indexed.</li> <li>Delete docs to remove vectors and records.</li> <li>Re\u2011embed to refresh vectors after changing embed model or chunking.</li> <li>Stats to view counts of docs/chunks and corpus metadata.</li> </ul> <p>These live on the same facade: <code>rag.list_corpora()</code>, <code>rag.list_docs(...)</code>, <code>rag.delete_docs(...)</code>, <code>rag.reembed(...)</code>, <code>rag.stats(...)</code>.</p>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#6-practical-recipes","title":"6. Practical recipes","text":"<ul> <li>Switch providers by changing <code>profile=</code> in <code>context.llm(...)</code> without touching your code elsewhere.</li> <li>Save docs as Artifacts (e.g., <code>save_text</code>, <code>save(path=...)</code>) and ingest by <code>{\"path\": local_path}</code> so RAG can cite their URIs.</li> <li>Log LLM outputs with <code>context.memory().record(...)</code> or <code>write_result(...)</code> to enable recency views, distillation, and RAG promotion later.</li> </ul>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#7-troubleshooting","title":"7. Troubleshooting","text":"<ul> <li>Auth/Endpoints: Check keys; for Azure, confirm deployment + endpoint. For LM Studio, the base URL must include <code>/v1</code>.</li> <li>No citations or odd snippets: Verify parsing (PDFs can be tricky). Consider storing originals as Artifacts alongside parsed text.</li> <li>Answers miss context: Increase <code>k</code>, adjust chunk sizes, or broaden your <code>where</code> filter when promoting events.</li> <li>Latency/Cost: Keep chunks compact, and filter ingestion to what you\u2019ll actually ask about.</li> </ul>"},{"location":"tutorials/t5-add-intelligence-llm-rag/#summary","title":"Summary","text":"<ul> <li>Configure LLM profiles via <code>.env</code> or runtime registration, then use <code>llm.chat()</code> / <code>llm.embed()</code>.</li> <li>Build RAG corpora from files and Memory events, then call <code>rag.answer(..., with_citations=True)</code> for grounded responses.</li> <li>Use Artifacts + Memory for provenance so you can trace what the model answered and why.</li> </ul> <p>See also: <code>context.llm()</code> \u00b7 <code>context.rag()</code> \u00b7 <code>context.memory().rag_*</code> \u00b7 <code>register_llm_client</code> \u00b7 <code>set_rag_llm_client</code> \u00b7 <code>llm.raw</code></p>"},{"location":"tutorials/t6-use-external-tools-mcp/","title":"Tutorial 6: Use External Tools \u2014 The MCP Example","text":"<p>AetherGraph supports external tool integration via the Model Context Protocol (MCP) \u2014 a simple JSON\u2011RPC 2.0 interface for listing and calling tools, reading resources, and managing structured outputs from remote services. In short, MCP lets your graph talk to anything that can expose a compliant interface: local CLI utilities, web services, or even another AI system.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#1-what-is-mcp","title":"1. What Is MCP?","text":"<p>The Model Context Protocol defines a standard way for an AI or orchestration framework to:</p> <ul> <li>List tools that an external process or service provides.</li> <li>Call those tools with structured arguments.</li> <li>List or read resources, such as files, datasets, or model outputs.</li> </ul> <p>AetherGraph\u2019s <code>MCPService</code> provides a unified layer for managing multiple MCP clients \u2014 e.g. a local subprocess (<code>StdioMCPClient</code>), a WebSocket endpoint (<code>WsMCPClient</code>), or an HTTP service (<code>HttpMCPClient</code>).</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#each-mcp-client-conforms-to-a-minimal-contract","title":"Each MCP client conforms to a minimal contract:","text":"<pre><code>class MCPClientProtocol:\n    async def list_tools(self) -&gt; List[MCPTool]: ...\n    async def call(self, tool: str, params: Dict[str, Any]) -&gt; Dict[str, Any]: ...\n    async def list_resources(self) -&gt; List[MCPResource]: ...\n    async def read_resource(self, uri: str) -&gt; Dict[str, Any]: ...\n</code></pre> <p>You can register many clients under names (e.g. <code>default</code>, <code>local</code>, <code>remote</code>), and access them via <code>context.mcp(name)</code>.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#2-minimal-example-query-a-local-mcp-tool","title":"2. Minimal Example \u2014 Query a Local MCP Tool","text":"<p>Suppose you have a local script or service exposing MCP over stdio. You can wrap it with <code>StdioMCPClient</code>:</p> <pre><code>from aethergraph.services import StdioMCPClient, MCPService\n\n# Initialize the service manually (usually handled by container)\nclient = StdioMCPClient([\"python\", \"my_mcp_server.py\"])\nmcp = MCPService({\"default\": client})\n\n# Example call to a tool named \"summarize_text\"\nasync def main():\n    await mcp.open(\"default\")\n    tools = await mcp.list_tools(\"default\")\n    print(\"Available tools:\", [t.name for t in tools])\n\n    result = await mcp.call(\"default\", \"summarize_text\", {\"text\": \"Hello MCP!\"})\n    print(\"Result:\", result)\n\nasyncio.run(main())\n</code></pre> <p>This works the same if you use a WebSocket or HTTP\u2011based MCP server \u2014 just replace the client class.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#3-inside-aethergraph-access-via-nodecontext","title":"3. Inside AetherGraph \u2014 Access via NodeContext","text":"<p>In real usage, you need to register the MCP service after starting the server:</p> <pre><code>from aethergraph import start_server\nfrom aethergraph.services import StdioMCPClient, MCPService\nfrom aethergraph.runtime import register_mcp_client \n\nstart_server()\nclient = StdioMCPClient([\"python\", \"my_mcp_server.py\"])\n\nregister_mcp_client(\"default\", client=client) # accessed by context.mcp(\"default)\n</code></pre> <p>The <code>NodeContext</code> injects it automatically, so any <code>graph_fn</code> or <code>@tool</code> can access it:</p> <pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"mcp_query\")\nasync def mcp_query(*, context: NodeContext):\n    # List tools available on the default MCP server\n    tools = await context.mcp(\"default\").list_tools()\n    context.logger().info(f\"Available MCP tools: {[t.name for t in tools]}\")\n\n    # Call a specific tool\n    result = await context.mcp(\"default\").call(\"summarize_text\", {\"text\": \"Explain MCP in one line.\"})\n    await context.channel().send_text(f\"Summary: {result['summary']}\")\n\n    return {\"result\": result}\n</code></pre> <p>Run this function via <code>run(graph, inputs)</code> or within a larger workflow \u2014 it will connect automatically to the configured MCP client.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#4-when-to-use-mcp","title":"4. When to Use MCP","text":"<p>MCP is useful when you want to:</p> <ul> <li>Bridge external AI systems (like a local LM Studio model or an in\u2011house LLM server) into AetherGraph.</li> <li>Integrate existing Python tools or APIs without writing new wrappers.</li> <li>Query live data services (e.g., weather, finance, or database APIs) through a JSON\u2011RPC layer.</li> </ul> <p>Because MCP uses async JSON\u2011RPC, it can easily be multiplexed across multiple nodes or graphs \u2014 even concurrently.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#5-available-clients","title":"5. Available Clients","text":"Client Type Class Transport Use Case Stdio <code>StdioMCPClient</code> stdin/stdout subprocess Run local CLI tools WebSocket <code>WsMCPClient</code> persistent WS channel Long\u2011lived AI services HTTP <code>HttpMCPClient</code> RESTful endpoint Web APIs with JSON\u2011RPC routes <p>You can register any number of them:</p> <pre><code>from aethergraph.services.mcp.ws_client import WsMCPClient\nmcp = MCPService({\n    \"default\": WsMCPClient(\"wss://example.com/mcp\"),\n    \"local\": StdioMCPClient([\"python\", \"my_local_tool.py\"])\n})\n</code></pre>"},{"location":"tutorials/t6-use-external-tools-mcp/#6-combined-example-http-tool-call-in-graph","title":"6. Combined Example \u2014 HTTP Tool Call in Graph","text":"<pre><code>from aethergraph import graph_fn, NodeContext\n\n@graph_fn(name=\"fetch_weather\")\nasync def fetch_weather(city: str, *, context: NodeContext):\n    # Connect to HTTP MCP backend\n    result = await context.mcp(\"default\").call(\"get_weather\", {\"city\": city})\n\n    report = result.get(\"report\", \"No data returned.\")\n    await context.channel().send_text(f\"Weather in {city}: {report}\")\n    return {\"city\": city, \"report\": report}\n</code></pre> <p>This looks like any other AetherGraph node \u2014 but the heavy lifting happens externally. MCP makes any compliant server a first\u2011class citizen in your graphs.</p>"},{"location":"tutorials/t6-use-external-tools-mcp/#7-notes-tips","title":"7. Notes &amp; Tips","text":"<ul> <li>Auto\u2011Reconnect: MCP clients auto\u2011reopen when disconnected, so you can safely call <code>context.mcp(\"local\")</code> multiple times.</li> <li>Multiple Servers: You can connect to multiple MCPs simultaneously for different tool domains.</li> </ul> <ul> <li>Chained Tools: Results from MCP calls are just Python dicts \u2014 they can be piped to other <code>@tool</code>s or stored as artifacts for later retrieval.</li> </ul>"},{"location":"tutorials/t6-use-external-tools-mcp/#summary","title":"Summary","text":"<p>MCP integration turns AetherGraph into a universal agent\u2011to\u2011agent protocol bridge. You can:</p> <ol> <li>Connect external AI or data tools via stdio, WebSocket, or HTTP.</li> <li>Access them with one unified API: <code>context.mcp(name)</code>.</li> <li>Call, list, and read resources without writing custom adapters.</li> </ol> <p>In the next section, we\u2019ll explore Extending Services \u2014 showing how to register your own MCP\u2011like service or log LLM prompts for inspection.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/","title":"Tutorial 7: Plug In Your World \u2014 Custom Services","text":"<p>Sometimes your agents need to talk to the rest of your world\u2014clusters, databases, storage systems, internal APIs. Instead of wiring that logic into every <code>graph_fn</code>, AetherGraph lets you attach custom services to the <code>context</code> object:</p> <pre><code># Later inside a graph or tool\nawait context.trainer().submit(spec)\nawait context.storage().put(\"/tmp/report.pdf\")\nstatus = await context.tracker().job_status(job_id)\n</code></pre> <p>This tutorial shows how to:</p> <ol> <li>Define a small service class (just Python).</li> <li>Register it so it appears as <code>context.&lt;name&gt;()</code>.</li> <li>Use it from <code>graph_fn</code> / <code>@tool</code> code.</li> <li>Apply practical patterns (HPC jobs, storage, external APIs).</li> </ol> <p>Goal: keep agent logic clean and move integration glue into reusable, testable services.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#1-what-is-a-custom-service-really","title":"1. What is a Custom Service, Really?","text":"<p>A custom service is a long\u2011lived Python object the runtime injects into every <code>NodeContext</code> under a chosen name.</p> <p>Once registered, it works anywhere:</p> <pre><code>@graph_fn(name=\"demo_trainer\")\nasync def demo_trainer(*, context):\n    job_id = await context.trainer().submit({\"epochs\": 10})\n    return {\"job_id\": job_id}\n</code></pre> <p>Key properties</p> <ul> <li>Named entrypoint \u2014 you choose the accessor (e.g., <code>trainer</code>, <code>storage</code>, <code>models</code>).</li> <li>Shared instance \u2014 one instance reused across nodes/runs (unless you design otherwise).</li> <li>Context\u2011aware \u2014 methods can access the current <code>NodeContext</code> (<code>run_id</code>, <code>graph_id</code>, <code>node_id</code>).</li> <li>Async\u2011first \u2014 works naturally with <code>await</code> and the event loop.</li> </ul> <p>Use a service when you have state or connectivity to share: clients, pools, caches, queues, background workers. For pure functions, a regular module is fine.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#2-minimal-service-from-zero-to-contexttrainer","title":"2. Minimal Service: from Zero to <code>context.trainer()</code>","text":""},{"location":"tutorials/t7-extend-runtime-custom-services/#step-1-define-a-service-class","title":"Step 1: Define a service class","text":"<p>Most custom services inherit from <code>Service</code> (aka <code>BaseContextService</code>) to get handy utilities: access to the current context, a service\u2011wide mutex, and helpers to run blocking code.</p> <pre><code>from aethergraph.services.runtime.base import Service\n\nclass Trainer(Service):\n    async def submit(self, spec: dict) -&gt; str:\n        \"\"\"Submit a training job to your cluster/scheduler.\"\"\"\n        ctx = self.ctx()  # NodeContext bound at call time\n        ctx.logger().info(\"trainer.submit\", extra={\"spec\": spec})\n\n        job_id = await self._submit_to_cluster(spec)  # implement backend call\n\n        # Optional: record to Memory for traceability\n        await ctx.memory().write_result(\n            topic=\"trainer.submit\",\n            outputs=[{\"name\": \"job_id\", \"kind\": \"text\", \"value\": job_id}],\n        )\n        return job_id\n\n    async def inspect_job(self, job_id: str) -&gt; dict:\n        status = await self._query_cluster(job_id)\n        return {\"job_id\": job_id, \"status\": status}\n\n    async def _submit_to_cluster(self, spec: dict) -&gt; str: ...\n    async def _query_cluster(self, job_id: str) -&gt; str: ...\n</code></pre> <p>Notes</p> <ul> <li><code>self.ctx()</code> gives you the current <code>NodeContext</code> at call time\u2014so logs, memory, and artifacts are run\u2011scoped automatically.</li> <li>The service can hold internal state (connection pools, caches) across calls.</li> </ul>"},{"location":"tutorials/t7-extend-runtime-custom-services/#step-2-register-the-service","title":"Step 2: Register the service","text":"<p>Register an instance at startup (e.g., when your sidecar/server boots):</p> <pre><code>from aethergraph import start_server\nfrom aethergraph.services.runtime.registry import register_context_service\n\nstart_server()  # start sidecar so services can be wired\n\ntrainer_service = Trainer()\nregister_context_service(\"trainer\", trainer_service)\n</code></pre> <p>From now on, inside any node:</p> <pre><code>job_id = await context.trainer().submit(spec)\n</code></pre> <p>Pattern: register once \u2192 call anywhere.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#3-using-services-inside-graph_fn-and-tool","title":"3. Using Services Inside <code>graph_fn</code> and <code>@tool</code>","text":"<p>Services behave like built\u2011ins on <code>context</code>.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#example-a-submit-and-track-a-job","title":"Example A \u2014 Submit and track a job","text":"<pre><code>from aethergraph import graph_fn, tool\n\n@graph_fn(name=\"train_and_wait\", outputs=[\"job_id\", \"done\"])\nasync def train_and_wait(spec: dict, *, context):\n    job_id = await context.trainer().submit(spec)\n    ready = await wait_for_training(job_id=job_id, context=context)\n    return {\"job_id\": job_id, \"done\": ready[\"ready\"]}\n\n@tool(name=\"wait_for_training\", outputs=[\"ready\"])\nasync def wait_for_training(job_id: str, *, context) -&gt; dict:\n    info = await context.trainer().inspect_job(job_id)\n    return {\"ready\": info[\"status\"] == \"COMPLETED\"}\n</code></pre> <p>Why this is nice:</p> <ul> <li>Cluster logic in one place (<code>Trainer</code>), not scattered across graphs.</li> <li>Tests can swap in a fake <code>Trainer</code> that returns canned statuses.</li> </ul>"},{"location":"tutorials/t7-extend-runtime-custom-services/#example-b-custom-storage-wrapper","title":"Example B \u2014 Custom storage wrapper","text":"<pre><code>class Storage(Service):\n    async def put(self, local_path: str, key: str) -&gt; str:\n        uri = await self._upload(local_path, key)  # implement upload\n        self.ctx().logger().info(\"storage.put\", extra={\"uri\": uri})\n        return uri\n\n    async def get(self, uri: str, dest: str) -&gt; None:\n        await self._download(uri, dest)\n\n@graph_fn(name=\"upload_report\", outputs=[\"uri\"])\nasync def upload_report(*, context):\n    uri = await context.storage().put(\"/tmp/report.pdf\", key=\"reports/2025-01-01.pdf\")\n    return {\"uri\": uri}\n</code></pre> <p>You can mix <code>context.storage()</code> with core features like <code>artifacts()</code> and <code>memory()</code>\u2014for example, storing the CAS URI next to an external bucket URI.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#4-concurrency-shared-state","title":"4. Concurrency &amp; Shared State","text":"<p>Because a service instance is shared, multiple nodes (or graphs) may hit it concurrently. If you expect concurrent accesses to a service, protect shared state inside the service, not at every call site.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#a-servicewide-mutex-recommended-pattern","title":"A) Service\u2011wide mutex (recommended pattern)","text":"<p>Use the built\u2011in <code>critical()</code> helper to guard a method. The pattern below binds the mutex to an instance method immediately after <code>__init__</code>, ensuring <code>self</code> exists:</p> <pre><code>import asyncio\nfrom aethergraph.services.runtime.base import Service\n\nclass CounterService(Service):\n    def __init__(self):\n        super().__init__()\n        self._value = 0\n        # Decorate incr with the bound service-wide mutex\n        # The entire method runs under a critical section\n        self.incr = self.critical()(self.incr)  # type: ignore\n\n    async def incr(self, n: int = 1) -&gt; int:\n        self._value += n\n        await asyncio.sleep(0)  # yield to event loop\n        return self._value\n</code></pre> <p>If you need finer\u2011grained control (e.g., per\u2011key locks, rate windows), design your own locking scheme inside the service. The point is to centralize concurrency policy in one place.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#b-offload-blocking-work","title":"B) Offload blocking work","text":"<pre><code>class Heavy(Service):\n    async def compute(self, x: int) -&gt; int:\n        return await self.run_blocking(self._slow_cpu_fn, x)  # threadpool offload\n\n    def _slow_cpu_fn(self, x: int) -&gt; int:\n        ...  # pure CPU work\n</code></pre> <p>This keeps agents responsive even when a service must do something synchronous or CPU\u2011heavy (e.g. heavy local simulation, training etc.).</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#5-service-lifecycle-startclose","title":"5. Service Lifecycle (start/close)","text":"<p>Some integrations need setup/teardown\u2014opening DB pools, authenticating SDKs, or warming models. Implement optional hooks on your service:</p> <pre><code>class Tracker(Service):\n    async def start(self):\n        self._client = ...  # open DB/HTTP client\n\n    async def close(self):\n        if getattr(self, \"_client\", None):\n            await self._client.aclose()\n</code></pre> <p>Call these from your process bootstrap/shutdown (sidecar, web server, CLI). The runtime doesn\u2019t force a pattern\u2014choose how you host services.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#6-testing-swapping-implementations","title":"6. Testing &amp; Swapping Implementations","text":"<p>Because services are registered by name, they\u2019re easy to replace in tests:</p> <pre><code>class FakeTrainer(Service):\n    async def submit(self, spec: dict) -&gt; str:\n        return \"job-test-123\"\n\n    async def inspect_job(self, job_id: str) -&gt; dict:\n        return {\"job_id\": job_id, \"status\": \"COMPLETED\"}\n\n# Test setup\nregister_context_service(\"trainer\", FakeTrainer())\n# All code using context.trainer() now talks to the fake.\n</code></pre>"},{"location":"tutorials/t7-extend-runtime-custom-services/#7-design-tips-common-patterns","title":"7. Design Tips &amp; Common Patterns","text":"<p>A few patterns that work well in real projects:</p> <ul> <li> <p>One concept \u2192 one service <code>context.trainer()</code> for orchestration, <code>context.storage()</code> for object stores, <code>context.materials()</code> for domain registries, etc.</p> </li> <li> <p>Keep names explicit   Prefer <code>context.k8s_jobs()</code> or <code>context.minio()</code> over vague <code>context.utils()</code>.</p> </li> <li> <p>Use services for anything stateful   HTTP clients, ORM sessions, caches, in\u2011memory registries, queues, schedulers.</p> </li> <li> <p>Don\u2019t replace built\u2011ins   Leave <code>context.memory()</code>, <code>context.artifacts()</code>, <code>context.channel()</code> alone. If you mirror to another system, create a separate service that consumes those.</p> </li> </ul> <p>More handy service ideas</p> Scenario Accessor What it wraps / does HPC / Training cluster <code>context.trainer()</code> Slurm/K8s jobs, Ray, internal queue External object storage <code>context.storage()</code> S3/GCS/MinIO, signed URLs, lifecycle/pinning Job/run tracking <code>context.tracker()</code> DB for job metadata, status dashboards Feature or embedding store <code>context.vectorstore()</code> Vector DB client, batch upserts, hybrid search Materials/parts registry <code>context.materials()</code> Domain DB + caching (e.g., refractive indices) Metrics/telemetry export <code>context.metrics()</code> Push to Prometheus/OTel/Grafana Lineage/BI export <code>context.lineage()</code> Push run/graph/node metadata to warehouse PDF/Doc processing <code>context.docs()</code> OCR, parsing, chunking utilities Secure secrets broker <code>context.secrets()</code> Rotation, envelope decryption Payment/billing <code>context.billing()</code> Client to your billing/ledger microservice License/Entitlements <code>context.license()</code> Gate features per user/org Remote execution (HPC/VM functions) <code>context.runner()</code> Dispatch Python/CLI jobs to remote workers Caching layer for expensive API calls <code>context.cache()</code> Memoization + TTL + invalidation Model hosting / inference gateway <code>context.predict()</code> Internal inference service with model registry"},{"location":"tutorials/t7-extend-runtime-custom-services/#8-optional-callable-services","title":"8. Optional: Callable Services","text":"<p>If you like compact call sites, implement <code>__call__</code>:</p> <pre><code>class Predictor(Service):\n    async def __call__(self, prompt: str) -&gt; str:\n        return await self.generate(prompt)\n\n    async def generate(self, prompt: str) -&gt; str:\n        ...\n\n# After registration as \"predictor\":\ntext = await context.predictor(\"hello\")               # calls __call__\ntext = await context.predictor().generate(\"hello\")    # explicit method\n</code></pre> <p>Sugar only; explicit method names (<code>submit</code>, <code>inspect_job</code>, <code>upload</code>, <code>generate</code>) are often clearer for teams.</p>"},{"location":"tutorials/t7-extend-runtime-custom-services/#9-how-this-fits-with-mcp-and-other-integrations","title":"9. How This Fits with MCP and Other Integrations","text":"<p>In the previous section, MCP treated external processes (HTTP/WebSocket/stdio) as tools your agent can call. Custom services are the other half:</p> <ul> <li>MCP: great when the external system already speaks MCP and you want tools/resources auto\u2011described.</li> <li>Custom services: great when you want a plain Python wrapper around internal systems\u2014no extra server, no protocol.</li> </ul> <p>Projects often mix both:</p> <ul> <li>Use an MCP server for generic capabilities (filesystem, SQL, web search).</li> <li>Use services like <code>context.trainer()</code> and <code>context.storage()</code> for tightly\u2011coupled, org\u2011specific infrastructure.</li> </ul> <p>With this pattern in place, you can keep adding capabilities by teaching the runtime new services, while keeping agent code small, readable, and testable.</p>"},{"location":"tutorials/t8-design-your-own-waits/","title":"Design Your Own Waits &amp; Adapters (Coming Soon)","text":"<p>This chapter is a roadmap for a feature that\u2019s not public yet: defining your own dual\u2011stage waits and adapters so external systems can safely pause and resume graphs.</p> <p>You can treat this as a design note: it explains what is coming and how to think about it, without locking you into any final API.</p>"},{"location":"tutorials/t8-design-your-own-waits/#1-why-custom-waits","title":"1. Why Custom Waits?","text":"<p>Today you\u2019ve seen two wait styles:</p> <ul> <li> <p>Cooperative waits via <code>context.channel().ask_*</code> inside <code>@graph_fn</code>.</p> </li> <li> <p>Great for interactive agents while the process is alive.</p> </li> <li>Not resumable after the Python process dies.</li> <li> <p>Dual\u2011stage waits via built\u2011in tools used in <code>@graphify</code> static graphs.</p> </li> <li> <p>The graph can pause indefinitely (e.g., waiting for Slack/Web reply).</p> </li> <li>You can cold\u2011resume with the same <code>run_id</code> much later.</li> </ul> <p>Custom waits are about taking that second pattern and making it available for your own backends:</p> <ul> <li>external approval systems</li> <li>internal job schedulers</li> <li>lab/experiment queues</li> <li>human\u2011in\u2011the\u2011loop tools that live outside channels like Slack/Telegram</li> </ul> <p>The goal: let you say \u201cpause here until X happens in system Y, then resume this node safely\u201d.</p>"},{"location":"tutorials/t8-design-your-own-waits/#2-what-exists-today","title":"2. What Exists Today","text":"<p>You already have:</p> <ul> <li>Cooperative waits on <code>NodeContext.channel()</code> (<code>ask_text</code>, <code>ask_approval</code>, <code>ask_files</code>, etc.).</li> <li> <p>Built\u2011in dual\u2011stage wait tools (used in the channel tutorial) that:</p> </li> <li> <p>create a continuation token</p> </li> <li>emit an outgoing event (e.g. to Slack/Web)</li> <li>persist a snapshot</li> <li>resume when an inbound event matches that token</li> </ul> <p>These built\u2011ins are wired to existing adapters (console, Slack, etc.) and work out of the box for common interaction flows.</p> <p>What\u2019s missing right now is a public, stable API for defining your own dual\u2011stage tools and adapters.</p> <p>That\u2019s what this \u201ccoming soon\u201d chapter is preparing you for.</p>"},{"location":"tutorials/t8-design-your-own-waits/#3-design-shape-of-a-dualstage-tool-conceptual","title":"3. Design Shape of a Dual\u2011Stage Tool (Conceptual)","text":"<p>At a high level, a dual\u2011stage tool will look like:</p> <ol> <li> <p>Stage A \u2013 schedule / emit</p> </li> <li> <p>Construct a request payload.</p> </li> <li>Register a continuation token with the runtime.</li> <li>Send an event to some external system (HTTP, MQ, email, etc.).</li> <li> <p>Return a WAITING status instead of a final result.</p> </li> <li> <p>Stage B \u2013 resume / handle reply</p> </li> <li> <p>An inbound event (webhook, poller, bridge) calls back with the same token.</p> </li> <li>Runtime restores the graph + node and hands you the reply payload.</li> <li>Your tool continues execution and returns a normal result.</li> </ol>"},{"location":"tutorials/t8-design-your-own-waits/#possible-future-shape-pseudocode-not-final","title":"Possible future shape (pseudo\u2011code, not final)","text":"<pre><code>class ApproveJob(DualStageTool):  # name TBD\n    async def build_request(self, spec: dict, *, context):\n        # Stage A: emit request + return a continuation descriptor\n        token = await context.create_continuation(kind=\"job_approval\", payload={\"spec\": spec})\n        await self.emit_request(spec=spec, token=token)\n        return self.wait(token)   # tells runtime: this node is now WAITING\n\n    async def on_resume(self, reply: dict, *, context):\n        # Stage B: this runs when the continuation is resumed\n        approved = bool(reply.get(\"approved\", False))\n        return {\"approved\": approved}\n</code></pre> <p>Again: this is illustrative only \u2014 the real base class and method names will be documented once the API is ready.</p> <p>The important idea is the split between \u201cset up a wait + emit a request\u201d and \u201chandle the resume payload\u201d, wired together by a continuation token.</p>"},{"location":"tutorials/t8-design-your-own-waits/#4-adapters-bridging-external-systems","title":"4. Adapters: Bridging External Systems","text":"<p>Custom waits are only half the story \u2014 you also need a way to bridge an external system to AetherGraph\u2019s continuation store.</p> <p>Conceptually, an adapter will:</p> <ol> <li>Listen for inbound events from your system (webhook, queue consumer, polling loop).</li> <li>Parse them into a payload <code>{token, data}</code>.</li> <li>Call the runtime to resolve the corresponding continuation and resume the graph.</li> </ol> <p>In practice this might look like (pseudo\u2011code):</p> <pre><code># somewhere in your web app / worker\n\n@app.post(\"/callbacks/job-approved\")\nasync def job_approved(req):\n    token = req.json()[\"token\"]\n    payload = {\"approved\": req.json()[\"approved\"]}\n    await runtime.resolve_continuation(token=token, payload=payload)\n    return {\"ok\": True}\n</code></pre> <p>Behind the scenes, the runtime will:</p> <ul> <li>load the snapshot for the relevant <code>run_id</code> / graph</li> <li>mark the waiting node as ready</li> <li>resume execution from that node using the payload</li> </ul> <p>The adapter API that makes this nicer to write will be documented with the dual\u2011stage tool support.</p>"},{"location":"tutorials/t8-design-your-own-waits/#5-how-this-relates-to-graph_fn-and-graphify","title":"5. How This Relates to <code>graph_fn</code> and <code>graphify</code>","text":"<p>It\u2019s useful to remember the three modes:</p> Mode Wait type available today Cold\u2011resume after process exit? <code>graph_fn</code> + <code>context.channel()</code> Cooperative waits (<code>ask_*</code>) \u274c No <code>graphify</code> + built\u2011in dual waits Dual\u2011stage tools \u2705 Yes <code>graphify</code> + custom dual waits Coming soon \u2705 Yes (for your own waits) <p>Once custom dual\u2011stage tools are available, you\u2019ll:</p> <ul> <li>Use <code>graph_fn</code> when you want live, in\u2011process agents. You can certainly combine <code>graph_fn</code> and <code>DualStageTool</code>, it's just <code>graph_fn</code> does not preserve states and resumption is not protected if the process gets interrupted. </li> <li>Use <code>graphify</code> + dual\u2011stage waits when you want hard guarantees about resuming long\u2011running flows.</li> <li>Wrap external systems (HPC, approval workflows, human review portals) as first\u2011class wait nodes.</li> </ul>"},{"location":"tutorials/t8-design-your-own-waits/#6-what-you-can-do-today","title":"6. What You Can Do Today","text":"<p>While the custom API is still baking, you can already:</p> <ul> <li>Use built\u2011in dual\u2011stage waits with channels (Slack/Web) in static graphs to pause and resume runs.</li> <li>Use cooperative waits (<code>context.channel().ask_*</code>) in <code>graph_fn</code> agents when you don\u2019t need cold\u2011resume.</li> <li>Front external systems with simple scripts or services that call existing channel tools (for example, sending a Slack approval that resumes a static graph).</li> </ul> <p>When the public dual\u2011stage API lands, you\u2019ll be able to replace those scripts with:</p> <ul> <li>explicit <code>@tool</code>\u2011style wait nodes, and</li> <li>small, typed adapters that speak your domain\u2019s language.</li> </ul>"},{"location":"tutorials/t8-design-your-own-waits/#7-looking-ahead","title":"7. Looking Ahead","text":"<p>This chapter is intentionally high\u2011level and labeled Coming Soon. The final documentation will include:</p> <ul> <li>A concrete base class or decorator for defining dual\u2011stage wait tools.</li> <li>A small adapter kit for wiring external callbacks into the continuation store.</li> <li>End\u2011to\u2011end examples: long\u2011running jobs, external approval systems, and custom interactive apps.</li> </ul> <p>Until then, you can design your flows with this mental model in mind:</p> <p>\u201cAnything that can emit a token and later send it back can be turned into a resumable node.\u201d</p> <p>Once the API is ready, you\u2019ll drop in the real primitives and your graphs will gain robust, resumable waits across all your systems.</p>"}]}